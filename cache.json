{"2024-03-18T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.10397v2","updated":"2024-03-18T12:00:31Z","published":"2024-03-15T15:31:13Z","title":"Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and\n  Depth Sensors","summary":"  Accurate positioning of remotely operated underwater vehicles (ROVs) in\nconfined environments is crucial for inspection and mapping tasks and is also a\nprerequisite for autonomous operations. Presently, there are no positioning\nsystems available that are suited for real-world use in confined underwater\nenvironments, unconstrained by environmental lighting and water turbidity\nlevels and have sufficient accuracy for long-term, reliable and repeatable\nnavigation. This shortage presents a significant barrier to enhancing the\ncapabilities of ROVs in such scenarios. This paper introduces an innovative\npositioning system for ROVs operating in confined, cluttered underwater\nsettings, achieved through the collaboration of an omnidirectional surface\nvehicle and an ROV. A formulation is proposed and evaluated in the simulation\nagainst ground truth. The experimental results from the simulation form a proof\nof principle of the proposed system and also demonstrate its deployability.\nUnlike many previous approaches, the system does not rely on fixed\ninfrastructure or tracking of features in the environment and can cover large\nenclosed areas without additional equipment.\n","authors":["{Xueliang Cheng","Barry Lennox","Keir Groves"],"pdf_url":"https://arxiv.org/pdf/2403.10397v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.11681v1","updated":"2024-03-18T11:35:18Z","published":"2024-03-18T11:35:18Z","title":"MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion","summary":"  Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.\n","authors":["Guiyong Zheng","Jinqi Jiang","Chen Feng","Shaojie Shen","Boyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.11681v1.pdf","comment":"Submitted to IROS2024. Code: https://github.com/SYSU-STAR/MASSTAR.\n  Project Page: https://github.com/SYSU-STAR/MASSTAR"},{"id":"http://arxiv.org/abs/2403.11679v1","updated":"2024-03-18T11:31:03Z","published":"2024-03-18T11:31:03Z","title":"NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using\n  3D Gaussian Splatting","summary":"  We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D\nGaussian representation, that enables robust 3D semantic mapping, accurate\ncamera tracking, and high-quality rendering in real-time. In the system, we\npropose a Spatially Consistent Feature Fusion model to reduce the effect of\nerroneous estimates from pre-trained segmentation head on semantic\nreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we\nemploy a lightweight encoder-decoder to compress the high-dimensional semantic\nfeatures into a compact 3D Gaussian representation, mitigating the burden of\nexcessive memory consumption. Furthermore, we leverage the advantage of 3D\nGaussian splatting, which enables efficient and differentiable novel view\nrendering, and propose a Virtual Camera View Pruning method to eliminate\noutlier GS points, thereby effectively enhancing the quality of scene\nrepresentations. Our NEDS-SLAM method demonstrates competitive performance over\nexisting dense semantic SLAM methods in terms of mapping and tracking accuracy\non Replica and ScanNet datasets, while also showing excellent capabilities in\n3D dense semantic mapping.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie"],"pdf_url":"https://arxiv.org/pdf/2403.11679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11662v1","updated":"2024-03-18T11:07:15Z","published":"2024-03-18T11:07:15Z","title":"FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames\n  with Events","summary":"  Keypoint detection and tracking in traditional image frames are often\ncompromised by image quality issues such as motion blur and extreme lighting\nconditions. Event cameras offer potential solutions to these challenges by\nvirtue of their high temporal resolution and high dynamic range. However, they\nhave limited performance in practical applications due to their inherent noise\nin event data. This paper advocates fusing the complementary information from\nimage frames and event streams to achieve more robust keypoint detection and\ntracking. Specifically, we propose a novel keypoint detection network that\nfuses the textural and structural information from image frames with the\nhigh-temporal-resolution motion information from event streams, namely FE-DeTr.\nThe network leverages a temporal response consistency for supervision, ensuring\nstable and efficient keypoint detection. Moreover, we use a spatio-temporal\nnearest-neighbor search strategy for robust keypoint tracking. Extensive\nexperiments are conducted on a new dataset featuring both image frames and\nevent data captured under extreme conditions. The experimental results confirm\nthe superior performance of our method over both existing frame-based and\nevent-based methods.\n","authors":["Xiangyuan Wang","Kuangyi Chen","Wen Yang","Lei Yu","Yannan Xing","Huai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11662v1.pdf","comment":"7 pages, Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11661v1","updated":"2024-03-18T11:04:21Z","published":"2024-03-18T11:04:21Z","title":"Combining Local and Global Perception for Autonomous Navigation on\n  Nano-UAVs","summary":"  A critical challenge in deploying unmanned aerial vehicles (UAVs) for\nautonomous tasks is their ability to navigate in an unknown environment. This\npaper introduces a novel vision-depth fusion approach for autonomous navigation\non nano-UAVs. We combine the visual-based PULP-Dronet convolutional neural\nnetwork for semantic information extraction, i.e., serving as the global\nperception, with 8x8px depth maps for close-proximity maneuvers, i.e., the\nlocal perception. When tested in-field, our integration strategy highlights the\ncomplementary strengths of both visual and depth sensory information. We\nachieve a 100% success rate over 15 flights in a complex navigation scenario,\nencompassing straight pathways, static obstacle avoidance, and 90{\\deg} turns.\n","authors":["Lorenzo Lamberti","Georg Rutishauser","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.11661v1.pdf","comment":"5 pages, 2 figures, 1 table, 1 video"},{"id":"http://arxiv.org/abs/2403.11643v1","updated":"2024-03-18T10:35:15Z","published":"2024-03-18T10:35:15Z","title":"Diffusion-Based Environment-Aware Trajectory Prediction","summary":"  The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.\n","authors":["Theodor Westny","Björn Olofsson","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.11643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11639v1","updated":"2024-03-18T10:21:05Z","published":"2024-03-18T10:21:05Z","title":"An Accurate and Real-time Relative Pose Estimation from Triple\n  Point-line Images by Decoupling Rotation and Translation","summary":"  Line features are valid complements for point features in man-made\nenvironments. 3D-2D constraints provided by line features have been widely used\nin Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how\nto accurately solve three-view relative motion only with 2D observations of\npoints and lines in real time has not been fully explored. In this paper, we\npropose a novel three-view pose solver based on rotation-translation decoupled\nestimation. First, a high-precision rotation estimation method based on normal\nvector coplanarity constraints that consider the uncertainty of observations is\nproposed, which can be solved by Levenberg-Marquardt (LM) algorithm\nefficiently. Second, a robust linear translation constraint that minimizes the\ndegree of the rotation components and feature observation components in\nequations is elaborately designed for estimating translations accurately.\nExperiments on synthetic data and real-world data show that the proposed\napproach improves both rotation and translation accuracy compared to the\nclassical trifocal-tensor-based method and the state-of-the-art two-view\nalgorithm in outdoor and indoor environments.\n","authors":["Zewen Xu","Yijia He","Hao Wei","Bo Xu","BinJian Xie","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11623v1","updated":"2024-03-18T09:55:22Z","published":"2024-03-18T09:55:22Z","title":"Synthesizing multi-log grasp poses","summary":"  Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs off the forest floor and onto forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy of 95%.\n","authors":["Arvid Fälldin","Erik Wallin","Tommy Löfstedt","Martin Servin"],"pdf_url":"https://arxiv.org/pdf/2403.11623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01962v2","updated":"2024-03-18T09:52:10Z","published":"2024-03-04T12:01:11Z","title":"An Efficient Model-Based Approach on Learning Agile Motor Skills without\n  Reinforcement","summary":"  Learning-based methods have improved locomotion skills of quadruped robots\nthrough deep reinforcement learning. However, the sim-to-real gap and low\nsample efficiency still limit the skill transfer. To address this issue, we\npropose an efficient model-based learning framework that combines a world model\nwith a policy network. We train a differentiable world model to predict future\nstates and use it to directly supervise a Variational Autoencoder (VAE)-based\npolicy network to imitate real animal behaviors. This significantly reduces the\nneed for real interaction data and allows for rapid policy updates. We also\ndevelop a high-level network to track diverse commands and trajectories. Our\nsimulated results show a tenfold sample efficiency increase compared to\nreinforcement learning methods such as PPO. In real-world testing, our policy\nachieves proficient command-following performance with only a two-minute data\ncollection period and generalizes well to new speeds and paths.\n","authors":["Haojie Shi","Tingguang Li","Qingxu Zhu","Jiapeng Sheng","Lei Han","Max Q. -H. Meng"],"pdf_url":"https://arxiv.org/pdf/2403.01962v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.11617v1","updated":"2024-03-18T09:50:05Z","published":"2024-03-18T09:50:05Z","title":"Frontier-Based Exploration for Multi-Robot Rendezvous in\n  Communication-Restricted Unknown Environments","summary":"  Multi-robot rendezvous and exploration are fundamental challenges in the\ndomain of mobile robotic systems. This paper addresses multi-robot rendezvous\nwithin an initially unknown environment where communication is only possible\nafter the rendezvous. Traditionally, exploration has been focused on rapidly\nmapping the environment, often leading to suboptimal rendezvous performance in\nlater stages. We adapt a standard frontier-based exploration technique to\nintegrate exploration and rendezvous into a unified strategy, with a mechanism\nthat allows robots to re-visit previously explored regions thus enhancing\nrendezvous opportunities. We validate our approach in 3D realistic simulations\nusing ROS, showcasing its effectiveness in achieving faster rendezvous times\ncompared to exploration strategies.\n","authors":["Mauro Tellaroli","Matteo Luperto","Michele Antonazzi","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11607v1","updated":"2024-03-18T09:31:59Z","published":"2024-03-18T09:31:59Z","title":"AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground\n  Robots in Occlusion-Prone Environments","summary":"  The exceptional mobility and long endurance of air-ground robots are raising\ninterest in their usage to navigate complex environments (e.g., forests and\nlarge buildings). However, such environments often contain occluded and unknown\nregions, and without accurate prediction of unobserved obstacles, the movement\nof the air-ground robot often suffers a suboptimal trajectory under existing\nmapping-based and learning-based navigation methods. In this work, we present\nAGRNav, a novel framework designed to search for safe and energy-saving\nair-ground hybrid paths. AGRNav contains a lightweight semantic scene\ncompletion network (SCONet) with self-attention to enable accurate obstacle\npredictions by capturing contextual information and occlusion area features.\nThe framework subsequently employs a query-based method for low-latency updates\nof prediction results to the grid map. Finally, based on the updated map, the\nhierarchical path planner efficiently searches for energy-saving paths for\nnavigation. We validate AGRNav's performance through benchmarks in both\nsimulated and real-world environments, demonstrating its superiority over\nclassical and state-of-the-art methods. The open-source code is available at\nhttps://github.com/jmwang0117/AGRNav.\n","authors":["Junming Wang","Zekai Sun","Xiuxian Guan","Tianxiang Shen","Zongyuan Zhang","Tianyang Duan","Dong Huang","Shixiong Zhao","Heming Cui"],"pdf_url":"https://arxiv.org/pdf/2403.11607v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11577v1","updated":"2024-03-18T08:53:03Z","published":"2024-03-18T08:53:03Z","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal\n  Calibration","summary":"  Reliable multimodal sensor fusion algorithms re- quire accurate\nspatiotemporal calibration. Recently, targetless calibration techniques based\non implicit neural representations have proven to provide precise and robust\nresults. Nevertheless, such methods are inherently slow to train given the high\ncompu- tational overhead caused by the large number of sampled points required\nfor volume rendering. With the recent introduction of 3D Gaussian Splatting as\na faster alternative to implicit representation methods, we propose to leverage\nthis new ren- dering approach to achieve faster multi-sensor calibration. We\nintroduce 3DGS-Calib, a new calibration method that relies on the speed and\nrendering accuracy of 3D Gaussian Splatting to achieve multimodal\nspatiotemporal calibration that is accurate, robust, and with a substantial\nspeed-up compared to methods relying on implicit neural representations. We\ndemonstrate the superiority of our proposal with experimental results on\nsequences from KITTI-360, a widely used driving dataset.\n","authors":["Quentin Herau","Moussab Bennehar","Arthur Moreau","Nathan Piasco","Luis Roldao","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2403.11577v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11567v1","updated":"2024-03-18T08:41:36Z","published":"2024-03-18T08:41:36Z","title":"R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based\n  Robots Ecosystems via Proposal Refinement","summary":"  We introduce a novel approach for scalable domain adaptation in cloud\nrobotics scenarios where robots rely on third-party AI inference services\npowered by large pre-trained deep neural networks. Our method is based on a\ndownstream proposal-refinement stage running locally on the robots, exploiting\na new lightweight DNN architecture, R2SNet. This architecture aims to mitigate\nperformance degradation from domain shifts by adapting the object detection\nprocess to the target environment, focusing on relabeling, rescoring, and\nsuppression of bounding-box proposals. Our method allows for local execution on\nrobots, addressing the scalability challenges of domain adaptation without\nincurring significant computational costs. Real-world results on mobile service\nrobots performing door detection show the effectiveness of the proposed method\nin achieving scalable domain adaptation.\n","authors":["Michele Antonazzi","Matteo Luperto","N. Alberto Borghese","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11552v1","updated":"2024-03-18T08:03:47Z","published":"2024-03-18T08:03:47Z","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feed- back through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain- specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nun- derscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v1.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2310.16838v2","updated":"2024-03-18T07:20:41Z","published":"2023-10-25T17:59:41Z","title":"SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous\n  Manipulation","summary":"  Humans demonstrate remarkable skill in transferring manipulation abilities\nacross objects of varying shapes, poses, and appearances, a capability rooted\nin their understanding of semantic correspondences between different instances.\nTo equip robots with a similar high-level comprehension, we present SparseDFF,\na novel DFF for 3D scenes utilizing large 2D vision models to extract semantic\nfeatures from sparse RGBD images, a domain where research is limited despite\nits relevance to many tasks with fixed-camera setups. SparseDFF generates\nview-consistent 3D DFFs, enabling efficient one-shot learning of dexterous\nmanipulations by mapping image features to a 3D point cloud. Central to\nSparseDFF is a feature refinement network, optimized with a contrastive loss\nbetween views and a point-pruning mechanism for feature continuity. This\nfacilitates the minimization of feature discrepancies w.r.t. end-effector\nparameters, bridging demonstrations and target manipulations. Validated in\nreal-world scenarios with a dexterous hand, SparseDFF proves effective in\nmanipulating both rigid and deformable objects, demonstrating significant\ngeneralization capabilities across object and scene variations.\n","authors":["Qianxu Wang","Haotong Zhang","Congyue Deng","Yang You","Hao Dong","Yixin Zhu","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2310.16838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17500v2","updated":"2024-03-18T07:10:02Z","published":"2024-01-30T23:18:35Z","title":"LeTO: Learning Constrained Visuomotor Policy with Differentiable\n  Trajectory Optimization","summary":"  This paper introduces LeTO, a method for learning constrained visuomotor\npolicy via differentiable trajectory optimization. Our approach uniquely\nintegrates a differentiable optimization layer into the neural network. By\nformulating the optimization layer as a trajectory optimization problem, we\nenable the model to end-to-end generate actions in a safe and controlled\nfashion without extra modules. Our method allows for the introduction of\nconstraints information during the training process, thereby balancing the\ntraining objectives of satisfying constraints, smoothing the trajectories, and\nminimizing errors with demonstrations. This \"gray box\" method marries the\noptimization-based safety and interpretability with the powerful\nrepresentational abilities of neural networks. We quantitatively evaluate LeTO\nin simulation and on the real robot. In simulation, LeTO achieves a success\nrate comparable to state-of-the-art imitation learning methods, but the\ngenerated trajectories are of less uncertainty, higher quality, and smoother.\nIn real-world experiments, we deployed LeTO to handle constraints-critical\ntasks. The results show the effectiveness of LeTO comparing with\nstate-of-the-art imitation learning approaches. We release our code at\nhttps://github.com/ZhengtongXu/LeTO.\n","authors":["Zhengtong Xu","Yu She"],"pdf_url":"https://arxiv.org/pdf/2401.17500v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11515v1","updated":"2024-03-18T07:01:21Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11513v1","updated":"2024-03-18T06:54:38Z","published":"2024-03-18T06:54:38Z","title":"Visual Preference Inference: An Image Sequence-Based Preference\n  Reasoning in Tabletop Object Manipulation","summary":"  In robotic object manipulation, human preferences can often be influenced by\nthe visual attributes of objects, such as color and shape. These properties\nplay a crucial role in operating a robot to interact with objects and align\nwith human intention. In this paper, we focus on the problem of inferring\nunderlying human preferences from a sequence of raw visual observations in\ntabletop manipulation environments with a variety of object types, named Visual\nPreference Inference (VPI). To facilitate visual reasoning in the context of\nmanipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR\nemploys a prompting mechanism that describes the difference between the\nconsecutive images (i.e., visual residuals) and incorporates such texts with a\nsequence of images to infer the user's preference. This approach significantly\nenhances the ability to understand and adapt to dynamic changes in its visual\nenvironment during manipulation tasks. Furthermore, we incorporate such texts\nalong with a sequence of images to infer the user's preferences. Our method\noutperforms baseline methods in terms of extracting human preferences from\nvisual sequences in both simulation and real-world environments. Code and\nvideos are available at:\n\\href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}\n","authors":["Joonhyung Lee","Sangbeom Park","Yongin Kwon","Jemin Lee","Minwook Ahn","Sungjoon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.11513v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.11511v1","updated":"2024-03-18T06:42:38Z","published":"2024-03-18T06:42:38Z","title":"Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation","summary":"  This paper focuses on the sim-to-real issue of RGB-D grasp detection and\nformulates it as a domain adaptation problem. In this case, we present a\nglobal-to-local method to address hybrid domain gaps in RGB and depth data and\ninsufficient multi-modal feature alignment. First, a self-supervised rotation\npre-training strategy is adopted to deliver robust initialization for RGB and\ndepth networks. We then propose a global-to-local alignment pipeline with\nindividual global domain classifiers for scene features of RGB and depth images\nas well as a local one specifically working for grasp features in the two\nmodalities. In particular, we propose a grasp prototype adaptation module,\nwhich aims to facilitate fine-grained local feature alignment by dynamically\nupdating and matching the grasp prototypes from the simulation and real-world\nscenarios throughout the training process. Due to such designs, the proposed\nmethod substantially reduces the domain shift and thus leads to consistent\nperformance improvements. Extensive experiments are conducted on the\nGraspNet-Planar benchmark and physical environment, and superior results are\nachieved which demonstrate the effectiveness of our method.\n","authors":["Haoxiang Ma","Ran Qin","Modi shi","Boyang Gao","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11511v1.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11496v1","updated":"2024-03-18T06:00:38Z","published":"2024-03-18T06:00:38Z","title":"MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception","summary":"  Perception plays a crucial role in various robot applications. However,\nexisting well-annotated datasets are biased towards autonomous driving\nscenarios, while unlabelled SLAM datasets are quickly over-fitted, and often\nlack environment and domain variations. To expand the frontier of these fields,\nwe introduce a comprehensive dataset named MCD (Multi-Campus Dataset),\nfeaturing a wide range of sensing modalities, high-accuracy ground truth, and\ndiverse challenging environments across three Eurasian university campuses. MCD\ncomprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive\nEpicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and\nUWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce\nsemantic annotations of 29 classes over 59k sparse NRE lidar scans across three\ndomains, thus providing a novel challenge to existing semantic segmentation\nresearch upon this largely unexplored lidar modality. Finally, we propose, for\nthe first time to the best of our knowledge, continuous-time ground truth based\non optimization-based registration of lidar-inertial data on large survey-grade\nprior maps, which are also publicly released, each several times the size of\nexisting ones. We conduct a rigorous evaluation of numerous state-of-the-art\nalgorithms on MCD, report their performance, and highlight the challenges\nawaiting solutions from the research community.\n","authors":["Thien-Minh Nguyen","Shenghai Yuan","Thien Hoang Nguyen","Pengyu Yin","Haozhi Cao","Lihua Xie","Maciej Wozniak","Patric Jensfelt","Marko Thiel","Justin Ziegenbein","Noel Blunder"],"pdf_url":"https://arxiv.org/pdf/2403.11496v1.pdf","comment":"Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024"},{"id":"http://arxiv.org/abs/2403.11492v1","updated":"2024-03-18T05:53:20Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v1.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11487v1","updated":"2024-03-18T05:38:07Z","published":"2024-03-18T05:38:07Z","title":"Can LLMs Generate Human-Like Wayfinding Instructions? Towards\n  Platform-Agnostic Embodied Instruction Synthesis","summary":"  We present a novel approach to automatically synthesize \"wayfinding\ninstructions\" for an embodied robot agent. In contrast to prior approaches that\nare heavily reliant on human-annotated datasets designed exclusively for\nspecific simulation platforms, our algorithm uses in-context learning to\ncondition an LLM to generate instructions using just a few references. Using an\nLLM-based Visual Question Answering strategy, we gather detailed information\nabout the environment which is used by the LLM for instruction synthesis. We\nimplement our approach on multiple simulation platforms including Matterport3D,\nAI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.\nWe subjectively evaluate our approach via a user study and observe that 83.3%\nof users find the synthesized instructions accurately capture the details of\nthe environment and show characteristics similar to those of human-generated\ninstructions. Further, we conduct zero-shot navigation with multiple approaches\non the REVERIE dataset using the generated instructions, and observe very close\ncorrelation with the baseline on standard success metrics (< 1% change in SR),\nquantifying the viability of generated instructions in replacing\nhuman-annotated data. To the best of our knowledge, ours is the first\nLLM-driven approach capable of generating \"human-like\" instructions in a\nplatform-agnostic manner, without requiring any form of training.\n","authors":["Vishnu Sashank Dorbala","Sanjoy Chowdhury","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.11487v1.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.11484v1","updated":"2024-03-18T05:14:40Z","published":"2024-03-18T05:14:40Z","title":"Robot Navigation in Unknown and Cluttered Workspace with Dynamical\n  System Modulation in Starshaped Roadmap","summary":"  This paper presents a novel reactive motion planning framework for navigating\nrobots in unknown and cluttered 2D workspace. Typical existing methods are\ndeveloped by enforcing the robot staying in free regions represented by the\nlocally extracted ellipse or polygon. Instead, we navigate the robot in free\nspace with an alternate starshaped decomposition, which is calculated directly\nfrom real-time sensor data. Additionally, a roadmap is constructed\nincrementally to maintain the connectivity information of the starshaped\nregions. Compared to the roadmap built upon connected polygons or ellipses in\nthe conventional approaches, the concave starshaped region is better suited to\ncapture the natural distribution of sensor data, so that the perception\ninformation can be fully exploited for robot navigation. In this sense,\nconservative and myopic behaviors are avoided with the proposed approach, and\nintricate obstacle configurations can be suitably accommodated in unknown and\ncluttered environments. Then, we design a heuristic exploration algorithm on\nthe roadmap to determine the frontier points of the starshaped regions, from\nwhich short-term goals are selected to attract the robot towards the goal\nconfiguration. It is noteworthy that, a recovery mechanism is developed on the\nroadmap that is triggered once a non-extendable short-term goal is reached.\nThis mechanism renders it possible to deal with dead-end situations that can be\ntypically encountered in unknown and cluttered environments. Furthermore, safe\nand smooth motion within the starshaped regions is generated by employing the\nDynamical System Modulation (DSM) approach on the constructed roadmap. Through\ncomprehensive evaluation in both simulations and real-world experiments, the\nproposed method outperforms the benchmark methods in terms of success rate and\ntraveling time.\n","authors":["Kai Chen","Haichao Liu","Yulin Li","Jianghua Duan","Lei Zhu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2403.11484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11461v1","updated":"2024-03-18T04:26:52Z","published":"2024-03-18T04:26:52Z","title":"VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation","summary":"  In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a\nnovel method designed to enhance 3D manipulation capabilities through\naction-aware view rendering. VIHE autoregressively refines actions in multiple\nstages by conditioning on rendered views posed from action predictions in the\nearlier stages. These virtual in-hand views provide a strong inductive bias for\neffectively recognizing the correct pose for the hand, especially for\nchallenging high-precision tasks such as peg insertion. On 18 manipulation\ntasks in RLBench simulated environments, VIHE achieves a new state-of-the-art,\nwith a 12% absolute improvement, increasing from 65% to 77% over the existing\nstate-of-the-art model using 100 demonstrations per task. In real-world\nscenarios, VIHE can learn manipulation tasks with just a handful of\ndemonstrations, highlighting its practical utility. Videos and code\nimplementation can be found at our project site: https://vihe-3d.github.io.\n","authors":["Weiyao Wang","Yutian Lei","Gregory D. Hage","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11459v1","updated":"2024-03-18T04:20:16Z","published":"2024-03-18T04:20:16Z","title":"ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot\n  Grasping","summary":"  To tackle the \"reality gap\" encountered in Sim-to-Real transfer, this study\nproposes a diffusion-based framework that minimizes inconsistencies in grasping\nactions between the simulation settings and realistic environments. The process\nbegins by training an adversarial supervision layout-to-image diffusion\nmodel(ALDM). Then, leverage the ALDM approach to enhance the simulation\nenvironment, rendering it with photorealistic fidelity, thereby optimizing\nrobotic grasp task training. Experimental results indicate this framework\noutperforms existing models in both success rates and adaptability to new\nenvironments through improvements in the accuracy and reliability of visual\ngrasping actions under a variety of conditions. Specifically, it achieves a\n75\\% success rate in grasping tasks under plain backgrounds and maintains a\n65\\% success rate in more complex scenarios. This performance demonstrates this\nframework excels at generating controlled image content based on text\ndescriptions, identifying object grasp points, and demonstrating zero-shot\nlearning in complex, unseen scenarios.\n","authors":["Yiwei Li","Zihao Wu","Huaqin Zhao","Tianze Yang","Zhengliang Liu","Peng Shu","Jin Sun","Ramviyas Parasuraman","Tianming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14134v2","updated":"2024-03-18T04:08:54Z","published":"2023-12-21T18:55:05Z","title":"Diffusion Reward: Learning Rewards via Conditional Video Diffusion","summary":"  Learning rewards from expert videos offers an affordable and effective\nsolution to specify the intended behaviors for reinforcement learning tasks. In\nthis work, we propose Diffusion Reward, a novel framework that learns rewards\nfrom expert videos via conditional video diffusion models for solving complex\nvisual RL problems. Our key insight is that lower generative diversity is\nobserved when conditioned on expert trajectories. Diffusion Reward is\naccordingly formalized by the negative of conditional entropy that encourages\nproductive exploration of expert-like behaviors. We show the efficacy of our\nmethod over 10 robotic manipulation tasks from MetaWorld and Adroit with visual\ninput and sparse reward. Moreover, Diffusion Reward could even solve unseen\ntasks successfully and effectively, largely surpassing baseline methods.\nProject page and code: https://diffusion-reward.github.io/.\n","authors":["Tao Huang","Guangqi Jiang","Yanjie Ze","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2312.14134v2.pdf","comment":"Project page and code: https://diffusion-reward.github.io/"},{"id":"http://arxiv.org/abs/2310.14402v2","updated":"2024-03-18T03:35:28Z","published":"2023-10-22T20:25:08Z","title":"Value of Assistance for Grasping","summary":"  In multiple realistic settings, a robot is tasked with grasping an object\nwithout knowing its exact pose and relies on a probabilistic estimation of the\npose to decide how to attempt the grasp. We support settings in which it is\npossible to provide the robot with an observation of the object before a grasp\nis attempted but this possibility is limited and there is a need to decide\nwhich sensing action would be most beneficial. We support this decision by\noffering a novel Value of Assistance (VOA) measure for assessing the expected\neffect a specific observation will have on the robot's ability to complete its\ntask. We evaluate our suggested measure in simulated and real-world\ncollaborative grasping settings.\n","authors":["Mohammad Masarwy","Yuval Goshen","David Dovrat","Sarah Keren"],"pdf_url":"https://arxiv.org/pdf/2310.14402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14150v7","updated":"2024-03-18T03:27:35Z","published":"2023-09-25T14:04:31Z","title":"Fast LiDAR Informed Visual Search in Unseen Indoor Environments","summary":"  This paper explores the problem of planning for visual search without prior\nmap information. We leverage the pixel-wise environment perception problem\nwhere one is given wide Field of View 2D scan data and must perform LiDAR\nsegmentation to contextually label points in the surroundings. These pixel\nclassifications provide an informed prior on which to plan next best viewpoints\nduring visual search tasks. We present LIVES: LiDAR Informed Visual Search, a\nmethod aimed at finding objects of interest in unknown indoor environments. A\nrobust map-free classifier is trained from expert data collected using a simple\ncart platform equipped with a map-based classifier. An autonomous exploration\nplanner takes the contextual data from scans and uses that prior to plan\nviewpoints more likely to yield detection of the search target. We propose a\nutility function that accounts for traditional metrics like information gain\nand path cost and for the contextual information. LIVES is baselined against\nseveral existing exploration methods in simulation to verify its performance.\nIt is validated in real-world experiments with single and multiple search\nobjects with a Spot robot in two unseen environments. Videos of experiments,\nimplementation details and open source code can be found at\nhttps://sites.google.com/view/lives-2024/home.\n","authors":["Ryan Gupta","Kyle Morgenstein","Steven Ortega","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2309.14150v7.pdf","comment":"6 pages + references. 6 figures. 1 algorithm. 1 table"},{"id":"http://arxiv.org/abs/2403.11432v1","updated":"2024-03-18T02:59:13Z","published":"2024-03-18T02:59:13Z","title":"Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle\n  Decision-Making","summary":"  With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in automated driving\ntasks has emerged as a chief application among them, taking the sensor data or\nthe higher-order kinematic variables as the input and providing a discrete\nchoice or continuous control output. However, the black-box nature of the\nmodels presents an overwhelming limitation that restricts the real-world\ndeployment of DRL in autonomous vehicles (AVs). Therefore, in this research\nwork, we focus on the interpretability of an attention-based DRL framework. We\nuse a continuous proximal policy optimization-based DRL algorithm as the\nbaseline model and add a multi-head attention framework in an open-source AV\nsimulation environment. We provide some analytical techniques for discussing\nthe interpretability of the trained models in terms of explainability and\ncausality for spatial and temporal correlations. We show that the weights in\nthe first head encode the positions of the neighboring vehicles while the\nsecond head focuses on the leader vehicle exclusively. Also, the ego vehicle's\naction is causally dependent on the vehicles in the target lane spatially and\ntemporally. Through these findings, we reliably show that these techniques can\nhelp practitioners decipher the results of the DRL algorithms.\n","authors":["Hanxi Wan","Pei Li","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2403.11432v1.pdf","comment":"Submitted for peer-review"},{"id":"http://arxiv.org/abs/2403.11412v1","updated":"2024-03-18T02:00:28Z","published":"2024-03-18T02:00:28Z","title":"Expert Composer Policy: Scalable Skill Repertoire for Quadruped Robots","summary":"  We propose the expert composer policy, a framework to reliably expand the\nskill repertoire of quadruped agents. The composer policy links pair of experts\nvia transitions to a sampled target state, allowing experts to be composed\nsequentially. Each expert specializes in a single skill, such as a locomotion\ngait or a jumping motion. Instead of a hierarchical or mixture-of-experts\narchitecture, we train a single composer policy in an independent process that\nis not conditioned on the other expert policies. By reusing the same composer\npolicy, our approach enables adding new experts without affecting existing\nones, enabling incremental repertoire expansion and preserving original motion\nquality. We measured the transition success rate of 72 transition pairs and\nachieved an average success rate of 99.99\\%, which is over 10\\% higher than the\nbaseline random approach, and outperforms other state-of-the-art methods. Using\ndomain randomization during training we ensure a successful transfer to the\nreal world, where we achieve an average transition success rate of 97.22\\%\n(N=360) in our experiments.\n","authors":["Guilherme Christmann","Ying-Sheng Luo","Wei-Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11412v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11396v1","updated":"2024-03-18T01:08:18Z","published":"2024-03-18T01:08:18Z","title":"Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot\n  Navigation and 3D Scene Understanding with FisherRF","summary":"  This work proposes a novel approach to bolster both the robot's risk\nassessment and safety measures while deepening its understanding of 3D scenes,\nwhich is achieved by leveraging Radiance Field (RF) models and 3D Gaussian\nSplatting. To further enhance these capabilities, we incorporate additional\nsampled views from the environment with the RF model. One of our key\ncontributions is the introduction of Risk-aware Environment Masking (RaEM),\nwhich prioritizes crucial information by selecting the next-best-view that\nmaximizes the expected information gain. This targeted approach aims to\nminimize uncertainties surrounding the robot's path and enhance the safety of\nits navigation. Our method offers a dual benefit: improved robot safety and\nincreased efficiency in risk-aware 3D scene reconstruction and understanding.\nExtensive experiments in real-world scenarios demonstrate the effectiveness of\nour proposed approach, highlighting its potential to establish a robust and\nsafety-focused framework for active robot exploration and 3D scene\nunderstanding.\n","authors":["Guangyi Liu","Wen Jiang","Boshu Lei","Vivek Pandey","Kostas Daniilidis","Nader Motee"],"pdf_url":"https://arxiv.org/pdf/2403.11396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11384v1","updated":"2024-03-18T00:22:30Z","published":"2024-03-18T00:22:30Z","title":"A Systematic Review of XR-based Remote Human-Robot Interaction Systems","summary":"  This survey provides an exhaustive review of the applications of extended\nreality (XR) technologies in the field of remote human-computer interaction\n(HRI). We developed a systematic search strategy based on the PRISMA\nmethodology. From the initial 2,561 articles selected, 100 research papers that\nmet our inclusion criteria were included. We categorized and summarized the\ndomain in detail, delving into XR technologies, including augmented reality\n(AR), virtual reality (VR), and mixed reality (MR), and their applications in\nfacilitating intuitive and effective remote control and interaction with\nrobotic systems.The survey highlights existing articles on the application of\nXR technologies, user experience enhancement, and various interaction designs\nfor XR in remote HRI, providing insights into current trends and future\ndirections. We also identified potential gaps and opportunities for future\nresearch to improve remote HRI systems through XR technology to guide and\ninform future XR and robotics research.\n","authors":["Xian Wang","Luyao Shen","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11383v1","updated":"2024-03-18T00:19:52Z","published":"2024-03-18T00:19:52Z","title":"On the Benefits of GPU Sample-Based Stochastic Predictive Controllers\n  for Legged Locomotion","summary":"  Quadrupedal robots excel in mobility, navigating complex terrains with\nagility. However, their complex control systems present challenges that are\nstill far from being fully addressed. In this paper, we introduce the use of\nSample-Based Stochastic control strategies for quadrupedal robots, as an\nalternative to traditional optimal control laws. We show that Sample-Based\nStochastic methods, supported by GPU acceleration, can be effectively applied\nto real quadruped robots. In particular, in this work, we focus on achieving\ngait frequency adaptation, a notable challenge in quadrupedal locomotion for\ngradient-based methods. To validate the effectiveness of Sample-Based\nStochastic controllers we test two distinct approaches for quadrupedal robots\nand compare them against a conventional gradient-based Model Predictive Control\nsystem. Our findings, validated both in simulation and on a real 21Kg Aliengo\nquadruped, demonstrate that our method is on par with a traditional Model\nPredictive Control strategy when the robot is subject to zero or moderate\ndisturbance, while it surpasses gradient-based methods in handling sustained\nexternal disturbances, thanks to the straightforward gait adaptation strategy\nthat is possible to achieve within their formulation.\n","authors":["Giulio Turrisi","Valerio Modugno","Lorenzo Amatucci","Dimitrios Kanoulas","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19115v2","updated":"2024-03-18T22:39:04Z","published":"2023-05-30T15:24:40Z","title":"High-Gain Disturbance Observer for Robust Trajectory Tracking of\n  Quadrotors","summary":"  This paper presents a simple method to boost the robustness of quadrotors in\ntrajectory tracking. The presented method features a high-gain disturbance\nobserver (HGDO) that provides disturbance estimates in real-time. The estimates\nare then used in a trajectory control law to compensate for disturbance\neffects. We present theoretical convergence results showing that the proposed\nHGDO can quickly converge to an adjustable neighborhood of actual disturbance\nvalues. We will then integrate the disturbance estimates with a typical robust\ntrajectory controller, namely sliding mode control (SMC), and present Lyapunov\nstability analysis to establish the boundedness of trajectory tracking errors.\nHowever, our stability analysis can be easily extended to other Lyapunov-based\ncontrollers to develop different HGDO-based controllers with formal stability\nguarantees. We evaluate the proposed HGDO-based control method using both\nsimulation and laboratory experiments in various scenarios and in the presence\nof external disturbances. Our results indicate that the addition of HGDO to a\nquadrotor trajectory controller can significantly improve the accuracy and\nprecision of trajectory tracking in the presence of external disturbances.\n","authors":["Mohammadreza Izadi","Reza Faieghi"],"pdf_url":"https://arxiv.org/pdf/2305.19115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12280v1","updated":"2024-03-18T21:55:25Z","published":"2024-03-18T21:55:25Z","title":"Reachability-based Trajectory Design via Exact Formulation of Implicit\n  Neural Signed Distance Functions","summary":"  Generating receding-horizon motion trajectories for autonomous vehicles in\nreal-time while also providing safety guarantees is challenging. This is\nbecause a future trajectory needs to be planned before the previously computed\ntrajectory is completely executed. This becomes even more difficult if the\ntrajectory is required to satisfy continuous-time collision-avoidance\nconstraints while accounting for a large number of obstacles. To address these\nchallenges, this paper proposes a novel real-time, receding-horizon motion\nplanning algorithm named REachability-based trajectory Design via Exact\nFormulation of Implicit NEural signed Distance functions (REDEFINED). REDEFINED\nfirst applies offline reachability analysis to compute zonotope-based reachable\nsets that overapproximate the motion of the ego vehicle. During online\nplanning, REDEFINED leverages zonotope arithmetic to construct a neural\nimplicit representation that computes the exact signed distance between a\nparameterized swept volume of the ego vehicle and obstacle vehicles. REDEFINED\nthen implements a novel, real-time optimization framework that utilizes the\nneural network to construct a collision avoidance constraint. REDEFINED is\ncompared to a variety of state-of-the-art techniques and is demonstrated to\nsuccessfully enable the vehicle to safely navigate through complex\nenvironments. Code, data, and video demonstrations can be found at\nhttps://roahmlab.github.io/redefined/.\n","authors":["Jonathan Michaux","Qingyi Chen","Challen Enninful Adu","Jinsun Liu","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2403.12280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12279v1","updated":"2024-03-18T21:54:04Z","published":"2024-03-18T21:54:04Z","title":"Scalable Networked Feature Selection with Randomized Algorithm for Robot\n  Navigation","summary":"  We address the problem of sparse selection of visual features for localizing\na team of robots navigating an unknown environment, where robots can exchange\nrelative position measurements with neighbors. We select a set of the most\ninformative features by anticipating their importance in robots localization by\nsimulating trajectories of robots over a prediction horizon. Through\ntheoretical proofs, we establish a crucial connection between graph Laplacian\nand the importance of features. We show that strong network connectivity\ntranslates to uniformity in feature importance, which enables uniform random\nsampling of features and reduces the overall computational complexity. We\nleverage a scalable randomized algorithm for sparse sums of positive\nsemidefinite matrices to efficiently select the set of the most informative\nfeatures and significantly improve the probabilistic performance bounds.\nFinally, we support our findings with extensive simulations.\n","authors":["Vivek Pandey","Arash Amini","Guangyi Liu","Ufuk Topcu","Qiyu Sun","Kostas Daniilidis","Nader Motee"],"pdf_url":"https://arxiv.org/pdf/2403.12279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12273v1","updated":"2024-03-18T21:41:09Z","published":"2024-03-18T21:41:09Z","title":"Multimodal Human-Autonomous Agents Interaction Using Pre-Trained\n  Language and Visual Foundation Models","summary":"  In this paper, we extended the method proposed in [17] to enable humans to\ninteract naturally with autonomous agents through vocal and textual\nconversations. Our extended method exploits the inherent capabilities of\npre-trained large language models (LLMs), multimodal visual language models\n(VLMs), and speech recognition (SR) models to decode the high-level natural\nlanguage conversations and semantic understanding of the robot's task\nenvironment, and abstract them to the robot's actionable commands or queries.\nWe performed a quantitative evaluation of our framework's natural vocal\nconversation understanding with participants from different racial backgrounds\nand English language accents. The participants interacted with the robot using\nboth spoken and textual instructional commands. Based on the logged interaction\ndata, our framework achieved 87.55% vocal commands decoding accuracy, 86.27%\ncommands execution success, and an average latency of 0.89 seconds from\nreceiving the participants' vocal chat commands to initiating the robot's\nactual physical action. The video demonstrations of this paper can be found at\nhttps://linusnep.github.io/MTCC-IRoNL/.\n","authors":["Linus Nwankwo","Elmar Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.12273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12245v1","updated":"2024-03-18T20:51:08Z","published":"2024-03-18T20:51:08Z","title":"Improving Out-of-Distribution Generalization of Learned Dynamics by\n  Learning Pseudometrics and Constraint Manifolds","summary":"  We propose a method for improving the prediction accuracy of learned robot\ndynamics models on out-of-distribution (OOD) states. We achieve this by\nleveraging two key sources of structure often present in robot dynamics: 1)\nsparsity, i.e., some components of the state may not affect the dynamics, and\n2) physical limits on the set of possible motions, in the form of nonholonomic\nconstraints. Crucially, we do not assume this structure is known \\textit{a\npriori}, and instead learn it from data. We use contrastive learning to obtain\na distance pseudometric that uncovers the sparsity pattern in the dynamics, and\nuse it to reduce the input space when learning the dynamics. We then learn the\nunknown constraint manifold by approximating the normal space of possible\nmotions from the data, which we use to train a Gaussian process (GP)\nrepresentation of the constraint manifold. We evaluate our approach on a\nphysical differential-drive robot and a simulated quadrotor, showing improved\nprediction accuracy on OOD data relative to baselines.\n","authors":["Yating Lin","Glen Chou","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2403.12245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12996v2","updated":"2024-03-18T20:45:17Z","published":"2023-11-21T21:05:21Z","title":"RLIF: Interactive Imitation Learning as Reinforcement Learning","summary":"  Although reinforcement learning methods offer a powerful framework for\nautomatic skill acquisition, for practical learning-based control problems in\ndomains such as robotics, imitation learning often provides a more convenient\nand accessible alternative. In particular, an interactive imitation learning\nmethod such as DAgger, which queries a near-optimal expert to intervene online\nto collect correction data for addressing the distributional shift challenges\nthat afflict na\\\"ive behavioral cloning, can enjoy good performance both in\ntheory and practice without requiring manually specified reward functions and\nother components of full reinforcement learning methods. In this paper, we\nexplore how off-policy reinforcement learning can enable improved performance\nunder assumptions that are similar but potentially even more practical than\nthose of interactive imitation learning. Our proposed method uses reinforcement\nlearning with user intervention signals themselves as rewards. This relaxes the\nassumption that intervening experts in interactive imitation learning should be\nnear-optimal and enables the algorithm to learn behaviors that improve over the\npotential suboptimal human expert. We also provide a unified framework to\nanalyze our RL method and DAgger; for which we present the asymptotic analysis\nof the suboptimal gap for both methods as well as the non-asymptotic sample\ncomplexity bound of our method. We then evaluate our method on challenging\nhigh-dimensional continuous control simulation benchmarks as well as real-world\nrobotic vision-based manipulation tasks. The results show that it strongly\noutperforms DAgger-like approaches across the different tasks, especially when\nthe intervening experts are suboptimal. Code and videos can be found on the\nproject website: https://rlif-page.github.io\n","authors":["Jianlan Luo","Perry Dong","Yuexiang Zhai","Yi Ma","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2311.12996v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12235v1","updated":"2024-03-18T20:33:06Z","published":"2024-03-18T20:33:06Z","title":"IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and\n  Rank Minimization","summary":"  Inverse kinematics (IK) is a fundamental problem frequently occurred in robot\ncontrol and motion planning. However, the problem is nonconvex because the\nkinematic map between the configuration and task spaces is generally nonlinear,\nwhich makes it challenging for fast and accurate solutions. The problem can be\nmore complicated with the existence of different physical constraints imposed\nby the robot structure. In this paper, we develop an inverse kinematics solver\nnamed IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK\nminimization) that can find solutions for robots with various structures,\nincluding open/closed kinematic chains, spherical, revolute, and/or prismatic\njoints. The solver works in the space of rotation matrices of the link\nreference frames and involves solving only convex semidefinite problems (SDPs).\nSpecifically, the IK problem is formulated as an SDP with an additional rank-1\nconstraint on symmetric matrices with constant traces. The solver first solves\nthis SDP disregarding the rank constraint to get a start point and then finds\nthe rank-1 solution iteratively via a rank minimization algorithm with proven\nlocal convergence. Compared to other work that performs SDP relaxation for IK\nproblems, our formulation is simpler, and uses variables with smaller sizes. We\nvalidate our approach via simulations on different robots, comparing against a\nstandard IK method.\n","authors":["Liangting Wu","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.12235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12223v1","updated":"2024-03-18T20:11:02Z","published":"2024-03-18T20:11:02Z","title":"HRI in Indian Education: Challenges Opportunities","summary":"  With the recent advancements in the field of robotics and the increased focus\non having general-purpose robots widely available to the general public, it has\nbecome increasingly necessary to pursue research into Human-robot interaction\n(HRI). While there have been a lot of works discussing frameworks for teaching\nHRI in educational institutions with a few institutions already offering\ncourses to students, a consensus on the course content still eludes the field.\nIn this work, we highlight a few challenges and opportunities while designing\nan HRI course from an Indian perspective. These topics warrant further\ndeliberations as they have a direct impact on the design of HRI courses and\nwider implications for the entire field.\n","authors":["Chinmaya Mishra","Anuj Nandanwar","Sashikala Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.12223v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.12214v1","updated":"2024-03-18T19:58:23Z","published":"2024-03-18T19:58:23Z","title":"Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot","summary":"  Robot art presents an opportunity to both showcase and advance\nstate-of-the-art robotics through the challenging task of creating art.\nCreating large-scale artworks in particular engages the public in a way that\nsmall-scale works cannot, and the distinct qualities of brush strokes\ncontribute to an organic and human-like quality. Combining the large scale of\nmurals with the strokes of the brush medium presents an especially impactful\nresult, but also introduces unique challenges in maintaining precise, dextrous\nmotion control of the brush across such a large workspace. In this work, we\npresent the first robot to our knowledge that can paint architectural-scale\nmurals with a brush. We create a hybrid robot consisting of a cable-driven\nparallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m\nby 3.7m mural on windows spanning 2-stories of a building. We discuss our\napproach to achieving both the scale and accuracy required for brush-painting a\nmural through a combination of novel mechanical design elements, coordinated\nplanning and control, and on-site calibration algorithms with experimental\nvalidations.\n","authors":["Gerry Chen","Tristan Al-Haddad","Frank Dellaert","Seth Hutchinson"],"pdf_url":"https://arxiv.org/pdf/2403.12214v1.pdf","comment":"8 pages IEEE conference format, submitted to IROS 2024,"},{"id":"http://arxiv.org/abs/2403.12203v1","updated":"2024-03-18T19:25:57Z","published":"2024-03-18T19:25:57Z","title":"Bootstrapping Reinforcement Learning with Imitation for Vision-Based\n  Agile Flight","summary":"  We combine the effectiveness of Reinforcement Learning (RL) and the\nefficiency of Imitation Learning (IL) in the context of vision-based,\nautonomous drone racing. We focus on directly processing visual input without\nexplicit state estimation. While RL offers a general framework for learning\ncomplex controllers through trial and error, it faces challenges regarding\nsample efficiency and computational demands due to the high dimensionality of\nvisual inputs. Conversely, IL demonstrates efficiency in learning from visual\ndemonstrations but is limited by the quality of those demonstrations and faces\nissues like covariate shift. To overcome these limitations, we propose a novel\ntraining framework combining RL and IL's advantages. Our framework involves\nthree stages: initial training of a teacher policy using privileged state\ninformation, distilling this policy into a student policy using IL, and\nperformance-constrained adaptive RL fine-tuning. Our experiments in both\nsimulated and real-world environments demonstrate that our approach achieves\nsuperior performance and robustness than IL or RL alone in navigating a\nquadrotor through a racing course using only visual information without\nexplicit state estimation.\n","authors":["Jiaxu Xing","Angel Romero","Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.12203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12194v1","updated":"2024-03-18T19:07:42Z","published":"2024-03-18T19:07:42Z","title":"The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating\n  Traverses across Lunar Polar Terrain under Extreme Lighting Conditions","summary":"  We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair\nimages of lunar-like terrain under polar lighting conditions designed to\nsimulate a straight-line traverse. Images from individual traverses with\ndifferent camera heights and pitches were recorded at 1 m intervals by moving a\nsuspended stereo bar across a test bed filled with regolith simulant and shaped\nto mimic lunar south polar terrain. Ground truth geometry and camera position\ninformation was also recorded. This dataset is intended for developing and\ntesting software algorithms that rely on stereo or monocular camera images,\nsuch as visual odometry, for use in the lunar polar environment, as well as to\nprovide insight into the expected lighting conditions in lunar polar regions.\n","authors":["Margaret Hansen","Uland Wong","Terrence Fong"],"pdf_url":"https://arxiv.org/pdf/2403.12194v1.pdf","comment":"6 pages, 5 figures, 3 tables. Associated dataset can be found at\n  https://ti.arc.nasa.gov/dataset/PolarTrav/"},{"id":"http://arxiv.org/abs/2403.12193v1","updated":"2024-03-18T19:04:56Z","published":"2024-03-18T19:04:56Z","title":"Continual Domain Randomization","summary":"  Domain Randomization (DR) is commonly used for sim2real transfer of\nreinforcement learning (RL) policies in robotics. Most DR approaches require a\nsimulator with a fixed set of tunable parameters from the start of the\ntraining, from which the parameters are randomized simultaneously to train a\nrobust model for use in the real world. However, the combined randomization of\nmany parameters increases the task difficulty and might result in sub-optimal\npolicies. To address this problem and to provide a more flexible training\nprocess, we propose Continual Domain Randomization (CDR) for RL that combines\ndomain randomization with continual learning to enable sequential training in\nsimulation on a subset of randomization parameters at a time. Starting from a\nmodel trained in a non-randomized simulation where the task is easier to solve,\nthe model is trained on a sequence of randomizations, and continual learning is\nemployed to remember the effects of previous randomizations. Our robotic\nreaching and grasping tasks experiments show that the model trained in this\nfashion learns effectively in simulation and performs robustly on the real\nrobot while matching or outperforming baselines that employ combined\nrandomization or sequential randomization without continual learning. Our code\nand videos are available at https://continual-dr.github.io/.\n","authors":["Josip Josifovski","Sayantan Auddy","Mohammadhossein Malmir","Justus Piater","Alois Knoll","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2403.12193v1.pdf","comment":"Under peer review"},{"id":"http://arxiv.org/abs/2310.00145v2","updated":"2024-03-18T18:51:09Z","published":"2023-09-29T21:09:02Z","title":"3D Reconstruction in Noisy Agricultural Environments: A Bayesian\n  Optimization Perspective for View Planning","summary":"  3D reconstruction is a fundamental task in robotics that gained attention due\nto its major impact in a wide variety of practical settings, including\nagriculture, underwater, and urban environments. This task can be carried out\nvia view planning (VP), which aims to optimally place a certain number of\ncameras in positions that maximize the visual information, improving the\nresulting 3D reconstruction. Nonetheless, in most real-world settings, existing\nenvironmental noise can significantly affect the performance of 3D\nreconstruction. To that end, this work advocates a novel geometric-based\nreconstruction quality function for VP, that accounts for the existing noise of\nthe environment, without requiring its closed-form expression. With no analytic\nexpression of the objective function, this work puts forth an adaptive Bayesian\noptimization algorithm for accurate 3D reconstruction in the presence of noise.\nNumerical tests on noisy agricultural environments showcase the merits of the\nproposed approach for 3D reconstruction with even a small number of available\ncameras.\n","authors":["Athanasios Bacharis","Konstantinos D. Polyzos","Henry J. Nelson","Georgios B. Giannakis","Nikolaos Papanikolopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.00145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12176v1","updated":"2024-03-18T18:49:20Z","published":"2024-03-18T18:49:20Z","title":"Safety Implications of Explainable Artificial Intelligence in End-to-End\n  Autonomous Driving","summary":"  The end-to-end learning pipeline is gradually creating a paradigm shift in\nthe ongoing development of highly autonomous vehicles, largely due to advances\nin deep learning, the availability of large-scale training datasets, and\nimprovements in integrated sensor devices. However, a lack of interpretability\nin real-time decisions with contemporary learning methods impedes user trust\nand attenuates the widespread deployment and commercialization of such\nvehicles. Moreover, the issue is exacerbated when these cars are involved in or\ncause traffic accidents. Such drawback raises serious safety concerns from\nsocietal and legal perspectives. Consequently, explainability in end-to-end\nautonomous driving is essential to enable the safety of vehicular automation.\nHowever, the safety and explainability aspects of autonomous driving have\ngenerally been investigated disjointly by researchers in today's state of the\nart. In this paper, we aim to bridge the gaps between these topics and seek to\nanswer the following research question: When and how can explanations improve\nsafety of autonomous driving? In this regard, we first revisit established\nsafety and state-of-the-art explainability techniques in autonomous driving.\nFurthermore, we present three critical case studies and show the pivotal role\nof explanations in enhancing self-driving safety. Finally, we describe our\nempirical investigation and reveal potential value, limitations, and caveats\nwith practical explainable AI methods on their role of assuring safety and\ntransparency for vehicle autonomy.\n","authors":["Shahin Atakishiyev","Mohammad Salameh","Randy Goebel"],"pdf_url":"https://arxiv.org/pdf/2403.12176v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.09875v2","updated":"2024-03-18T18:46:13Z","published":"2024-03-14T21:09:59Z","title":"Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting","summary":"  In this work, we propose a novel method to supervise 3D Gaussian Splatting\n(3DGS) scenes using optical tactile sensors. Optical tactile sensors have\nbecome widespread in their use in robotics for manipulation and object\nrepresentation; however, raw optical tactile sensor data is unsuitable to\ndirectly supervise a 3DGS scene. Our representation leverages a Gaussian\nProcess Implicit Surface to implicitly represent the object, combining many\ntouches into a unified representation with uncertainty. We merge this model\nwith a monocular depth estimation network, which is aligned in a two stage\nprocess, coarsely aligning with a depth camera and then finely adjusting to\nmatch our touch data. For every training image, our method produces a\ncorresponding fused depth and uncertainty map. Utilizing this additional\ninformation, we propose a new loss function, variance weighted depth supervised\nloss, for training the 3DGS scene model. We leverage the DenseTact optical\ntactile sensor and RealSense RGB-D camera to show that combining touch and\nvision in this manner leads to quantitatively and qualitatively better results\nthan vision or touch alone in a few-view scene syntheses on opaque as well as\non reflective and transparent objects. Please see our project page at\nhttp://armlabstanford.github.io/touch-gs\n","authors":["Aiden Swann","Matthew Strong","Won Kyung Do","Gadiel Sznaier Camps","Mac Schwager","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.09875v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.12170v1","updated":"2024-03-18T18:39:51Z","published":"2024-03-18T18:39:51Z","title":"Sim2Real Manipulation on Unknown Objects with Tactile-based\n  Reinforcement Learning","summary":"  Using tactile sensors for manipulation remains one of the most challenging\nproblems in robotics. At the heart of these challenges is generalization: How\ncan we train a tactile-based policy that can manipulate unseen and diverse\nobjects? In this paper, we propose to perform Reinforcement Learning with only\nvisual tactile sensing inputs on diverse objects in a physical simulator. By\ntraining with diverse objects in simulation, it enables the policy to\ngeneralize to unseen objects. However, leveraging simulation introduces the\nSim2Real transfer problem. To mitigate this problem, we study different tactile\nrepresentations and evaluate how each affects real-robot manipulation results\nafter transfer. We conduct our experiments on diverse real-world objects and\nshow significant improvements over baselines for the pivoting task. Our project\npage is available at https://tactilerl.github.io/.\n","authors":["Entong Su","Chengzhe Jia","Yuzhe Qin","Wenxuan Zhou","Annabella Macaluso","Binghao Huang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12162v1","updated":"2024-03-18T18:23:36Z","published":"2024-03-18T18:23:36Z","title":"Intelligent Execution through Plan Analysis","summary":"  Intelligent robots need to generate and execute plans. In order to deal with\nthe complexity of real environments, planning makes some assumptions about the\nworld. When executing plans, the assumptions are usually not met. Most works\nhave focused on the negative impact of this fact and the use of replanning\nafter execution failures. Instead, we focus on the positive impact, or\nopportunities to find better plans. When planning, the proposed technique finds\nand stores those opportunities. Later, during execution, the monitoring system\ncan use them to focus perception and repair the plan, instead of replanning\nfrom scratch. Experiments in several paradigmatic robotic tasks show how the\napproach outperforms standard replanning strategies.\n","authors":["Daniel Borrajo","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2403.12162v1.pdf","comment":"Published at IROS 21, 6 pages"},{"id":"http://arxiv.org/abs/2403.12149v1","updated":"2024-03-18T18:03:08Z","published":"2024-03-18T18:03:08Z","title":"Ergonomic Optimization in Worker-Robot Bimanual Object Handover:\n  Implementing REBA Using Reinforcement Learning in Virtual Reality","summary":"  Robots can serve as safety catalysts on construction job sites by taking over\nhazardous and repetitive tasks while alleviating the risks associated with\nexisting manual workflows. Research on the safety of physical human-robot\ninteraction (pHRI) is traditionally focused on addressing the risks associated\nwith potential collisions. However, it is equally important to ensure that the\nworkflows involving a collaborative robot are inherently safe, even though they\nmay not result in an accident. For example, pHRI may require the human\ncounterpart to use non-ergonomic body postures to conform to the robot hardware\nand physical configurations. Frequent and long-term exposure to such situations\nmay result in chronic health issues. Safety and ergonomics assessment measures\ncan be understood by robots if they are presented in algorithmic fashions so\noptimization for body postures is attainable. While frameworks such as Rapid\nEntire Body Assessment (REBA) have been an industry standard for many decades,\nthey lack a rigorous mathematical structure which poses challenges in using\nthem immediately for pHRI safety optimization purposes. Furthermore, learnable\napproaches have limited robustness outside of their training data, reducing\ngeneralizability. In this paper, we propose a novel framework that approaches\noptimization through Reinforcement Learning, ensuring precise, online ergonomic\nscores as compared to approximations, while being able to generalize and tune\nthe regiment to any human and any task. To ensure practicality, the training is\ndone in virtual reality utilizing Inverse Kinematics to simulate human movement\nmechanics. Experimental findings are compared to ergonomically naive object\nhandover heuristics and indicate promising results where the developed\nframework can find the optimal object handover coordinates in pHRI contexts for\nmanual material handling exemplary situations.\n","authors":["Mani Amani","Reza Akhavian"],"pdf_url":"https://arxiv.org/pdf/2403.12149v1.pdf","comment":"Submitted to Safety Science"},{"id":"http://arxiv.org/abs/2403.12039v1","updated":"2024-03-18T17:59:48Z","published":"2024-03-18T17:59:48Z","title":"StereoNavNet: Learning to Navigate using Stereo Cameras with Auxiliary\n  Occupancy Voxels","summary":"  Visual navigation has received significant attention recently. Most of the\nprior works focus on predicting navigation actions based on semantic features\nextracted from visual encoders. However, these approaches often rely on large\ndatasets and exhibit limited generalizability. In contrast, our approach draws\ninspiration from traditional navigation planners that operate on geometric\nrepresentations, such as occupancy maps. We propose StereoNavNet (SNN), a novel\nvisual navigation approach employing a modular learning framework comprising\nperception and policy modules. Within the perception module, we estimate an\nauxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric\nfeatures from it. These features, along with user-defined goals, are utilized\nby the policy module to predict navigation actions. Through extensive empirical\nevaluation, we demonstrate that SNN outperforms baseline approaches in terms of\nsuccess rates, success weighted by path length, and navigation error.\nFurthermore, SNN exhibits better generalizability, characterized by maintaining\nleading performance when navigating across previously unseen environments.\n","authors":["Hongyu Li","Taskin Padir","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12023v1","updated":"2024-03-18T17:55:52Z","published":"2024-03-18T17:55:52Z","title":"Aligning Learning with Communication in Shared Autonomy","summary":"  Assistive robot arms can help humans by partially automating their desired\ntasks. Consider an adult with motor impairments controlling an assistive robot\narm to eat dinner. The robot can reduce the number of human inputs -- and how\nprecise those inputs need to be -- by recognizing what the human wants (e.g., a\nfork) and assisting for that task (e.g., moving towards the fork). Prior\nresearch has largely focused on learning the human's task and providing\nmeaningful assistance. But as the robot learns and assists, we also need to\nensure that the human understands the robot's intent (e.g., does the human know\nthe robot is reaching for a fork?). In this paper, we study the effects of\ncommunicating learned assistance from the robot back to the human operator. We\ndo not focus on the specific interfaces used for communication. Instead, we\ndevelop experimental and theoretical models of a) how communication changes the\nway humans interact with assistive robot arms, and b) how robots can harness\nthese changes to better align with the human's intent. We first conduct online\nand in-person user studies where participants operate robots that provide\npartial assistance, and we measure how the human's inputs change with and\nwithout communication. With communication, we find that humans are more likely\nto intervene when the robot incorrectly predicts their intent, and more likely\nto release control when the robot correctly understands their task. We then use\nthese findings to modify an established robot learning algorithm so that the\nrobot can correctly interpret the human's inputs when communication is present.\nOur results from a second in-person user study suggest that this combination of\ncommunication and learning outperforms assistive systems that isolate either\nlearning or communication.\n","authors":["Joshua Hoegerman","Shahabedin Sagheb","Benjamin A. Christie","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2403.12023v1.pdf","comment":"7 pages, under review for IROS 2024"},{"id":"http://arxiv.org/abs/2310.15928v2","updated":"2024-03-18T17:36:33Z","published":"2023-10-24T15:26:57Z","title":"AO-Grasp: Articulated Object Grasp Generation","summary":"  We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps\nthat enable robots to interact with articulated objects, such as opening and\nclosing cabinets and appliances. AO-Grasp consists of two main contributions:\nthe AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point\ncloud of a single articulated object, the AO-Grasp Model predicts the best\ngrasp points on the object with an Actionable Grasp Point Predictor. Then, it\nfinds corresponding grasp orientations for each of these points, resulting in\nstable and actionable grasp proposals. We train the AO-Grasp Model on our new\nAO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on\nsynthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp\nsuccess rate, whereas the highest performing baseline achieves a 35.0% success\nrate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects\nwith varied geometries, articulation axes, and joint states, where AO-Grasp\nproduces successful grasps on 67.5% of scenes, while the baseline only produces\nsuccessful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is\nthe first method for generating 6 DoF grasps on articulated objects directly\nfrom partial point clouds without requiring part detection or hand-designed\ngrasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp\n","authors":["Carlota Parés Morlans","Claire Chen","Yijia Weng","Michelle Yi","Yuying Huang","Nick Heppert","Linqi Zhou","Leonidas Guibas","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2310.15928v2.pdf","comment":"Project website: https://stanford-iprl-lab.github.io/ao-grasp"},{"id":"http://arxiv.org/abs/2403.11985v1","updated":"2024-03-18T17:22:43Z","published":"2024-03-18T17:22:43Z","title":"SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial\n  Observation","summary":"  When exploring new areas, robotic systems generally exclusively plan and\nexecute controls over geometry that has been directly measured. When entering\nspace that was previously obstructed from view such as turning corners in\nhallways or entering new rooms, robots often pause to plan over the newly\nobserved space. To address this we present SceneScene, a real-time 3D diffusion\nmodel for synthesizing 3D occupancy information from partial observations that\neffectively predicts these occluded or out of view geometries for use in future\nplanning and control frameworks. SceneSense uses a running occupancy map and a\nsingle RGB-D camera to generate predicted geometry around the platform at\nruntime, even when the geometry is occluded or out of view. Our architecture\nensures that SceneSense never overwrites observed free or occupied space. By\npreserving the integrity of the observed map, SceneSense mitigates the risk of\ncorrupting the observed space with generative predictions. While SceneSense is\nshown to operate well using a single RGB-D camera, the framework is flexible\nenough to extend to additional modalities. SceneSense operates as part of any\nsystem that generates a running occupancy map `out of the box', removing\nconditioning from the framework. Alternatively, for maximum performance in new\nmodalities, the perception backbone can be replaced and the model retrained for\ninference in new applications. Unlike existing models that necessitate multiple\nviews and offline scene synthesis, or are focused on filling gaps in observed\ndata, our findings demonstrate that SceneSense is an effective approach to\nestimating unobserved local occupancy information at runtime. Local occupancy\npredictions from SceneSense are shown to better represent the ground truth\noccupancy distribution during the test exploration trajectories than the\nrunning occupancy map.\n","authors":["Alec Reed","Brendan Crowe","Doncey Albin","Lorin Achey","Bradley Hayes","Christoffer Heckman"],"pdf_url":"https://arxiv.org/pdf/2403.11985v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.16316v2","updated":"2024-03-18T17:02:28Z","published":"2023-06-28T15:47:49Z","title":"Learning Continuous Control with Geometric Regularity from Robot\n  Intrinsic Symmetry","summary":"  Geometric regularity, which leverages data symmetry, has been successfully\nincorporated into deep learning architectures such as CNNs, RNNs, GNNs, and\nTransformers. While this concept has been widely applied in robotics to address\nthe curse of dimensionality when learning from high-dimensional data, the\ninherent reflectional and rotational symmetry of robot structures has not been\nadequately explored. Drawing inspiration from cooperative multi-agent\nreinforcement learning, we introduce novel network structures for single-agent\ncontrol learning that explicitly capture these symmetries. Moreover, we\ninvestigate the relationship between the geometric prior and the concept of\nParameter Sharing in multi-agent reinforcement learning. Last but not the\nleast, we implement the proposed framework in online and offline learning\nmethods to demonstrate its ease of use. Through experiments conducted on\nvarious challenging continuous control tasks on simulators and real robots, we\nhighlight the significant potential of the proposed geometric regularity in\nenhancing robot learning capabilities.\n","authors":["Shengchao Yan","Baohe Zhang","Yuan Zhang","Joschka Boedecker","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2306.16316v2.pdf","comment":"accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11955v1","updated":"2024-03-18T16:52:45Z","published":"2024-03-18T16:52:45Z","title":"Inferring Belief States in Partially-Observable Human-Robot Teams","summary":"  We investigate the real-time estimation of human situation awareness using\nobservations from a robot teammate with limited visibility. In human factors\nand human-autonomy teaming, it is recognized that individuals navigate their\nenvironments using an internal mental simulation, or mental model. The mental\nmodel informs cognitive processes including situation awareness, contextual\nreasoning, and task planning. In teaming domains, the mental model includes a\nteam model of each teammate's beliefs and capabilities, enabling fluent\nteamwork without the need for explicit communication. However, little work has\napplied team models to human-robot teaming. We compare the performance of two\ncurrent methods at estimating user situation awareness over varying visibility\nconditions. Our results indicate that the methods are largely resilient to\nlow-visibility conditions in our domain, however opportunities exist to improve\ntheir overall performance.\n","authors":["Jack Kolb","Karen M. Feigh"],"pdf_url":"https://arxiv.org/pdf/2403.11955v1.pdf","comment":"Under review, project page: https://jackkolb.com/tmm-hri"},{"id":"http://arxiv.org/abs/2403.11948v1","updated":"2024-03-18T16:42:39Z","published":"2024-03-18T16:42:39Z","title":"Learning Dynamical Systems Encoding Non-Linearity within Space Curvature","summary":"  Dynamical Systems (DS) are an effective and powerful means of shaping\nhigh-level policies for robotics control. They provide robust and reactive\ncontrol while ensuring the stability of the driving vector field. The\nincreasing complexity of real-world scenarios necessitates DS with a higher\ndegree of non-linearity, along with the ability to adapt to potential changes\nin environmental conditions, such as obstacles. Current learning strategies for\nDSs often involve a trade-off, sacrificing either stability guarantees or\noffline computational efficiency in order to enhance the capabilities of the\nlearned DS. Online local adaptation to environmental changes is either not\ntaken into consideration or treated as a separate problem. In this paper, our\nobjective is to introduce a method that enhances the complexity of the learned\nDS without compromising efficiency during training or stability guarantees.\nFurthermore, we aim to provide a unified approach for seamlessly integrating\nthe initially learned DS's non-linearity with any local non-linearities that\nmay arise due to changes in the environment. We propose a geometrical approach\nto learn asymptotically stable non-linear DS for robotics control. Each DS is\nmodeled as a harmonic damped oscillator on a latent manifold. By learning the\nmanifold's Euclidean embedded representation, our approach encodes the\nnon-linearity of the DS within the curvature of the space. Having an explicit\nembedded representation of the manifold allows us to showcase obstacle\navoidance by directly inducing local deformations of the space. We demonstrate\nthe effectiveness of our methodology through two scenarios: first, the 2D\nlearning of synthetic vector fields, and second, the learning of 3D robotic\nend-effector motions in real-world settings.\n","authors":["Bernardo Fichera","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2403.11948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11914v1","updated":"2024-03-18T16:13:02Z","published":"2024-03-18T16:13:02Z","title":"Single-Agent Actor Critic for Decentralized Cooperative Driving","summary":"  Active traffic management incorporating autonomous vehicles (AVs) promises a\nfuture with diminished congestion and enhanced traffic flow. However,\ndeveloping algorithms for real-world application requires addressing the\nchallenges posed by continuous traffic flow and partial observability. To\nbridge this gap and advance the field of active traffic management towards\ngreater decentralization, we introduce a novel asymmetric actor-critic model\naimed at learning decentralized cooperative driving policies for autonomous\nvehicles using single-agent reinforcement learning. Our approach employs\nattention neural networks with masking to handle the dynamic nature of\nreal-world traffic flow and partial observability. Through extensive\nevaluations against baseline controllers across various traffic scenarios, our\nmodel shows great potential for improving traffic flow at diverse bottleneck\nlocations within the road system. Additionally, we explore the challenge\nassociated with the conservative driving behaviors of autonomous vehicles that\nadhere strictly to traffic regulations. The experiment results illustrate that\nour proposed cooperative policy can mitigate potential traffic slowdowns\nwithout compromising safety.\n","authors":["Shengchao Yan","Lukas König","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.11914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11898v1","updated":"2024-03-18T15:56:44Z","published":"2024-03-18T15:56:44Z","title":"Visuo-Tactile Pretraining for Cable Plugging","summary":"  Tactile information is a critical tool for fine-grain manipulation. As\nhumans, we rely heavily on tactile information to understand objects in our\nenvironments and how to interact with them. We use touch not only to perform\nmanipulation tasks but also to learn how to perform these tasks. Therefore, to\ncreate robotic agents that can learn to complete manipulation tasks at a human\nor super-human level of performance, we need to properly incorporate tactile\ninformation into both skill execution and skill learning. In this paper, we\ninvestigate how we can incorporate tactile information into imitation learning\nplatforms to improve performance on complex tasks. To do this, we tackle the\nchallenge of plugging in a USB cable, a dexterous manipulation task that relies\non fine-grain visuo-tactile serving. By incorporating tactile information into\nimitation learning frameworks, we are able to train a robotic agent to plug in\na USB cable - a first for imitation learning. Additionally, we explore how\ntactile information can be used to train non-tactile agents through a\ncontrastive-loss pretraining process. Our results show that by pretraining with\ntactile information, the performance of a non-tactile agent can be\nsignificantly improved, reaching a level on par with visuo-tactile agents.\n  For demonstration videos and access to our codebase, see the project website:\nhttps://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home\n","authors":["Abraham George","Selam Gano","Pranav Katragadda","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.11898v1.pdf","comment":"8 pages, 6 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2211.13024v3","updated":"2024-03-18T15:39:47Z","published":"2022-11-23T15:27:31Z","title":"Comparison of Motion Encoding Frameworks on Human Manipulation Actions","summary":"  Movement generation, and especially generalisation to unseen situations,\nplays an important role in robotics. Different types of movement generation\nmethods exist such as spline based methods, dynamical system based methods, and\nmethods based on Gaussian mixture models (GMMs). Using a large, new dataset on\nhuman manipulations, in this paper we provide a highly detailed comparison of\nfive fundamentally different and widely used movement encoding and generation\nframeworks: dynamic movement primitives (DMPs), time based Gaussian mixture\nregression (tbGMR), stable estimator of dynamical systems (SEDS), Probabilistic\nMovement Primitives (ProMP) and Optimal Control Primitives (OCP). We compare\nthese frameworks with respect to their movement encoding efficiency,\nreconstruction accuracy, and movement generalisation capabilities. The new\ndataset consists of nine object manipulation actions performed by 12 humans:\npick and place, put on top/take down, put inside/take out, hide/uncover, and\npush/pull with a total of 7,652 movement examples. Our analysis shows that for\nmovement encoding and reconstruction DMPs and OCPs are the most efficient with\nrespect to the number of parameters and reconstruction accuracy, if a\nsufficient number of kernels is used. In case of movement generalisation to new\nstart- and end-point situations, DMPs, OCPs and task parameterized GMM (TP-GMM,\nmovement generalisation framework based on tbGMR) lead to similar performance,\nwhich ProMPs only achieve when using many demonstrations for learning. All\nmodels outperform SEDS, which additionally proves to be difficult to fit.\nFurthermore we observe that TP-GMM and SEDS suffer from problems reaching the\nend-points of generalizations.These different quantitative results will help\nselecting the most appropriate models and designing trajectory representations\nin an improved task-dependent way in future robotic applications.\n","authors":["Lennart Jahn","Florentin Wörgötter","Tomas Kulvicius"],"pdf_url":"https://arxiv.org/pdf/2211.13024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11876v1","updated":"2024-03-18T15:28:35Z","published":"2024-03-18T15:28:35Z","title":"Deep Bayesian Future Fusion for Self-Supervised, High-Resolution,\n  Off-Road Mapping","summary":"  The limited sensing resolution of resource-constrained off-road vehicles\nposes significant challenges towards reliable off-road autonomy. To overcome\nthis limitation, we propose a general framework based on fusing the future\ninformation (i.e. future fusion) for self-supervision. Recent approaches\nexploit this future information alongside the hand-crafted heuristics to\ndirectly supervise the targeted downstream tasks (e.g. traversability\nestimation). However, in this paper, we opt for a more general line of\ndevelopment - time-efficient completion of the highest resolution (i.e. 2cm per\npixel) BEV map in a self-supervised manner via future fusion, which can be used\nfor any downstream tasks for better longer range prediction. To this end,\nfirst, we create a high-resolution future-fusion dataset containing pairs of\n(RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to\naccommodate the noise and sparsity of the sensory information, especially in\nthe distal regions, we design an efficient realization of the Bayes filter onto\nthe vanilla convolutional network via the recurrent mechanism. Equipped with\nthe ideas from SOTA generative models, our Bayesian structure effectively\npredicts high-quality BEV maps in the distal regions. Extensive evaluation on\nboth the quality of completion and downstream task on our future-fusion dataset\ndemonstrates the potential of our approach.\n","authors":["Shubhra Aich","Wenshan Wang","Parv Maheshwari","Matthew Sivaprakasam","Samuel Triest","Cherie Ho","Jason M. Gregory","John G. Rogers III","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2403.11876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11863v1","updated":"2024-03-18T15:17:15Z","published":"2024-03-18T15:17:15Z","title":"Context-aware LLM-based Safe Control Against Latent Risks","summary":"  It is challenging for autonomous control systems to perform complex tasks in\nthe presence of latent risks. Motivated by this challenge, this paper proposes\nan integrated framework that involves Large Language Models (LLMs), stochastic\ngradient descent (SGD), and optimization-based control. In the first phrase,\nthe proposed framework breaks down complex tasks into a sequence of smaller\nsubtasks, whose specifications account for contextual information and latent\nrisks. In the second phase, these subtasks and their parameters are refined\nthrough a dual process involving LLMs and SGD. LLMs are used to generate rough\nguesses and failure explanations, and SGD is used to fine-tune parameters. The\nproposed framework is tested using simulated case studies of robots and\nvehicles. The experiments demonstrate that the proposed framework can mediate\nactions based on the context and latent risks and learn complex behaviors\nefficiently.\n","authors":["Quan Khanh Luu","Xiyu Deng","Anh Van Ho","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13139v2","updated":"2024-03-18T14:57:03Z","published":"2023-09-22T18:48:54Z","title":"Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of\n  Vision Algorithms","summary":"  Visual Odometry (VO) is one of the fundamental tasks in computer vision for\nrobotics. However, its performance is deeply affected by High Dynamic Range\n(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches\nto mitigate this have appeared, their comparison in a reproducible manner is\nproblematic. This stems from the fact that the behavior of AE depends on the\nenvironment, and it affects the image acquisition process. Consequently, AE has\ntraditionally only been benchmarked in an online manner, making the experiments\nnon-reproducible. To solve this, we propose a new methodology based on an\nemulator that can generate images at any exposure time. It leverages BorealHDR,\na unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories\nwith challenging illumination conditions. Moreover, it includes\nlidar-inertial-based global maps with pose estimation for each image frame as\nwell as Global Navigation Satellite System (GNSS) data, for comparison. We show\nthat using these images acquired at different exposure times, we can emulate\nrealistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared\nto ground truth images. To demonstrate the practicality of our approach for\noffline benchmarking, we compared three state-of-the-art AE algorithms on key\nelements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,\nagainst four baselines. Consequently, reproducible evaluation of AE is now\npossible, speeding up the development of future approaches. Our code and\ndataset are available online at this link:\nhttps://github.com/norlab-ulaval/BorealHDR\n","authors":["Olivier Gamache","Jean-Michel Fortin","Matěj Boxan","Maxime Vaidis","François Pomerleau","Philippe Giguère"],"pdf_url":"https://arxiv.org/pdf/2309.13139v2.pdf","comment":"6 pages, 6 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.11796v1","updated":"2024-03-18T13:53:48Z","published":"2024-03-18T13:53:48Z","title":"OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy\n  Representation","summary":"  3D reconstruction has been widely used in autonomous navigation fields of\nmobile robotics. However, the former research can only provide the basic\ngeometry structure without the capability of open-world scene understanding,\nlimiting advanced tasks like human interaction and visual navigation. Moreover,\ntraditional 3D scene understanding approaches rely on expensive labeled 3D\ndatasets to train a model for a single task with supervision. Thus, geometric\nreconstruction with zero-shot scene understanding i.e. Open vocabulary 3D\nUnderstanding and Reconstruction, is crucial for the future development of\nmobile robots. In this paper, we propose OpenOcc, a novel framework unifying\nthe 3D scene reconstruction and open vocabulary understanding with neural\nradiance fields. We model the geometric structure of the scene with occupancy\nrepresentation and distill the pre-trained open vocabulary model into a 3D\nlanguage field via volume rendering for zero-shot inference. Furthermore, a\nnovel semantic-aware confidence propagation (SCP) method has been proposed to\nrelieve the issue of language field representation degeneracy caused by\ninconsistent measurements in distilled features. Experimental results show that\nour approach achieves competitive performance in 3D scene understanding tasks,\nespecially for small and long-tail objects.\n","authors":["Haochen Jiang","Yueming Xu","Yihan Zeng","Hang Xu","Wei Zhang","Jianfeng Feng","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11788v1","updated":"2024-03-18T13:46:32Z","published":"2024-03-18T13:46:32Z","title":"Locomotion Generation for a Rat Robot based on Environmental Changes via\n  Reinforcement Learning","summary":"  This research focuses on developing reinforcement learning approaches for the\nlocomotion generation of small-size quadruped robots. The rat robot NeRmo is\nemployed as the experimental platform. Due to the constrained volume,\nsmall-size quadruped robots typically possess fewer and weaker sensors,\nresulting in difficulty in accurately perceiving and responding to\nenvironmental changes. In this context, insufficient and imprecise feedback\ndata from sensors makes it difficult to generate adaptive locomotion based on\nreinforcement learning. To overcome these challenges, this paper proposes a\nnovel reinforcement learning approach that focuses on extracting effective\nperceptual information to enhance the environmental adaptability of small-size\nquadruped robots. According to the frequency of a robot's gait stride, key\ninformation of sensor data is analyzed utilizing sinusoidal functions derived\nfrom Fourier transform results. Additionally, a multifunctional reward\nmechanism is proposed to generate adaptive locomotion in different tasks.\nExtensive simulations are conducted to assess the effectiveness of the proposed\nreinforcement learning approach in generating rat robot locomotion in various\nenvironments. The experiment results illustrate the capability of the proposed\napproach to maintain stable locomotion of a rat robot across different\nterrains, including ramps, stairs, and spiral stairs.\n","authors":["Xinhui Shan","Yuhong Huang","Zhenshan Bing","Zitao Zhang","Xiangtong Yao","Kai Huang","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.11788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02599v2","updated":"2024-03-18T13:44:22Z","published":"2023-12-05T09:18:12Z","title":"MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor\n  Positioning","summary":"  A Magnetic field Aided Inertial Navigation System (MAINS) for indoor\nnavigation is proposed in this paper. MAINS leverages an array of magnetometers\nto measure spatial variations in the magnetic field, which are then used to\nestimate the displacement and orientation changes of the system, thereby aiding\nthe inertial navigation system (INS). Experiments show that MAINS significantly\noutperforms the stand-alone INS, demonstrating a remarkable two orders of\nmagnitude reduction in position error. Furthermore, when compared to the\nstate-of-the-art magnetic-field-aided navigation approach, the proposed method\nexhibits slightly improved horizontal position accuracy. On the other hand, it\nhas noticeably larger vertical error on datasets with large magnetic field\nvariations. However, one of the main advantages of MAINS compared to the\nstate-of-the-art is that it enables flexible sensor configurations. The\nexperimental results show that the position error after 2 minutes of navigation\nin most cases is less than 3 meters when using an array of 30 magnetometers.\nThus, the proposed navigation solution has the potential to solve one of the\nkey challenges faced with current magnetic-field simultaneous localization and\nmapping (SLAM) solutions: the very limited allowable length of the exploration\nphase during which unvisited areas are mapped.\n","authors":["Chuan Huang","Gustaf Hendeby","Hassen Fourati","Christophe Prieur","Isaac Skog"],"pdf_url":"https://arxiv.org/pdf/2312.02599v2.pdf","comment":"Accepted to IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2403.11784v1","updated":"2024-03-18T13:42:06Z","published":"2024-03-18T13:42:06Z","title":"ForzaETH Race Stack -- Scaled Autonomous Head-to-Head Racing on Fully\n  Commercial off-the-Shelf Hardware","summary":"  Autonomous racing in robotics combines high-speed dynamics with the necessity\nfor reliability and real-time decision-making. While such racing pushes\nsoftware and hardware to their limits, many existing full-system solutions\nnecessitate complex, custom hardware and software, and usually focus on\nTime-Trials rather than full unrestricted Head-to-Head racing, due to financial\nand safety constraints. This limits their reproducibility, making advancements\nand replication feasible mostly for well-resourced laboratories with\ncomprehensive expertise in mechanical, electrical, and robotics fields.\nResearchers interested in the autonomy domain but with only partial experience\nin one of these fields, need to spend significant time with familiarization and\nintegration. The ForzaETH Race Stack addresses this gap by providing an\nautonomous racing software platform designed for F1TENTH, a 1:10 scaled\nHead-to-Head autonomous racing competition, which simplifies replication by\nusing commercial off-the-shelf hardware. This approach enhances the competitive\naspect of autonomous racing and provides an accessible platform for research\nand development in the field. The ForzaETH Race Stack is designed with\nmodularity and operational ease of use in mind, allowing customization and\nadaptability to various environmental conditions, such as track friction and\nlayout. Capable of handling both Time-Trials and Head-to-Head racing, the stack\nhas demonstrated its effectiveness, robustness, and adaptability in the field\nby winning the official F1TENTH international competition multiple times.\n","authors":["Nicolas Baumann","Edoardo Ghignone","Jonas Kühne","Niklas Bastuck","Jonathan Becker","Nadine Imholz","Tobias Kränzlin","Tian Yi Lim","Michael Lötscher","Luca Schwarzenbach","Luca Tognoni","Christian Vogt","Andrea Carron","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.11784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13694v2","updated":"2024-03-18T13:38:35Z","published":"2023-10-20T17:57:30Z","title":"Studying speed-accuracy trade-offs in best-of-n collective\n  decision-making through heterogeneous mean-field modeling","summary":"  To succeed in their objectives, groups of individuals must be able to make\nquick and accurate collective decisions on the best option among a set of\nalternatives with different qualities. Group-living animals aim to do that all\nthe time. Plants and fungi are thought to do so too. Swarms of autonomous\nrobots can also be programmed to make best-of-n decisions for solving tasks\ncollaboratively. Ultimately, humans critically need it and so many times they\nshould be better at it. Thanks to their mathematical tractability, simple\nmodels like the voter model and the local majority rule model have proven\nuseful to describe the dynamics of such collective decision-making processes.\nTo reach a consensus, individuals change their opinion by interacting with\nneighbors in their social network. At least among animals and robots, options\nwith a better quality are exchanged more often and therefore spread faster than\nlower-quality options, leading to the collective selection of the best option.\nWith our work, we study the impact of individuals making errors in pooling\nothers' opinions caused, for example, by the need to reduce the cognitive load.\nOur analysis is grounded on the introduction of a model that generalizes the\ntwo existing models (local majority rule and voter model), showing a\nspeed-accuracy trade-off regulated by the cognitive effort of individuals. We\nalso investigate the impact of the interaction network topology on the\ncollective dynamics. To do so, we extend our model and, by using the\nheterogeneous mean-field approach, we show the presence of another\nspeed-accuracy trade-off regulated by network connectivity. An interesting\nresult is that reduced network connectivity corresponds to an increase in\ncollective decision accuracy.\n","authors":["Andreagiovanni Reina","Thierry Njougouo","Elio Tuci","Timoteo Carletti"],"pdf_url":"https://arxiv.org/pdf/2310.13694v2.pdf","comment":"29 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.11761v1","updated":"2024-03-18T13:14:46Z","published":"2024-03-18T13:14:46Z","title":"BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation","summary":"  Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a\ncrucial role in facilitating planning and decision-making for mobile robots.\nAlthough recent vision-only methods have demonstrated notable advancements in\nperformance, they often struggle under adverse illumination conditions such as\nrain or nighttime. While active sensors offer a solution to this challenge, the\nprohibitively high cost of LiDARs remains a limiting factor. Fusing camera data\nwith automotive radars poses a more inexpensive alternative but has received\nless attention in prior research. In this work, we aim to advance this\npromising avenue by introducing BEVCar, a novel approach for joint BEV object\nand map segmentation. The core novelty of our approach lies in first learning a\npoint-based encoding of raw radar data, which is then leveraged to efficiently\ninitialize the lifting of image features into the BEV space. We perform\nextensive experiments on the nuScenes dataset and demonstrate that BEVCar\noutperforms the current state of the art. Moreover, we show that incorporating\nradar information significantly enhances robustness in challenging\nenvironmental conditions and improves segmentation performance for distant\nobjects. To foster future research, we provide the weather split of the\nnuScenes dataset used in our experiments, along with our code and trained\nmodels at http://bevcar.cs.uni-freiburg.de.\n","authors":["Jonas Schramm","Niclas Vödisch","Kürsat Petek","B Ravi Kiran","Senthil Yogamani","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.11761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11742v1","updated":"2024-03-18T12:54:33Z","published":"2024-03-18T12:54:33Z","title":"Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization","summary":"  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own \\gls{ocp}, with ADMM\nfacilitating consistency between their optimizations. This approach not only\ndecreases the computational time but also allows for effective scaling with\nmore complex robot configurations, facilitating the integration of additional\nsubsystems such as articulated arms on a quadruped robot. We demonstrate,\nthrough numerical evaluations, the convergence of our approach on two systems\nwith increasing complexity. In addition, we showcase that our approach\nconverges towards the same solution when compared to a state-of-the-art\ncentralized whole-body MPC implementation. Moreover, we quantitatively compare\nthe computational efficiency of our method to the centralized approach,\nrevealing up to a 75\\% reduction in computational time. Overall, our approach\noffers a promising avenue for accelerating MPC solutions for legged robots,\npaving the way for more effective utilization of the computational performance\nof modern hardware.\n","authors":["Lorenzo Amatucci","Giulio Turrisi","Angelo Bratta","Victor Barasuol","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07658v2","updated":"2024-03-18T12:47:27Z","published":"2024-01-15T13:00:35Z","title":"Robustness Evaluation of Localization Techniques for Autonomous Racing","summary":"  This work introduces SynPF, an MCL-based algorithm tailored for high-speed\nracing environments. Benchmarked against Cartographer, a state-of-the-art\npose-graph SLAM algorithm, SynPF leverages synergies from previous\nparticle-filtering methods and synthesizes them for the high-performance racing\ndomain. Our extensive in-field evaluations reveal that while Cartographer\nexcels under nominal conditions, it struggles when subjected to wheel-slip, a\ncommon phenomenon in a racing scenario due to varying grip levels and\naggressive driving behaviour. Conversely, SynPF demonstrates robustness in\nthese challenging conditions and a low-latency computation time of 1.25 ms on\non-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled\nautonomous racing vehicle, this work not only highlights the vulnerabilities of\nexisting algorithms in high-speed scenarios, tested up until 7.6 m/s, but also\nemphasizes the potential of SynPF as a viable alternative, especially in\ndeteriorating odometry conditions.\n","authors":["Tian Yi Lim","Edoardo Ghignone","Nicolas Baumann","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2401.07658v2.pdf","comment":"Accepted at the Design, Automation and Test in Europe Conference 2024\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2403.11737v1","updated":"2024-03-18T12:44:54Z","published":"2024-03-18T12:44:54Z","title":"SMT-Based Dynamic Multi-Robot Task Allocation","summary":"  Multi-Robot Task Allocation (MRTA) is a problem that arises in many\napplication domains including package delivery, warehouse robotics, and\nhealthcare. In this work, we consider the problem of MRTA for a dynamic stream\nof tasks with task deadlines and capacitated agents (capacity for more than one\nsimultaneous task). Previous work commonly focuses on the static case, uses\nspecialized algorithms for restrictive task specifications, or lacks\nguarantees. We propose an approach to Dynamic MRTA for capacitated robots that\nis based on Satisfiability Modulo Theories (SMT) solving and addresses these\nconcerns. We show our approach is both sound and complete, and that the SMT\nencoding is general, enabling extension to a broader class of task\nspecifications. We show how to leverage the incremental solving capabilities of\nSMT solvers, keeping learned information when allocating new tasks arriving\nonline, and to solve non-incrementally, which we provide runtime comparisons\nof. Additionally, we provide an algorithm to start with a smaller but\npotentially incomplete encoding that can iteratively be adjusted to the\ncomplete encoding. We evaluate our method on a parameterized set of benchmarks\nencoding multi-robot delivery created from a graph abstraction of a\nhospital-like environment. The effectiveness of our approach is demonstrated\nusing a range of encodings, including quantifier-free theories of uninterpreted\nfunctions and linear or bitvector arithmetic across multiple solvers.\n","authors":["Victoria Marie Tuck","Pei-Wei Chen","Georgios Fainekos","Bardh Hoxha","Hideki Okamoto","S. Shankar Sastry","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2403.11737v1.pdf","comment":"26 pages, 6 figures, to be published in NASA Formal Methods Symposium\n  2024"},{"id":"http://arxiv.org/abs/2403.11729v1","updated":"2024-03-18T12:38:30Z","published":"2024-03-18T12:38:30Z","title":"Hardware Design and Learning-Based Software Architecture of\n  Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications","summary":"  Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness.\n","authors":["Kento Kawaharazuka","Akihiro Miki","Masahiro Bando","Temma Suzuki","Yoshimoto Ribayashi","Yasunori Toshimitsu","Yuya Nagamatsu","Kei Okada","and Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11729v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2403.11728v1","updated":"2024-03-18T12:37:41Z","published":"2024-03-18T12:37:41Z","title":"PITA: Physics-Informed Trajectory Autoencoder","summary":"  Validating robotic systems in safety-critical appli-cations requires testing\nin many scenarios including rare edgecases that are unlikely to occur,\nrequiring to complement real-world testing with testing in simulation.\nGenerative models canbe used to augment real-world datasets with generated data\ntoproduce edge case scenarios by sampling in a learned latentspace.\nAutoencoders can learn said latent representation for aspecific domain by\nlearning to reconstruct the input data froma lower-dimensional intermediate\nrepresentation. However, theresulting trajectories are not necessarily\nphysically plausible, butinstead typically contain noise that is not present in\nthe inputtrajectory. To resolve this issue, we propose the novel\nPhysics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates\na physical dynamics model into the loss functionof the autoencoder. This\nresults in smooth trajectories that notonly reconstruct the input trajectory\nbut also adhere to thephysical model. We evaluate PITA on a real-world dataset\nofvehicle trajectories and compare its performance to a normalautoencoder and a\nstate-of-the-art action-space autoencoder.\n","authors":["Johannes Fischer","Kevin Rösch","Martin Lauer","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2403.11728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05716v4","updated":"2024-03-18T11:57:09Z","published":"2023-06-09T07:22:12Z","title":"Transferring Foundation Models for Generalizable Robotic Manipulation","summary":"  Improving the generalization capabilities of general-purpose robotic\nmanipulation agents in the real world has long been a significant challenge.\nExisting approaches often rely on collecting large-scale robotic data which is\ncostly and time-consuming, such as the RT-1 dataset. However, due to\ninsufficient diversity of data, these approaches typically suffer from limiting\ntheir capability in open-domain scenarios with new objects and diverse\nenvironments. In this paper, we propose a novel paradigm that effectively\nleverages language-reasoning segmentation mask generated by internet-scale\nfoundation models, to condition robot manipulation tasks. By integrating the\nmask modality, which incorporates semantic, geometric, and temporal correlation\npriors derived from vision foundation models, into the end-to-end policy model,\nour approach can effectively and robustly perceive object pose and enable\nsample-efficient generalization learning, including new object instances,\nsemantic categories, and unseen backgrounds. We first introduce a series of\nfoundation models to ground natural language demands across multiple tasks.\nSecondly, we develop a two-stream 2D policy model based on imitation learning,\nwhich processes raw images and object masks to predict robot actions with a\nlocal-global perception manner. Extensive realworld experiments conducted on a\nFranka Emika robot arm demonstrate the effectiveness of our proposed paradigm\nand policy architecture. Demos can be found in our submitted video, and more\ncomprehensive ones can be found in link1 or link2.\n","authors":["Jiange Yang","Wenhui Tan","Chuhao Jin","Keling Yao","Bei Liu","Jianlong Fu","Ruihua Song","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2306.05716v4.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11577v1","updated":"2024-03-18T08:53:03Z","published":"2024-03-18T08:53:03Z","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal\n  Calibration","summary":"  Reliable multimodal sensor fusion algorithms require accurate spatiotemporal\ncalibration. Recently, targetless calibration techniques based on implicit\nneural representations have proven to provide precise and robust results.\nNevertheless, such methods are inherently slow to train given the high\ncomputational overhead caused by the large number of sampled points required\nfor volume rendering. With the recent introduction of 3D Gaussian Splatting as\na faster alternative to implicit representation methods, we propose to leverage\nthis new rendering approach to achieve faster multi-sensor calibration. We\nintroduce 3DGS-Calib, a new calibration method that relies on the speed and\nrendering accuracy of 3D Gaussian Splatting to achieve multimodal\nspatiotemporal calibration that is accurate, robust, and with a substantial\nspeed-up compared to methods relying on implicit neural representations. We\ndemonstrate the superiority of our proposal with experimental results on\nsequences from KITTI-360, a widely used driving dataset.\n","authors":["Quentin Herau","Moussab Bennehar","Arthur Moreau","Nathan Piasco","Luis Roldao","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2403.11577v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11552v1","updated":"2024-03-18T08:03:47Z","published":"2024-03-18T08:03:47Z","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v1.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.11708v1","updated":"2024-03-18T12:12:45Z","published":"2024-03-18T12:12:45Z","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n","authors":["Kaijie Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11708v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09401v2","updated":"2024-03-18T12:08:01Z","published":"2024-03-14T13:52:03Z","title":"Unsupervised Modality-Transferable Video Highlight Detection with\n  Representation Activation Sequence Learning","summary":"  Identifying highlight moments of raw video materials is crucial for improving\nthe efficiency of editing videos that are pervasive on internet platforms.\nHowever, the extensive work of manually labeling footage has created obstacles\nto applying supervised methods to videos of unseen categories. The absence of\nan audio modality that contains valuable cues for highlight detection in many\nvideos also makes it difficult to use multimodal strategies. In this paper, we\npropose a novel model with cross-modal perception for unsupervised highlight\ndetection. The proposed model learns representations with visual-audio level\nsemantics from image-audio pair data via a self-reconstruction task. To achieve\nunsupervised highlight detection, we investigate the latent representations of\nthe network and propose the representation activation sequence learning (RASL)\nmodule with k-point contrastive learning to learn significant representation\nactivations. To connect the visual modality with the audio modality, we use the\nsymmetric contrastive learning (SCL) module to learn the paired visual and\naudio representations. Furthermore, an auxiliary task of masked feature vector\nsequence (FVS) reconstruction is simultaneously conducted during pretraining\nfor representation enhancement. During inference, the cross-modal pretrained\nmodel can generate representations with paired visual-audio semantics given\nonly the visual modality. The RASL module is used to output the highlight\nscores. The experimental results show that the proposed framework achieves\nsuperior performance compared to other state-of-the-art approaches.\n","authors":["Tingtian Li","Zixun Sun","Xinyu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09401v2.pdf","comment":"Accepted by IEEE Transactions on Image Processing, 2024"},{"id":"http://arxiv.org/abs/2403.11703v1","updated":"2024-03-18T12:04:11Z","published":"2024-03-18T12:04:11Z","title":"LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images","summary":"  Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.\n","authors":["Ruyi Xu","Yuan Yao","Zonghao Guo","Junbo Cui","Zanlin Ni","Chunjiang Ge","Tat-Seng Chua","Zhiyuan Liu","Maosong Sun","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11703v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.11699v1","updated":"2024-03-18T11:56:32Z","published":"2024-03-18T11:56:32Z","title":"A Spatial-Temporal Progressive Fusion Network for Breast Lesion\n  Segmentation in Ultrasound Videos","summary":"  Ultrasound video-based breast lesion segmentation provides a valuable\nassistance in early breast lesion detection and treatment. However, existing\nworks mainly focus on lesion segmentation based on ultrasound breast images\nwhich usually can not be adapted well to obtain desirable results on ultrasound\nvideos. The main challenge for ultrasound video-based breast lesion\nsegmentation is how to exploit the lesion cues of both intra-frame and\ninter-frame simultaneously. To address this problem, we propose a novel\nSpatial-Temporal Progressive Fusion Network (STPFNet) for video based breast\nlesion segmentation problem. The main aspects of the proposed STPFNet are\nthreefold. First, we propose to adopt a unified network architecture to capture\nboth spatial dependences within each ultrasound frame and temporal correlations\nbetween different frames together for ultrasound data representation. Second,\nwe propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to\nfuse spatial and temporal cues together for lesion detection. MSFF can help to\ndetermine the boundary contour of lesion region to overcome the issue of lesion\nboundary blurring. Third, we propose to exploit the segmentation result of\nprevious frame as the prior knowledge to suppress the noisy background and\nlearn more robust representation. In particular, we introduce a new publicly\navailable ultrasound video breast lesion segmentation dataset, termed UVBLS200,\nwhich is specifically dedicated to breast lesion segmentation. It contains 200\nvideos, including 80 videos of benign lesions and 120 videos of malignant\nlesions. Experiments on the proposed dataset demonstrate that the proposed\nSTPFNet achieves better breast lesion detection performance than\nstate-of-the-art methods.\n","authors":["Zhengzheng Tu","Zigang Zhu","Yayang Duan","Bo Jiang","Qishun Wang","Chaoxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11697v1","updated":"2024-03-18T11:54:35Z","published":"2024-03-18T11:54:35Z","title":"Urban Scene Diffusion through Semantic Occupancy Map","summary":"  Generating unbounded 3D scenes is crucial for large-scale scene understanding\nand simulation. Urban scenes, unlike natural landscapes, consist of various\ncomplex man-made objects and structures such as roads, traffic signs, vehicles,\nand buildings. To create a realistic and detailed urban scene, it is crucial to\naccurately represent the geometry and semantics of the underlying objects,\ngoing beyond their visual appearance. In this work, we propose UrbanDiffusion,\na 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and\ngenerates an urban scene with geometry and semantics in the form of semantic\noccupancy map. Our model introduces a novel paradigm that learns the data\ndistribution of scene-level structures within a latent space and further\nenables the expansion of the synthesized scene into an arbitrary scale. After\ntraining on real-world driving datasets, our model can generate a wide range of\ndiverse urban scenes given the BEV maps from the held-out set and also\ngeneralize to the synthesized maps from a driving simulator. We further\ndemonstrate its application to scene image synthesis with a pretrained image\ngenerator as a prior.\n","authors":["Junge Zhang","Qihang Zhang","Li Zhang","Ramana Rao Kompella","Gaowen Liu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.11697v1.pdf","comment":"The project website is https://metadriverse.github.io/urbandiff/"},{"id":"http://arxiv.org/abs/2403.11695v1","updated":"2024-03-18T11:48:41Z","published":"2024-03-18T11:48:41Z","title":"TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction","summary":"  Autonomous driving systems are a rapidly evolving technology that enables\ndriverless car production. Trajectory prediction is a critical component of\nautonomous driving systems, enabling cars to anticipate the movements of\nsurrounding objects for safe navigation. Trajectory prediction using Lidar\npoint-cloud data performs better than 2D images due to providing 3D\ninformation. However, processing point-cloud data is more complicated and\ntime-consuming than 2D images. Hence, state-of-the-art 3D trajectory\npredictions using point-cloud data suffer from slow and erroneous predictions.\nThis paper introduces TrajectoryNAS, a pioneering method that focuses on\nutilizing point cloud data for trajectory prediction. By leveraging Neural\nArchitecture Search (NAS), TrajectoryNAS automates the design of trajectory\nprediction models, encompassing object detection, tracking, and forecasting in\na cohesive manner. This approach not only addresses the complex\ninterdependencies among these tasks but also emphasizes the importance of\naccuracy and efficiency in trajectory modeling. Through empirical studies,\nTrajectoryNAS demonstrates its effectiveness in enhancing the performance of\nautonomous driving systems, marking a significant advancement in the\nfield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8\nhigger accuracy and 1.1* lower latency over competing methods on the NuScenes\ndataset.\n","authors":["Ali Asghar Sharifi","Ali Zoljodi","Masoud Daneshtalab"],"pdf_url":"https://arxiv.org/pdf/2403.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11694v1","updated":"2024-03-18T11:48:20Z","published":"2024-03-18T11:48:20Z","title":"Object Segmentation-Assisted Inter Prediction for Versatile Video Coding","summary":"  In modern video coding standards, block-based inter prediction is widely\nadopted, which brings high compression efficiency. However, in natural videos,\nthere are usually multiple moving objects of arbitrary shapes, resulting in\ncomplex motion fields that are difficult to compactly represent. This problem\nhas been tackled by more flexible block partitioning methods in the Versatile\nVideo Coding (VVC) standard, but the more flexible partitions require more\noverhead bits to signal and still cannot be made arbitrary shaped. To address\nthis limitation, we propose an object segmentation-assisted inter prediction\nmethod (SAIP), where objects in the reference frames are segmented by some\nadvanced technologies. With a proper indication, the object segmentation mask\nis translated from the reference frame to the current frame as the\narbitrary-shaped partition of different regions without any extra signal. Using\nthe segmentation mask, motion compensation is separately performed for\ndifferent regions, achieving higher prediction accuracy. The segmentation mask\nis further used to code the motion vectors of different regions more\nefficiently. Moreover, segmentation mask is considered in the joint\nrate-distortion optimization for motion estimation and partition estimation to\nderive the motion vector of different regions and partition more accurately.\nThe proposed method is implemented into the VVC reference software, VTM version\n12.0. Experimental results show that the proposed method achieves up to 1.98%,\n1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common\ntest sequences, under the Low-delay P, Low-delay B, and Random Access\nconfigurations, respectively.\n","authors":["Zhuoyuan Li","Zikun Yuan","Li Li","Dong Liu","Xiaohu Tang","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11694v1.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.11691v1","updated":"2024-03-18T11:41:55Z","published":"2024-03-18T11:41:55Z","title":"TTT-KD: Test-Time Training for 3D Semantic Segmentation through\n  Knowledge Distillation from Foundation Models","summary":"  Test-Time Training (TTT) proposes to adapt a pre-trained network to changing\ndata distributions on-the-fly. In this work, we propose the first TTT method\nfor 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD)\nfrom foundation models (e.g. DINOv2) as a self-supervised objective for\nadaptation to distribution shifts at test-time. Given access to paired\nimage-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for\nthe main task of semantic segmentation using the pointclouds and the task of 2D\n$\\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model. At\ntest-time, our TTT-KD updates the 3D segmentation backbone for each test\nsample, by using the self-supervised task of knowledge distillation, before\nperforming the final prediction. Extensive evaluations on multiple indoor and\noutdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves\nperformance for both in-distribution (ID) and out-of-distribution (ODO) test\ndatasets. We achieve a gain of up to 13% mIoU (7% on average) when the train\nand test distributions are similar and up to 45% (20% on average) when adapting\nto OOD test samples.\n","authors":["Lisa Weijler","Muhammad Jehanzeb Mirza","Leon Sick","Can Ekkazan","Pedro Hermosilla"],"pdf_url":"https://arxiv.org/pdf/2403.11691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11689v1","updated":"2024-03-18T11:38:47Z","published":"2024-03-18T11:38:47Z","title":"MoreStyle: Relax Low-frequency Constraint of Fourier-based Image\n  Reconstruction in Generalizable Medical Image Segmentation","summary":"  The task of single-source domain generalization (SDG) in medical image\nsegmentation is crucial due to frequent domain shifts in clinical image\ndatasets. To address the challenge of poor generalization across different\ndomains, we introduce a Plug-and-Play module for data augmentation called\nMoreStyle. MoreStyle diversifies image styles by relaxing low-frequency\nconstraints in Fourier space, guiding the image reconstruction network. With\nthe help of adversarial learning, MoreStyle further expands the style range and\npinpoints the most intricate style combinations within latent features. To\nhandle significant style variations, we introduce an uncertainty-weighted loss.\nThis loss emphasizes hard-to-classify pixels resulting only from style shifts\nwhile mitigating true hard-to-classify pixels in both MoreStyle-generated and\noriginal images. Extensive experiments on two widely used benchmarks\ndemonstrate that the proposed MoreStyle effectively helps to achieve good\ndomain generalization ability, and has the potential to further boost the\nperformance of some state-of-the-art SDG methods.\n","authors":["Haoyu Zhao","Wenhui Dong","Rui Yu","Zhou Zhao","Du Bo","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.11689v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.11681v1","updated":"2024-03-18T11:35:18Z","published":"2024-03-18T11:35:18Z","title":"MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion","summary":"  Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.\n","authors":["Guiyong Zheng","Jinqi Jiang","Chen Feng","Shaojie Shen","Boyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.11681v1.pdf","comment":"Submitted to IROS2024. Code: https://github.com/SYSU-STAR/MASSTAR.\n  Project Page: https://github.com/SYSU-STAR/MASSTAR"},{"id":"http://arxiv.org/abs/2403.11679v1","updated":"2024-03-18T11:31:03Z","published":"2024-03-18T11:31:03Z","title":"NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using\n  3D Gaussian Splatting","summary":"  We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D\nGaussian representation, that enables robust 3D semantic mapping, accurate\ncamera tracking, and high-quality rendering in real-time. In the system, we\npropose a Spatially Consistent Feature Fusion model to reduce the effect of\nerroneous estimates from pre-trained segmentation head on semantic\nreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we\nemploy a lightweight encoder-decoder to compress the high-dimensional semantic\nfeatures into a compact 3D Gaussian representation, mitigating the burden of\nexcessive memory consumption. Furthermore, we leverage the advantage of 3D\nGaussian splatting, which enables efficient and differentiable novel view\nrendering, and propose a Virtual Camera View Pruning method to eliminate\noutlier GS points, thereby effectively enhancing the quality of scene\nrepresentations. Our NEDS-SLAM method demonstrates competitive performance over\nexisting dense semantic SLAM methods in terms of mapping and tracking accuracy\non Replica and ScanNet datasets, while also showing excellent capabilities in\n3D dense semantic mapping.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie"],"pdf_url":"https://arxiv.org/pdf/2403.11679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11678v1","updated":"2024-03-18T11:29:43Z","published":"2024-03-18T11:29:43Z","title":"Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous\n  Scenes","summary":"  We present a method enabling the scaling of NeRFs to learn a large number of\nsemantically-similar scenes. We combine two techniques to improve the required\ntraining time and memory cost per scene. First, we learn a 3D-aware latent\nspace in which we train Tri-Plane scene representations, hence reducing the\nresolution at which scenes are learned. Moreover, we present a way to share\ncommon information across scenes, hence allowing for a reduction of model\ncomplexity to learn a particular scene. Our method reduces effective per-scene\nmemory costs by 44% and per-scene time costs by 86% when training 1000 scenes.\nOur project page can be found at https://3da-ae.github.io .\n","authors":["Antoine Schnepf","Karim Kassab","Jean-Yves Franceschi","Laurent Caraffa","Flavian Vasile","Jeremie Mary","Andrew Comport","Valérie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2403.11678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11675v1","updated":"2024-03-18T11:23:02Z","published":"2024-03-18T11:23:02Z","title":"Better (pseudo-)labels for semi-supervised instance segmentation","summary":"  Despite the availability of large datasets for tasks like image\nclassification and image-text alignment, labeled data for more complex\nrecognition tasks, such as detection and segmentation, is less abundant. In\nparticular, for instance segmentation annotations are time-consuming to\nproduce, and the distribution of instances is often highly skewed across\nclasses. While semi-supervised teacher-student distillation methods show\npromise in leveraging vast amounts of unlabeled data, they suffer from\nmiscalibration, resulting in overconfidence in frequently represented classes\nand underconfidence in rarer ones. Additionally, these methods encounter\ndifficulties in efficiently learning from a limited set of examples. We\nintroduce a dual-strategy to enhance the teacher model's training process,\nsubstantially improving the performance on few-shot learning. Secondly, we\npropose a calibration correction mechanism that that enables the student model\nto correct the teacher's calibration errors. Using our approach, we observed\nmarked improvements over a state-of-the-art supervised baseline performance on\nthe LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3%\ngain in AP for rare classes.\n","authors":["François Porcher","Camille Couprie","Marc Szafraniec","Jakob Verbeek"],"pdf_url":"https://arxiv.org/pdf/2403.11675v1.pdf","comment":"Appeared at the Practical ML for Low Resource Settings workshop at\n  ICLR 2024"},{"id":"http://arxiv.org/abs/2403.11674v1","updated":"2024-03-18T11:21:52Z","published":"2024-03-18T11:21:52Z","title":"Towards Generalizing to Unseen Domains with Few Labels","summary":"  We approach the challenge of addressing semi-supervised domain generalization\n(SSDG). Specifically, our aim is to obtain a model that learns\ndomain-generalizable features by leveraging a limited subset of labelled data\nalongside a substantially larger pool of unlabeled data. Existing domain\ngeneralization (DG) methods which are unable to exploit unlabeled data perform\npoorly compared to semi-supervised learning (SSL) methods under SSDG setting.\nNevertheless, SSL methods have considerable room for performance improvement\nwhen compared to fully-supervised DG training. To tackle this underexplored,\nyet highly practical problem of SSDG, we make the following core contributions.\nFirst, we propose a feature-based conformity technique that matches the\nposterior distributions from the feature space with the pseudo-label from the\nmodel's output space. Second, we develop a semantics alignment loss to learn\nsemantically-compatible representations by regularizing the semantic structure\nin the feature space. Our method is plug-and-play and can be readily integrated\nwith different SSL-based SSDG baselines without introducing any additional\nparameters. Extensive experimental results across five challenging DG\nbenchmarks with four strong SSL baselines suggest that our method provides\nconsistent and notable gains in two different SSDG settings.\n","authors":["Chamuditha Jayanga Galappaththige","Sanoojan Baliah","Malitha Gunawardhana","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2403.11674v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11672v1","updated":"2024-03-18T11:20:11Z","published":"2024-03-18T11:20:11Z","title":"WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT\n  Denoising","summary":"  In clinical examinations and diagnoses, low-dose computed tomography (LDCT)\nis crucial for minimizing health risks compared with normal-dose computed\ntomography (NDCT). However, reducing the radiation dose compromises the\nsignal-to-noise ratio, leading to degraded quality of CT images. To address\nthis, we analyze LDCT denoising task based on experimental results from the\nfrequency perspective, and then introduce a novel self-supervised CT image\ndenoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND\ncomprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware\nMulti-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by\nmainly adding noise to the high-frequency components, which is the main\ndifference between LDCT and NDCT. Second, to better capture high-frequency\ncomponents and detailed information, Frequency-Aware Multi-scale Loss (FAM) is\nproposed by effectively utilizing multi-scale feature space. Extensive\nexperiments on two public LDCT denoising datasets demonstrate that our\nWIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art\nweakly-supervised and self-supervised methods.\n","authors":["Haoyu Zhao","Guyu Liang","Zhou Zhao","Bo Du","Yongchao Xu","Rui Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11672v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11667v1","updated":"2024-03-18T11:15:03Z","published":"2024-03-18T11:15:03Z","title":"Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for\n  Unsupervised Anomaly Detection","summary":"  The high performance of denoising diffusion models for image generation has\npaved the way for their application in unsupervised medical anomaly detection.\nAs diffusion-based methods require a lot of GPU memory and have long sampling\ntimes, we present a novel and fast unsupervised anomaly detection approach\nbased on latent Bernoulli diffusion models. We first apply an autoencoder to\ncompress the input images into a binary latent representation. Next, a\ndiffusion model that follows a Bernoulli noise schedule is employed to this\nlatent space and trained to restore binary latent representations from\nperturbed ones. The binary nature of this diffusion model allows us to identify\nentries in the latent space that have a high probability of flipping their\nbinary code during the denoising process, which indicates out-of-distribution\ndata. We propose a masking algorithm based on these probabilities, which\nimproves the anomaly detection scores. We achieve state-of-the-art performance\ncompared to other diffusion-based unsupervised anomaly detection algorithms\nwhile significantly reducing sampling time and memory consumption. The code is\navailable at https://github.com/JuliaWolleb/Anomaly_berdiff.\n","authors":["Julia Wolleb","Florentin Bieder","Paul Friedrich","Peter Zhang","Alicia Durrer","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2403.11667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11665v1","updated":"2024-03-18T11:12:39Z","published":"2024-03-18T11:12:39Z","title":"Normalized Validity Scores for DNNs in Regression based Eye Feature\n  Extraction","summary":"  We propose an improvement to the landmark validity loss. Landmark detection\nis widely used in head pose estimation, eyelid shape extraction, as well as\npupil and iris segmentation. There are numerous additional applications where\nlandmark detection is used to estimate the shape of complex objects. One part\nof this process is the accurate and fine-grained detection of the shape. The\nother part is the validity or inaccuracy per landmark, which can be used to\ndetect unreliable areas, where the shape possibly does not fit, and to improve\nthe accuracy of the entire shape extraction by excluding inaccurate landmarks.\nWe propose a normalization in the loss formulation, which improves the accuracy\nof the entire approach due to the numerical balance of the normalized\ninaccuracy. In addition, we propose a margin for the inaccuracy to reduce the\nimpact of gradients, which are produced by negligible errors close to the\nground truth.\n","authors":["Wolfgang Fuhl"],"pdf_url":"https://arxiv.org/pdf/2403.11665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10040v2","updated":"2024-03-18T11:02:11Z","published":"2024-03-15T06:20:09Z","title":"Histo-Genomic Knowledge Distillation For Cancer Prognosis From\n  Histopathology Whole Slide Images","summary":"  Histo-genomic multi-modal methods have recently emerged as a powerful\nparadigm, demonstrating significant potential for improving cancer prognosis.\nHowever, genome sequencing, unlike histopathology imaging, is still not widely\naccessible in underdeveloped regions, limiting the application of these\nmulti-modal approaches in clinical settings. To address this, we propose a\nnovel Genome-informed Hyper-Attention Network, termed G-HANet, which is capable\nof effectively distilling the histo-genomic knowledge during training to\nelevate uni-modal whole slide image (WSI)-based inference for the first time.\nCompared with traditional knowledge distillation methods (i.e., teacher-student\narchitecture) in other tasks, our end-to-end model is superior in terms of\ntraining efficiency and learning cross-modal interactions. Specifically, the\nnetwork comprises the cross-modal associating branch (CAB) and hyper-attention\nsurvival branch (HSB). Through the genomic data reconstruction from WSIs, CAB\neffectively distills the associations between functional genotypes and\nmorphological phenotypes and offers insights into the gene expression profiles\nin the feature space. Subsequently, HSB leverages the distilled histo-genomic\nassociations as well as the generated morphology-based weights to achieve the\nhyper-attention modeling of the patients from both histopathology and genomic\nperspectives to improve cancer prognosis. Extensive experiments are conducted\non five TCGA benchmarking datasets and the results demonstrate that G-HANet\nsignificantly outperforms the state-of-the-art WSI-based methods and achieves\ncompetitive performance with genome-based and multi-modal methods. G-HANet is\nexpected to be explored as a useful tool by the research community to address\nthe current bottleneck of insufficient histo-genomic data pairing in the\ncontext of cancer prognosis and precision oncology.\n","authors":["Zhikang Wang","Yumeng Zhang","Yingxue Xu","Seiya Imoto","Hao Chen","Jiangning Song"],"pdf_url":"https://arxiv.org/pdf/2403.10040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14899v2","updated":"2024-03-18T10:55:36Z","published":"2024-02-22T17:36:34Z","title":"Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning\n  Meets Adversarial Images","summary":"  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand\nimages. However, like traditional vision models, they are still vulnerable to\nadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely\nexplored on MLLMs, which not only improves model's performance, but also\nenhances model's explainability by giving intermediate reasoning steps.\nNevertheless, there is still a lack of study regarding MLLMs' adversarial\nrobustness with CoT and an understanding of what the rationale looks like when\nMLLMs infer wrong answers with adversarial images. Our research evaluates the\nadversarial robustness of MLLMs when employing CoT reasoning, finding that CoT\nmarginally improves adversarial robustness against existing attack methods.\nMoreover, we introduce a novel stop-reasoning attack technique that effectively\nbypasses the CoT-induced robustness enhancements. Finally, we demonstrate the\nalterations in CoT reasoning when MLLMs confront adversarial images, shedding\nlight on their reasoning process under adversarial attacks.\n","authors":["Zefeng Wang","Zhen Han","Shuo Chen","Fan Xue","Zifeng Ding","Xun Xiao","Volker Tresp","Philip Torr","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2402.14899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13316v2","updated":"2024-03-18T10:54:03Z","published":"2023-12-20T11:00:54Z","title":"ECAMP: Entity-centered Context-aware Medical Vision Language\n  Pre-training","summary":"  Despite significant advancements in medical vision-language pre-training,\nexisting methods have largely overlooked the inherent entity-specific context\nwithin radiology reports and the complex cross-modality contextual\nrelationships between text and images. To close this gap, we propose a novel\nEntity-centered Context-aware Medical Vision-language Pre-training (ECAMP)\nframework, which is designed to enable a more entity-centered and\ncontext-sensitive interpretation of medical data. Utilizing the recent powerful\nlarge language model, we distill entity-centered context from medical reports,\nwhich enables ECAMP to gain more effective supervision from the text modality.\nBy further pre-training our model with carefully designed entity-aware,\ncontext-enhanced masked language modeling and context-guided super-resolution\ntasks, ECAMP significantly refines the interplay between text and image\nmodalities, leading to an enhanced ability to extract entity-centered\ncontextual features. Besides, our proposed multi-scale context fusion design\nalso improves the semantic integration of both coarse and fine-level image\nrepresentations, prompting better performance for multi-scale downstream\napplications. Combining these components leads to significant performance leaps\nover current state-of-the-art methods and establishes a new standard for\ncross-modality learning in medical imaging, whose effectiveness is demonstrated\nby our extensive experiments on various tasks including classification,\nsegmentation, and detection across several public datasets. Code and models are\navailable at https://github.com/ToniChopp/ECAMP.\n","authors":["Rongsheng Wang","Qingsong Yao","Haoran Lai","Zhiyang He","Xiaodong Tao","Zihang Jiang","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.13316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11656v1","updated":"2024-03-18T10:53:00Z","published":"2024-03-18T10:53:00Z","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model","summary":"  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n","authors":["Yuxin Cao","Jinghao Li","Xi Xiao","Derui Wang","Minhui Xue","Hao Ge","Wei Liu","Guangwu Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11656v1.pdf","comment":"Accepted to 2024 IEEE Security and Privacy Workshops (SPW)"},{"id":"http://arxiv.org/abs/2403.11650v1","updated":"2024-03-18T10:45:50Z","published":"2024-03-18T10:45:50Z","title":"Prioritized Semantic Learning for Zero-shot Instance Navigation","summary":"  We study zero-shot instance navigation, in which the agent navigates to a\nspecific object without using object annotations for training. Previous object\nnavigation approaches apply the image-goal navigation (ImageNav) task (go to\nthe location of an image) for pretraining, and transfer the agent to achieve\nobject goals using a vision-language model. However, these approaches lead to\nissues of semantic neglect, where the model fails to learn meaningful semantic\nalignments. In this paper, we propose a Prioritized Semantic Learning (PSL)\nmethod to improve the semantic understanding ability of navigation agents.\nSpecifically, a semantic-enhanced PSL agent is proposed and a prioritized\nsemantic training strategy is introduced to select goal images that exhibit\nclear semantic supervision and relax the reward function from strict exact view\nmatching. At inference time, a semantic expansion inference scheme is designed\nto preserve the same granularity level of the goal-semantic as training.\nFurthermore, for the popular HM3D environment, we present an Instance\nNavigation (InstanceNav) task that requires going to a specific object instance\nwith detailed descriptions, as opposed to the Object Navigation (ObjectNav)\ntask where the goal is defined merely by the object category. Our PSL agent\noutperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in\nterms of success rate and is also superior on the new InstanceNav task. Code\nwill be released at https://anonymous.4open. science/r/PSL/.\n","authors":["Xander Sun","Louis Lau","Hoyard Zhi","Ronghe Qiu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2403.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11649v1","updated":"2024-03-18T10:45:27Z","published":"2024-03-18T10:45:27Z","title":"Gridless 2D Recovery of Lines using the Sliding Frank-Wolfe Algorithm","summary":"  We present a new approach leveraging the Sliding Frank--Wolfe algorithm to\naddress the challenge of line recovery in degraded images. Building upon\nadvances in conditional gradient methods for sparse inverse problems with\ndifferentiable measurement models, we propose two distinct models tailored for\nline detection tasks within the realm of blurred line deconvolution and ridge\ndetection of linear chirps in spectrogram images.\n","authors":["Kévin Polisano","Basile Dubois-Bonnaire","Sylvain Meignen"],"pdf_url":"https://arxiv.org/pdf/2403.11649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11646v1","updated":"2024-03-18T10:42:24Z","published":"2024-03-18T10:42:24Z","title":"MedMerge: Merging Models for Effective Transfer Learning to Medical\n  Imaging Tasks","summary":"  Transfer learning has become a powerful tool to initialize deep learning\nmodels to achieve faster convergence and higher performance. This is especially\nuseful in the medical imaging analysis domain, where data scarcity limits\npossible performance gains for deep learning models. Some advancements have\nbeen made in boosting the transfer learning performance gain by merging models\nstarting from the same initialization. However, in the medical imaging analysis\ndomain, there is an opportunity in merging models starting from different\ninitialisations, thus combining the features learnt from different tasks. In\nthis work, we propose MedMerge, a method whereby the weights of different\nmodels can be merged, and their features can be effectively utilized to boost\nperformance on a new task. With MedMerge, we learn kernel-level weights that\ncan later be used to merge the models into a single model, even when starting\nfrom different initializations. Testing on various medical imaging analysis\ntasks, we show that our merged model can achieve significant performance gains,\nwith up to 3% improvement on the F1 score. The code implementation of this work\nwill be available at www.github.com/BioMedIA-MBZUAI/MedMerge.\n","authors":["Ibrahim Almakky","Santosh Sanjeev","Anees Ur Rehman Hashmi","Mohammad Areeb Qazi","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.11646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11643v1","updated":"2024-03-18T10:35:15Z","published":"2024-03-18T10:35:15Z","title":"Diffusion-Based Environment-Aware Trajectory Prediction","summary":"  The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.\n","authors":["Theodor Westny","Björn Olofsson","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.11643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11641v1","updated":"2024-03-18T10:32:51Z","published":"2024-03-18T10:32:51Z","title":"Arc2Face: A Foundation Model of Human Faces","summary":"  This paper presents Arc2Face, an identity-conditioned face foundation model,\nwhich, given the ArcFace embedding of a person, can generate diverse\nphoto-realistic images with an unparalleled degree of face similarity than\nexisting models. Despite previous attempts to decode face recognition features\ninto detailed images, we find that common high-resolution datasets (e.g. FFHQ)\nlack sufficient identities to reconstruct any subject. To that end, we\nmeticulously upsample a significant portion of the WebFace42M database, the\nlargest public dataset for face recognition (FR). Arc2Face builds upon a\npretrained Stable Diffusion model, yet adapts it to the task of ID-to-face\ngeneration, conditioned solely on ID vectors. Deviating from recent works that\ncombine ID with text embeddings for zero-shot personalization of text-to-image\nmodels, we emphasize on the compactness of FR features, which can fully capture\nthe essence of the human face, as opposed to hand-crafted prompts. Crucially,\ntext-augmented models struggle to decouple identity and text, usually\nnecessitating some description of the given face to achieve satisfactory\nsimilarity. Arc2Face, however, only needs the discriminative features of\nArcFace to guide the generation, offering a robust prior for a plethora of\ntasks where ID consistency is of paramount importance. As an example, we train\na FR model on synthetic images from our model and achieve superior performance\nto existing synthetic datasets.\n","authors":["Foivos Paraperas Papantoniou","Alexandros Lattas","Stylianos Moschoglou","Jiankang Deng","Bernhard Kainz","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2403.11641v1.pdf","comment":"29 pages, 20 figures. Project page: https://arc2face.github.io/"},{"id":"http://arxiv.org/abs/2403.11639v1","updated":"2024-03-18T10:21:05Z","published":"2024-03-18T10:21:05Z","title":"An Accurate and Real-time Relative Pose Estimation from Triple\n  Point-line Images by Decoupling Rotation and Translation","summary":"  Line features are valid complements for point features in man-made\nenvironments. 3D-2D constraints provided by line features have been widely used\nin Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how\nto accurately solve three-view relative motion only with 2D observations of\npoints and lines in real time has not been fully explored. In this paper, we\npropose a novel three-view pose solver based on rotation-translation decoupled\nestimation. First, a high-precision rotation estimation method based on normal\nvector coplanarity constraints that consider the uncertainty of observations is\nproposed, which can be solved by Levenberg-Marquardt (LM) algorithm\nefficiently. Second, a robust linear translation constraint that minimizes the\ndegree of the rotation components and feature observation components in\nequations is elaborately designed for estimating translations accurately.\nExperiments on synthetic data and real-world data show that the proposed\napproach improves both rotation and translation accuracy compared to the\nclassical trifocal-tensor-based method and the state-of-the-art two-view\nalgorithm in outdoor and indoor environments.\n","authors":["Zewen Xu","Yijia He","Hao Wei","Bo Xu","BinJian Xie","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11634v1","updated":"2024-03-18T10:13:53Z","published":"2024-03-18T10:13:53Z","title":"Personalized 3D Human Pose and Shape Refinement","summary":"  Recently, regression-based methods have dominated the field of 3D human pose\nand shape estimation. Despite their promising results, a common issue is the\nmisalignment between predictions and image observations, often caused by minor\njoint rotation errors that accumulate along the kinematic chain. To address\nthis issue, we propose to construct dense correspondences between initial human\nmodel estimates and the corresponding images that can be used to refine the\ninitial predictions. To this end, we utilize renderings of the 3D models to\npredict per-pixel 2D displacements between the synthetic renderings and the RGB\nimages. This allows us to effectively integrate and exploit appearance\ninformation of the persons. Our per-pixel displacements can be efficiently\ntransformed to per-visible-vertex displacements and then used for 3D model\nrefinement by minimizing a reprojection loss. To demonstrate the effectiveness\nof our approach, we refine the initial 3D human mesh predictions of multiple\nmodels using different refinement procedures on 3DPW and RICH. We show that our\napproach not only consistently leads to better image-model alignment, but also\nto improved 3D accuracy.\n","authors":["Tom Wehrbein","Bodo Rosenhahn","Iain Matthews","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2403.11634v1.pdf","comment":"Accepted to 2023 IEEE/CVF International Conference on Computer Vision\n  Workshops (ICCVW)"},{"id":"http://arxiv.org/abs/2403.11631v1","updated":"2024-03-18T10:09:28Z","published":"2024-03-18T10:09:28Z","title":"Compositional Kronecker Context Optimization for Vision-Language Models","summary":"  Context Optimization (CoOp) has emerged as a simple yet effective technique\nfor adapting CLIP-like vision-language models to downstream image recognition\ntasks. Nevertheless, learning compact context with satisfactory base-to-new,\ndomain and cross-task generalization ability while adapting to new tasks is\nstill a challenge. To tackle such a challenge, we propose a lightweight yet\ngeneralizable approach termed Compositional Kronecker Context Optimization\n(CK-CoOp). Technically, the prompt's context words in CK-CoOp are learnable\nvectors, which are crafted by linearly combining base vectors sourced from a\ndictionary. These base vectors consist of a non-learnable component obtained by\nquantizing the weights in the token embedding layer, and a learnable component\nconstructed by applying Kronecker product on several learnable tiny matrices.\nIntuitively, the compositional structure mitigates the risk of overfitting on\ntraining data by remembering more pre-trained knowledge. Meantime, the\nKronecker product breaks the non-learnable restrictions of the dictionary,\nthereby enhancing representation ability with minimal additional parameters.\nExtensive experiments confirm that CK-CoOp achieves state-of-the-art\nperformance under base-to-new, domain and cross-task generalization evaluation,\nbut also has the metrics of fewer learnable parameters and efficient training\nand inference speed.\n","authors":["Kun Ding","Xiaohui Li","Qiang Yu","Ying Wang","Haojian Zhang","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.11631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11627v1","updated":"2024-03-18T09:58:52Z","published":"2024-03-18T09:58:52Z","title":"LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept\n  Customization in Training-Free Diffusion Models","summary":"  Customization generation techniques have significantly advanced the synthesis\nof specific concepts across varied contexts. Multi-concept customization\nemerges as the challenging task within this domain. Existing approaches often\nrely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRA\nto merge various concepts into a single image. However, we identify this\nstraightforward method faces two major challenges: 1) concept confusion, which\noccurs when the model cannot preserve distinct individual characteristics, and\n2) concept vanishing, where the model fails to generate the intended subjects.\nTo address these issues, we introduce LoRA-Composer, a training-free framework\ndesigned for seamlessly integrating multiple LoRAs, thereby enhancing the\nharmony among different concepts within generated images. LoRA-Composer\naddresses concept vanishing through Concept Injection Constraints, enhancing\nconcept visibility via an expanded cross-attention mechanism. To combat concept\nconfusion, Concept Isolation Constraints are introduced, refining the\nself-attention computation. Furthermore, Latent Re-initialization is proposed\nto effectively stimulate concept-specific latent within designated regions. Our\nextensive testing showcases a notable enhancement in LoRA-Composer's\nperformance compared to standard baselines, especially when eliminating the\nimage-based conditions like canny edge or pose estimations. Code is released at\nhttps://github.com/Young98CN/LoRA\\_Composer.\n","authors":["Yang Yang","Wen Wang","Liang Peng","Chaotian Song","Yao Chen","Hengjia Li","Xiaolong Yang","Qinglin Lu","Deng Cai","Boxi Wu","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11626v1","updated":"2024-03-18T09:58:43Z","published":"2024-03-18T09:58:43Z","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","summary":"  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n","authors":["Zhizhen Zhou","Yejing Huo","Guoheng Huang","An Zeng","Xuhang Chen","Lian Huang","Zinuo Li"],"pdf_url":"https://arxiv.org/pdf/2403.11626v1.pdf","comment":"Accepted by The Visual Computer Journal"},{"id":"http://arxiv.org/abs/2403.11625v1","updated":"2024-03-18T09:56:48Z","published":"2024-03-18T09:56:48Z","title":"GaussNav: Gaussian Splatting for Visual Navigation","summary":"  In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to\nlocate a specific object depicted in a goal image within an unexplored\nenvironment. The primary difficulty of IIN stems from the necessity of\nrecognizing the target object across varying viewpoints and rejecting potential\ndistractors.\n  Existing map-based navigation methods largely adopt the representation form\nof Bird's Eye View (BEV) maps, which, however, lack the representation of\ndetailed textures in a scene.\n  To address the above issues, we propose a new Gaussian Splatting Navigation\n(abbreviated as GaussNav) framework for IIN task, which constructs a novel map\nrepresentation based on 3D Gaussian Splatting (3DGS).\n  The proposed framework enables the agent to not only memorize the geometry\nand semantic information of the scene, but also retain the textural features of\nobjects.\n  Our GaussNav framework demonstrates a significant leap in performance,\nevidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to\n0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.\n  Our code will be made publicly available.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.11625v1.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2403.08227v2","updated":"2024-03-18T09:55:51Z","published":"2024-03-13T04:11:38Z","title":"Matching Non-Identical Objects","summary":"  Not identical but similar objects are everywhere in the world. Examples\ninclude four-legged animals such as dogs and cats, cars of different models,\nakin flowers in various colors, and countless others. In this study, we address\na novel task of matching such non-identical objects. We propose a simple\nweighting scheme of descriptors that enhances various sparse image matching\nmethods, which were originally designed for matching identical objects captured\nfrom different perspectives, and achieve semantically robust matching. The\nexperiments show successful matching between non-identical objects in various\ncases including domain shift. Further, we present a first evaluation of the\nrobustness of the image matching methods under common corruptions, which is a\nsort of domain shift, and the proposed method improves the matching in this\ncase as well.\n","authors":["Yusuke Marumo","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2403.08227v2.pdf","comment":"10+7 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.13602v3","updated":"2024-03-18T09:50:00Z","published":"2024-02-21T08:09:05Z","title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving","summary":"  Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.\n","authors":["Mehdi Azarafza","Mojtaba Nayyeri","Charles Steinmetz","Steffen Staab","Achim Rettberg"],"pdf_url":"https://arxiv.org/pdf/2402.13602v3.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11616v1","updated":"2024-03-18T09:47:41Z","published":"2024-03-18T09:47:41Z","title":"Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level\n  Perception","summary":"  For training a video-based action recognition model that accepts multi-view\nvideo, annotating frame-level labels is tedious and difficult. However, it is\nrelatively easy to annotate sequence-level labels. This kind of coarse\nannotations are called as weak labels. However, training a multi-view\nvideo-based action recognition model with weak labels for frame-level\nperception is challenging. In this paper, we propose a novel learning\nframework, where the weak labels are first used to train a multi-view\nvideo-based base model, which is subsequently used for downstream frame-level\nperception tasks. The base model is trained to obtain individual latent\nembeddings for each view in the multi-view input. For training the model using\nthe weak labels, we propose a novel latent loss function. We also propose a\nmodel that uses the view-specific latent embeddings for downstream frame-level\naction recognition and detection tasks. The proposed framework is evaluated\nusing the MM Office dataset by comparing several baseline algorithms. The\nresults show that the proposed base model is effectively trained using weak\nlabels and the latent embeddings help the downstream models improve accuracy.\n","authors":["Vijay John","Yasutomo Kawanishi"],"pdf_url":"https://arxiv.org/pdf/2403.11616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03411v2","updated":"2024-03-18T09:47:24Z","published":"2024-01-07T08:03:06Z","title":"GRAM: Global Reasoning for Multi-Page VQA","summary":"  The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document tokens, we propose a tailored bias adaptation\nmethod. For additional computational savings during decoding, we introduce an\noptional compression stage using our compression-transformer\n(C-Former),reducing the encoded sequence length, thereby allowing a tradeoff\nbetween quality and latency. Extensive experiments showcase GRAM's\nstate-of-the-art performance on the benchmarks for multi-page DocVQA,\ndemonstrating the effectiveness of our approach.\n","authors":["Tsachi Blau","Sharon Fogel","Roi Ronen","Alona Golts","Roy Ganz","Elad Ben Avraham","Aviad Aberdam","Shahar Tsiper","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2401.03411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11614v1","updated":"2024-03-18T09:44:44Z","published":"2024-03-18T09:44:44Z","title":"CRS-Diff: Controllable Generative Remote Sensing Foundation Model","summary":"  The emergence of diffusion models has revolutionized the field of image\ngeneration, providing new methods for creating high-quality, high-resolution\nimages across various applications. However, the potential of these models for\ngenerating domain-specific images, particularly remote sensing (RS) images,\nremains largely untapped. RS images that are notable for their high resolution,\nextensive coverage, and rich information content, bring new challenges that\ngeneral diffusion models may not adequately address. This paper proposes\nCRS-Diff, a pioneering diffusion modeling framework specifically tailored for\ngenerating remote sensing imagery, leveraging the inherent advantages of\ndiffusion models while integrating advanced control mechanisms to ensure that\nthe imagery is not only visually clear but also enriched with geographic and\ntemporal information. The model integrates global and local control inputs,\nenabling precise combinations of generation conditions to refine the generation\nprocess. A comprehensive evaluation of CRS-Diff has demonstrated its superior\ncapability to generate RS imagery both in a single condition and multiple\nconditions compared with previous methods in terms of image quality and\ndiversity.\n","authors":["Datao Tang","Xiangyong Cao","Xingsong Hou","Zhongyuan Jiang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.11614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07800v2","updated":"2024-03-18T09:42:20Z","published":"2024-03-12T16:36:27Z","title":"BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\n  learning objectives","summary":"  This work addresses the Brain Magnetic Resonance Image Synthesis for Tumor\nSegmentation (BraSyn) challenge, which was hosted as part of the Brain Tumor\nSegmentation (BraTS) challenge in 2023. In this challenge, researchers are\ninvited to synthesize a missing magnetic resonance image sequence, given other\navailable sequences, to facilitate tumor segmentation pipelines trained on\ncomplete sets of image sequences. This problem can be tackled using deep\nlearning within the framework of paired image-to-image translation. In this\nstudy, we propose investigating the effectiveness of a commonly used deep\nlearning framework, such as Pix2Pix, trained under the supervision of different\nimage-quality loss functions. Our results indicate that the use of different\nloss functions significantly affects the synthesis quality. We systematically\nstudy the impact of various loss functions in the multi-sequence MR image\nsynthesis setting of the BraSyn challenge. Furthermore, we demonstrate how\nimage synthesis performance can be optimized by combining different learning\nobjectives beneficially.\n","authors":["Ivo M. Baltruschat","Parvaneh Janbakhshi","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2403.07800v2.pdf","comment":"minor changes, to be published as part of the 9th BrainLes:\n  International MICCAI Brain Lesion Workshop"},{"id":"http://arxiv.org/abs/2402.16086v2","updated":"2024-03-18T09:33:47Z","published":"2024-02-25T13:22:17Z","title":"Deep Homography Estimation for Visual Place Recognition","summary":"  Visual place recognition (VPR) is a fundamental task for many applications\nsuch as robot localization and augmented reality. Recently, the hierarchical\nVPR methods have received considerable attention due to the trade-off between\naccuracy and efficiency. They usually first use global features to retrieve the\ncandidate images, then verify the spatial consistency of matched local features\nfor re-ranking. However, the latter typically relies on the RANSAC algorithm\nfor fitting homography, which is time-consuming and non-differentiable. This\nmakes existing methods compromise to train the network only in global feature\nextraction. Here, we propose a transformer-based deep homography estimation\n(DHE) network that takes the dense feature map extracted by a backbone network\nas input and fits homography for fast and learnable geometric verification.\nMoreover, we design a re-projection error of inliers loss to train the DHE\nnetwork without additional homography labels, which can also be jointly trained\nwith the backbone network to help it extract the features that are more\nsuitable for local matching. Extensive experiments on benchmark datasets show\nthat our method can outperform several state-of-the-art methods. And it is more\nthan one order of magnitude faster than the mainstream hierarchical VPR methods\nusing RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.\n","authors":["Feng Lu","Shuting Dong","Lijun Zhang","Bingxi Liu","Xiangyuan Lan","Dongmei Jiang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.16086v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.06645v2","updated":"2024-03-18T09:22:01Z","published":"2024-03-11T12:07:33Z","title":"Ricci flow-based brain surface covariance descriptors for diagnosing\n  Alzheimer's disease","summary":"  Automated feature extraction from MRI brain scans and diagnosis of\nAlzheimer's disease are ongoing challenges. With advances in 3D imaging\ntechnology, 3D data acquisition is becoming more viable and efficient than its\n2D counterpart. Rather than using feature-based vectors, in this paper, for the\nfirst time, we suggest a pipeline to extract novel covariance-based descriptors\nfrom the cortical surface using the Ricci energy optimization. The covariance\ndescriptors are components of the nonlinear manifold of symmetric\npositive-definite matrices, thus we focus on using the Gaussian radial basis\nfunction to apply manifold-based classification to the 3D shape problem.\nApplying this novel signature to the analysis of abnormal cortical brain\nmorphometry allows for diagnosing Alzheimer's disease. Experimental studies\nperformed on about two hundred 3D MRI brain models, gathered from Alzheimer's\nDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of\nour descriptors in achieving remarkable classification accuracy.\n","authors":["Fatemeh Ahmadi","Mohamad Ebrahim Shiri","Behroz Bidabad","Maral Sedaghat","Pooran Memari"],"pdf_url":"https://arxiv.org/pdf/2403.06645v2.pdf","comment":"Accepted for publication in Biomedical Signal Processing and Control\n  journal"},{"id":"http://arxiv.org/abs/2403.11593v1","updated":"2024-03-18T09:12:16Z","published":"2024-03-18T09:12:16Z","title":"End-to-end multi-modal product matching in fashion e-commerce","summary":"  Product matching, the task of identifying different representations of the\nsame product for better discoverability, curation, and pricing, is a key\ncapability for online marketplace and e-commerce companies. We present a robust\nmulti-modal product matching system in an industry setting, where large\ndatasets, data distribution shifts and unseen domains pose challenges. We\ncompare different approaches and conclude that a relatively straightforward\nprojection of pretrained image and text encoders, trained through contrastive\nlearning, yields state-of-the-art results, while balancing cost and\nperformance. Our solution outperforms single modality matching systems and\nlarge pretrained models, such as CLIP. Furthermore we show how a\nhuman-in-the-loop process can be combined with model-based predictions to\nachieve near perfect precision in a production system.\n","authors":["Sándor Tóth","Stephen Wilson","Alexia Tsoukara","Enric Moreu","Anton Masalovich","Lars Roemheld"],"pdf_url":"https://arxiv.org/pdf/2403.11593v1.pdf","comment":"9 pages, submitted to SIGKDD"},{"id":"http://arxiv.org/abs/2403.11590v1","updated":"2024-03-18T09:08:41Z","published":"2024-03-18T09:08:41Z","title":"HSEmotion Team at the 6th ABAW Competition: Facial Expressions,\n  Valence-Arousal and Emotion Intensity Prediction","summary":"  This article presents our results for the sixth Affective Behavior Analysis\nin-the-wild (ABAW) competition. To improve the trustworthiness of facial\nanalysis, we study the possibility of using pre-trained deep models that\nextract reliable emotional features without the need to fine-tune the neural\nnetworks for a downstream task. In particular, we introduce several lightweight\nmodels based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN\narchitectures trained in multi-task scenarios to recognize facial expressions,\nvalence, and arousal on static photos. These neural networks extract\nframe-level features fed into a simple classifier, e.g., linear feed-forward\nneural network, to predict emotion intensity, compound expressions, action\nunits, facial expressions, and valence/arousal. Experimental results for five\ntasks from the sixth ABAW challenge demonstrate that our approach lets us\nsignificantly improve quality metrics on validation sets compared to existing\nnon-ensemble techniques.\n","authors":["Andrey V. Savchenko"],"pdf_url":"https://arxiv.org/pdf/2403.11590v1.pdf","comment":"10 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2310.16640v2","updated":"2024-03-18T09:07:03Z","published":"2023-10-25T13:43:36Z","title":"EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) is a crucial task in affective computing,\nbut its conventional focus on the seven basic emotions limits its applicability\nto the complex and expanding emotional spectrum. To address the issue of new\nand unseen emotions present in dynamic in-the-wild FER, we propose a novel\nvision-language model that utilises sample-level text descriptions (i.e.\ncaptions of the context, expressions or emotional cues) as natural language\nsupervision, aiming to enhance the learning of rich latent representations, for\nzero-shot classification. To test this, we evaluate using zero-shot\nclassification of the model trained on sample-level descriptions on four\npopular dynamic FER datasets. Our findings show that this approach yields\nsignificant improvements when compared to baseline methods. Specifically, for\nzero-shot video FER, we outperform CLIP by over 10\\% in terms of Weighted\nAverage Recall and 5\\% in terms of Unweighted Average Recall on several\ndatasets. Furthermore, we evaluate the representations obtained from the\nnetwork trained using sample-level descriptions on the downstream task of\nmental health symptom estimation, achieving performance comparable or superior\nto state-of-the-art methods and strong agreement with human experts. Namely, we\nachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophrenia\nsymptom severity estimation, which is comparable to human experts' agreement.\nThe code is publicly available at: https://github.com/NickyFot/EmoCLIP.\n","authors":["Niki Maria Foteinopoulou","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2310.16640v2.pdf","comment":"Accepted at FG'2024"},{"id":"http://arxiv.org/abs/2403.11589v1","updated":"2024-03-18T09:03:56Z","published":"2024-03-18T09:03:56Z","title":"UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures\n  for Human Avatar Modeling","summary":"  Reconstructing photo-realistic drivable human avatars from multi-view image\nsequences has been a popular and challenging topic in the field of computer\nvision and graphics. While existing NeRF-based methods can achieve high-quality\nnovel view rendering of human models, both training and inference processes are\ntime-consuming. Recent approaches have utilized 3D Gaussians to represent the\nhuman body, enabling faster training and rendering. However, they undermine the\nimportance of the mesh guidance and directly predict Gaussians in 3D space with\ncoarse mesh guidance. This hinders the learning procedure of the Gaussians and\ntends to produce blurry textures. Therefore, we propose UV Gaussians, which\nmodels the 3D human body by jointly learning mesh deformations and 2D UV-space\nGaussian textures. We utilize the embedding of UV map to learn Gaussian\ntextures in 2D space, leveraging the capabilities of powerful 2D networks to\nextract features. Additionally, through an independent Mesh network, we\noptimize pose-dependent geometric deformations, thereby guiding Gaussian\nrendering and significantly enhancing rendering quality. We collect and process\na new dataset of human motion, which includes multi-view images, scanned\nmodels, parametric model registration, and corresponding texture maps.\nExperimental results demonstrate that our method achieves state-of-the-art\nsynthesis of novel view and novel pose. The code and data will be made\navailable on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the\npaper is accepted.\n","authors":["Yujiao Jiang","Qingmin Liao","Xiaoyu Li","Li Ma","Qi Zhang","Chaopeng Zhang","Zongqing Lu","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2403.11589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18605v3","updated":"2024-03-18T09:03:05Z","published":"2023-11-30T15:02:13Z","title":"Learning Triangular Distribution in Visual World","summary":"  Convolution neural network is successful in pervasive vision tasks, including\nlabel distribution learning, which usually takes the form of learning an\ninjection from the non-linear visual features to the well-defined labels.\nHowever, how the discrepancy between features is mapped to the label\ndiscrepancy is ambient, and its correctness is not guaranteed.To address these\nproblems, we study the mathematical connection between feature and its label,\npresenting a general and simple framework for label distribution learning. We\npropose a so-called Triangular Distribution Transform (TDT) to build an\ninjective function between feature and label, guaranteeing that any symmetric\nfeature discrepancy linearly reflects the difference between labels. The\nproposed TDT can be used as a plug-in in mainstream backbone networks to\naddress different label distribution learning tasks. Experiments on Facial Age\nRecognition, Illumination Chromaticity Estimation, and Aesthetics assessment\nshow that TDT achieves on-par or better results than the prior arts.\n","authors":["Ping Chen","Xingpeng Zhang","Chengtao Zhou","Dichao Fan","Peng Tu","Le Zhang","Yanlin Qian"],"pdf_url":"https://arxiv.org/pdf/2311.18605v3.pdf","comment":"Accepet by CVPR 2024 (11 pages, 5 figures)"},{"id":"http://arxiv.org/abs/2312.12833v2","updated":"2024-03-18T09:02:20Z","published":"2023-12-20T08:30:07Z","title":"Learning Exhaustive Correlation for Spectral Super-Resolution: Where\n  Spatial-Spectral Attention Meets Linear Dependence","summary":"  Spectral super-resolution that aims to recover hyperspectral image (HSI) from\neasily obtainable RGB image has drawn increasing interest in the field of\ncomputational photography. The crucial aspect of spectral super-resolution lies\nin exploiting the correlation within HSIs. However, two types of bottlenecks in\nexisting Transformers limit performance improvement and practical applications.\nFirst, existing Transformers often separately emphasize either spatial-wise or\nspectral-wise correlation, disrupting the 3D features of HSI and hindering the\nexploitation of unified spatial-spectral correlation. Second, existing\nself-attention mechanism always establishes full-rank correlation matrix by\nlearning the correlation between pairs of tokens, leading to its inability to\ndescribe linear dependence widely existing in HSI among multiple tokens. To\naddress these issues, we propose a novel Exhaustive Correlation Transformer\n(ECT) for spectral super-resolution. First, we propose a Spectral-wise\nDiscontinuous 3D (SD3D) splitting strategy, which models unified\nspatial-spectral correlation by integrating spatial-wise continuous splitting\nstrategy and spectral-wise discontinuous splitting strategy. Second, we propose\na Dynamic Low-Rank Mapping (DLRM) model, which captures linear dependence among\nmultiple tokens through a dynamically calculated low-rank dependence map. By\nintegrating unified spatial-spectral attention and linear dependence, our ECT\ncan model exhaustive correlation within HSI. The experimental results on both\nsimulated and real data indicate that our method achieves state-of-the-art\nperformance. Codes and pretrained models will be available later.\n","authors":["Hongyuan Wang","Lizhi Wang","Jiang Xu","Chang Chen","Xue Hu","Fenglong Song","Youliang Yan"],"pdf_url":"https://arxiv.org/pdf/2312.12833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11944v2","updated":"2024-03-18T09:02:03Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Wenhu Chen","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11586v1","updated":"2024-03-18T08:58:48Z","published":"2024-03-18T08:58:48Z","title":"DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface\n  Reconstruction","summary":"  This paper explores the problem of reconstructing temporally consistent\nsurfaces from a 3D point cloud sequence without correspondence. To address this\nchallenging task, we propose DynoSurf, an unsupervised learning framework\nintegrating a template surface representation with a learnable deformation\nfield. Specifically, we design a coarse-to-fine strategy for learning the\ntemplate surface based on the deformable tetrahedron representation.\nFurthermore, we propose a learnable deformation representation based on the\nlearnable control points and blending weights, which can deform the template\nsurface non-rigidly while maintaining the consistency of the local shape.\nExperimental results demonstrate the significant superiority of DynoSurf over\ncurrent state-of-the-art approaches, showcasing its potential as a powerful\ntool for dynamic mesh reconstruction. The code is publicly available at\nhttps://github.com/yaoyx689/DynoSurf.\n","authors":["Yuxin Yao","Siyu Ren","Junhui Hou","Zhi Deng","Juyong Zhang","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11582v1","updated":"2024-03-18T08:55:48Z","published":"2024-03-18T08:55:48Z","title":"OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive\n  Semantic Segmentation","summary":"  Multi-target domain adaptation (MTDA) for semantic segmentation poses a\nsignificant challenge, as it involves multiple target domains with varying\ndistributions. The goal of MTDA is to minimize the domain discrepancies among a\nsingle source and multi-target domains, aiming to train a single model that\nexcels across all target domains. Previous MTDA approaches typically employ\nmultiple teacher architectures, where each teacher specializes in one target\ndomain to simplify the task. However, these architectures hinder the student\nmodel from fully assimilating comprehensive knowledge from all target-specific\nteachers and escalate training costs with increasing target domains. In this\npaper, we propose an ouroboric domain bridging (OurDB) framework, offering an\nefficient solution to the MTDA problem using a single teacher architecture.\nThis framework dynamically cycles through multiple target domains, aligning\neach domain individually to restrain the biased alignment problem, and utilizes\nFisher information to minimize the forgetting of knowledge from previous target\ndomains. We also propose a context-guided class-wise mixup (CGMix) that\nleverages contextual information tailored to diverse target contexts in MTDA.\nExperimental evaluations conducted on four urban driving datasets (i.e., GTA5,\nCityscapes, IDD, and Mapillary) demonstrate the superiority of our method over\nexisting state-of-the-art approaches.\n","authors":["Seungbeom Woo","Geonwoo Baek","Taehoon Kim","Jaemin Na","Joong-won Hwang","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16701v2","updated":"2024-03-18T08:55:36Z","published":"2023-08-15T17:38:55Z","title":"Is it Really Negative? Evaluating Natural Language Video Localization\n  Performance on Multiple Reliable Videos Pool","summary":"  With the explosion of multimedia content in recent years, Video Corpus Moment\nRetrieval (VCMR), which aims to detect a video moment that matches a given\nnatural language query from multiple videos, has become a critical problem.\nHowever, existing VCMR studies have a significant limitation since they have\nregarded all videos not paired with a specific query as negative, neglecting\nthe possibility of including false negatives when constructing the negative\nvideo set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)\ntask that aims to localize video frames within a massive video set, mitigating\nthe possibility of falsely distinguishing positive and negative videos. For\nthis task, we suggest an automatic dataset construction framework by employing\ntextual and visual semantic matching evaluation methods on the existing video\nmoment search datasets and introduce three MVMR datasets. To solve MVMR task,\nwe further propose a strong method, CroCs, which employs cross-directional\ncontrastive learning that selectively identifies the reliable and informative\nnegatives, enhancing the robustness of a model on MVMR task. Experimental\nresults on the introduced datasets reveal that existing video moment search\nmodels are easily distracted by negative video frames, whereas our model shows\nsignificant performance.\n","authors":["Nakyeong Yang","Minsung Kim","Seunghyun Yoon","Joongbo Shin","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2309.16701v2.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.11577v1","updated":"2024-03-18T08:53:03Z","published":"2024-03-18T08:53:03Z","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal\n  Calibration","summary":"  Reliable multimodal sensor fusion algorithms re- quire accurate\nspatiotemporal calibration. Recently, targetless calibration techniques based\non implicit neural representations have proven to provide precise and robust\nresults. Nevertheless, such methods are inherently slow to train given the high\ncompu- tational overhead caused by the large number of sampled points required\nfor volume rendering. With the recent introduction of 3D Gaussian Splatting as\na faster alternative to implicit representation methods, we propose to leverage\nthis new ren- dering approach to achieve faster multi-sensor calibration. We\nintroduce 3DGS-Calib, a new calibration method that relies on the speed and\nrendering accuracy of 3D Gaussian Splatting to achieve multimodal\nspatiotemporal calibration that is accurate, robust, and with a substantial\nspeed-up compared to methods relying on implicit neural representations. We\ndemonstrate the superiority of our proposal with experimental results on\nsequences from KITTI-360, a widely used driving dataset.\n","authors":["Quentin Herau","Moussab Bennehar","Arthur Moreau","Nathan Piasco","Luis Roldao","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2403.11577v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11576v1","updated":"2024-03-18T08:52:23Z","published":"2024-03-18T08:52:23Z","title":"MISS: Memory-efficient Instance Segmentation Framework By Visual\n  Inductive Priors Flow Propagation","summary":"  Instance segmentation, a cornerstone task in computer vision, has\nwide-ranging applications in diverse industries. The advent of deep learning\nand artificial intelligence has underscored the criticality of training\neffective models, particularly in data-scarce scenarios - a concern that\nresonates in both academic and industrial circles. A significant impediment in\nthis domain is the resource-intensive nature of procuring high-quality,\nannotated data for instance segmentation, a hurdle that amplifies the challenge\nof developing robust models under resource constraints. In this context, the\nstrategic integration of a visual prior into the training dataset emerges as a\npotential solution to enhance congruity with the testing data distribution,\nconsequently reducing the dependency on computational resources and the need\nfor highly complex models. However, effectively embedding a visual prior into\nthe learning process remains a complex endeavor. Addressing this challenge, we\nintroduce the MISS (Memory-efficient Instance Segmentation System) framework.\nMISS leverages visual inductive prior flow propagation, integrating intrinsic\nprior knowledge from the Synergy-basketball dataset at various stages: data\npreprocessing, augmentation, training, and inference. Our empirical evaluations\nunderscore the efficacy of MISS, demonstrating commendable performance in\nscenarios characterized by limited data availability and memory constraints.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00766v2","updated":"2024-03-18T08:51:58Z","published":"2024-01-01T14:14:35Z","title":"Exposure Bracketing is All You Need for Unifying Image Restoration and\n  Enhancement Tasks","summary":"  It is highly desired but challenging to acquire high-quality photos with\nclear content in low-light environments. Although multi-image processing\nmethods (using burst, dual-exposure, or multi-exposure images) have made\nsignificant progress in addressing this issue, they typically focus on specific\nrestoration or enhancement problems, being insufficient in exploiting\nmulti-image. Motivated by that multi-exposure images are complementary in\ndenoising, deblurring, high dynamic range imaging, and super-resolution, we\npropose to utilize exposure bracketing photography to unify restoration and\nenhancement tasks in this work. Due to the difficulty in collecting real-world\npairs, we suggest a solution that first pre-trains the model with synthetic\npaired data and then adapts it to real-world unlabeled images. In particular, a\ntemporally modulated recurrent network (TMRNet) and self-supervised adaptation\nmethod are proposed. Moreover, we construct a data simulation pipeline to\nsynthesize pairs and collect real-world images from 200 nighttime scenarios.\nExperiments on both datasets show that our method performs favorably against\nthe state-of-the-art multi-image processing ones. The dataset, code, and\npre-trained models are available at https://github.com/cszhilu1998/BracketIRE.\n","authors":["Zhilu Zhang","Shuohao Zhang","Renlong Wu","Zifei Yan","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.00766v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2403.11573v1","updated":"2024-03-18T08:50:04Z","published":"2024-03-18T08:50:04Z","title":"Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for\n  Resolving Class-imbalance Problem","summary":"  Typical LiDAR-based 3D object detection models are trained in a supervised\nmanner with real-world data collection, which is often imbalanced over classes\n(or long-tailed). To deal with it, augmenting minority-class examples by\nsampling ground truth (GT) LiDAR points from a database and pasting them into a\nscene of interest is often used, but challenges still remain: inflexibility in\nlocating GT samples and limited sample diversity. In this work, we propose to\nleverage pseudo-LiDAR point clouds generated (at a low cost) from videos\ncapturing a surround view of miniatures or real-world objects of minor classes.\nOur method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of\nthree main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D\nview synthesis model, (ii) object-level domain alignment with LiDAR intensity\nestimation and (iii) a hybrid context-aware placement method from ground and\nmap information. We demonstrate the superiority and generality of our method\nthrough performance improvements in extensive experiments conducted on three\npopular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the\ndatasets with large domain gaps captured by different LiDAR configurations. Our\ncode and data will be publicly available upon publication.\n","authors":["Mincheol Chang","Siyeong Lee","Jinkyu Kim","Namil Kim"],"pdf_url":"https://arxiv.org/pdf/2403.11573v1.pdf","comment":"28 pages, 12 figures, 11 tables; Submitted to ECCV 2024"},{"id":"http://arxiv.org/abs/2311.10529v3","updated":"2024-03-18T08:47:03Z","published":"2023-11-17T13:49:00Z","title":"Enhancing the Reliability of Segment Anything Model for Auto-Prompting\n  Medical Image Segmentation with Uncertainty Rectification","summary":"  The Segment Anything Model (SAM) has recently emerged as a groundbreaking\nfoundation model for prompt-driven image segmentation tasks. However, both the\noriginal SAM and its medical variants require slice-by-slice manual prompting\nof target structures, which directly increase the burden for applications.\nDespite attempts of auto-prompting to turn SAM into a fully automatic manner,\nit still exhibits subpar performance and lacks of reliability especially in the\nfield of medical imaging. In this paper, we propose UR-SAM, an uncertainty\nrectified SAM framework to enhance the reliability for auto-prompting medical\nimage segmentation. Building upon a localization framework for automatic prompt\ngeneration, our method incorporates a prompt augmentation module to obtain a\nseries of input prompts for SAM for uncertainty estimation and an\nuncertainty-based rectification module to further utilize the distribution of\nestimated uncertainty to improve the segmentation performance. Extensive\nexperiments on two public 3D medical datasets covering the segmentation of 35\norgans demonstrate that without supplementary training or fine-tuning, our\nmethod further improves the segmentation performance with up to 10.7 % and 13.8\n% in dice similarity coefficient, demonstrating efficiency and broad\ncapabilities for medical image segmentation without manual prompting.\n","authors":["Yichi Zhang","Shiyao Hu","Sijie Ren","Chen Jiang","Yuan Cheng","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2311.10529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14405v2","updated":"2024-03-18T08:45:52Z","published":"2024-01-25T18:59:58Z","title":"Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities","summary":"  We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.\n","authors":["Yiyuan Zhang","Xiaohan Ding","Kaixiong Gong","Yixiao Ge","Ying Shan","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2401.14405v2.pdf","comment":"CVPR 2024. Code and models are available at\n  https://github.com/AILab-CVC/M2PT"},{"id":"http://arxiv.org/abs/2403.11572v1","updated":"2024-03-18T08:44:40Z","published":"2024-03-18T08:44:40Z","title":"Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance\n  Segmentation Framework for Sport-scenes","summary":"  Instance segmentation is a fundamental task in computer vision with broad\napplications across various industries. In recent years, with the proliferation\nof deep learning and artificial intelligence applications, how to train\neffective models with limited data has become a pressing issue for both\nacademia and industry. In the Visual Inductive Priors challenge (VIPriors2023),\nparticipants must train a model capable of precisely locating individuals on a\nbasketball court, all while working with limited data and without the use of\ntransfer learning or pre-trained models. We propose Memory effIciency inStance\nSegmentation framework based on visual inductive prior flow propagation that\neffectively incorporates inherent prior information from the dataset into both\nthe data preprocessing and data augmentation stages, as well as the inference\nphase. Our team (ACVLAB) experiments demonstrate that our model achieves\npromising performance (0.509 AP@0.50:0.95) even under limited data and memory\nconstraints.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee","Ming-Shyen Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11570v1","updated":"2024-03-18T08:43:42Z","published":"2024-03-18T08:43:42Z","title":"LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense\n  Knowledge","summary":"  Large text-to-image models have achieved astonishing performance in\nsynthesizing diverse and high-quality images guided by texts. With\ndetail-oriented conditioning control, even finer-grained spatial control can be\nachieved. However, some generated images still appear unreasonable, even with\nplentiful object features and a harmonious style. In this paper, we delve into\nthe underlying causes and find that deep-level logical information, serving as\ncommon-sense knowledge, plays a significant role in understanding and\nprocessing images. Nonetheless, almost all models have neglected the importance\nof logical relations in images, resulting in poor performance in this aspect.\nFollowing this observation, we propose LogicalDefender, which combines images\nwith the logical knowledge already summarized by humans in text. This\nencourages models to learn logical knowledge faster and better, and\nconcurrently, extracts the widely applicable logical knowledge from both images\nand human knowledge. Experiments show that our model has achieved better\nlogical performance, and the extracted logical knowledge can be effectively\napplied to other scenarios.\n","authors":["Yuhe Liu","Mengxue Kang","Zengchang Qin","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2403.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11568v1","updated":"2024-03-18T08:42:08Z","published":"2024-03-18T08:42:08Z","title":"EffiVED:Efficient Video Editing via Text-instruction Diffusion Models","summary":"  Large-scale text-to-video models have shown remarkable abilities, but their\ndirect application in video editing remains challenging due to limited\navailable datasets. Current video editing methods commonly require per-video\nfine-tuning of diffusion models or specific inversion optimization to ensure\nhigh-fidelity edits. In this paper, we introduce EffiVED, an efficient\ndiffusion-based model that directly supports instruction-guided video editing.\nTo achieve this, we present two efficient workflows to gather video editing\npairs, utilizing augmentation and fundamental vision-language techniques. These\nworkflows transform vast image editing datasets and open-world videos into a\nhigh-quality dataset for training EffiVED. Experimental results reveal that\nEffiVED not only generates high-quality editing videos but also executes\nrapidly. Finally, we demonstrate that our data collection method significantly\nimproves editing performance and can potentially tackle the scarcity of video\nediting data. The datasets will be made publicly available upon publication.\n","authors":["Zhenghao Zhang","Zuozhuo Dai","Long Qin","Weizhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04164v2","updated":"2024-03-18T08:40:48Z","published":"2024-03-07T02:48:42Z","title":"ProMISe: Promptable Medical Image Segmentation using SAM","summary":"  With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for\nmedical image segmentation (MIS) has become popular. However, due to the large\nsize of the SAM model and the significant domain gap between natural and\nmedical images, fine-tuning-based strategies are costly with potential risk of\ninstability, feature damage and catastrophic forgetting. Furthermore, some\nmethods of transferring SAM to a domain-specific MIS through fine-tuning\nstrategies disable the model's prompting capability, severely limiting its\nutilization scenarios. In this paper, we propose an Auto-Prompting Module\n(APM), which provides SAM-based foundation model with Euclidean adaptive\nprompts in the target domain. Our experiments demonstrate that such adaptive\nprompts significantly improve SAM's non-fine-tuned performance in MIS. In\naddition, we propose a novel non-invasive method called Incremental Pattern\nShifting (IPS) to adapt SAM to specific medical domains. Experimental results\nshow that the IPS enables SAM to achieve state-of-the-art or competitive\nperformance in MIS without the need for fine-tuning. By coupling these two\nmethods, we propose ProMISe, an end-to-end non-fine-tuned framework for\nPromptable Medical Image Segmentation. Our experiments demonstrate that both\nusing our methods individually or in combination achieves satisfactory\nperformance in low-cost pattern shifting, with all of SAM's parameters frozen.\n","authors":["Jinfeng Wang","Sifan Song","Xinkun Wang","Yiyi Wang","Yiyi Miao","Jionglong Su","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.04164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15599v2","updated":"2024-03-18T08:37:24Z","published":"2023-11-27T07:48:50Z","title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,\n  Video, Point Cloud, Time-Series and Image Recognition","summary":"  Large-kernel convolutional neural networks (ConvNets) have recently received\nextensive research attention, but two unresolved and critical issues demand\nfurther investigation. 1) The architectures of existing large-kernel ConvNets\nlargely follow the design principles of conventional ConvNets or transformers,\nwhile the architectural design for large-kernel ConvNets remains\nunder-addressed. 2) As transformers have dominated multiple modalities, it\nremains to be investigated whether ConvNets also have a strong universal\nperception ability in domains beyond vision. In this paper, we contribute from\ntwo aspects. 1) We propose four architectural guidelines for designing\nlarge-kernel ConvNets, the core of which is to exploit the essential\ncharacteristics of large kernels that distinguish them from small kernels -\nthey can see wide without going deep. Following such guidelines, our proposed\nlarge-kernel ConvNet shows leading performance in image recognition (ImageNet\naccuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),\ndemonstrating better performance and higher speed than the recent powerful\ncompetitors. 2) We discover large kernels are the key to unlocking the\nexceptional performance of ConvNets in domains where they were originally not\nproficient. With certain modality-related preprocessing approaches, the\nproposed model achieves state-of-the-art performance on time-series forecasting\nand audio recognition tasks even without modality-specific customization to the\narchitecture. All the code and models are publicly available on GitHub and\nHuggingface.\n","authors":["Xiaohan Ding","Yiyuan Zhang","Yixiao Ge","Sijie Zhao","Lin Song","Xiangyu Yue","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.15599v2.pdf","comment":"CVPR 2024. Code, all the models, reproducible training scripts at\n  https://github.com/AILab-CVC/UniRepLKNet"},{"id":"http://arxiv.org/abs/2310.12149v2","updated":"2024-03-18T08:36:02Z","published":"2023-10-18T17:59:02Z","title":"Object-aware Inversion and Reassembly for Image Editing","summary":"  By comparing the original and target prompts, we can obtain numerous editing\npairs, each comprising an object and its corresponding editing target. To allow\neditability while maintaining fidelity to the input image, existing editing\nmethods typically involve a fixed number of inversion steps that project the\nwhole input image to its noisier latent representation, followed by a denoising\nprocess guided by the target prompt. However, we find that the optimal number\nof inversion steps for achieving ideal editing results varies significantly\namong different editing pairs, owing to varying editing difficulties.\nTherefore, the current literature, which relies on a fixed number of inversion\nsteps, produces sub-optimal generation quality, especially when handling\nmultiple editing pairs in a natural image. To this end, we propose a new image\nediting paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enable\nobject-level fine-grained editing. Specifically, we design a new search metric,\nwhich determines the optimal inversion steps for each editing pair, by jointly\nconsidering the editability of the target and the fidelity of the non-editing\nregion. We use our search metric to find the optimal inversion step for each\nediting pair when editing an image. We then edit these editing pairs separately\nto avoid concept mismatch. Subsequently, we propose an additional reassembly\nstep to seamlessly integrate the respective editing results and the non-editing\nregion to obtain the final edited image. To systematically evaluate the\neffectiveness of our method, we collect two datasets called OIRBench for\nbenchmarking single- and multi-object editing, respectively. Experiments\ndemonstrate that our method achieves superior performance in editing object\nshapes, colors, materials, categories, etc., especially in multi-object editing\nscenarios.\n","authors":["Zhen Yang","Ganggui Ding","Wen Wang","Hao Chen","Bohan Zhuang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2310.12149v2.pdf","comment":"Project Page: https://aim-uofa.github.io/OIR-Diffusion/"},{"id":"http://arxiv.org/abs/2403.11561v1","updated":"2024-03-18T08:29:47Z","published":"2024-03-18T08:29:47Z","title":"Learning Unified Reference Representation for Unsupervised Multi-class\n  Anomaly Detection","summary":"  In the field of multi-class anomaly detection, reconstruction-based methods\nderived from single-class anomaly detection face the well-known challenge of\n``learning shortcuts'', wherein the model fails to learn the patterns of normal\nsamples as it should, opting instead for shortcuts such as identity mapping or\nartificial noise elimination. Consequently, the model becomes unable to\nreconstruct genuine anomalies as normal instances, resulting in a failure of\nanomaly detection. To counter this issue, we present a novel unified feature\nreconstruction-based anomaly detection framework termed RLR (Reconstruct\nfeatures from a Learnable Reference representation). Unlike previous methods,\nRLR utilizes learnable reference representations to compel the model to learn\nnormal feature patterns explicitly, thereby prevents the model from succumbing\nto the ``learning shortcuts'' issue. Additionally, RLR incorporates locality\nconstraints into the learnable reference to facilitate more effective normal\npattern capture and utilizes a masked learnable key attention mechanism to\nenhance robustness. Evaluation of RLR on the 15-category MVTec-AD dataset and\nthe 12-category VisA dataset shows superior performance compared to\nstate-of-the-art methods under the unified setting. The code of RLR will be\npublicly available.\n","authors":["Liren He","Zhengkai Jiang","Jinlong Peng","Liang Liu","Qiangang Du","Xiaobin Hu","Wenbing Zhu","Mingmin Chi","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14272v2","updated":"2024-03-18T08:15:48Z","published":"2023-11-24T04:16:32Z","title":"CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning","summary":"  Machine learning pipelines for classification tasks often train a universal\nmodel to achieve accuracy across a broad range of classes. However, a typical\nuser encounters only a limited selection of classes regularly. This disparity\nprovides an opportunity to enhance computational efficiency by tailoring models\nto focus on user-specific classes. Existing works rely on unstructured pruning,\nwhich introduces randomly distributed non-zero values in the model, making it\nunsuitable for hardware acceleration. Alternatively, some approaches employ\nstructured pruning, such as channel pruning, but these tend to provide only\nminimal compression and may lead to reduced model accuracy. In this work, we\npropose CRISP, a novel pruning framework leveraging a hybrid structured\nsparsity pattern that combines both fine-grained N:M structured sparsity and\ncoarse-grained block sparsity. Our pruning strategy is guided by a\ngradient-based class-aware saliency score, allowing us to retain weights\ncrucial for user-specific classes. CRISP achieves high accuracy with minimal\nmemory consumption for popular models like ResNet-50, VGG-16, and MobileNetV2\non ImageNet and CIFAR-100 datasets. Moreover, CRISP delivers up to 14$\\times$\nreduction in latency and energy consumption compared to existing pruning\nmethods while maintaining comparable accuracy. Our code is available at\nhttps://github.com/shivmgg/CRISP/.\n","authors":["Shivam Aggarwal","Kuluhan Binici","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2311.14272v2.pdf","comment":"6 pages, accepted in Design, Automation & Test in Europe Conference &\n  Exhibition (DATE) 2024"},{"id":"http://arxiv.org/abs/2403.11556v1","updated":"2024-03-18T08:13:26Z","published":"2024-03-18T08:13:26Z","title":"Hierarchical Frequency-based Upsampling and Refining for Compressed\n  Video Quality Enhancement","summary":"  Video compression artifacts arise due to the quantization operation in the\nfrequency domain. The goal of video quality enhancement is to reduce\ncompression artifacts and reconstruct a visually-pleasant result. In this work,\nwe propose a hierarchical frequency-based upsampling and refining neural\nnetwork (HFUR) for compressed video quality enhancement. HFUR consists of two\nmodules: implicit frequency upsampling module (ImpFreqUp) and hierarchical and\niterative refinement module (HIR). ImpFreqUp exploits DCT-domain prior derived\nthrough implicit DCT transform, and accurately reconstructs the DCT-domain loss\nvia a coarse-to-fine transfer. Consequently, HIR is introduced to facilitate\ncross-collaboration and information compensation between the scales, thus\nfurther refine the feature maps and promote the visual quality of the final\noutput. We demonstrate the effectiveness of the proposed modules via ablation\nexperiments and visualized results. Extensive experiments on public benchmarks\nshow that HFUR achieves state-of-the-art performance for both constant bit rate\nand constant QP modes.\n","authors":["Qianyu Zhang","Bolun Zheng","Xinying Chen","Quan Chen","Zhunjie Zhu","Canjin Wang","Zongpeng Li","Chengang Yan"],"pdf_url":"https://arxiv.org/pdf/2403.11556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11550v1","updated":"2024-03-18T08:01:23Z","published":"2024-03-18T08:01:23Z","title":"TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling","summary":"  As a cross-modal task, visual storytelling aims to generate a story for an\nordered image sequence automatically. Different from the image captioning task,\nvisual storytelling requires not only modeling the relationships between\nobjects in the image but also mining the connections between adjacent images.\nRecent approaches primarily utilize either end-to-end frameworks or multi-stage\nframeworks to generate relevant stories, but they usually overlook latent topic\ninformation. In this paper, in order to generate a more coherent and relevant\nstory, we propose a novel method, Topic Aware Reinforcement Network for VIsual\nStoryTelling (TARN-VIST). In particular, we pre-extracted the topic information\nof stories from both visual and linguistic perspectives. Then we apply two\ntopic-consistent reinforcement learning rewards to identify the discrepancy\nbetween the generated story and the human-labeled story so as to refine the\nwhole generation process. Extensive experimental results on the VIST dataset\nand human evaluation demonstrate that our proposed model outperforms most of\nthe competitive models across multiple evaluation metrics.\n","authors":["Weiran Chen","Xin Li","Jiaqi Su","Guiqian Zhu","Ying Li","Yi Ji","Chunping Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11549v1","updated":"2024-03-18T08:00:23Z","published":"2024-03-18T08:00:23Z","title":"Boosting Continual Learning of Vision-Language Models via\n  Mixture-of-Experts Adapters","summary":"  Continual learning can empower vision-language models to continuously acquire\nnew knowledge, without the need for access to the entire historical dataset.\nHowever, mitigating the performance degradation in large-scale models is\nnon-trivial due to (i) parameter shifts throughout lifelong learning and (ii)\nsignificant computational burdens associated with full-model tuning. In this\nwork, we present a parameter-efficient continual learning framework to\nalleviate long-term forgetting in incremental learning with vision-language\nmodels. Our approach involves the dynamic expansion of a pre-trained CLIP\nmodel, through the integration of Mixture-of-Experts (MoE) adapters in response\nto new tasks. To preserve the zero-shot recognition capability of\nvision-language models, we further introduce a Distribution Discriminative\nAuto-Selector (DDAS) that automatically routes in-distribution and\nout-of-distribution inputs to the MoE Adapter and the original CLIP,\nrespectively. Through extensive experiments across various settings, our\nproposed method consistently outperforms previous state-of-the-art approaches\nwhile concurrently reducing parameter training burdens by 60%. Our code locates\nat https://github.com/JiazuoYu/MoE-Adapters4CL\n","authors":["Jiazuo Yu","Yunzhi Zhuge","Lu Zhang","Dong Wang","Huchuan Lu","You He"],"pdf_url":"https://arxiv.org/pdf/2403.11549v1.pdf","comment":"This work is accepted by CVPR2024. More modifications may be\n  performed"},{"id":"http://arxiv.org/abs/2301.00371v2","updated":"2024-03-18T07:57:19Z","published":"2023-01-01T08:38:07Z","title":"Robust Domain Adaptive Object Detection with Unified Multi-Granularity\n  Alignment","summary":"  Domain adaptive detection aims to improve the generalization of detectors on\ntarget domain. To reduce discrepancy in feature distributions between two\ndomains, recent approaches achieve domain adaption through feature alignment in\ndifferent granularities via adversarial learning. However, they neglect the\nrelationship between multiple granularities and different features in\nalignment, degrading detection. Addressing this, we introduce a unified\nmulti-granularity alignment (MGA)-based detection framework for\ndomain-invariant feature learning. The key is to encode the dependencies across\ndifferent granularities including pixel-, instance-, and category-levels\nsimultaneously to align two domains. Specifically, based on pixel-level\nfeatures, we first develop an omni-scale gated fusion (OSGF) module to\naggregate discriminative representations of instances with scale-aware\nconvolutions, leading to robust multi-scale detection. Besides, we introduce\nmulti-granularity discriminators to identify where, either source or target\ndomains, different granularities of samples come from. Note that, MGA not only\nleverages instance discriminability in different categories but also exploits\ncategory consistency between two domains for detection. Furthermore, we present\nan adaptive exponential moving average (AEMA) strategy that explores model\nassessments for model update to improve pseudo labels and alleviate local\nmisalignment problem, boosting detection robustness. Extensive experiments on\nmultiple domain adaption scenarios validate the superiority of MGA over other\napproaches on FCOS and Faster R-CNN detectors. Code will be released at\nhttps://github.com/tiankongzhang/MGA.\n","authors":["Libo Zhang","Wenzhang Zhou","Heng Fan","Tiejian Luo","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2301.00371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11541v1","updated":"2024-03-18T07:51:22Z","published":"2024-03-18T07:51:22Z","title":"Hierarchical Spatial Proximity Reasoning for Vision-and-Language\n  Navigation","summary":"  Most Vision-and-Language Navigation (VLN) algorithms tend to make decision\nerrors, primarily due to a lack of visual common sense and insufficient\nreasoning capabilities. To address this issue, this paper proposes a\nHierarchical Spatial Proximity Reasoning (HSPR) model. Firstly, we design a\nScene Understanding Auxiliary Task (SUAT) to assist the agent in constructing a\nknowledge base of hierarchical spatial proximity for reasoning navigation.\nSpecifically, this task utilizes panoramic views and object features to\nidentify regions in the navigation environment and uncover the adjacency\nrelationships between regions, objects, and region-object pairs. Secondly, we\ndynamically construct a semantic topological map through agent-environment\ninteractions and propose a Multi-step Reasoning Navigation Algorithm (MRNA)\nbased on the map. This algorithm continuously plans various feasible paths from\none region to another, utilizing the constructed proximity knowledge base,\nenabling more efficient exploration. Additionally, we introduce a Proximity\nAdaptive Attention Module (PAAM) and Residual Fusion Method (RFM) to enable the\nmodel to obtain more accurate navigation decision confidence. Finally, we\nconduct experiments on publicly available datasets including REVERIE, SOON,\nR2R, and R4R to validate the effectiveness of the proposed approach.\n","authors":["Ming Xu","Zilong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.11541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11537v1","updated":"2024-03-18T07:43:14Z","published":"2024-03-18T07:43:14Z","title":"Semantic Prompting with Image-Token for Continual Learning","summary":"  Continual learning aims to refine model parameters for new tasks while\nretaining knowledge from previous tasks. Recently, prompt-based learning has\nemerged to leverage pre-trained models to be prompted to learn subsequent tasks\nwithout the reliance on the rehearsal buffer. Although this approach has\ndemonstrated outstanding results, existing methods depend on preceding\ntask-selection process to choose appropriate prompts. However, imperfectness in\ntask-selection may lead to negative impacts on the performance particularly in\nthe scenarios where the number of tasks is large or task distributions are\nimbalanced. To address this issue, we introduce I-Prompt, a task-agnostic\napproach focuses on the visual semantic information of image tokens to\neliminate task prediction. Our method consists of semantic prompt matching,\nwhich determines prompts based on similarities between tokens, and image\ntoken-level prompting, which applies prompts directly to image tokens in the\nintermediate layers. Consequently, our method achieves competitive performance\non four benchmarks while significantly reducing training time compared to\nstate-of-the-art methods. Moreover, we demonstrate the superiority of our\nmethod across various scenarios through extensive experiments.\n","authors":["Jisu Han","Jaemin Na","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.11537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11536v1","updated":"2024-03-18T07:41:39Z","published":"2024-03-18T07:41:39Z","title":"OCR is All you need: Importing Multi-Modality into Image-based Defect\n  Detection System","summary":"  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing\nprocess, predominantly leveraging high-resolution imaging instruments for\nscanning purposes. It detects anomalies by analyzing image textures or\npatterns, making it an essential tool in industrial manufacturing and quality\ncontrol. Despite its importance, the deployment of models for AOI often faces\nchallenges. These include limited sample sizes, which hinder effective feature\nlearning, variations among source domains, and sensitivities to changes in\nlighting and camera positions during imaging. These factors collectively\ncompromise the accuracy of model predictions. Traditional AOI often fails to\ncapitalize on the rich mechanism-parameter information from machines or inside\nimages, including statistical parameters, which typically benefit AOI\nclassification. To address this, we introduce an external modality-guided data\nmining framework, primarily rooted in optical character recognition (OCR), to\nextract statistical features from images as a second modality to enhance\nperformance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the\nalignment of external modality features, extracted using a single\nmodality-aware model, with image features encoded by a convolutional neural\nnetwork. This synergy enables a more refined fusion of semantic representations\nfrom different modalities. We further introduce feature refinement and a gating\nfunction in our OANet to optimize the combination of these features, enhancing\ninference and decision-making capabilities. Experimental outcomes show that our\nmethodology considerably boosts the recall rate of the defect detection model\nand maintains high robustness even in challenging scenarios.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee","Chun-Hung Sun","Kuang-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11535v1","updated":"2024-03-18T07:41:19Z","published":"2024-03-18T07:41:19Z","title":"EchoReel: Enhancing Action Generation of Existing Video Diffusion Models","summary":"  Recent large-scale video datasets have facilitated the generation of diverse\nopen-domain videos of Video Diffusion Models (VDMs). Nonetheless, the efficacy\nof VDMs in assimilating complex knowledge from these datasets remains\nconstrained by their inherent scale, leading to suboptimal comprehension and\nsynthesis of numerous actions. In this paper, we introduce EchoReel, a novel\napproach to augment the capability of VDMs in generating intricate actions by\nemulating motions from pre-existing videos, which are readily accessible from\ndatabases or online repositories. EchoReel seamlessly integrates with existing\nVDMs, enhancing their ability to produce realistic motions without compromising\ntheir fundamental capabilities. Specifically, the Action Prism (AP), is\nintroduced to distill motion information from reference videos, which requires\ntraining on only a small dataset. Leveraging the knowledge from pre-trained\nVDMs, EchoReel incorporates new action features into VDMs through the\nadditional layers, eliminating the need for any further fine-tuning of\nuntrained actions. Extensive experiments demonstrate that EchoReel is not\nmerely replicating the whole content from references, and it significantly\nimproves the generation of realistic actions, even in situations where existing\nVDMs might directly fail.\n","authors":["Jianzhi liu","Junchen Zhu","Lianli Gao","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2403.11535v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.11532v1","updated":"2024-03-18T07:35:25Z","published":"2024-03-18T07:35:25Z","title":"Out-of-Distribution Detection Should Use Conformal Prediction (and\n  Vice-versa?)","summary":"  Research on Out-Of-Distribution (OOD) detection focuses mainly on building\nscores that efficiently distinguish OOD data from In Distribution (ID) data. On\nthe other hand, Conformal Prediction (CP) uses non-conformity scores to\nconstruct prediction sets with probabilistic coverage guarantees. In this work,\nwe propose to use CP to better assess the efficiency of OOD scores.\nSpecifically, we emphasize that in standard OOD benchmark settings, evaluation\nmetrics can be overly optimistic due to the finite sample size of the test\ndataset. Based on the work of (Bates et al., 2022), we define new conformal\nAUROC and conformal FRP@TPR95 metrics, which are corrections that provide\nprobabilistic conservativeness guarantees on the variability of these metrics.\nWe show the effect of these corrections on two reference OOD and anomaly\ndetection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,\n2022). We also show that the benefits of using OOD together with CP apply the\nother way around by using OOD scores as non-conformity scores, which results in\nimproving upon current CP methods. One of the key messages of these\ncontributions is that since OOD is concerned with designing scores and CP with\ninterpreting these scores, the two fields may be inherently intertwined.\n","authors":["Paul Novello","Joseba Dalmau","Léo Andeol"],"pdf_url":"https://arxiv.org/pdf/2403.11532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11530v1","updated":"2024-03-18T07:33:56Z","published":"2024-03-18T07:33:56Z","title":"Continual Forgetting for Pre-trained Vision Models","summary":"  For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners. These requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify two key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal. To\naddress them, we propose Group Sparse LoRA (GS-LoRA). Specifically, towards\n(i), we use LoRA modules to fine-tune the FFN layers in Transformer blocks for\neach forgetting task independently, and towards (ii), a simple group sparse\nregularization is adopted, enabling automatic selection of specific LoRA groups\nand zeroing out the others. GS-LoRA is effective, parameter-efficient,\ndata-efficient, and easy to implement. We conduct extensive experiments on face\nrecognition, object detection and image classification and demonstrate that\nGS-LoRA manages to forget specific classes with minimal impact on other\nclasses. Codes will be released on \\url{https://github.com/bjzhb666/GS-LoRA}.\n","authors":["Hongbo Zhao","Bolin Ni","Haochen Wang","Junsong Fan","Fei Zhu","Yuxi Wang","Yuntao Chen","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11530v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11529v1","updated":"2024-03-18T07:31:39Z","published":"2024-03-18T07:31:39Z","title":"Video Object Segmentation with Dynamic Query Modulation","summary":"  Storing intermediate frame segmentations as memory for long-range context\nmodeling, spatial-temporal memory-based methods have recently showcased\nimpressive results in semi-supervised video object segmentation (SVOS).\nHowever, these methods face two key limitations: 1) relying on non-local\npixel-level matching to read memory, resulting in noisy retrieved features for\nsegmentation; 2) segmenting each object independently without interaction.\nThese shortcomings make the memory-based methods struggle in similar object and\nmulti-object segmentation. To address these issues, we propose a query\nmodulation method, termed QMVOS. This method summarizes object features into\ndynamic queries and then treats them as dynamic filters for mask prediction,\nthereby providing high-level descriptions and object-level perception for the\nmodel. Efficient and effective multi-object interactions are realized through\ninter-query attention. Extensive experiments demonstrate that our method can\nbring significant improvements to the memory-based SVOS method and achieve\ncompetitive performance on standard SVOS benchmarks. The code is available at\nhttps://github.com/zht8506/QMVOS.\n","authors":["Hantao Zhou","Runze Hu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.11529v1.pdf","comment":"Accepted by ICME2024"},{"id":"http://arxiv.org/abs/2311.00318v2","updated":"2024-03-18T07:21:27Z","published":"2023-11-01T06:02:59Z","title":"Flooding Regularization for Stable Training of Generative Adversarial\n  Networks","summary":"  Generative Adversarial Networks (GANs) have shown remarkable performance in\nimage generation. However, GAN training suffers from the problem of\ninstability. One of the main approaches to address this problem is to modify\nthe loss function, often using regularization terms in addition to changing the\ntype of adversarial losses. This paper focuses on directly regularizing the\nadversarial loss function. We propose a method that applies flooding, an\noverfitting suppression method in supervised learning, to GANs to directly\nprevent the discriminator's loss from becoming excessively low. Flooding\nrequires tuning the flood level, but when applied to GANs, we propose that the\nappropriate range of flood level settings is determined by the adversarial loss\nfunction, supported by theoretical analysis of GANs using the binary cross\nentropy loss. We experimentally verify that flooding stabilizes GAN training\nand can be combined with other stabilization techniques. We also show that by\nrestricting the discriminator's loss to be no less than the flood level, the\ntraining proceeds stably even when the flood level is somewhat high.\n","authors":["Iu Yahiro","Takashi Ishida","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2311.00318v2.pdf","comment":"25 pages, 9 figures, 18 tables"},{"id":"http://arxiv.org/abs/2403.04306v2","updated":"2024-03-18T07:21:01Z","published":"2024-03-07T08:25:27Z","title":"Effectiveness Assessment of Recent Large Vision-Language Models","summary":"  The advent of large vision-language models (LVLMs) represents a noteworthy\nadvancement towards the pursuit of artificial general intelligence. However,\nthe extent of their efficacy across both specialized and general tasks warrants\nfurther investigation. This article endeavors to evaluate the competency of\npopular LVLMs in specialized and general tasks, respectively, aiming to offer a\ncomprehensive comprehension of these innovative methodologies. To gauge their\nefficacy in specialized tasks, we tailor a comprehensive testbed comprising\nthree distinct scenarios: natural, healthcare, and industrial, encompassing six\nchallenging tasks. These tasks include salient, camouflaged, and transparent\nobject detection, as well as polyp and skin lesion detection, alongside\nindustrial anomaly detection. We examine the performance of three recent\nopen-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of\nvisual recognition and localization. Moreover, we conduct empirical\ninvestigations utilizing the aforementioned models alongside GPT-4V, assessing\ntheir multi-modal understanding capacities in general tasks such as object\ncounting, absurd question answering, affordance reasoning, attribute\nrecognition, and spatial relation reasoning. Our investigations reveal that\nthese models demonstrate limited proficiency not only in specialized tasks but\nalso in general tasks. We delve deeper into this inadequacy and suggest several\npotential factors, including limited cognition in specialized tasks, object\nhallucination, text-to-image interference, and decreased robustness in complex\nproblems. We hope this study would provide valuable insights for the future\ndevelopment of LVLMs, augmenting their power in coping with both general and\nspecialized applications.\n","authors":["Yao Jiang","Xinyu Yan","Ge-Peng Ji","Keren Fu","Meijun Sun","Huan Xiong","Deng-Ping Fan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16838v2","updated":"2024-03-18T07:20:41Z","published":"2023-10-25T17:59:41Z","title":"SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous\n  Manipulation","summary":"  Humans demonstrate remarkable skill in transferring manipulation abilities\nacross objects of varying shapes, poses, and appearances, a capability rooted\nin their understanding of semantic correspondences between different instances.\nTo equip robots with a similar high-level comprehension, we present SparseDFF,\na novel DFF for 3D scenes utilizing large 2D vision models to extract semantic\nfeatures from sparse RGBD images, a domain where research is limited despite\nits relevance to many tasks with fixed-camera setups. SparseDFF generates\nview-consistent 3D DFFs, enabling efficient one-shot learning of dexterous\nmanipulations by mapping image features to a 3D point cloud. Central to\nSparseDFF is a feature refinement network, optimized with a contrastive loss\nbetween views and a point-pruning mechanism for feature continuity. This\nfacilitates the minimization of feature discrepancies w.r.t. end-effector\nparameters, bridging demonstrations and target manipulations. Validated in\nreal-world scenarios with a dexterous hand, SparseDFF proves effective in\nmanipulating both rigid and deformable objects, demonstrating significant\ngeneralization capabilities across object and scene variations.\n","authors":["Qianxu Wang","Haotong Zhang","Congyue Deng","Yang You","Hao Dong","Yixin Zhu","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2310.16838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11515v1","updated":"2024-03-18T07:01:21Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11511v1","updated":"2024-03-18T06:42:38Z","published":"2024-03-18T06:42:38Z","title":"Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation","summary":"  This paper focuses on the sim-to-real issue of RGB-D grasp detection and\nformulates it as a domain adaptation problem. In this case, we present a\nglobal-to-local method to address hybrid domain gaps in RGB and depth data and\ninsufficient multi-modal feature alignment. First, a self-supervised rotation\npre-training strategy is adopted to deliver robust initialization for RGB and\ndepth networks. We then propose a global-to-local alignment pipeline with\nindividual global domain classifiers for scene features of RGB and depth images\nas well as a local one specifically working for grasp features in the two\nmodalities. In particular, we propose a grasp prototype adaptation module,\nwhich aims to facilitate fine-grained local feature alignment by dynamically\nupdating and matching the grasp prototypes from the simulation and real-world\nscenarios throughout the training process. Due to such designs, the proposed\nmethod substantially reduces the domain shift and thus leads to consistent\nperformance improvements. Extensive experiments are conducted on the\nGraspNet-Planar benchmark and physical environment, and superior results are\nachieved which demonstrate the effectiveness of our method.\n","authors":["Haoxiang Ma","Ran Qin","Modi shi","Boyang Gao","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11511v1.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2305.12681v2","updated":"2024-03-18T06:34:01Z","published":"2023-05-22T03:38:59Z","title":"Phased Data Augmentation for Training a Likelihood-Based Generative\n  Model with Limited Data","summary":"  Generative models excel in creating realistic images, yet their dependency on\nextensive datasets for training presents significant challenges, especially in\ndomains where data collection is costly or challenging. Current data-efficient\nmethods largely focus on GAN architectures, leaving a gap in training other\ntypes of generative models. Our study introduces \"phased data augmentation\" as\na novel technique that addresses this gap by optimizing training in limited\ndata scenarios without altering the inherent data distribution. By limiting the\naugmentation intensity throughout the learning phases, our method enhances the\nmodel's ability to learn from limited data, thus maintaining fidelity. Applied\nto a model integrating PixelCNNs with VQ-VAE-2, our approach demonstrates\nsuperior performance in both quantitative and qualitative evaluations across\ndiverse datasets. This represents an important step forward in the efficient\ntraining of likelihood-based models, extending the usefulness of data\naugmentation techniques beyond just GANs.\n","authors":["Yuta Mimura"],"pdf_url":"https://arxiv.org/pdf/2305.12681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11510v1","updated":"2024-03-18T06:32:23Z","published":"2024-03-18T06:32:23Z","title":"GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel\n  Objects","summary":"  Despite the progress of learning-based methods for 6D object pose estimation,\nthe trade-off between accuracy and scalability for novel objects still exists.\nSpecifically, previous methods for novel objects do not make good use of the\ntarget object's 3D shape information since they focus on generalization by\nprocessing the shape indirectly, making them less effective. We present\nGenFlow, an approach that enables both accuracy and generalization to novel\nobjects with the guidance of the target object's shape. Our method predicts\noptical flow between the rendered image and the observed image and refines the\n6D pose iteratively. It boosts the performance by a constraint of the 3D shape\nand the generalizable geometric knowledge learned from an end-to-end\ndifferentiable system. We further improve our model by designing a cascade\nnetwork architecture to exploit the multi-scale correlations and coarse-to-fine\nrefinement. GenFlow ranked first on the unseen object pose estimation\nbenchmarks in both the RGB and RGB-D cases. It also achieves performance\ncompetitive with existing state-of-the-art methods for the seen object pose\nestimation without any fine-tuning.\n","authors":["Sungphill Moon","Hyeontae Son","Dongcheol Hur","Sangwook Kim"],"pdf_url":"https://arxiv.org/pdf/2403.11510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11507v1","updated":"2024-03-18T06:25:41Z","published":"2024-03-18T06:25:41Z","title":"Circle Representation for Medical Instance Object Segmentation","summary":"  Recently, circle representation has been introduced for medical imaging,\ndesigned specifically to enhance the detection of instance objects that are\nspherically shaped (e.g., cells, glomeruli, and nuclei). Given its outstanding\neffectiveness in instance detection, it is compelling to consider the\napplication of circle representation for segmenting instance medical objects.\nIn this study, we introduce CircleSnake, a simple end-to-end segmentation\napproach that utilizes circle contour deformation for segmenting ball-shaped\nmedical objects at the instance level. The innovation of CircleSnake lies in\nthese three areas: (1) It substitutes the complex bounding box-to-octagon\ncontour transformation with a more consistent and rotation-invariant bounding\ncircle-to-circle contour adaptation. This adaptation specifically targets\nball-shaped medical objects. (2) The circle representation employed in\nCircleSnake significantly reduces the degrees of freedom to two, compared to\neight in the octagon representation. This reduction enhances both the\nrobustness of the segmentation performance and the rotational consistency of\nthe method. (3) CircleSnake is the first end-to-end deep instance segmentation\npipeline to incorporate circle representation, encompassing consistent circle\ndetection, circle contour proposal, and circular convolution in a unified\nframework. This integration is achieved through the novel application of\ncircular graph convolution within the context of circle detection and instance\nsegmentation. In practical applications, such as the detection of glomeruli,\nnuclei, and eosinophils in pathological images, CircleSnake has demonstrated\nsuperior performance and greater rotation invariance when compared to\nbenchmarks. The code has been made publicly available:\nhttps://github.com/hrlblab/CircleSnake.\n","authors":["Juming Xiong","Ethan H. Nguyen","Yilin Liu","Ruining Deng","Regina N Tyree","Hernan Correa","Girish Hiremath","Yaohong Wang","Haichun Yang","Agnes B. Fogo","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2403.11507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11506v1","updated":"2024-03-18T06:24:46Z","published":"2024-03-18T06:24:46Z","title":"End-To-End Underwater Video Enhancement: Dataset and Model","summary":"  Underwater video enhancement (UVE) aims to improve the visibility and frame\nquality of underwater videos, which has significant implications for marine\nresearch and exploration. However, existing methods primarily focus on\ndeveloping image enhancement algorithms to enhance each frame independently.\nThere is a lack of supervised datasets and models specifically tailored for UVE\ntasks. To fill this gap, we construct the Synthetic Underwater Video\nEnhancement (SUVE) dataset, comprising 840 diverse underwater-style videos\npaired with ground-truth reference videos. Based on this dataset, we train a\nnovel underwater video enhancement model, UVENet, which utilizes inter-frame\nrelationships to achieve better enhancement performance. Through extensive\nexperiments on both synthetic and real underwater videos, we demonstrate the\neffectiveness of our approach. This study represents the first comprehensive\nexploration of UVE to our knowledge. The code is available at\nhttps://anonymous.4open.science/r/UVENet.\n","authors":["Dazhao Du","Enhan Li","Lingyu Si","Fanjiang Xu","Jianwei Niu"],"pdf_url":"https://arxiv.org/pdf/2403.11506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11505v1","updated":"2024-03-18T06:20:49Z","published":"2024-03-18T06:20:49Z","title":"Covid-19 detection from CT scans using EfficientNet and Attention\n  mechanism","summary":"  Manual diagnosis and analysis of COVID-19 through the examination of lung\nComputed Tomography (CT) scan images by physicians tends to result in\ninefficiency, especially with high patient volumes and numerous images per\npatient. We address the need for automation by developing a deep learning\nmodel-based pipeline for COVID-19 detection from CT scan images of the lungs.\nThe Domain adaptation, Explainability, and Fairness in AI for Medical Image\nAnalysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D)\nprovides an opportunity to assess our designed pipeline for COVID-19 detection\nfrom CT scan images. The proposed pipeline incorporates EfficientNet with an\nAttention mechanism with a pre-processing step. Our pipeline outperforms last\nyear's teams on the validation set of the competition dataset.\n","authors":["Ramy Farag","Parth Upadhyay","Guilhermen DeSouza"],"pdf_url":"https://arxiv.org/pdf/2403.11505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11504v1","updated":"2024-03-18T06:19:37Z","published":"2024-03-18T06:19:37Z","title":"MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray\n  Self-Supervised Representation Learning","summary":"  Self-supervised learning (SSL) is potentially useful in reducing the need for\nmanual annotation and making deep learning models accessible for medical image\nanalysis tasks. By leveraging the representations learned from unlabeled data,\nself-supervised models perform well on tasks that require little to no\nfine-tuning. However, for medical images, like chest X-rays, which are\ncharacterized by complex anatomical structures and diverse clinical conditions,\nthere arises a need for representation learning techniques that can encode\nfine-grained details while preserving the broader contextual information. In\nthis context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration\nfor Chest X-ray Self-Supervised Representation Learning), an approach to\ncapture rich representations in the form of embeddings from chest X-ray images.\nCentral to our approach is a novel multi-level variance and covariance\nexploration strategy that empowers the model to detect diagnostically\nmeaningful patterns while reducing redundancy effectively. By enhancing the\nvariance and covariance of the learned embeddings, MLVICX promotes the\nretention of critical medical insights by adapting both global and local\ncontextual details. We demonstrate the performance of MLVICX in advancing\nself-supervised chest X-ray representation learning through comprehensive\nexperiments. The performance enhancements we observe across various downstream\ntasks highlight the significance of the proposed approach in enhancing the\nutility of chest X-ray embeddings for precision medical diagnosis and\ncomprehensive image analysis. For pertaining, we used the NIH-Chest X-ray\ndataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,\nRSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more\nthan 3% performance gains over SOTA SSL approaches in various downstream tasks.\n","authors":["Azad Singh","Vandan Gorade","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.11504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11503v1","updated":"2024-03-18T06:18:59Z","published":"2024-03-18T06:18:59Z","title":"Diffusion Models are Geometry Critics: Single Image 3D Editing Using\n  Pre-Trained Diffusion Priors","summary":"  We propose a novel image editing technique that enables 3D manipulations on\nsingle images, such as object rotation and translation. Existing 3D-aware image\nediting approaches typically rely on synthetic multi-view datasets for training\nspecialized models, thus constraining their effectiveness on open-domain images\nfeaturing significantly more varied layouts and styles. In contrast, our method\ndirectly leverages powerful image diffusion models trained on a broad spectrum\nof text-image pairs and thus retain their exceptional generalization abilities.\nThis objective is realized through the development of an iterative novel view\nsynthesis and geometry alignment algorithm. The algorithm harnesses diffusion\nmodels for dual purposes: they provide appearance prior by predicting novel\nviews of the selected object using estimated depth maps, and they act as a\ngeometry critic by correcting misalignments in 3D shapes across the sampled\nviews. Our method can generate high-quality 3D-aware image edits with large\nviewpoint transformations and high appearance and shape consistency with the\ninput image, pushing the boundaries of what is possible with single-image\n3D-aware editing.\n","authors":["Ruicheng Wang","Jianfeng Xiang","Jiaolong Yang","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2403.11503v1.pdf","comment":"Project page: https://wangrc.site/DiffCriticEdit/"},{"id":"http://arxiv.org/abs/2403.11498v1","updated":"2024-03-18T06:07:45Z","published":"2024-03-18T06:07:45Z","title":"Domain Adaptation Using Pseudo Labels for COVID-19 Detection","summary":"  In response to the need for rapid and accurate COVID-19 diagnosis during the\nglobal pandemic, we present a two-stage framework that leverages pseudo labels\nfor domain adaptation to enhance the detection of COVID-19 from CT scans. By\nutilizing annotated data from one domain and non-annotated data from another,\nthe model overcomes the challenge of data scarcity and variability, common in\nemergent health crises. The innovative approach of generating pseudo labels\nenables the model to iteratively refine its learning process, thereby improving\nits accuracy and adaptability across different hospitals and medical centres.\nExperimental results on COV19-CT-DB database showcase the model's potential to\nachieve high diagnostic precision, significantly contributing to efficient\npatient management and alleviating the strain on healthcare systems. Our method\nachieves 0.92 Macro F1 Score on the validation set of Covid-19 domain\nadaptation challenge.\n","authors":["Runtian Yuan","Qingqiu Li","Junlin Hou","Jilan Xu","Yuejie Zhang","Rui Feng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11497v1","updated":"2024-03-18T06:04:02Z","published":"2024-03-18T06:04:02Z","title":"Do CLIPs Always Generalize Better than ImageNet Models?","summary":"  Large vision language models, such as CLIPs, have revolutionized modern\nmachine learning. CLIPs have demonstrated great generalizability under\ndistribution shifts, supported by an increasing body of literature. However,\nthe evaluation datasets for CLIPs are variations primarily designed for\nImageNet benchmarks, which may not fully reflect the extent to which CLIPs,\ne.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap,\nwe collect a real-world dataset called CounterAnimal that contains realistic\nspurious features found in animal photos. CounterAnimal consists of a) the\ncommon group: comprising animals on common backgrounds, and b) the counter\ngroup: including animals on unusual backgrounds. The performance drops from the\ncommon to counter groups quantify the reliance of models on spurious features\n(i.e., backgrounds) to predict the animals. We find that CLIPs trained on\neither LAION or the OpenAI data exhibit notable performance drops on the\ncounter group. Surprisingly, we observe that single-modal models trained on\nImageNet are more robust than CLIPs. We provide both theoretical and empirical\nexplanations for why CLIPs still learn spurious features. Our findings suggest\nthat distribution shifts remain an open problem for CLIPs, and one needs to be\ncautious about test setups when evaluating foundation models pre-trained on a\nsignificantly different scale and distribution.\n","authors":["Qizhou Wang","Yong Lin","Yongqiang Chen","Ludwig Schmidt","Bo Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11497v1.pdf","comment":"Qizhou Wang, Yong Lin, and Yongqiang Chen contributed equally.\n  Project page: https://counteranimal.github.io"},{"id":"http://arxiv.org/abs/2403.11494v1","updated":"2024-03-18T05:58:13Z","published":"2024-03-18T05:58:13Z","title":"CCC++: Optimized Color Classified Colorization with Segment Anything\n  Model (SAM) Empowered Object Selective Color Harmonization","summary":"  In this paper, we formulate the colorization problem into a multinomial\nclassification problem and then apply a weighted function to classes. We\npropose a set of formulas to transform color values into color classes and vice\nversa. To optimize the classes, we experiment with different bin sizes for\ncolor class transformation. Observing class appearance, standard deviation, and\nmodel parameters on various extremely large-scale real-time images in practice\nwe propose 532 color classes for our classification task. During training, we\npropose a class-weighted function based on true class appearance in each batch\nto ensure proper saturation of individual objects. We adjust the weights of the\nmajor classes, which are more frequently observed, by lowering them, while\nescalating the weights of the minor classes, which are less commonly observed.\nIn our class re-weight formula, we propose a hyper-parameter for finding the\noptimal trade-off between the major and minor appeared classes. As we apply\nregularization to enhance the stability of the minor class, occasional minor\nnoise may appear at the object's edges. We propose a novel object-selective\ncolor harmonization method empowered by the Segment Anything Model (SAM) to\nrefine and enhance these edges. We propose two new color image evaluation\nmetrics, the Color Class Activation Ratio (CCAR), and the True Activation Ratio\n(TAR), to quantify the richness of color components. We compare our proposed\nmodel with state-of-the-art models using six different dataset: Place, ADE,\nCeleba, COCO, Oxford 102 Flower, and ImageNet, in qualitative and quantitative\napproaches. The experimental results show that our proposed model outstrips\nother models in visualization, CNR and in our proposed CCAR and TAR measurement\ncriteria while maintaining satisfactory performance in regression (MSE, PSNR),\nsimilarity (SSIM, LPIPS, UIUI), and generative criteria (FID).\n","authors":["Mrityunjoy Gain","Avi Deb Raha","Rameswar Debnath"],"pdf_url":"https://arxiv.org/pdf/2403.11494v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.01476"},{"id":"http://arxiv.org/abs/2210.15563v3","updated":"2024-03-18T05:55:34Z","published":"2022-10-27T15:53:38Z","title":"Multimodal Transformer Distillation for Audio-Visual Synchronization","summary":"  Audio-visual synchronization aims to determine whether the mouth movements\nand speech in the video are synchronized. VocaLiST reaches state-of-the-art\nperformance by incorporating multimodal Transformers to model audio-visual\ninteract information. However, it requires high computing resources, making it\nimpractical for real-world applications. This paper proposed an MTDVocaLiST\nmodel, which is trained by our proposed multimodal Transformer distillation\n(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the\ncross-attention distribution and value-relation in the Transformer of VocaLiST.\nAdditionally, we harness uncertainty weighting to fully exploit the interaction\ninformation across all layers. Our proposed method is effective in two aspects:\nFrom the distillation method perspective, MTD loss outperforms other strong\ndistillation baselines. From the distilled model's performance perspective: 1)\nMTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match\nmodels by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST\nby 83.52%, yet still maintaining similar performance.\n","authors":["Xuanjun Chen","Haibin Wu","Chung-Che Wang","Hung-yi Lee","Jyh-Shing Roger Jang"],"pdf_url":"https://arxiv.org/pdf/2210.15563v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.11492v1","updated":"2024-03-18T05:53:20Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v1.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2401.13937v2","updated":"2024-03-18T05:45:07Z","published":"2024-01-25T04:39:48Z","title":"Self-supervised Video Object Segmentation with Distillation Learning of\n  Deformable Attention","summary":"  Video object segmentation is a fundamental research problem in computer\nvision. Recent techniques have often applied attention mechanism to object\nrepresentation learning from video sequences. However, due to temporal changes\nin the video data, attention maps may not well align with the objects of\ninterest across video frames, causing accumulated errors in long-term video\nprocessing. In addition, existing techniques have utilised complex\narchitectures, requiring highly computational complexity and hence limiting the\nability to integrate video object segmentation into low-powered devices. To\naddress these issues, we propose a new method for self-supervised video object\nsegmentation based on distillation learning of deformable attention.\nSpecifically, we devise a lightweight architecture for video object\nsegmentation that is effectively adapted to temporal changes. This is enabled\nby deformable attention mechanism, where the keys and values capturing the\nmemory of a video sequence in the attention module have flexible locations\nupdated across frames. The learnt object representations are thus adaptive to\nboth the spatial and temporal dimensions. We train the proposed architecture in\na self-supervised fashion through a new knowledge distillation paradigm where\ndeformable attention maps are integrated into the distillation loss. We\nqualitatively and quantitatively evaluate our method and compare it with\nexisting methods on benchmark datasets including DAVIS 2016/2017 and\nYouTube-VOS 2018/2019. Experimental results verify the superiority of our\nmethod via its achieved state-of-the-art performance and optimal memory usage.\n","authors":["Quang-Trung Truong","Duc Thanh Nguyen","Binh-Son Hua","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2401.13937v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2403.11481v1","updated":"2024-03-18T05:07:59Z","published":"2024-03-18T05:07:59Z","title":"VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding","summary":"  We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.\n","authors":["Yue Fan","Xiaojian Ma","Rujie Wu","Yuntao Du","Jiaqi Li","Zhi Gao","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.11481v1.pdf","comment":"Project page: videoagent.github.io; First two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2403.08282v2","updated":"2024-03-18T05:03:53Z","published":"2024-03-13T06:22:17Z","title":"Hierarchical Auto-Organizing System for Open-Ended Multi-Agent\n  Navigation","summary":"  Due to the dynamic and unpredictable open-world setting, navigating complex\nenvironments in Minecraft poses significant challenges for multi-agent systems.\nAgents must interact with the environment and coordinate their actions with\nother agents to achieve common objectives. However, traditional approaches\noften struggle to efficiently manage inter-agent communication and task\ndistribution, crucial for effective multi-agent navigation. Furthermore,\nprocessing and integrating multi-modal information (such as visual, textual,\nand auditory data) is essential for agents to comprehend their goals and\nnavigate the environment successfully and fully. To address this issue, we\ndesign the HAS framework to auto-organize groups of LLM-based agents to\ncomplete navigation tasks. In our approach, we devise a hierarchical\nauto-organizing navigation system, which is characterized by 1) a hierarchical\nsystem for multi-agent organization, ensuring centralized planning and\ndecentralized execution; 2) an auto-organizing and intra-communication\nmechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal\ninformation platform, facilitating multi-modal perception to perform the three\nnavigation tasks with one system. To assess organizational behavior, we design\na series of navigation tasks in the Minecraft environment, which includes\nsearching and exploring. We aim to develop embodied organizations that push the\nboundaries of embodied AI, moving it towards a more human-like organizational\nstructure.\n","authors":["Zhonghan Zhao","Kewei Chen","Dongxu Guo","Wenhao Chai","Tian Ye","Yanting Zhang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08282v2.pdf","comment":"ICLR 2024 Workshop on LLM Agents"},{"id":"http://arxiv.org/abs/2403.11480v1","updated":"2024-03-18T05:03:07Z","published":"2024-03-18T05:03:07Z","title":"Towards understanding the nature of direct functional connectivity in\n  visual brain network","summary":"  Recent advances in neuroimaging have enabled studies in functional\nconnectivity (FC) of human brain, alongside investigation of the neuronal basis\nof cognition. One important FC study is the representation of vision in human\nbrain. The release of publicly available dataset BOLD5000 has made it possible\nto study the brain dynamics during visual tasks in greater detail. In this\npaper, a comprehensive analysis of fMRI time series (TS) has been performed to\nexplore different types of visual brain networks (VBN). The novelty of this\nwork lies in (1) constructing VBN with consistently significant direct\nconnectivity using both marginal and partial correlation, which is further\nanalyzed using graph theoretic measures, (2) classification of VBNs as formed\nby image complexity-specific TS, using graphical features. In image\ncomplexity-specific VBN classification, XGBoost yields average accuracy in the\nrange of 86.5% to 91.5% for positively correlated VBN, which is 2% greater than\nthat using negative correlation. This result not only reflects the\ndistinguishing graphical characteristics of each image complexity-specific VBN,\nbut also highlights the importance of studying both positively correlated and\nnegatively correlated VBN to understand the how differently brain functions\nwhile viewing different complexities of real-world images.\n","authors":["Debanjali Bhattacharya","Neelam Sinha"],"pdf_url":"https://arxiv.org/pdf/2403.11480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11469v1","updated":"2024-03-18T04:41:59Z","published":"2024-03-18T04:41:59Z","title":"Generative Motion Stylization within Canonical Motion Space","summary":"  Stylized motion breathes life into characters. However, the fixed skeleton\nstructure and style representation hinder existing data-driven motion synthesis\nmethods from generating stylized motion for various characters. In this work,\nwe propose a generative motion stylization pipeline, named MotionS, for\nsynthesizing diverse and stylized motion on cross-structure characters using\ncross-modality style prompts. Our key insight is to embed motion style into a\ncross-modality latent space and perceive the cross-structure skeleton\ntopologies, allowing for motion stylization within a canonical motion space.\nSpecifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP)\nmodel is leveraged to construct the cross-modality latent space, enabling\nflexible style representation within this space. Additionally, two\ntopology-encoded tokens are learned to capture the canonical and specific\nskeleton topologies, facilitating cross-structure topology shifting.\nSubsequently, the topology-shifted stylization diffusion is designed to\ngenerate motion content for the specific skeleton and stylize it in the shifted\ncanonical motion space using multi-modality style descriptions. Through an\nextensive set of examples, we demonstrate the flexibility and generalizability\nof our pipeline across various characters and style descriptions. Qualitative\nand quantitative experiments underscore the superiority of our pipeline over\nstate-of-the-art methods, consistently delivering high-quality stylized motion\nacross a broad spectrum of skeletal structures.\n","authors":["Jiaxu Zhang","Xin Chen","Gang Yu","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2403.11469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11468v1","updated":"2024-03-18T04:41:38Z","published":"2024-03-18T04:41:38Z","title":"Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V","summary":"  Recent advancements in generative AI have suggested that by taking visual\nprompt, GPT-4V can demonstrate significant proficiency in image recognition\ntask. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier for its wide use. To address\nthis challenge, our work introduces Collage Prompting, a budget-friendly\nprompting approach that concatenates multiple images into a single visual\ninput. With collage prompt, GPT-4V is able to perform image recognition on\nseveral images simultaneously. Based on the observation that the accuracy of\nGPT-4V's image recognition varies significantly with the order of images within\nthe collage prompt, our method further learns to optimize the arrangement of\nimages for maximum recognition accuracy. A graph predictor is trained to\nindicate the accuracy of each collage prompt, then we propose an optimization\nmethod to navigate the search space of possible image arrangements. Experiment\nresults across various datasets demonstrate the cost-efficiency score of\ncollage prompt is much larger than standard prompt. Additionally, collage\nprompt with learned arrangement achieves clearly better accuracy than collage\nprompt with random arrangement in GPT-4V's visual recognition.\n","authors":["Siyu Xu","Yunke Wang","Daochang Liu","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.11468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17555v3","updated":"2024-03-18T04:34:27Z","published":"2023-05-27T19:10:19Z","title":"Diffeomorphic Mesh Deformation via Efficient Optimal Transport for\n  Cortical Surface Reconstruction","summary":"  Mesh deformation plays a pivotal role in many 3D vision tasks including\ndynamic simulations, rendering, and reconstruction. However, defining an\nefficient discrepancy between predicted and target meshes remains an open\nproblem. A prevalent approach in current deep learning is the set-based\napproach which measures the discrepancy between two surfaces by comparing two\nrandomly sampled point-clouds from the two meshes with Chamfer pseudo-distance.\nNevertheless, the set-based approach still has limitations such as lacking a\ntheoretical guarantee for choosing the number of points in sampled\npoint-clouds, and the pseudo-metricity and the quadratic complexity of the\nChamfer divergence. To address these issues, we propose a novel metric for\nlearning mesh deformation. The metric is defined by sliced Wasserstein distance\non meshes represented as probability measures that generalize the set-based\napproach. By leveraging probability measure space, we gain flexibility in\nencoding meshes using diverse forms of probability measures, such as\ncontinuous, empirical, and discrete measures via varifold representation. After\nhaving encoded probability measures, we can compare meshes by using the sliced\nWasserstein distance which is an effective optimal transport distance with\nlinear computational complexity and can provide a fast statistical rate for\napproximating the surface of meshes. To the end, we employ a neural ordinary\ndifferential equation (ODE) to deform the input surface into the target shape\nby modeling the trajectories of the points on the surface. Our experiments on\ncortical surface reconstruction demonstrate that our approach surpasses other\ncompeting methods in multiple datasets and metrics.\n","authors":["Tung Le","Khai Nguyen","Shanlin Sun","Kun Han","Nhat Ho","Xiaohui Xie"],"pdf_url":"https://arxiv.org/pdf/2305.17555v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.11463v1","updated":"2024-03-18T04:30:31Z","published":"2024-03-18T04:30:31Z","title":"Siamese Learning with Joint Alignment and Regression for\n  Weakly-Supervised Video Paragraph Grounding","summary":"  Video Paragraph Grounding (VPG) is an emerging task in video-language\nunderstanding, which aims at localizing multiple sentences with semantic\nrelations and temporal order from an untrimmed video. However, existing VPG\napproaches are heavily reliant on a considerable number of temporal labels that\nare laborious and time-consuming to acquire. In this work, we introduce and\nexplore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the\nneed of temporal annotations. Different from previous weakly-supervised\ngrounding frameworks based on multiple instance learning or reconstruction\nlearning for two-stage candidate ranking, we propose a novel siamese learning\nframework that jointly learns the cross-modal feature alignment and temporal\ncoordinate regression without timestamp labels to achieve concise one-stage\nlocalization for WSVPG. Specifically, we devise a Siamese Grounding TRansformer\n(SiamGTR) consisting of two weight-sharing branches for learning complementary\nsupervision. An Augmentation Branch is utilized for directly regressing the\ntemporal boundaries of a complete paragraph within a pseudo video, and an\nInference Branch is designed to capture the order-guided feature correspondence\nfor localizing multiple sentences in a normal video. We demonstrate by\nextensive experiments that our paradigm has superior practicability and\nflexibility to achieve efficient weakly-supervised or semi-supervised learning,\noutperforming state-of-the-art methods trained with the same or stronger\nsupervision.\n","authors":["Chaolei Tan","Jianhuang Lai","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11463v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11460v1","updated":"2024-03-18T04:26:18Z","published":"2024-03-18T04:26:18Z","title":"Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning","summary":"  In this work, we present Fed3DGS, a scalable 3D reconstruction framework\nbased on 3D Gaussian splatting (3DGS) with federated learning. Existing\ncity-scale reconstruction methods typically adopt a centralized approach, which\ngathers all data in a central server and reconstructs scenes. The approach\nhampers scalability because it places a heavy load on the server and demands\nextensive data storage when reconstructing scenes on a scale beyond city-scale.\nIn pursuit of a more scalable 3D reconstruction, we propose a federated\nlearning framework with 3DGS, which is a decentralized framework and can\npotentially use distributed computational resources across millions of clients.\nWe tailor a distillation-based model update scheme for 3DGS and introduce\nappearance modeling for handling non-IID data in the scenario of 3D\nreconstruction with federated learning. We simulate our method on several\nlarge-scale benchmarks, and our method demonstrates rendered image quality\ncomparable to centralized approaches. In addition, we also simulate our method\nwith data collected in different seasons, demonstrating that our framework can\nreflect changes in the scenes and our appearance modeling captures changes due\nto seasonal variations.\n","authors":["Teppei Suzuki"],"pdf_url":"https://arxiv.org/pdf/2403.11460v1.pdf","comment":"Code: https://github.com/DensoITLab/Fed3DGS"},{"id":"http://arxiv.org/abs/2310.08044v2","updated":"2024-03-18T04:25:43Z","published":"2023-10-12T05:34:45Z","title":"EC-Depth: Exploring the consistency of self-supervised monocular depth\n  estimation in challenging scenes","summary":"  Self-supervised monocular depth estimation holds significant importance in\nthe fields of autonomous driving and robotics. However, existing methods are\ntypically trained and tested on standard datasets, overlooking the impact of\nvarious adverse conditions prevalent in real-world applications, such as rainy\ndays. As a result, it is commonly observed that these methods struggle to\nhandle these challenging scenarios. To address this issue, we present EC-Depth,\na novel self-supervised two-stage framework to achieve a robust depth\nestimation. In the first stage, we propose depth consistency regularization to\npropagate reliable supervision from standard to challenging scenes. In the\nsecond stage, we adopt the Mean Teacher paradigm and propose a novel\nconsistency-based pseudo-label filtering strategy to improve the quality of\npseudo-labels, further improving both the accuracy and robustness of our model.\nExtensive experiments demonstrate that our method achieves accurate and\nconsistent depth predictions in both standard and challenging scenarios,\nsurpassing existing state-of-the-art methods on KITTI, KITTI-C, DrivingStereo,\nand NuScenes-Night benchmarks.\n","authors":["Ziyang Song","Ruijie Zhu","Chuxin Wang","Jiacheng Deng","Jianfeng He","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08044v2.pdf","comment":"Project page: https://ruijiezhu94.github.io/ECDepth_page"},{"id":"http://arxiv.org/abs/2306.10900v2","updated":"2024-03-18T04:14:50Z","published":"2023-06-19T12:58:17Z","title":"MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators","summary":"  Generating realistic human motion from given action descriptions has\nexperienced significant advancements because of the emerging requirement of\ndigital humans. While recent works have achieved impressive results in\ngenerating motion directly from textual action descriptions, they often support\nonly a single modality of the control signal, which limits their application in\nthe real digital human industry. This paper presents a Motion General-Purpose\ngeneraTor (MotionGPT) that can use multimodal control signals, e.g., text and\nsingle-frame poses, for generating consecutive human motions by treating\nmultimodal signals as special input tokens in large language models (LLMs).\nSpecifically, we first quantize multimodal control signals into discrete codes\nand then formulate them in a unified prompt instruction to ask the LLMs to\ngenerate the motion answer. Our MotionGPT demonstrates a unified human motion\ngeneration model with multimodal control signals by tuning a mere 0.4% of LLM\nparameters. To the best of our knowledge, MotionGPT is the first method to\ngenerate human motion by multimodal control signals, which we hope can shed\nlight on this new direction. Visit our webpage at\nhttps://qiqiapink.github.io/MotionGPT/.\n","authors":["Yaqi Zhang","Di Huang","Bin Liu","Shixiang Tang","Yan Lu","Lu Chen","Lei Bai","Qi Chu","Nenghai Yu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2306.10900v2.pdf","comment":"18 pages, 8 figures, accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.14134v2","updated":"2024-03-18T04:08:54Z","published":"2023-12-21T18:55:05Z","title":"Diffusion Reward: Learning Rewards via Conditional Video Diffusion","summary":"  Learning rewards from expert videos offers an affordable and effective\nsolution to specify the intended behaviors for reinforcement learning tasks. In\nthis work, we propose Diffusion Reward, a novel framework that learns rewards\nfrom expert videos via conditional video diffusion models for solving complex\nvisual RL problems. Our key insight is that lower generative diversity is\nobserved when conditioned on expert trajectories. Diffusion Reward is\naccordingly formalized by the negative of conditional entropy that encourages\nproductive exploration of expert-like behaviors. We show the efficacy of our\nmethod over 10 robotic manipulation tasks from MetaWorld and Adroit with visual\ninput and sparse reward. Moreover, Diffusion Reward could even solve unseen\ntasks successfully and effectively, largely surpassing baseline methods.\nProject page and code: https://diffusion-reward.github.io/.\n","authors":["Tao Huang","Guangqi Jiang","Yanjie Ze","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2312.14134v2.pdf","comment":"Project page and code: https://diffusion-reward.github.io/"},{"id":"http://arxiv.org/abs/2311.10319v3","updated":"2024-03-18T04:01:53Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages the advancements in self-supervised\nand semi-supervised learning. These techniques engage in auxiliary tasks that\ndo not require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that\nself-supervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v3.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.11453v1","updated":"2024-03-18T04:01:26Z","published":"2024-03-18T04:01:26Z","title":"Bridging 3D Gaussian and Mesh for Freeview Video Rendering","summary":"  This is only a preview version of GauMesh. Recently, primitive-based\nrendering has been proven to achieve convincing results in solving the problem\nof modeling and rendering the 3D dynamic scene from 2D images. Despite this, in\nthe context of novel view synthesis, each type of primitive has its inherent\ndefects in terms of representation ability. It is difficult to exploit the mesh\nto depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D\nGaussian Splatting) method usually produces artifacts or blurry pixels in the\narea with smooth geometry and sharp textures. As a result, it is difficult,\neven not impossible, to represent the complex and dynamic scene with a single\ntype of primitive. To this end, we propose a novel approach, GauMesh, to bridge\nthe 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a\nsequence of tracked mesh as initialization, our goal is to simultaneously\noptimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,\nand the deformation field. At a specific time, we perform $\\alpha$-blending on\nthe RGB and opacity values based on the merged and re-ordered z-buffers from\nmesh and 3D Gaussian rasterizations. This produces the final rendering, which\nis supervised by the ground-truth image. Experiments demonstrate that our\napproach adapts the appropriate type of primitives to represent the different\nparts of the dynamic scene and outperforms all the baseline methods in both\nquantitative and qualitative comparisons without losing render speed.\n","authors":["Yuting Xiao","Xuan Wang","Jiafei Li","Hongrui Cai","Yanbo Fan","Nan Xue","Minghui Yang","Yujun Shen","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.11453v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.11451v1","updated":"2024-03-18T03:59:43Z","published":"2024-03-18T03:59:43Z","title":"CasSR: Activating Image Power for Real-World Image Super-Resolution","summary":"  The objective of image super-resolution is to generate clean and\nhigh-resolution images from degraded versions. Recent advancements in diffusion\nmodeling have led to the emergence of various image super-resolution techniques\nthat leverage pretrained text-to-image (T2I) models. Nevertheless, due to the\nprevalent severe degradation in low-resolution images and the inherent\ncharacteristics of diffusion models, achieving high-fidelity image restoration\nremains challenging. Existing methods often exhibit issues including semantic\nloss, artifacts, and the introduction of spurious content not present in the\noriginal image. To tackle this challenge, we propose Cascaded diffusion for\nSuper-Resolution, CasSR , a novel method designed to produce highly detailed\nand realistic images. In particular, we develop a cascaded controllable\ndiffusion model that aims to optimize the extraction of information from\nlow-resolution images. This model generates a preliminary reference image to\nfacilitate initial information extraction and degradation mitigation.\nFurthermore, we propose a multi-attention mechanism to enhance the T2I model's\ncapability in maximizing the restoration of the original image content. Through\na comprehensive blend of qualitative and quantitative analyses, we substantiate\nthe efficacy and superiority of our approach.\n","authors":["Haolan Chen","Jinhua Hao","Kai Zhao","Kun Yuan","Ming Sun","Chao Zhou","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11450v1","updated":"2024-03-18T03:59:24Z","published":"2024-03-18T03:59:24Z","title":"Zero-shot Compound Expression Recognition with Visual Language Model at\n  the 6th ABAW Challenge","summary":"  Conventional approaches to facial expression recognition primarily focus on\nthe classification of six basic facial expressions. Nevertheless, real-world\nsituations present a wider range of complex compound expressions that consist\nof combinations of these basics ones due to limited availability of\ncomprehensive training datasets. The 6th Workshop and Competition on Affective\nBehavior Analysis in-the-wild (ABAW) offered unlabeled datasets containing\ncompound expressions. In this study, we propose a zero-shot approach for\nrecognizing compound expressions by leveraging a pretrained visual language\nmodel integrated with some traditional CNN networks.\n","authors":["Jiahe Wang","Jiale Huang","Bingzhao Cai","Yifan Cao","Xin Yun","Shangfei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11450v1.pdf","comment":"USTC-AC's paper for Compound Expression (CE) Recognition Challenge in\n  6th Workshop and Competition on Affective Behavior Analysis in-the-wild\n  (ABAW)"},{"id":"http://arxiv.org/abs/2403.11448v1","updated":"2024-03-18T03:54:01Z","published":"2024-03-18T03:54:01Z","title":"Robust Overfitting Does Matter: Test-Time Adversarial Purification With\n  FGSM","summary":"  Numerous studies have demonstrated the susceptibility of deep neural networks\n(DNNs) to subtle adversarial perturbations, prompting the development of many\nadvanced adversarial defense methods aimed at mitigating adversarial attacks.\nCurrent defense strategies usually train DNNs for a specific adversarial attack\nmethod and can achieve good robustness in defense against this type of\nadversarial attack. Nevertheless, when subjected to evaluations involving\nunfamiliar attack modalities, empirical evidence reveals a pronounced\ndeterioration in the robustness of DNNs. Meanwhile, there is a trade-off\nbetween the classification accuracy of clean examples and adversarial examples.\nMost defense methods often sacrifice the accuracy of clean examples in order to\nimprove the adversarial robustness of DNNs. To alleviate these problems and\nenhance the overall robust generalization of DNNs, we propose the Test-Time\nPixel-Level Adversarial Purification (TPAP) method. This approach is based on\nthe robust overfitting characteristic of DNNs to the fast gradient sign method\n(FGSM) on training and test datasets. It utilizes FGSM for adversarial\npurification, to process images for purifying unknown adversarial perturbations\nfrom pixels at testing time in a \"counter changes with changelessness\" manner,\nthereby enhancing the defense capability of DNNs against various unknown\nadversarial attacks. Extensive experimental results show that our method can\neffectively improve both overall robust generalization of DNNs, notably over\nprevious methods.\n","authors":["Linyu Tang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11448v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11447v1","updated":"2024-03-18T03:46:26Z","published":"2024-03-18T03:46:26Z","title":"Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene\n  Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene\nreconstruction. However, existing methods focus mainly on extending static 3DGS\ninto a time-variant representation, while overlooking the rich motion\ninformation carried by 2D observations, thus suffering from performance\ndegradation and model redundancy. To address the above problem, we propose a\nnovel motion-aware enhancement framework for dynamic scene reconstruction,\nwhich mines useful motion cues from optical flow to improve different paradigms\nof dynamic 3DGS. Specifically, we first establish a correspondence between 3D\nGaussian movements and pixel-level flow. Then a novel flow augmentation method\nis introduced with additional insights into uncertainty and loss collaboration.\nMoreover, for the prevalent deformation-based paradigm that presents a harder\noptimization problem, a transient-aware deformation auxiliary module is\nproposed. We conduct extensive experiments on both multi-view and monocular\nscenes to verify the merits of our work. Compared with the baselines, our\nmethod shows significant superiority in both rendering quality and efficiency.\n","authors":["Zhiyang Guo","Wengang Zhou","Li Li","Min Wang","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.11447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11490v5","updated":"2024-03-18T03:41:09Z","published":"2023-05-19T07:44:39Z","title":"LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and\n  Generation","summary":"  Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.\n","authors":["Suhyeon Lee","Won Jun Kim","Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2305.11490v5.pdf","comment":"21 pages, 8 figures; ICLR 2024 (poster)"},{"id":"http://arxiv.org/abs/2403.09065v2","updated":"2024-03-18T03:37:54Z","published":"2024-03-14T03:12:02Z","title":"When Semantic Segmentation Meets Frequency Aliasing","summary":"  Despite recent advancements in semantic segmentation, where and what pixels\nare hard to segment remains largely unexplored. Existing research only\nseparates an image into easy and hard regions and empirically observes the\nlatter are associated with object boundaries. In this paper, we conduct a\ncomprehensive analysis of hard pixel errors, categorizing them into three\ntypes: false responses, merging mistakes, and displacements. Our findings\nreveal a quantitative association between hard pixels and aliasing, which is\ndistortion caused by the overlapping of frequency components in the Fourier\ndomain during downsampling. To identify the frequencies responsible for\naliasing, we propose using the equivalent sampling rate to calculate the\nNyquist frequency, which marks the threshold for aliasing. Then, we introduce\nthe aliasing score as a metric to quantify the extent of aliasing. While\npositively correlated with the proposed aliasing score, three types of hard\npixels exhibit different patterns. Here, we propose two novel de-aliasing\nfilter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\ndegradation by accurately removing or adjusting frequencies higher than the\nNyquist frequency. The DAF precisely removes the frequencies responsible for\naliasing before downsampling, while the FreqMix dynamically selects\nhigh-frequency components within the encoder block. Experimental results\ndemonstrate consistent improvements in semantic segmentation and low-light\ninstance segmentation tasks. The code is available at:\nhttps://github.com/Linwei-Chen/Seg-Aliasing.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09065v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2310.18917v4","updated":"2024-03-18T03:37:31Z","published":"2023-10-29T06:10:46Z","title":"TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields","summary":"  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. In this paper, we propose a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, tracking process and mapping process, are\nsimultaneously maintained in our framework. For the tracking process, all input\nimages are uniformly sampled, then progressively trained in a self-supervised\nparadigm. For the mapping process, we leverage motion masks to distinguish\ndynamic objects from static background, and sample more pixels from dynamic\nareas. Secondly, the parameter optimization for both processes consists of two\nstages: the first stage associates time with 3D positions to convert the\ndeformation field to the canonical field. And the second stage associates time\nwith the embeddings of canonical field to obtain colors and Signed Distance\nFunction (SDF). Lastly, we propose a novel keyframe selection strategy based on\nthe overlapping rate. We evaluate our approach on two synthetic datasets and\none real-world dataset. And the experiments validate that our method achieves\ncompetitive results in both tracking and mapping when compared to existing\nstate-of-the-art NeRF-based methods.\n","authors":["Chengyao Duan","Zhiliu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.18917v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11440v1","updated":"2024-03-18T03:28:01Z","published":"2024-03-18T03:28:01Z","title":"Boosting Continuous Emotion Recognition with Self-Pretraining using\n  Masked Autoencoders, Temporal Convolutional Networks, and Transformers","summary":"  Human emotion recognition holds a pivotal role in facilitating seamless\nhuman-computer interaction. This paper delineates our methodology in tackling\nthe Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification\nChallenge, and Action Unit (AU) Detection Challenge within the ambit of the 6th\nWorkshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our\nstudy advocates a novel approach aimed at refining continuous emotion\nrecognition. We achieve this by initially harnessing pre-training with Masked\nAutoencoders (MAE) on facial datasets, followed by fine-tuning on the aff-wild2\ndataset annotated with expression (Expr) labels. The pre-trained model serves\nas an adept visual feature extractor, thereby enhancing the model's robustness.\nFurthermore, we bolster the performance of continuous emotion recognition by\nintegrating Temporal Convolutional Network (TCN) modules and Transformer\nEncoder modules into our framework.\n","authors":["Weiwei Zhou","Jiada Lu","Chenkun Ling","Weifeng Wang","Shaowei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19160v2","updated":"2024-03-18T03:26:02Z","published":"2024-02-29T13:44:19Z","title":"Effective Message Hiding with Order-Preserving Mechanisms","summary":"  Message hiding, a technique that conceals secret message bits within a cover\nimage, aims to achieve an optimal balance among message capacity, recovery\naccuracy, and imperceptibility. While convolutional neural networks have\nnotably improved message capacity and imperceptibility, achieving high recovery\naccuracy remains challenging. This challenge arises because convolutional\noperations struggle to preserve the sequential order of message bits and\neffectively address the discrepancy between these two modalities. To address\nthis, we propose StegaFormer, an innovative MLP-based framework designed to\npreserve bit order and enable global fusion between modalities. Specifically,\nStegaFormer incorporates three crucial components: Order-Preserving Message\nEncoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and\nOPMD aim to preserve the order of message bits by segmenting the entire\nsequence into equal-length segments and incorporating sequential information\nduring encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion\nmechanism to effectively fuse the features from the two uncorrelated\nmodalities. Experimental results on the COCO and DIV2K datasets demonstrate\nthat StegaFormer surpasses existing state-of-the-art methods in terms of\nrecovery accuracy, message capacity, and imperceptibility. We will make our\ncode publicly available.\n","authors":["Gao Yu","Qiu Xuchong","Ye Zihan"],"pdf_url":"https://arxiv.org/pdf/2402.19160v2.pdf","comment":"7 Pages"},{"id":"http://arxiv.org/abs/2402.16594v3","updated":"2024-03-18T03:21:31Z","published":"2024-02-26T14:18:12Z","title":"CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition","summary":"  To achieve greater accuracy, hypergraph matching algorithms require\nexponential increases in computational resources. Recent kd-tree-based\napproximate nearest neighbor (ANN) methods, despite the sparsity of their\ncompatibility tensor, still require exhaustive calculations for large-scale\ngraph matching. This work utilizes CUR tensor decomposition and introduces a\nnovel cascaded second and third-order hypergraph matching framework (CURSOR)\nfor efficient hypergraph matching. A CUR-based second-order graph matching\nalgorithm is used to provide a rough match, and then the core of CURSOR, a\nfiber-CUR-based tensor generation method, directly calculates entries of the\ncompatibility tensor by leveraging the initial second-order match result. This\nsignificantly decreases the time complexity and tensor density. A probability\nrelaxation labeling (PRL)-based matching algorithm, especially suitable for\nsparse tensors, is developed. Experiment results on large-scale synthetic\ndatasets and widely-adopted benchmark sets demonstrate the superiority of\nCURSOR over existing methods. The tensor generation method in CURSOR can be\nintegrated seamlessly into existing hypergraph matching methods to improve\ntheir performance and lower their computational costs.\n","authors":["Qixuan Zheng","Ming Zhang","Hong Yan"],"pdf_url":"https://arxiv.org/pdf/2402.16594v3.pdf","comment":"Accepted to CVPR 2024. The final camera-ready version. 15 pages with\n  supplementary materials and 11 figures. Minor grammarly and syntax errors\n  fixed. Irrelavant hyperrefs removed. Authorship information amended"},{"id":"http://arxiv.org/abs/2403.09136v2","updated":"2024-03-18T03:18:37Z","published":"2024-03-14T07:21:46Z","title":"Biophysics Informed Pathological Regularisation for Brain Tumour\n  Segmentation","summary":"  Recent advancements in deep learning have significantly improved brain tumour\nsegmentation techniques; however, the results still lack confidence and\nrobustness as they solely consider image data without biophysical priors or\npathological information. Integrating biophysics-informed regularisation is one\neffective way to change this situation, as it provides an prior regularisation\nfor automated end-to-end learning. In this paper, we propose a novel approach\nthat designs brain tumour growth Partial Differential Equation (PDE) models as\na regularisation with deep learning, operational with any network model. Our\nmethod introduces tumour growth PDE models directly into the segmentation\nprocess, improving accuracy and robustness, especially in data-scarce\nscenarios. This system estimates tumour cell density using a periodic\nactivation function. By effectively integrating this estimation with\nbiophysical models, we achieve a better capture of tumour characteristics. This\napproach not only aligns the segmentation closer to actual biological behaviour\nbut also strengthens the model's performance under limited data conditions. We\ndemonstrate the effectiveness of our framework through extensive experiments on\nthe BraTS 2023 dataset, showcasing significant improvements in both precision\nand reliability of tumour segmentation.\n","authors":["Lipei Zhang","Yanqi Cheng","Lihao Liu","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2403.09136v2.pdf","comment":"11 pages, 4 figures and 1 table"},{"id":"http://arxiv.org/abs/2401.08178v2","updated":"2024-03-18T03:15:39Z","published":"2024-01-16T07:51:00Z","title":"Key-point Guided Deformable Image Manipulation Using Diffusion Model","summary":"  In this paper, we introduce a Key-point-guided Diffusion probabilistic Model\n(KDM) that gains precise control over images by manipulating the object's\nkey-point. We propose a two-stage generative model incorporating an optical\nflow map as an intermediate output. By doing so, a dense pixel-wise\nunderstanding of the semantic relation between the image and sparse key point\nis configured, leading to more realistic image generation. Additionally, the\nintegration of optical flow helps regulate the inter-frame variance of\nsequential images, demonstrating an authentic sequential image generation. The\nKDM is evaluated with diverse key-point conditioned image synthesis tasks,\nincluding facial image generation, human pose synthesis, and echocardiography\nvideo prediction, demonstrating the KDM is proving consistency enhanced and\nphoto-realistic images compared with state-of-the-art models.\n","authors":["Seok-Hwan Oh","Guil Jung","Myeong-Gee Kim","Sang-Yun Kim","Young-Min Kim","Hyeon-Jik Lee","Hyuk-Sool Kwon","Hyeon-Min Bae"],"pdf_url":"https://arxiv.org/pdf/2401.08178v2.pdf","comment":"1. The ideas and approaches for the existing network have undergone\n  significant revisions, along with changes in the dataset, resulting in an\n  overall overhaul. I am planning to upload the newly written paper. 2. All\n  authors have agreed to these decisions"},{"id":"http://arxiv.org/abs/2401.14846v2","updated":"2024-03-18T03:09:57Z","published":"2024-01-26T13:27:15Z","title":"Understanding Domain Generalization: A Noise Robustness Perspective","summary":"  Despite the rapid development of machine learning algorithms for domain\ngeneralization (DG), there is no clear empirical evidence that the existing DG\nalgorithms outperform the classic empirical risk minimization (ERM) across\nstandard benchmarks. To better understand this phenomenon, we investigate\nwhether there are benefits of DG algorithms over ERM through the lens of label\nnoise. Specifically, our finite-sample analysis reveals that label noise\nexacerbates the effect of spurious correlations for ERM, undermining\ngeneralization. Conversely, we illustrate that DG algorithms exhibit implicit\nlabel-noise robustness during finite-sample training even when spurious\ncorrelation is present. Such desirable property helps mitigate spurious\ncorrelations and improve generalization in synthetic experiments. However,\nadditional comprehensive experiments on real-world benchmark datasets indicate\nthat label-noise robustness does not necessarily translate to better\nperformance compared to ERM. We conjecture that the failure mode of ERM arising\nfrom spurious correlations may be less pronounced in practice.\n","authors":["Rui Qiao","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2401.14846v2.pdf","comment":"Accepted to the 12th International Conference on Learning\n  Representations (ICLR 2024). Code is available at\n  https://github.com/qiaoruiyt/NoiseRobustDG"},{"id":"http://arxiv.org/abs/2403.06467v2","updated":"2024-03-18T02:56:26Z","published":"2024-03-11T07:07:39Z","title":"Point Mamba: A Novel Point Cloud Backbone Based on State Space Model\n  with Octree-Based Ordering Strategy","summary":"  Recently, state space model (SSM) has gained great attention due to its\npromising performance, linear complexity, and long sequence modeling ability in\nboth language and image domains. However, it is non-trivial to extend SSM to\nthe point cloud field, because of the causality requirement of SSM and the\ndisorder and irregularity nature of point clouds. In this paper, we propose a\nnovel SSM-based point cloud processing backbone, named Point Mamba, with a\ncausality-aware ordering mechanism. To construct the causal dependency\nrelationship, we design an octree-based ordering strategy on raw irregular\npoints, globally sorting points in a z-order sequence and also retaining their\nspatial proximity. Our method achieves state-of-the-art performance compared\nwith transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU\nrespectively on the ModelNet40 classification dataset and ScanNet semantic\nsegmentation dataset. Furthermore, our Point Mamba has linear complexity, which\nis more efficient than transformer-based methods. Our method demonstrates the\ngreat potential that SSM can serve as a generic backbone in point cloud\nunderstanding. Codes are released at https://github.com/IRMVLab/Point-Mamba.\n","authors":["Jiuming Liu","Ruiji Yu","Yian Wang","Yu Zheng","Tianchen Deng","Weicai Ye","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07030v2","updated":"2024-03-18T02:45:04Z","published":"2024-03-11T03:34:14Z","title":"AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge\n  Distillation","summary":"  Due to privacy or patent concerns, a growing number of large models are\nreleased without granting access to their training data, making transferring\ntheir knowledge inefficient and problematic. In response, Data-Free Knowledge\nDistillation (DFKD) methods have emerged as direct solutions. However, simply\nadopting models derived from DFKD for real-world applications suffers\nsignificant performance degradation, due to the discrepancy between teachers'\ntraining data and real-world scenarios (student domain). The degradation stems\nfrom the portions of teachers' knowledge that are not applicable to the student\ndomain. They are specific to the teacher domain and would undermine students'\nperformance. Hence, selectively transferring teachers' appropriate knowledge\nbecomes the primary challenge in DFKD. In this work, we propose a simple but\neffective method AuG-KD. It utilizes an uncertainty-guided and sample-specific\nanchor to align student-domain data with the teacher domain and leverages a\ngenerative method to progressively trade off the learning process between OOD\nknowledge distillation and domain-specific information learning via mixup\nlearning. Extensive experiments in 3 datasets and 8 settings demonstrate the\nstability and superiority of our approach. Code available at\nhttps://github.com/IshiKura-a/AuG-KD .\n","authors":["Zihao Tang","Zheqi Lv","Shengyu Zhang","Yifan Zhou","Xinyu Duan","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.07030v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2310.04152v2","updated":"2024-03-18T02:44:59Z","published":"2023-10-06T10:55:34Z","title":"Improving Neural Radiance Field using Near-Surface Sampling with Point\n  Cloud Generation","summary":"  Neural radiance field (NeRF) is an emerging view synthesis method that\nsamples points in a three-dimensional (3D) space and estimates their existence\nand color probabilities. The disadvantage of NeRF is that it requires a long\ntraining time since it samples many 3D points. In addition, if one samples\npoints from occluded regions or in the space where an object is unlikely to\nexist, the rendering quality of NeRF can be degraded. These issues can be\nsolved by estimating the geometry of 3D scene. This paper proposes a\nnear-surface sampling framework to improve the rendering quality of NeRF. To\nthis end, the proposed method estimates the surface of a 3D object using depth\nimages of the training set and sampling is performed around there only. To\nobtain depth information on a novel view, the paper proposes a 3D point cloud\ngeneration method and a simple refining method for projected depth from a point\ncloud. Experimental results show that the proposed near-surface sampling NeRF\nframework can significantly improve the rendering quality, compared to the\noriginal NeRF and three different state-of-the-art NeRF. In addition, one can\nsignificantly accelerate the training time of a NeRF model with the proposed\nnear-surface sampling framework.\n","authors":["Hye Bin Yoo","Hyun Min Han","Sung Soo Hwang","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2310.04152v2.pdf","comment":"14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.11427v1","updated":"2024-03-18T02:44:46Z","published":"2024-03-18T02:44:46Z","title":"BAGS: Building Animatable Gaussian Splatting from a Monocular Video with\n  Diffusion Priors","summary":"  Animatable 3D reconstruction has significant applications across various\nfields, primarily relying on artists' handcraft creation. Recently, some\nstudies have successfully constructed animatable 3D models from monocular\nvideos. However, these approaches require sufficient view coverage of the\nobject within the input video and typically necessitate significant time and\ncomputational costs for training and rendering. This limitation restricts the\npractical applications. In this work, we propose a method to build animatable\n3D Gaussian Splatting from monocular video with diffusion priors. The 3D\nGaussian representations significantly accelerate the training and rendering\nprocess, and the diffusion priors allow the method to learn 3D models with\nlimited viewpoints. We also present the rigid regularization to enhance the\nutilization of the priors. We perform an extensive evaluation across various\nreal-world videos, demonstrating its superior performance compared to the\ncurrent state-of-the-art methods.\n","authors":["Tingyang Zhang","Qingzhe Gao","Weiyu Li","Libin Liu","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11427v1.pdf","comment":"https://talegqz.github.io/BAGS/"},{"id":"http://arxiv.org/abs/2310.14566v4","updated":"2024-03-18T02:42:10Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11424v1","updated":"2024-03-18T02:39:21Z","published":"2024-03-18T02:39:21Z","title":"Benchmarking the Robustness of UAV Tracking Against Common Corruptions","summary":"  The robustness of unmanned aerial vehicle (UAV) tracking is crucial in many\ntasks like surveillance and robotics. Despite its importance, little attention\nis paid to the performance of UAV trackers under common corruptions due to lack\nof a dedicated platform. Addressing this, we propose UAV-C, a large-scale\nbenchmark for assessing robustness of UAV trackers under common corruptions.\nSpecifically, UAV-C is built upon two popular UAV datasets by introducing 18\ncommon corruptions from 4 representative categories including adversarial,\nsensor, blur, and composite corruptions in different levels. Finally, UAV-C\ncontains more than 10K sequences. To understand the robustness of existing UAV\ntrackers against corruptions, we extensively evaluate 12 representative\nalgorithms on UAV-C. Our study reveals several key findings: 1) Current\ntrackers are vulnerable to corruptions, indicating more attention needed in\nenhancing the robustness of UAV trackers; 2) When accompanying together,\ncomposite corruptions result in more severe degradation to trackers; and 3)\nWhile each tracker has its unique performance profile, some trackers may be\nmore sensitive to specific corruptions. By releasing UAV-C, we hope it, along\nwith comprehensive analysis, serves as a valuable resource for advancing the\nrobustness of UAV tracking against corruption. Our UAV-C will be available at\nhttps://github.com/Xiaoqiong-Liu/UAV-C.\n","authors":["Xiaoqiong Liu","Yunhe Feng","Shu Hu","Xiaohui Yuan","Heng Fan"],"pdf_url":"https://arxiv.org/pdf/2403.11424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11423v1","updated":"2024-03-18T02:38:55Z","published":"2024-03-18T02:38:55Z","title":"VmambaIR: Visual State Space Model for Image Restoration","summary":"  Image restoration is a critical task in low-level computer vision, aiming to\nrestore high-quality images from degraded inputs. Various models, such as\nconvolutional neural networks (CNNs), generative adversarial networks (GANs),\ntransformers, and diffusion models (DMs), have been employed to address this\nproblem with significant impact. However, CNNs have limitations in capturing\nlong-range dependencies. DMs require large prior models and computationally\nintensive denoising steps. Transformers have powerful modeling capabilities but\nface challenges due to quadratic complexity with input image size. To address\nthese challenges, we propose VmambaIR, which introduces State Space Models\n(SSMs) with linear complexity into comprehensive image restoration tasks. We\nutilize a Unet architecture to stack our proposed Omni Selective Scan (OSS)\nblocks, consisting of an OSS module and an Efficient Feed-Forward Network\n(EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional\nmodeling limitation of SSMs by efficiently modeling image information flows in\nall six directions. Furthermore, we conducted a comprehensive evaluation of our\nVmambaIR across multiple image restoration tasks, including image deraining,\nsingle image super-resolution, and real-world image super-resolution. Extensive\nexperimental results demonstrate that our proposed VmambaIR achieves\nstate-of-the-art (SOTA) performance with much fewer computational resources and\nparameters. Our research highlights the potential of state space models as\npromising alternatives to the transformer and CNN architectures in serving as\nfoundational frameworks for next-generation low-level visual tasks.\n","authors":["Yuan Shi","Bin Xia","Xiaoyu Jin","Xing Wang","Tianyu Zhao","Xin Xia","Xuefeng Xiao","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11423v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.09195v2","updated":"2024-03-18T02:30:17Z","published":"2024-03-14T09:07:34Z","title":"SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash\n  Attention to Achieve 30 times Acceleration","summary":"  Segment Anything Model (SAM) has garnered significant attention in\nsegmentation tasks due to their zero-shot generalization ability. However, a\nbroader application of SAMs to real-world practice has been restricted by their\nlow inference speed and high computational memory demands, which mainly stem\nfrom the attention mechanism. Existing work concentrated on optimizing the\nencoder, yet has not adequately addressed the inefficiency of the attention\nmechanism itself, even when distilled to a smaller model, which thus leaves\nspace for further improvement. In response, we introduce SAM-Lightening, a\nvariant of SAM, that features a re-engineered attention mechanism, termed\nDilated Flash Attention. It not only facilitates higher parallelism, enhancing\nprocessing efficiency but also retains compatibility with the existing\nFlashAttention. Correspondingly, we propose a progressive distillation to\nenable an efficient knowledge transfer from the vanilla SAM without costly\ntraining from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening\nsignificantly outperforms the state-of-the-art methods in both run-time\nefficiency and segmentation accuracy. Specifically, it can achieve an inference\nspeed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels,\nwhich is 30.1 times faster than the vanilla SAM and 2.1 times than the\nstate-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the\nvanilla SAM. The code and weights are available at\nhttps://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.\n","authors":["Yanfei Song","Bangzheng Pu","Peng Wang","Hongxu Jiang","Dong Dong","Yongxiang Cao","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2403.09195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11171v2","updated":"2024-03-18T02:12:44Z","published":"2023-11-18T21:27:04Z","title":"LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation","summary":"  This work proposes a non-iterative, scalable, and statistically optimal way\nto triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that\nminimize the reprojection ($L_2$) error, LOSTU will still provide the maximum\nlikelihood estimate when there are errors in camera pose or parameters. This\ngeneric framework is used to contextualize other triangulation methods like the\ndirect linear transform (DLT) or the midpoint. Synthetic experiments show that\nLOSTU can be substantially faster than using uncertainty-aware\nLevenberg-Marquardt (or similar) optimization schemes, while providing results\nof comparable precision. Finally, LOSTU is implemented in sequential\nreconstruction in conjunction with uncertainty-aware pose estimation, where it\nyields better reconstruction metrics.\n","authors":["Sébastien Henry","John A. Christian"],"pdf_url":"https://arxiv.org/pdf/2311.11171v2.pdf","comment":"19 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.11415v1","updated":"2024-03-18T02:08:58Z","published":"2024-03-18T02:08:58Z","title":"DreamSampler: Unifying Diffusion Sampling and Score Distillation for\n  Image Manipulation","summary":"  Reverse sampling and score-distillation have emerged as main workhorses in\nrecent years for image manipulation using latent diffusion models (LDMs). While\nreverse diffusion sampling often requires adjustments of LDM architecture or\nfeature engineering, score distillation offers a simple yet powerful\nmodel-agnostic approach, but it is often prone to mode-collapsing. To address\nthese limitations and leverage the strengths of both approaches, here we\nintroduce a novel framework called {\\em DreamSampler}, which seamlessly\nintegrates these two distinct approaches through the lens of regularized latent\noptimization. Similar to score-distillation, DreamSampler is a model-agnostic\napproach applicable to any LDM architecture, but it allows both distillation\nand reverse sampling with additional guidance for image editing and\nreconstruction. Through experiments involving image editing, SVG reconstruction\nand etc, we demonstrate the competitive performance of DreamSampler compared to\nexisting approaches, while providing new applications.\n","authors":["Jeongsol Kim","Geon Yeong Park","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.11415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10022v2","updated":"2024-03-18T01:57:08Z","published":"2024-03-15T05:08:59Z","title":"Lifelong Person Re-Identification with Backward-Compatibility","summary":"  Lifelong person re-identification (LReID) assumes a practical scenario where\nthe model is sequentially trained on continuously incoming datasets while\nalleviating the catastrophic forgetting in the old datasets. However, not only\nthe training datasets but also the gallery images are incrementally\naccumulated, that requires a huge amount of computational complexity and\nstorage space to extract the features at the inference phase. In this paper, we\naddress the above mentioned problem by incorporating the backward-compatibility\nto LReID for the first time. We train the model using the continuously incoming\ndatasets while maintaining the model's compatibility toward the previously\ntrained old models without re-computing the features of the old gallery images.\nTo this end, we devise the cross-model compatibility loss based on the\ncontrastive learning with respect to the replay features across all the old\ndatasets. Moreover, we also develop the knowledge consolidation method based on\nthe part classification to learn the shared representation across different\ndatasets for the backward-compatibility. We suggest a more practical\nmethodology for performance evaluation as well where all the gallery and query\nimages are considered together. Experimental results demonstrate that the\nproposed method achieves a significantly higher performance of the\nbackward-compatibility compared with the existing methods. It is a promising\ntool for more practical scenarios of LReID.\n","authors":["Minyoung Oh","Jae-Young Sim"],"pdf_url":"https://arxiv.org/pdf/2403.10022v2.pdf","comment":"17 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.05369v3","updated":"2024-03-18T01:53:42Z","published":"2024-03-08T15:00:44Z","title":"Frequency-Adaptive Dilated Convolution for Semantic Segmentation","summary":"  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.05369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04293v3","updated":"2024-03-18T01:50:08Z","published":"2023-12-07T13:27:37Z","title":"GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion\n  Recognition","summary":"  Recently, GPT-4 with Vision (GPT-4V) has demonstrated remarkable visual\ncapabilities across various tasks, but its performance in emotion recognition\nhas not been fully evaluated. To bridge this gap, we present the quantitative\nevaluation results of GPT-4V on 21 benchmark datasets covering 6 tasks: visual\nsentiment analysis, tweet sentiment analysis, micro-expression recognition,\nfacial emotion recognition, dynamic facial emotion recognition, and multimodal\nemotion recognition. This paper collectively refers to these tasks as\n``Generalized Emotion Recognition (GER)''. Through experimental analysis, we\nobserve that GPT-4V exhibits strong visual understanding capabilities in GER\ntasks. Meanwhile, GPT-4V shows the ability to integrate multimodal clues and\nexploit temporal information, which is also critical for emotion recognition.\nHowever, it's worth noting that GPT-4V is primarily designed for general\ndomains and cannot recognize micro-expressions that require specialized\nknowledge. To the best of our knowledge, this paper provides the first\nquantitative assessment of GPT-4V for GER tasks. We have open-sourced the code\nand encourage subsequent researchers to broaden the evaluation scope by\nincluding more tasks and datasets. Our code and evaluation results are\navailable at: https://github.com/zeroQiaoba/gpt4v-emotion.\n","authors":["Zheng Lian","Licai Sun","Haiyang Sun","Kang Chen","Zhuofan Wen","Hao Gu","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2312.04293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08927v2","updated":"2024-03-18T01:22:04Z","published":"2023-09-16T08:46:59Z","title":"DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic\n  Neural Radiance Fields","summary":"  The accurate reconstruction of dynamic scenes with neural radiance fields is\nsignificantly dependent on the estimation of camera poses. Widely used\nstructure-from-motion pipelines encounter difficulties in accurately tracking\nthe camera trajectory when faced with separate dynamics of the scene content\nand the camera movement. To address this challenge, we propose DynaMoN. DynaMoN\nutilizes semantic segmentation and generic motion masks to handle dynamic\ncontent for initial camera pose estimation and statics-focused ray sampling for\nfast and accurate novel-view synthesis. Our novel iterative learning scheme\nswitches between training the NeRF and updating the pose parameters for an\nimproved reconstruction and trajectory estimation quality. The proposed\npipeline shows significant acceleration of the training process. We extensively\nevaluate our approach on two real-world dynamic datasets, the TUM RGB-D and the\nBONN RGB-D Dynamic dataset. DynaMoN improves over the state-of-the-art both in\nterms of reconstruction quality and trajectory accuracy. We plan to make our\ncode public to enhance research in this area.\n","authors":["Nicolas Schischka","Hannah Schieber","Mert Asim Karaoglu","Melih Görgülü","Florian Grötzner","Alexander Ladikos","Daniel Roth","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.08927v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11401v1","updated":"2024-03-18T01:18:48Z","published":"2024-03-18T01:18:48Z","title":"Scene-LLM: Extending Language Model for 3D Visual Understanding and\n  Reasoning","summary":"  This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.\n","authors":["Rao Fu","Jingyu Liu","Xilun Chen","Yixin Nie","Wenhan Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.11401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10075v2","updated":"2024-03-18T01:16:04Z","published":"2024-03-15T07:34:08Z","title":"A survey of synthetic data augmentation methods in computer vision","summary":"  The standard approach to tackling computer vision problems is to train deep\nconvolutional neural network (CNN) models using large-scale image datasets\nwhich are representative of the target task. However, in many scenarios, it is\noften challenging to obtain sufficient image data for the target task. Data\naugmentation is a way to mitigate this challenge. A common practice is to\nexplicitly transform existing images in desired ways so as to create the\nrequired volume and variability of training data necessary to achieve good\ngeneralization performance. In situations where data for the target domain is\nnot accessible, a viable workaround is to synthesize training data from\nscratch--i.e., synthetic data augmentation. This paper presents an extensive\nreview of synthetic data augmentation techniques. It covers data synthesis\napproaches based on realistic 3D graphics modeling, neural style transfer\n(NST), differential neural rendering, and generative artificial intelligence\n(AI) techniques such as generative adversarial networks (GANs) and variational\nautoencoders (VAEs). For each of these classes of methods, we focus on the\nimportant data generation and augmentation techniques, general scope of\napplication and specific use-cases, as well as existing limitations and\npossible workarounds. Additionally, we provide a summary of common synthetic\ndatasets for training computer vision models, highlighting the main features,\napplication domains and supported tasks. Finally, we discuss the effectiveness\nof synthetic data augmentation methods. Since this is the first paper to\nexplore synthetic data augmentation methods in great detail, we are hoping to\nequip readers with the necessary background information and in-depth knowledge\nof existing methods and their attendant issues.\n","authors":["Alhassan Mumuni","Fuseini Mumuni","Nana Kobina Gerrar"],"pdf_url":"https://arxiv.org/pdf/2403.10075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11397v1","updated":"2024-03-18T01:11:53Z","published":"2024-03-18T01:11:53Z","title":"Defense Against Adversarial Attacks on No-Reference Image Quality Models\n  with Gradient Norm Regularization","summary":"  The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the\nquality score of an input image without additional information. NR-IQA models\nplay a crucial role in the media industry, aiding in performance evaluation and\noptimization guidance. However, these models are found to be vulnerable to\nadversarial attacks, which introduce imperceptible perturbations to input\nimages, resulting in significant changes in predicted scores. In this paper, we\npropose a defense method to improve the stability in predicted scores when\nattacked by small perturbations, thus enhancing the adversarial robustness of\nNR-IQA models. To be specific, we present theoretical evidence showing that the\nmagnitude of score changes is related to the $\\ell_1$ norm of the model's\ngradient with respect to the input image. Building upon this theoretical\nfoundation, we propose a norm regularization training strategy aimed at\nreducing the $\\ell_1$ norm of the gradient, thereby boosting the robustness of\nNR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate\nthe effectiveness of our strategy in reducing score changes in the presence of\nadversarial attacks. To the best of our knowledge, this work marks the first\nattempt to defend against adversarial attacks on NR-IQA models. Our study\noffers valuable insights into the adversarial robustness of NR-IQA models and\nprovides a foundation for future research in this area.\n","authors":["Yujia Liu","Chenxi Yang","Dingquan Li","Jianhao Ding","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.11397v1.pdf","comment":"accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11391v1","updated":"2024-03-18T00:48:58Z","published":"2024-03-18T00:48:58Z","title":"Investigating the Benefits of Projection Head for Representation\n  Learning","summary":"  An effective technique for obtaining high-quality representations is adding a\nprojection head on top of the encoder during training, then discarding it and\nusing the pre-projection representations. Despite its proven practical\neffectiveness, the reason behind the success of this technique is poorly\nunderstood. The pre-projection representations are not directly optimized by\nthe loss function, raising the question: what makes them better? In this work,\nwe provide a rigorous theoretical answer to this question. We start by\nexamining linear models trained with self-supervised contrastive loss. We\nreveal that the implicit bias of training algorithms leads to layer-wise\nprogressive feature weighting, where features become increasingly unequal as we\ngo deeper into the layers. Consequently, lower layers tend to have more\nnormalized and less specialized representations. We theoretically characterize\nscenarios where such representations are more beneficial, highlighting the\nintricate interplay between data augmentation and input features. Additionally,\nwe demonstrate that introducing non-linearity into the network allows lower\nlayers to learn features that are completely absent in higher layers. Finally,\nwe show how this mechanism improves the robustness in supervised contrastive\nlearning and supervised learning. We empirically validate our results through\nvarious experiments on CIFAR-10/100, UrbanCars and shifted versions of\nImageNet. We also introduce a potential alternative to projection head, which\noffers a more interpretable and controllable design.\n","authors":["Yihao Xue","Eric Gan","Jiayi Ni","Siddharth Joshi","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2403.11391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08747v3","updated":"2024-03-18T00:45:45Z","published":"2022-09-19T03:46:13Z","title":"On Robust Cross-View Consistency in Self-Supervised Monocular Depth\n  Estimation","summary":"  Remarkable progress has been made in self-supervised monocular depth\nestimation (SS-MDE) by exploring cross-view consistency, e.g., photometric\nconsistency and 3D point cloud consistency. However, they are very vulnerable\nto illumination variance, occlusions, texture-less regions, as well as moving\nobjects, making them not robust enough to deal with various scenes. To address\nthis challenge, we study two kinds of robust cross-view consistency in this\npaper. Firstly, the spatial offset field between adjacent frames is obtained by\nreconstructing the reference frame from its neighbors via deformable alignment,\nwhich is used to align the temporal depth features via a Depth Feature\nAlignment (DFA) loss. Secondly, the 3D point clouds of each reference frame and\nits nearby frames are calculated and transformed into voxel space, where the\npoint density in each voxel is calculated and aligned via a Voxel Density\nAlignment (VDA) loss. In this way, we exploit the temporal coherence in both\ndepth feature space and 3D voxel space for SS-MDE, shifting the\n\"point-to-point\" alignment paradigm to the \"region-to-region\" one. Compared\nwith the photometric consistency loss as well as the rigid point cloud\nalignment loss, the proposed DFA and VDA losses are more robust owing to the\nstrong representation power of deep features as well as the high tolerance of\nvoxel density to the aforementioned challenges. Experimental results on several\noutdoor benchmarks show that our method outperforms current state-of-the-art\ntechniques. Extensive ablation study and analysis validate the effectiveness of\nthe proposed losses, especially in challenging scenes. The code and models are\navailable at https://github.com/sunnyHelen/RCVC-depth.\n","authors":["Haimei Zhao","Jing Zhang","Zhuo Chen","Bo Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2209.08747v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11380v1","updated":"2024-03-18T00:13:41Z","published":"2024-03-18T00:13:41Z","title":"Boosting Order-Preserving and Transferability for Neural Architecture\n  Search: a Joint Architecture Refined Search and Fine-tuning Approach","summary":"  Supernet is a core component in many recent Neural Architecture Search (NAS)\nmethods. It not only helps embody the search space but also provides a\n(relative) estimation of the final performance of candidate architectures.\nThus, it is critical that the top architectures ranked by a supernet should be\nconsistent with those ranked by true performance, which is known as the\norder-preserving ability. In this work, we analyze the order-preserving ability\non the whole search space (global) and a sub-space of top architectures\n(local), and empirically show that the local order-preserving for current\ntwo-stage NAS methods still need to be improved. To rectify this, we propose a\nnovel concept of Supernet Shifting, a refined search strategy combining\narchitecture searching with supernet fine-tuning. Specifically, apart from\nevaluating, the training loss is also accumulated in searching and the supernet\nis updated every iteration. Since superior architectures are sampled more\nfrequently in evolutionary searching, the supernet is encouraged to focus on\ntop architectures, thus improving local order-preserving. Besides, a\npre-trained supernet is often un-reusable for one-shot methods. We show that\nSupernet Shifting can fulfill transferring supernet to a new dataset.\nSpecifically, the last classifier layer will be unset and trained through\nevolutionary searching. Comprehensive experiments show that our method has\nbetter order-preserving ability and can find a dominating architecture.\nMoreover, the pre-trained supernet can be easily transferred into a new dataset\nwith no loss of performance.\n","authors":["Beichen Zhang","Xiaoxing Wang","Xiaohan Qin","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.11380v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.11376v1","updated":"2024-03-18T00:03:48Z","published":"2024-03-18T00:03:48Z","title":"ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation","summary":"  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: https://github.com/UARK-AICV/ShapeFormer\n","authors":["Minh Tran","Winston Bounsavy","Khoa Vo","Anh Nguyen","Tri Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2403.11376v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.11375v1","updated":"2024-03-18T00:02:48Z","published":"2024-03-18T00:02:48Z","title":"Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival\n  Outcome Prediction","summary":"  For predicting cancer survival outcomes, standard approaches in clinical\nresearch are often based on two main modalities: pathology images for observing\ncell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene\nexpressions. However, existing pathology-genomic multi-modal algorithms face\nsignificant challenges: (1) Valuable biological insights regarding genes and\ngene-gene interactions are frequently overlooked; (2) one modality often\ndominates the optimization process, causing inadequate training for the other\nmodality. In this paper, we introduce a new multi-modal ``Path-GPTOmic\"\nframework for cancer survival outcome prediction. First, to extract valuable\nbiological insights, we regulate the embedding space of a foundation model,\nscGPT, initially trained on single-cell RNA-seq data, making it adaptable for\nbulk RNA-seq data. Second, to address the imbalance-between-modalities problem,\nwe propose a gradient modulation mechanism tailored to the Cox partial\nlikelihood loss for survival prediction. The contributions of the modalities\nare dynamically monitored and adjusted during the training process, encouraging\nthat both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer\nGenome Atlas) datasets, our model achieves substantially improved survival\nprediction accuracy.\n","authors":["Hongxiao Wang","Yang Yang","Zhuo Zhao","Pengfei Gu","Nishchal Sapkota","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11375v1.pdf","comment":"Accepted by IEEE International Symposium on Biomedical Imaging (ISBI\n  2024)"}],"Graphics":[{"id":"http://arxiv.org/abs/2403.11626v1","updated":"2024-03-18T09:58:43Z","published":"2024-03-18T09:58:43Z","title":"QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation","summary":"  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n","authors":["Zhizhen Zhou","Yejing Huo","Guoheng Huang","An Zeng","Xuhang Chen","Lian Huang","Zinuo Li"],"pdf_url":"https://arxiv.org/pdf/2403.11626v1.pdf","comment":"Accepted by The Visual Computer Journal"},{"id":"http://arxiv.org/abs/2403.11469v1","updated":"2024-03-18T04:41:59Z","published":"2024-03-18T04:41:59Z","title":"Generative Motion Stylization within Canonical Motion Space","summary":"  Stylized motion breathes life into characters. However, the fixed skeleton\nstructure and style representation hinder existing data-driven motion synthesis\nmethods from generating stylized motion for various characters. In this work,\nwe propose a generative motion stylization pipeline, named MotionS, for\nsynthesizing diverse and stylized motion on cross-structure characters using\ncross-modality style prompts. Our key insight is to embed motion style into a\ncross-modality latent space and perceive the cross-structure skeleton\ntopologies, allowing for motion stylization within a canonical motion space.\nSpecifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP)\nmodel is leveraged to construct the cross-modality latent space, enabling\nflexible style representation within this space. Additionally, two\ntopology-encoded tokens are learned to capture the canonical and specific\nskeleton topologies, facilitating cross-structure topology shifting.\nSubsequently, the topology-shifted stylization diffusion is designed to\ngenerate motion content for the specific skeleton and stylize it in the shifted\ncanonical motion space using multi-modality style descriptions. Through an\nextensive set of examples, we demonstrate the flexibility and generalizability\nof our pipeline across various characters and style descriptions. Qualitative\nand quantitative experiments underscore the superiority of our pipeline over\nstate-of-the-art methods, consistently delivering high-quality stylized motion\nacross a broad spectrum of skeletal structures.\n","authors":["Jiaxu Zhang","Xin Chen","Gang Yu","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2403.11469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11453v1","updated":"2024-03-18T04:01:26Z","published":"2024-03-18T04:01:26Z","title":"Bridging 3D Gaussian and Mesh for Freeview Video Rendering","summary":"  This is only a preview version of GauMesh. Recently, primitive-based\nrendering has been proven to achieve convincing results in solving the problem\nof modeling and rendering the 3D dynamic scene from 2D images. Despite this, in\nthe context of novel view synthesis, each type of primitive has its inherent\ndefects in terms of representation ability. It is difficult to exploit the mesh\nto depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D\nGaussian Splatting) method usually produces artifacts or blurry pixels in the\narea with smooth geometry and sharp textures. As a result, it is difficult,\neven not impossible, to represent the complex and dynamic scene with a single\ntype of primitive. To this end, we propose a novel approach, GauMesh, to bridge\nthe 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a\nsequence of tracked mesh as initialization, our goal is to simultaneously\noptimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,\nand the deformation field. At a specific time, we perform $\\alpha$-blending on\nthe RGB and opacity values based on the merged and re-ordered z-buffers from\nmesh and 3D Gaussian rasterizations. This produces the final rendering, which\nis supervised by the ground-truth image. Experiments demonstrate that our\napproach adapts the appropriate type of primitives to represent the different\nparts of the dynamic scene and outperforms all the baseline methods in both\nquantitative and qualitative comparisons without losing render speed.\n","authors":["Yuting Xiao","Xuan Wang","Jiafei Li","Hongrui Cai","Yanbo Fan","Nan Xue","Minghui Yang","Yujun Shen","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.11453v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.10075v2","updated":"2024-03-18T01:16:04Z","published":"2024-03-15T07:34:08Z","title":"A survey of synthetic data augmentation methods in computer vision","summary":"  The standard approach to tackling computer vision problems is to train deep\nconvolutional neural network (CNN) models using large-scale image datasets\nwhich are representative of the target task. However, in many scenarios, it is\noften challenging to obtain sufficient image data for the target task. Data\naugmentation is a way to mitigate this challenge. A common practice is to\nexplicitly transform existing images in desired ways so as to create the\nrequired volume and variability of training data necessary to achieve good\ngeneralization performance. In situations where data for the target domain is\nnot accessible, a viable workaround is to synthesize training data from\nscratch--i.e., synthetic data augmentation. This paper presents an extensive\nreview of synthetic data augmentation techniques. It covers data synthesis\napproaches based on realistic 3D graphics modeling, neural style transfer\n(NST), differential neural rendering, and generative artificial intelligence\n(AI) techniques such as generative adversarial networks (GANs) and variational\nautoencoders (VAEs). For each of these classes of methods, we focus on the\nimportant data generation and augmentation techniques, general scope of\napplication and specific use-cases, as well as existing limitations and\npossible workarounds. Additionally, we provide a summary of common synthetic\ndatasets for training computer vision models, highlighting the main features,\napplication domains and supported tasks. Finally, we discuss the effectiveness\nof synthetic data augmentation methods. Since this is the first paper to\nexplore synthetic data augmentation methods in great detail, we are hoping to\nequip readers with the necessary background information and in-depth knowledge\nof existing methods and their attendant issues.\n","authors":["Alhassan Mumuni","Fuseini Mumuni","Nana Kobina Gerrar"],"pdf_url":"https://arxiv.org/pdf/2403.10075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10357v2","updated":"2024-03-18T23:00:57Z","published":"2024-03-15T14:45:38Z","title":"ANIM: Accurate Neural Implicit Model for Human Reconstruction from a\n  single RGB-D image","summary":"  Recent progress in human shape learning, shows that neural implicit models\nare effective in generating 3D human surfaces from limited number of views, and\neven from a single RGB image. However, existing monocular approaches still\nstruggle to recover fine geometric details such as face, hands or cloth\nwrinkles. They are also easily prone to depth ambiguities that result in\ndistorted geometries along the camera optical axis. In this paper, we explore\nthe benefits of incorporating depth observations in the reconstruction process\nby introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes\nfrom single-view RGB-D images with an unprecedented level of accuracy. Our\nmodel learns geometric details from both multi-resolution pixel-aligned and\nvoxel-aligned features to leverage depth information and enable spatial\nrelationships, mitigating depth ambiguities. We further enhance the quality of\nthe reconstructed shape by introducing a depth-supervision strategy, which\nimproves the accuracy of the signed distance field estimation of points that\nlie on the reconstructed surface. Experiments demonstrate that ANIM outperforms\nstate-of-the-art works that use RGB, surface normals, point cloud or RGB-D data\nas input. In addition, we introduce ANIM-Real, a new multi-modal dataset\ncomprising high-quality scans paired with consumer-grade RGB-D camera, and our\nprotocol to fine-tune ANIM, enabling high-quality reconstruction from\nreal-world human capture.\n","authors":["Marco Pesavento","Yuanlu Xu","Nikolaos Sarafianos","Robert Maier","Ziyan Wang","Chun-Han Yao","Marco Volino","Edmond Boyer","Adrian Hilton","Tony Tung"],"pdf_url":"https://arxiv.org/pdf/2403.10357v2.pdf","comment":"Accepted to CVPR24; Project page:\n  https://marcopesavento.github.io/ANIM/"},{"id":"http://arxiv.org/abs/2403.12198v1","updated":"2024-03-18T19:13:02Z","published":"2024-03-18T19:13:02Z","title":"FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo\n  Endoscopic Videos","summary":"  Reconstruction of endoscopic scenes is an important asset for various medical\napplications, from post-surgery analysis to educational training. Neural\nrendering has recently shown promising results in endoscopic reconstruction\nwith deforming tissue. However, the setup has been restricted to a static\nendoscope, limited deformation, or required an external tracking device to\nretrieve camera pose information of the endoscopic camera. With FLex we adress\nthe challenging setup of a moving endoscope within a highly dynamic environment\nof deforming tissue. We propose an implicit scene separation into multiple\noverlapping 4D neural radiance fields (NeRFs) and a progressive optimization\nscheme jointly optimizing for reconstruction and camera poses from scratch.\nThis improves the ease-of-use and allows to scale reconstruction capabilities\nin time to process surgical videos of 5,000 frames and more; an improvement of\nmore than ten times compared to the state of the art while being agnostic to\nexternal tracking information. Extensive evaluations on the StereoMIS dataset\nshow that FLex significantly improves the quality of novel view synthesis while\nmaintaining competitive pose accuracy.\n","authors":["Florian Philipp Stilz","Mert Asim Karaoglu","Felix Tristram","Nassir Navab","Benjamin Busam","Alexander Ladikos"],"pdf_url":"https://arxiv.org/pdf/2403.12198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12036v1","updated":"2024-03-18T17:59:40Z","published":"2024-03-18T17:59:40Z","title":"One-Step Image Translation with Text-to-Image Models","summary":"  In this work, we address two limitations of existing conditional diffusion\nmodels: their slow inference speed due to the iterative denoising process and\ntheir reliance on paired data for model fine-tuning. To tackle these issues, we\nintroduce a general method for adapting a single-step diffusion model to new\ntasks and domains through adversarial learning objectives. Specifically, we\nconsolidate various modules of the vanilla latent diffusion model into a single\nend-to-end generator network with small trainable weights, enhancing its\nability to preserve the input image structure while reducing overfitting. We\ndemonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms\nexisting GAN-based and diffusion-based methods for various scene translation\ntasks, such as day-to-night conversion and adding/removing weather effects like\nfog, snow, and rain. We extend our method to paired settings, where our model\npix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and\nEdge2Image, but with a single-step inference. This work suggests that\nsingle-step diffusion models can serve as strong backbones for a range of GAN\nlearning objectives. Our code and models are available at\nhttps://github.com/GaParmar/img2img-turbo.\n","authors":["Gaurav Parmar","Taesung Park","Srinivasa Narasimhan","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12036v1.pdf","comment":"Github: https://github.com/GaParmar/img2img-turbo"},{"id":"http://arxiv.org/abs/2403.12034v1","updated":"2024-03-18T17:59:12Z","published":"2024-03-18T17:59:12Z","title":"VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion\n  Models","summary":"  This paper presents a novel paradigm for building scalable 3D generative\nmodels utilizing pre-trained video diffusion models. The primary obstacle in\ndeveloping foundation 3D generative models is the limited availability of 3D\ndata. Unlike images, texts, or videos, 3D data are not readily accessible and\nare difficult to acquire. This results in a significant disparity in scale\ncompared to the vast quantities of other types of data. To address this issue,\nwe propose using a video diffusion model, trained with extensive volumes of\ntext, images, and videos, as a knowledge source for 3D data. By unlocking its\nmulti-view generative capabilities through fine-tuning, we generate a\nlarge-scale synthetic multi-view dataset to train a feed-forward 3D generative\nmodel. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view\ndata, can generate a 3D asset from a single image in seconds and achieves\nsuperior performance when compared to current SOTA feed-forward 3D generative\nmodels, with users preferring our results over 70% of the time.\n","authors":["Junlin Han","Filippos Kokkinos","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2403.12034v1.pdf","comment":"Project page: https://junlinhan.github.io/projects/vfusion3d.html"},{"id":"http://arxiv.org/abs/2403.12010v1","updated":"2024-03-18T17:48:15Z","published":"2024-03-18T17:48:15Z","title":"VideoMV: Consistent Multi-View Generation Based on Large Video\n  Generative Model","summary":"  Generating multi-view images based on text or single-image prompts is a\ncritical capability for the creation of 3D content. Two fundamental questions\non this topic are what data we use for training and how to ensure multi-view\nconsistency. This paper introduces a novel framework that makes fundamental\ncontributions to both questions. Unlike leveraging images from 2D diffusion\nmodels for training, we propose a dense consistent multi-view generation model\nthat is fine-tuned from off-the-shelf video generative models. Images from\nvideo generative models are more suitable for multi-view generation because the\nunderlying network architecture that generates them employs a temporal module\nto enforce frame consistency. Moreover, the video data sets used to train these\nmodels are abundant and diverse, leading to a reduced train-finetuning domain\ngap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising\nSampling, which first employs a feed-forward reconstruction module to get an\nexplicit global 3D model, and then adopts a sampling strategy that effectively\ninvolves images rendered from the global 3D model into the denoising sampling\nloop to improve the multi-view consistency of the final images. As a\nby-product, this module also provides a fast way to create 3D assets\nrepresented by 3D Gaussians within a few seconds. Our approach can generate 24\ndense views and converges much faster in training than state-of-the-art\napproaches (4 GPU hours versus many thousand GPU hours) with comparable visual\nquality and consistency. By further fine-tuning, our approach outperforms\nexisting state-of-the-art methods in both quantitative metrics and visual\neffects. Our project page is aigc3d.github.io/VideoMV.\n","authors":["Qi Zuo","Xiaodong Gu","Lingteng Qiu","Yuan Dong","Zhengyi Zhao","Weihao Yuan","Rui Peng","Siyu Zhu","Zilong Dong","Liefeng Bo","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12010v1.pdf","comment":"Project page: aigc3d.github.io/VideoMV/"},{"id":"http://arxiv.org/abs/2403.11868v1","updated":"2024-03-18T15:22:09Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11865v1","updated":"2024-03-18T15:18:55Z","published":"2024-03-18T15:18:55Z","title":"Exploring Multi-modal Neural Scene Representations With Applications on\n  Thermal Imaging","summary":"  Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard\nfor the task of novel view synthesis when trained on a set of RGB images. In\nthis paper, we conduct a comprehensive evaluation of neural scene\nrepresentations, such as NeRFs, in the context of multi-modal learning.\nSpecifically, we present four different strategies of how to incorporate a\nsecond modality, other than RGB, into NeRFs: (1) training from scratch\nindependently on both modalities; (2) pre-training on RGB and fine-tuning on\nthe second modality; (3) adding a second branch; and (4) adding a separate\ncomponent to predict (color) values of the additional modality. We chose\nthermal imaging as second modality since it strongly differs from RGB in terms\nof radiosity, making it challenging to integrate into neural scene\nrepresentations. For the evaluation of the proposed strategies, we captured a\nnew publicly available multi-view dataset, ThermalMix, consisting of six common\nobjects and about 360 RGB and thermal images in total. We employ cross-modality\ncalibration prior to data capturing, leading to high-quality alignments between\nRGB and thermal images. Our findings reveal that adding a second branch to NeRF\nperforms best for novel view synthesis on thermal images while also yielding\ncompelling results on RGB. Finally, we also show that our analysis generalizes\nto other modalities, including near-infrared images and depth maps. Project\npage: https://mert-o.github.io/ThermalNeRF/.\n","authors":["Mert Özer","Maximilian Weiherer","Martin Hundhausen","Bernhard Egger"],"pdf_url":"https://arxiv.org/pdf/2403.11865v1.pdf","comment":"24 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.11821v1","updated":"2024-03-18T14:24:20Z","published":"2024-03-18T14:24:20Z","title":"Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality\n  Metrics","summary":"  Recent advances in text-to-image synthesis have been enabled by exploiting a\ncombination of language and vision through foundation models. These models are\npre-trained on tremendous amounts of text-image pairs sourced from the World\nWide Web or other large-scale databases. As the demand for high-quality image\ngeneration shifts towards ensuring content alignment between text and image,\nnovel evaluation metrics have been developed with the aim of mimicking human\njudgments. Thus, researchers have started to collect datasets with increasingly\ncomplex annotations to study the compositionality of vision-language models and\ntheir incorporation as a quality measure of compositional alignment between\ntext and image contents. In this work, we provide a comprehensive overview of\nexisting text-to-image evaluation metrics and propose a new taxonomy for\ncategorizing these metrics. We also review frequently adopted text-image\nbenchmark datasets before discussing techniques to optimize text-to-image\nsynthesis models towards quality and human preferences. Ultimately, we derive\nguidelines for improving text-to-image evaluation and discuss the open\nchallenges and current limitations.\n","authors":["Sebastian Hartwig","Dominik Engel","Leon Sick","Hannah Kniesel","Tristan Payer"," Poonam","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2403.11821v1.pdf","comment":"preprint, 18 pages, 2 figures, 2 tables"}]},"2024-03-17T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.11370v1","updated":"2024-03-17T23:23:40Z","published":"2024-03-17T23:23:40Z","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic\n  Environments using Graph Neural Networks","summary":"  The assumption of a static environment is common in many geometric computer\nvision tasks like SLAM but limits their applicability in highly dynamic scenes.\nSince these tasks rely on identifying point correspondences between input\nimages within the static part of the environment, we propose a graph neural\nnetwork-based sparse feature matching network designed to perform robust\nmatching under challenging conditions while excluding keypoints on moving\nobjects. We employ a similar scheme of attentional aggregation over graph edges\nto enhance keypoint representations as state-of-the-art feature-matching\nnetworks but augment the graph with epipolar and temporal information and\nvastly reduce the number of graph edges. Furthermore, we introduce a\nself-supervised training scheme to extract pseudo labels for image pairs in\ndynamic environments from exclusively unprocessed visual-inertial data. A\nseries of experiments show the superior performance of our network as it\nexcludes keypoints on moving objects compared to state-of-the-art feature\nmatching networks while still achieving similar results regarding conventional\nmatching metrics. When integrated into a SLAM system, our network significantly\nimproves performance, especially in highly dynamic scenes.\n","authors":["Theresa Huber","Simon Schaefer","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.11370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11368v1","updated":"2024-03-17T23:07:13Z","published":"2024-03-17T23:07:13Z","title":"Driving Style Alignment for LLM-powered Driver Agent","summary":"  Recently, LLM-powered driver agents have demonstrated considerable potential\nin the field of autonomous driving, showcasing human-like reasoning and\ndecision-making abilities.However, current research on aligning driver agent\nbehaviors with human driving styles remains limited, partly due to the scarcity\nof high-quality natural language data from human driving behaviors.To address\nthis research gap, we propose a multi-alignment framework designed to align\ndriver agents with human driving styles through demonstrations and feedback.\nNotably, we construct a natural language dataset of human driver behaviors\nthrough naturalistic driving experiments and post-driving interviews, offering\nhigh-quality human demonstrations for LLM alignment. The framework's\neffectiveness is validated through simulation experiments in the CARLA urban\ntraffic simulator and further corroborated by human evaluations. Our research\noffers valuable insights into designing driving agents with diverse driving\nstyles.The implementation of the framework and details of the dataset can be\nfound at the link.\n","authors":["Ruoxuan Yang","Xinyue Zhang","Anais Fernandez-Laaksonen","Xin Ding","Jiangtao Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11367v1","updated":"2024-03-17T23:06:12Z","published":"2024-03-17T23:06:12Z","title":"3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual\n  ReLocalization","summary":"  This paper presents a novel system designed for 3D mapping and visual\nrelocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and\ncamera data to create accurate and visually plausible representations of the\nenvironment. By leveraging LiDAR data to initiate the training of the 3D\nGaussian Splatting map, our system constructs maps that are both detailed and\ngeometrically accurate. To mitigate excessive GPU memory usage and facilitate\nrapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.\nThis preparation makes our method well-suited for visual localization tasks,\nenabling efficient identification of correspondences between the query image\nand the rendered image from the Gaussian Splatting map via normalized\ncross-correlation (NCC). Additionally, we refine the camera pose of the query\nimage using feature-based matching and the Perspective-n-Point (PnP) technique.\nThe effectiveness, adaptability, and precision of our system are demonstrated\nthrough extensive evaluation on the KITTI360 dataset.\n","authors":["Peng Jiang","Gaurav Pandey","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.11367v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.08978v2","updated":"2024-03-17T22:17:41Z","published":"2023-08-17T13:33:15Z","title":"Quantifying the biomimicry gap in biohybrid robot-fish pairs","summary":"  Biohybrid systems in which robotic lures interact with animals have become\ncompelling tools for probing and identifying the mechanisms underlying\ncollective animal behavior. One key challenge lies in the transfer of social\ninteraction models from simulations to reality, using robotics to validate the\nmodeling hypotheses. This challenge arises in bridging what we term the\n\"biomimicry gap\", which is caused by imperfect robotic replicas, communication\ncues and physics constraints not incorporated in the simulations, that may\nelicit unrealistic behavioral responses in animals. In this work, we used a\nbiomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a\nneural network (NN) model for generating biomimetic social interactions.\nThrough experiments with a biohybrid pair comprising a fish and the robotic\nlure, a pair of real fish, and simulations of pairs of fish, we demonstrate\nthat our biohybrid system generates social interactions mirroring those of\ngenuine fish pairs. Our analyses highlight that: 1) the lure and NN maintain\nminimal deviation in real-world interactions compared to simulations and\nfish-only experiments, 2) our NN controls the robot efficiently in real-time,\nand 3) a comprehensive validation is crucial to bridge the biomimicry gap,\nensuring realistic biohybrid systems.\n","authors":["Vaios Papaspyros","Guy Theraulaz","Clément Sire","Francesco Mondada"],"pdf_url":"https://arxiv.org/pdf/2308.08978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04619v2","updated":"2024-03-17T20:46:14Z","published":"2023-07-10T15:07:29Z","title":"Learning Fine Pinch-Grasp Skills using Tactile Sensing from A Few\n  Real-world Demonstrations","summary":"  Imitation learning for robot dexterous manipulation, especially with a real\nrobot setup, typically requires a large number of demonstrations. In this\npaper, we present a data-efficient learning from demonstration framework which\nexploits the use of rich tactile sensing data and achieves fine bimanual pinch\ngrasping. Specifically, we employ a convolutional autoencoder network that can\neffectively extract and encode high-dimensional tactile information. Further,\nWe develop a framework that achieves efficient multi-sensor fusion for\nimitation learning, allowing the robot to learn contact-aware sensorimotor\nskills from demonstrations. Our comparision study against the framework without\nusing encoded tactile features highlighted the effectiveness of incorporating\nrich contact information, which enabled dexterous bimanual grasping with active\ncontact searching. Extensive experiments demonstrated the robustness of the\nfine pinch grasp policy directly learned from few-shot demonstration, including\ngrasping of the same object with different initial poses, generalizing to ten\nunseen new objects, robust and firm grasping against external pushes, as well\nas contact-aware and reactive re-grasping in case of dropping objects under\nvery large perturbations. Furthermore, the saliency map analysis method is used\nto describe weight distribution across various modalities during pinch\ngrasping, confirming the effectiveness of our framework at leveraging\nmultimodal information.\n","authors":["Xiaofeng Mao","Yucheng Xu","Ruoshi Wen","Mohammadreza Kasaei","Wanming Yu","Efi Psomopoulou","Nathan F. Lepora","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2307.04619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17892v2","updated":"2024-03-17T20:44:31Z","published":"2024-02-27T21:12:31Z","title":"SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking","summary":"  Modern robotic systems are required to operate in dense dynamic environments,\nrequiring highly accurate real-time track identification and estimation. For 3D\nmulti-object tracking, recent approaches process a single measurement frame\nrecursively with greedy association and are prone to errors in ambiguous\nassociation decisions. Our method, Sliding Window Tracker (SWTrack), yields\nmore accurate association and state estimation by batch processing many frames\nof sensor data while being capable of running online in real-time. The most\nprobable track associations are identified by evaluating all possible track\nhypotheses across the temporal sliding window. A novel graph optimization\napproach is formulated to solve the multidimensional assignment problem with\nlifted graph edges introduced to account for missed detections and graph\nsparsity enforced to retain real-time efficiency. We evaluate our SWTrack\nimplementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate\nimproved tracking performance.\n","authors":["Sandro Papais","Robert Ren","Steven Waslander"],"pdf_url":"https://arxiv.org/pdf/2402.17892v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11334v1","updated":"2024-03-17T20:29:01Z","published":"2024-03-17T20:29:01Z","title":"Bridging the Gap between Discrete Agent Strategies in Game Theory and\n  Continuous Motion Planning in Dynamic Environments","summary":"  Generating competitive strategies and performing continuous motion planning\nsimultaneously in an adversarial setting is a challenging problem. In addition,\nunderstanding the intent of other agents is crucial to deploying autonomous\nsystems in adversarial multi-agent environments. Existing approaches either\ndiscretize agent action by grouping similar control inputs, sacrificing\nperformance in motion planning, or plan in uninterpretable latent spaces,\nproducing hard-to-understand agent behaviors. This paper proposes an agent\nstrategy representation via Policy Characteristic Space that maps the agent\npolicies to a pre-specified low-dimensional space. Policy Characteristic Space\nenables the discretization of agent policy switchings while preserving\ncontinuity in control. Also, it provides intepretability of agent policies and\nclear intentions of policy switchings. Then, regret-based game-theoretic\napproaches can be applied in the Policy Characteristic Space to obtain high\nperformance in adversarial environments. Our proposed method is assessed by\nconducting experiments in an autonomous racing scenario using scaled vehicles.\nStatistical evidence shows that our method significantly improves the win rate\nof ego agent and the method also generalizes well to unseen environments.\n","authors":["Hongrui Zheng","Zhijun Zhuang","Stephanie Wu","Shuo Yang","Rahul Mangharam"],"pdf_url":"https://arxiv.org/pdf/2403.11334v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2209.14875v2","updated":"2024-03-17T20:10:16Z","published":"2022-09-29T15:40:12Z","title":"Accelerating Laboratory Automation Through Robot Skill Learning For\n  Sample Scraping","summary":"  The use of laboratory robotics for autonomous experiments offers an\nattractive route to alleviate scientists from tedious tasks while accelerating\nmaterial discovery for topical issues such as climate change and\npharmaceuticals. While some experimental workflows can already benefit from\nautomation, sample preparation is still carried out manually due to the high\nlevel of motor function and dexterity required when dealing with different\ntools, chemicals, and glassware. A fundamental workflow in chemical fields is\ncrystallisation, where one application is polymorph screening, i.e., obtaining\na three dimensional molecular structure from a crystal. For this process, it is\nof utmost importance to recover as much of the sample as possible since\nsynthesising molecules is both costly in time and money. To this aim, chemists\nscrape vials to retrieve sample contents prior to imaging plate transfer.\nAutomating this process is challenging as it goes beyond robotic insertion\ntasks due to a fundamental requirement of having to execute fine-granular\nmovements within a constrained environment (sample vial). Motivated by how\nhuman chemists carry out this process of scraping powder from vials, our work\nproposes a model-free reinforcement learning method for learning a scraping\npolicy, leading to a fully autonomous sample scraping procedure. We first\ncreate a scenario-specific simulation environment with a Panda Franka Emika\nrobot using a laboratory scraper that is inserted into a simulated vial, to\ndemonstrate how a scraping policy can be learned successfully in simulation. We\nthen train and evaluate our method on a real robotic manipulator in laboratory\nsettings, and show that our method can autonomously scrape powder across\nvarious setups.\n","authors":["Gabriella Pizzuto","Hetong Wang","Hatem Fakhruldeen","Bei Peng","Kevin S. Luck","Andrew I. Cooper"],"pdf_url":"https://arxiv.org/pdf/2209.14875v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05570v2","updated":"2024-03-17T19:49:58Z","published":"2024-02-22T02:38:51Z","title":"A Motion Planning Algorithm in a Figure Eight Track","summary":"  We design a motion planning algorithm to coordinate the movements of two\nrobots along a figure eight track, in such a way that no collisions occur. We\nuse a topological approach to robot motion planning that relates instabilities\nin motion planning algorithms to topological features of configuration spaces.\nThe topological complexity of a configuration space is an invariant that\nmeasures the complexity of motion planning algorithms. We show that the\ntopological complexity of our problem is 3 and construct an explicit algorithm\nwith three continuous instructions.\n","authors":["Cristian Jardon","Brian Sheppard","Veet Zaveri"],"pdf_url":"https://arxiv.org/pdf/2403.05570v2.pdf","comment":"25 pages, 45 figures, First published in PUMP Journal of\n  Undergraduate Research. This research paper was completed under the\n  supervision of Prof. Hellen Colman at Wilbur Wright College"},{"id":"http://arxiv.org/abs/2403.11313v1","updated":"2024-03-17T19:23:46Z","published":"2024-03-17T19:23:46Z","title":"Leveraging Simulation-Based Model Preconditions for Fast Action\n  Parameter Optimization with Multiple Models","summary":"  Optimizing robotic action parameters is a significant challenge for\nmanipulation tasks that demand high levels of precision and generalization.\nUsing a model-based approach, the robot must quickly reason about the outcomes\nof different actions using a predictive model to find a set of parameters that\nwill have the desired effect. The model may need to capture the behaviors of\nrigid and deformable objects, as well as objects of various shapes and sizes.\nPredictive models often need to trade-off speed for prediction accuracy and\ngeneralization. This paper proposes a framework that leverages the strengths of\nmultiple predictive models, including analytical, learned, and simulation-based\nmodels, to enhance the efficiency and accuracy of action parameter\noptimization. Our approach uses Model Deviation Estimators (MDEs) to determine\nthe most suitable predictive model for any given state-action parameters,\nallowing the robot to select models to make fast and precise predictions. We\nextend the MDE framework by not only learning sim-to-real MDEs, but also\nsim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide\nsignificantly faster parameter optimization as well as a basis for efficiently\nlearning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim\ntraining data also allows the robot to learn MDEs based directly on visual\ninputs and local material properties.\n","authors":["M. Yunus Seker","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2403.11313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11304v1","updated":"2024-03-17T18:53:46Z","published":"2024-03-17T18:53:46Z","title":"Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving","summary":"  Planning the trajectory of the controlled ego vehicle is a key challenge in\nautomated driving. As for human drivers, predicting the motions of surrounding\nvehicles is important to plan the own actions. Recent motion prediction methods\nutilize equivariant neural networks to exploit geometric symmetries in the\nscene. However, no existing method combines motion prediction and trajectory\nplanning in a joint step while guaranteeing equivariance under\nroto-translations of the input space. We address this gap by proposing a\nlightweight equivariant planning model that generates multi-modal joint\npredictions for all vehicles and selects one mode as the ego plan. The\nequivariant network design improves sample efficiency, guarantees output\nstability, and reduces model parameters. We further propose equivariant route\nattraction to guide the ego vehicle along a high-level route provided by an\noff-the-shelf GPS navigation system. This module creates a momentum from\nembedded vehicle positions toward the route in latent space while keeping the\nequivariance property. Route attraction enables goal-oriented behavior without\nforcing the vehicle to stick to the exact route. We conduct experiments on the\nchallenging nuScenes dataset to investigate the capability of our planner. The\nresults show that the planned trajectory is stable under roto-translations of\nthe input scene which demonstrates the equivariance of our model. Despite using\nonly a small split of the dataset for training, our method improves L2 distance\nat 3 s by 20.6 % and surpasses the state of the art.\n","authors":["Steffen Hagedorn","Marcel Milich","Alexandru P. Condurache"],"pdf_url":"https://arxiv.org/pdf/2403.11304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11298v1","updated":"2024-03-17T18:40:29Z","published":"2024-03-17T18:40:29Z","title":"Multi-Sample Long Range Path Planning under Sensing Uncertainty for\n  Off-Road Autonomous Driving","summary":"  We focus on the problem of long-range dynamic replanning for off-road\nautonomous vehicles, where a robot plans paths through a previously unobserved\nenvironment while continuously receiving noisy local observations. An effective\napproach for planning under sensing uncertainty is determinization, where one\nconverts a stochastic world into a deterministic one and plans under this\nsimplification. This makes the planning problem tractable, but the cost of\nfollowing the planned path in the real world may be different than in the\ndeterminized world. This causes collisions if the determinized world\noptimistically ignores obstacles, or causes unnecessarily long routes if the\ndeterminized world pessimistically imagines more obstacles. We aim to be robust\nto uncertainty over potential worlds while still achieving the efficiency\nbenefits of determinization. We evaluate algorithms for dynamic replanning on a\nlarge real-world dataset of challenging long-range planning problems from the\nDARPA RACER program. Our method, Dynamic Replanning via Evaluating and\nAggregating Multiple Samples (DREAMS), outperforms other determinization-based\napproaches in terms of combined traversal time and collision cost.\nhttps://sites.google.com/cs.washington.edu/dreams/\n","authors":["Matt Schmittle","Rohan Baijal","Brian Hou","Siddhartha Srinivasa","Byron Boots"],"pdf_url":"https://arxiv.org/pdf/2403.11298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11289v1","updated":"2024-03-17T17:59:59Z","published":"2024-03-17T17:59:59Z","title":"ManipVQA: Injecting Robotic Affordance and Physically Grounded\n  Information into Multi-Modal Large Language Models","summary":"  The integration of Multimodal Large Language Models (MLLMs) with robotic\nsystems has significantly enhanced the ability of robots to interpret and act\nupon natural language instructions. Despite these advancements, conventional\nMLLMs are typically trained on generic image-text pairs, lacking essential\nrobotics knowledge such as affordances and physical knowledge, which hampers\ntheir efficacy in manipulation tasks. To bridge this gap, we introduce\nManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric\nknowledge through a Visual Question-Answering format. This approach not only\nencompasses tool detection and affordance recognition but also extends to a\ncomprehensive understanding of physical concepts. Our approach starts with\ncollecting a varied set of images displaying interactive objects, which\npresents a broad range of challenges in tool object detection, affordance, and\nphysical concept predictions. To seamlessly integrate this robotic-specific\nknowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a\nunified VQA format and devise a fine-tuning strategy that preserves the\noriginal vision-reasoning abilities while incorporating the new robotic\ninsights. Empirical evaluations conducted in robotic simulators and across\nvarious vision task benchmarks demonstrate the robust performance of ManipVQA.\nCode and dataset will be made publicly available at\nhttps://github.com/SiyuanHuang95/ManipVQA.\n","authors":["Siyuan Huang","Iaroslav Ponomarenko","Zhengkai Jiang","Xiaoqi Li","Xiaobin Hu","Peng Gao","Hongsheng Li","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11289v1.pdf","comment":"Code and dataset will be made publicly available at\n  https://github.com/SiyuanHuang95/ManipVQA"},{"id":"http://arxiv.org/abs/2403.11279v1","updated":"2024-03-17T17:37:21Z","published":"2024-03-17T17:37:21Z","title":"Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance","summary":"  We propose a hybrid feedback control scheme for the autonomous robot\nnavigation problem in three-dimensional environments with arbitrarily-shaped\nconvex obstacles. The proposed hybrid control strategy, which consists in\nswitching between the move-to-target mode and the obstacle-avoidance mode,\nguarantees global asymptotic stability of the target location in the\nobstacle-free workspace. We also provide a procedure for the implementation of\nthe proposed hybrid controller in a priori unknown environments and validate\nits effectiveness through simulation results.\n","authors":["Mayur Sawant","Ilia Polushin","Abdelhamid Tayebi"],"pdf_url":"https://arxiv.org/pdf/2403.11279v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11252v1","updated":"2024-03-17T15:59:41Z","published":"2024-03-17T15:59:41Z","title":"Zutu: A Platform for Localization and Navigation of Swarm Robots Using\n  Virtual Grids","summary":"  Swarm robots, which are inspired from the way insects behave collectively in\norder to achieve a common goal, have become a major part of research with\napplications involving search and rescue, area exploration, surveillance etc.\nIn this paper, we present a swarm of robots that do not require individual\nextrinsic sensors to sense the environment but instead use a single central\ncamera to locate and map the swarm. The robots can be easily built using\nreadily available components with the main chassis being 3D printed, making the\nsystem low-cost, low-maintenance, and easy to replicate. We describe Zutu's\nhardware and software architecture, the algorithms to map the robots to the\nreal world, and some experiments conducted using four of our robots.\nEventually, we conclude the possible applications of our system in research,\neducation, and industries.\n","authors":[" Prateek","Pawan Wadhwani","Reshesh Kumar Pathak","Mayur Bhosale","Dr. A Helen Victoria"],"pdf_url":"https://arxiv.org/pdf/2403.11252v1.pdf","comment":"Accepted at 7th International Conference on Robotics and Automation\n  Engineering, ICRAE 2022, Singapore, November 18 - November 20, 2022"},{"id":"http://arxiv.org/abs/2403.11247v1","updated":"2024-03-17T15:41:35Z","published":"2024-03-17T15:41:35Z","title":"Compact 3D Gaussian Splatting For Dense Visual SLAM","summary":"  Recent work has shown that 3D Gaussian-based SLAM enables high-quality\nreconstruction, accurate pose estimation, and real-time rendering of scenes.\nHowever, these approaches are built on a tremendous number of redundant 3D\nGaussian ellipsoids, leading to high memory and storage costs, and slow\ntraining speed. To address the limitation, we propose a compact 3D Gaussian\nSplatting SLAM system that reduces the number and the parameter size of\nGaussian ellipsoids. A sliding window-based masking strategy is first proposed\nto reduce the redundant ellipsoids. Then we observe that the covariance matrix\n(geometry) of most 3D Gaussian ellipsoids are extremely similar, which\nmotivates a novel geometry codebook to compress 3D Gaussian geometric\nattributes, i.e., the parameters. Robust and accurate pose estimation is\nachieved by a global bundle adjustment method with reprojection loss. Extensive\nexperiments demonstrate that our method achieves faster training and rendering\nspeed while maintaining the state-of-the-art (SOTA) quality of the scene\nrepresentation.\n","authors":["Tianchen Deng","Yaohui Chen","Leyan Zhang","Jianfei Yang","Shenghai Yuan","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11233v1","updated":"2024-03-17T14:42:05Z","published":"2024-03-17T14:42:05Z","title":"STAIR: Semantic-Targeted Active Implicit Reconstruction","summary":"  Many autonomous robotic applications require object-level understanding when\ndeployed. Actively reconstructing objects of interest, i.e. objects with\nspecific semantic meanings, is therefore relevant for a robot to perform\ndownstream tasks in an initially unknown environment. In this work, we propose\na novel framework for semantic-targeted active reconstruction using posed RGB-D\nmeasurements and 2D semantic labels as input. The key components of our\nframework are a semantic implicit neural representation and a compatible\nplanning utility function based on semantic rendering and uncertainty\nestimation, enabling adaptive view planning to target objects of interest. Our\nplanning approach achieves better reconstruction performance in terms of mesh\nand novel view rendering quality compared to implicit reconstruction baselines\nthat do not consider semantics for view planning. Our framework further\noutperforms a state-of-the-art semantic-targeted active reconstruction pipeline\nbased on explicit maps, justifying our choice of utilising implicit neural\nrepresentations to tackle semantic-targeted active reconstruction problems.\n","authors":["Liren Jin","Haofei Kuang","Yue Pan","Cyrill Stachniss","Marija Popović"],"pdf_url":"https://arxiv.org/pdf/2403.11233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11205v1","updated":"2024-03-17T13:08:41Z","published":"2024-03-17T13:08:41Z","title":"Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL\n  Using Reinforcement Learning","summary":"  We have developed a parallel wire-driven monopedal robot, RAMIEL, which has\nboth speed and power due to the parallel wire mechanism and a long acceleration\ndistance. RAMIEL is capable of jumping high and continuously, and so has high\nperformance in traveling. On the other hand, one of the drawbacks of a minimal\nparallel wire-driven robot without joint encoders is that the current joint\nvelocities estimated from the wire lengths oscillate due to the elongation of\nthe wires, making the values unreliable. Therefore, despite its high\nperformance, the control of the robot is unstable, and in 10 out of 16 jumps,\nthe robot could only jump up to two times continuously. In this study, we\npropose a method to realize a continuous jumping motion by reinforcement\nlearning in simulation, and its application to the actual robot. Because the\njoint velocities oscillate with the elongation of the wires, they are not used\ndirectly, but instead are inferred from the time series of joint angles. At the\nsame time, noise that imitates the vibration caused by the elongation of the\nwires is added for transfer to the actual robot. The results show that the\nsystem can be applied to the actual robot RAMIEL as well as to the stable\ncontinuous jumping motion in simulation.\n","authors":["Kento Kawaharazuka","Temma Suzuki","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11205v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2403.11198v1","updated":"2024-03-17T12:50:05Z","published":"2024-03-17T12:50:05Z","title":"Learning-Based Wiping Behavior of Low-Rigidity Robots Considering\n  Various Surface Materials and Task Definitions","summary":"  Wiping behavior is a task of tracing the surface of an object while feeling\nthe force with the palm of the hand. It is necessary to adjust the force and\nposture appropriately considering the various contact conditions felt by the\nhand. Several studies have been conducted on the wiping motion, however, these\nstudies have only dealt with a single surface material, and have only\nconsidered the application of the amount of appropriate force, lacking\nintelligent movements to ensure that the force is applied either evenly to the\nentire surface or to a certain area. Depending on the surface material, the\nhand posture and pressing force should be varied appropriately, and this is\nhighly dependent on the definition of the task. Also, most of the movements are\nexecuted by high-rigidity robots that are easy to model, and few movements are\nexecuted by robots that are low-rigidity but therefore have a small risk of\ndamage due to excessive contact. So, in this study, we develop a method of\nmotion generation based on the learned prediction of contact force during the\nwiping motion of a low-rigidity robot. We show that MyCobot, which is made of\nlow-rigidity resin, can appropriately perform wiping behaviors on a plane with\nmultiple surface materials based on various task definitions.\n","authors":["Kento Kawaharazuka","Naoaki Kanazawa","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11198v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2307.00533v3","updated":"2024-03-17T12:34:00Z","published":"2023-07-02T10:02:08Z","title":"Representing Robot Geometry as Distance Fields: Applications to\n  Whole-body Manipulation","summary":"  In this work, we propose a novel approach to represent robot geometry as\ndistance fields (RDF) that extends the principle of signed distance fields\n(SDFs) to articulated kinematic chains. Our method employs a combination of\nBernstein polynomials to encode the signed distance for each robot link with\nhigh accuracy and efficiency while ensuring the mathematical continuity and\ndifferentiability of SDFs. We further leverage the kinematics chain of the\nrobot to produce the SDF representation in joint space, allowing robust\ndistance queries in arbitrary joint configurations. The proposed RDF\nrepresentation is differentiable and smooth in both task and joint spaces,\nenabling its direct integration to optimization problems. Additionally, the\n0-level set of the robot corresponds to the robot surface, which can be\nseamlessly integrated into whole-body manipulation tasks. We conduct various\nexperiments in both simulations and with 7-axis Franka Emika robots, comparing\nagainst baseline methods, and demonstrating its effectiveness in collision\navoidance and whole-body manipulation tasks. Project page:\nhttps://sites.google.com/view/lrdf/home\n","authors":["Yiming Li","Yan Zhang","Amirreza Razmjoo","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2307.00533v3.pdf","comment":"IEEE International Conference on Robotics and Automation, ICRA, 2024"},{"id":"http://arxiv.org/abs/2307.00085v2","updated":"2024-03-17T12:32:08Z","published":"2023-06-30T18:43:24Z","title":"Parallel Self-assembly for a Multi-USV System on Water Surface with\n  Obstacles","summary":"  Parallel self-assembly is an efficient approach to accelerate the assembly\nprocess for modular robots. However, these approaches cannot accommodate\ncomplicated environments with obstacles, which restricts their applications.\nThis paper considers the surrounding stationary obstacles and proposes a\nparallel self-assembly planning algorithm named SAPOA. With this algorithm,\nmodular robots can avoid immovable obstacles when performing docking actions,\nwhich adapts the parallel self-assembly process to complex scenes. To validate\nthe efficiency and scalability, we have designed 25 distinct grid maps with\ndifferent obstacle configurations to simulate the algorithm. From the results\ncompared to the existing parallel self-assembly algorithms, our algorithm shows\na significantly higher success rate, which is more than 80%. For verification\nin real-world applications, a multi-agent hardware testbed system is developed.\nThe algorithm is successfully deployed on four omnidirectional unmanned surface\nvehicles, CuBoats. The navigation strategy that translates the discrete\nplanner, SAPOA, to the continuous controller on the CuBoats is presented. The\nalgorithm's feasibility and flexibility were demonstrated through successful\nself-assembly experiments on 5 maps with varying obstacle configurations.\n","authors":["Lianxin Zhang","Yihan Huang","Zhongzhong Cao","Yang Jiao","Huihuan Qian"],"pdf_url":"https://arxiv.org/pdf/2307.00085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02044v3","updated":"2024-03-17T10:37:08Z","published":"2023-10-03T13:35:49Z","title":"How Physics and Background Attributes Impact Video Transformers in\n  Robotic Manipulation: A Case Study on Planar Pushing","summary":"  As model and dataset sizes continue to scale in robot learning, the need to\nunderstand what is the specific factor in the dataset that affects model\nperformance becomes increasingly urgent to ensure cost-effective data\ncollection and model performance. In this work, we empirically investigate how\nphysics attributes (color, friction coefficient, shape) and scene background\ncharacteristics, such as the complexity and dynamics of interactions with\nbackground objects, influence the performance of Video Transformers in\npredicting planar pushing trajectories. We aim to investigate three primary\nquestions: How do physics attributes and background scene characteristics\ninfluence model performance? What kind of changes in attributes are most\ndetrimental to model generalization? What proportion of fine-tuning data is\nrequired to adapt models to novel scenarios? To facilitate this research, we\npresent CloudGripper-Push-1K, a large real-world vision-based robot pushing\ndataset comprising 1278 hours and 460,000 videos of planar pushing interactions\nwith objects with different physics and background attributes. We also propose\nVideo Occlusion Transformer (VOT), a generic modular video-transformer-based\ntrajectory prediction framework which features 3 choices of 2D-spatial encoders\nas the subject of our case study. Dataset and codes will be available at\nhttps://cloudgripper.org.\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v3.pdf","comment":"Under review at IEEE/RSJ IROS 2024"},{"id":"http://arxiv.org/abs/2310.00398v2","updated":"2024-03-17T09:59:40Z","published":"2023-09-30T14:40:12Z","title":"Optimal Impact Angle Guidance via First-Order Optimization under\n  Nonconvex Constraints","summary":"  Most of the optimal guidance problems can be formulated as nonconvex\noptimization problems, which can be solved indirectly by relaxation,\nconvexification, or linearization. Although these methods are guaranteed to\nconverge to the global optimum of the modified problems, the obtained solution\nmay not guarantee global optimality or even the feasibility of the original\nnonconvex problems. In this paper, we propose a computational optimal guidance\napproach that directly handles the nonconvex constraints encountered in\nformulating the guidance problems. The proposed computational guidance approach\nalternately solves the least squares problems and projects the solution onto\nnonconvex feasible sets, which rapidly converges to feasible suboptimal\nsolutions or sometimes to the globally optimal solutions. The proposed\nalgorithm is verified via a series of numerical simulations on impact angle\nguidance problems under state dependent maneuver vector constraints, and it is\ndemonstrated that the proposed algorithm provides superior guidance performance\nthan conventional techniques.\n","authors":["Gyubin Park","Jiwoo Choi","Da Hoon Jeong","Jong-Han Kim"],"pdf_url":"https://arxiv.org/pdf/2310.00398v2.pdf","comment":"To appear at 2024 American Control Conference"},{"id":"http://arxiv.org/abs/2403.11146v1","updated":"2024-03-17T08:51:17Z","published":"2024-03-17T08:51:17Z","title":"Toward Adaptive Cooperation: Model-Based Shared Control Using\n  LQ-Differential Games","summary":"  This paper introduces a novel model-based adaptive shared control to allow\nfor the identification and design challenge for shared-control systems, in\nwhich humans and automation share control tasks. The main challenge is the\nadaptive behavior of the human in such shared control interactions.\nConsequently, merely identifying human behavior without considering automation\nis insufficient and often leads to inadequate automation design. Therefore,\nthis paper proposes a novel solution involving online identification of the\nhuman and the adaptation of shared control using Linear-Quadratic differential\ngames. The effectiveness of the proposed online adaptation is analyzed in\nsimulations and compared with a non-adaptive shared control from the state of\nthe art. Finally, the proposed approach is tested through human-in-the-loop\nexperiments, highlighting its suitability for real-time applications.\n","authors":["Balint Varga"],"pdf_url":"https://arxiv.org/pdf/2403.11146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15241v2","updated":"2024-03-17T05:30:40Z","published":"2023-11-26T08:59:30Z","title":"CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration\n  Network","summary":"  The fusion of LiDARs and cameras has been increasingly adopted in autonomous\ndriving for perception tasks. The performance of such fusion-based algorithms\nlargely depends on the accuracy of sensor calibration, which is challenging due\nto the difficulty of identifying common features across different data\nmodalities. Previously, many calibration methods involved specific targets\nand/or manual intervention, which has proven to be cumbersome and costly.\nLearning-based online calibration methods have been proposed, but their\nperformance is barely satisfactory in most cases. These methods usually suffer\nfrom issues such as sparse feature maps, unreliable cross-modality association,\ninaccurate calibration parameter regression, etc. In this paper, to address\nthese issues, we propose CalibFormer, an end-to-end network for automatic\nLiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR\nimage features to achieve high-resolution representations. A multi-head\ncorrelation module is utilized to identify correlations between features more\naccurately. Lastly, we employ transformer architectures to estimate accurate\ncalibration parameters from the correlation information. Our method achieved a\nmean translation error of $0.8751 \\mathrm{cm}$ and a mean rotation error of\n$0.0562 ^{\\circ}$ on the KITTI dataset, surpassing existing state-of-the-art\nmethods and demonstrating strong robustness, accuracy, and generalization\ncapabilities.\n","authors":["Yuxuan Xiao","Yao Li","Chengzhen Meng","Xingchen Li","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.15241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11095v1","updated":"2024-03-17T05:23:43Z","published":"2024-03-17T05:23:43Z","title":"PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for\n  Aerial Wildfire Monitoring in Partially Observable Environments","summary":"  Motivated by agility, 3D mobility, and low-risk operation compared to\nhuman-operated management systems of autonomous unmanned aerial vehicles\n(UAVs), this work studies UAV-based active wildfire monitoring where a UAV\ndetects fire incidents in remote areas and tracks the fire frontline. A UAV\npath planning solution is proposed considering realistic wildfire management\nmissions, where a single low-altitude drone with limited power and flight time\nis available. Noting the limited field of view of commercial low-altitude UAVs,\nthe problem formulates as a partially observable Markov decision process\n(POMDP), in which wildfire progression outside the field of view causes\ninaccurate state representation that prevents the UAV from finding the optimal\npath to track the fire front in limited time. Common deep reinforcement\nlearning (DRL)-based trajectory planning solutions require diverse\ndrone-recorded wildfire data to generalize pre-trained models to real-time\nsystems, which is not currently available at a diverse and standard scale. To\nnarrow down the gap caused by partial observability in the space of possible\npolicies, a belief-based state representation with broad, extensive simulated\ndata is proposed where the beliefs (i.e., ignition probabilities of different\ngrid areas) are updated using a Bayesian framework for the cells within the\nfield of view. The performance of the proposed solution in terms of the ratio\nof detected fire cells and monitored ignited area (MIA) is evaluated in a\ncomplex fire scenario with multiple rapidly growing fire batches, indicating\nthat the belief state representation outperforms the observation state\nrepresentation both in fire coverage and the distance to fire frontline.\n","authors":["Sahand Khoshdel","Qi Luo","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2403.11095v1.pdf","comment":"7 pages, Accepted in American Control Conference (ACC) 2024, July\n  10-12th, Toronto, ON, Canada"},{"id":"http://arxiv.org/abs/2311.03351v4","updated":"2024-03-17T04:40:06Z","published":"2023-11-06T18:58:59Z","title":"Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with\n  Multi-Step On-Policy Optimization","summary":"  Combining offline and online reinforcement learning (RL) is crucial for\nefficient and safe learning. However, previous approaches treat offline and\nonline learning as separate procedures, resulting in redundant designs and\nlimited performance. We ask: Can we achieve straightforward yet effective\noffline and online learning without introducing extra conservatism or\nregularization? In this study, we propose Uni-o4, which utilizes an on-policy\nobjective for both offline and online learning. Owning to the alignment of\nobjectives in two phases, the RL agent can transfer between offline and online\nlearning seamlessly. This property enhances the flexibility of the learning\nparadigm, allowing for arbitrary combinations of pretraining, fine-tuning,\noffline, and online learning. In the offline phase, specifically, Uni-o4\nleverages diverse ensemble policies to address the mismatch issues between the\nestimated behavior policy and the offline dataset. Through a simple offline\npolicy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy\nimprovement safely. We demonstrate that by employing the method above, the\nfusion of these two paradigms can yield superior offline initialization as well\nas stable and rapid online fine-tuning capabilities. Through real-world robot\ntasks, we highlight the benefits of this paradigm for rapid deployment in\nchallenging, previously unseen real-world environments. Additionally, through\ncomprehensive evaluations using numerous simulated benchmarks, we substantiate\nthat our method achieves state-of-the-art performance in both offline and\noffline-to-online fine-tuning learning. Our website:\nhttps://lei-kun.github.io/uni-o4/ .\n","authors":["Kun Lei","Zhengmao He","Chenhao Lu","Kaizhe Hu","Yang Gao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2311.03351v4.pdf","comment":"Our website: https://lei-kun.github.io/uni-o4/"},{"id":"http://arxiv.org/abs/2403.11057v1","updated":"2024-03-17T02:06:49Z","published":"2024-03-17T02:06:49Z","title":"Large Language Models Powered Context-aware Motion Prediction","summary":"  Motion prediction is among the most fundamental tasks in autonomous driving.\nTraditional methods of motion forecasting primarily encode vector information\nof maps and historical trajectory data of traffic participants, lacking a\ncomprehensive understanding of overall traffic semantics, which in turn affects\nthe performance of prediction tasks. In this paper, we utilized Large Language\nModels (LLMs) to enhance the global traffic context understanding for motion\nprediction tasks. We first conducted systematic prompt engineering, visualizing\ncomplex traffic environments and historical trajectory information of traffic\nparticipants into image prompts -- Transportation Context Map (TC-Map),\naccompanied by corresponding text prompts. Through this approach, we obtained\nrich traffic context information from the LLM. By integrating this information\ninto the motion prediction model, we demonstrate that such context can enhance\nthe accuracy of motion predictions. Furthermore, considering the cost\nassociated with LLMs, we propose a cost-effective deployment strategy:\nenhancing the accuracy of motion prediction tasks at scale with 0.7\\%\nLLM-augmented datasets. Our research offers valuable insights into enhancing\nthe understanding of traffic scenes of LLMs and the motion prediction\nperformance of autonomous driving.\n","authors":["Xiaoji Zheng","Lixiu Wu","Zhijie Yan","Yuanrong Tang","Hao Zhao","Chen Zhong","Bokui Chen","Jiangtao Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11057v1.pdf","comment":"6 pages,4 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.08459v2","updated":"2024-03-17T23:45:01Z","published":"2023-12-13T19:01:07Z","title":"FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head\n  Models","summary":"  We introduce FaceTalk, a novel generative approach designed for synthesizing\nhigh-fidelity 3D motion sequences of talking human heads from input audio\nsignal. To capture the expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to couple speech signal\nwith the latent space of neural parametric head models to create high-fidelity,\ntemporally coherent motion sequences. We propose a new latent diffusion model\nfor this task, operating in the expression space of neural parametric head\nmodels, to synthesize audio-driven realistic head sequences. In the absence of\na dataset with corresponding NPHM expressions to audio, we optimize for these\ncorrespondences to produce a dataset of temporally-optimized NPHM expressions\nfit to audio-video recordings of people talking. To the best of our knowledge,\nthis is the first work to propose a generative approach for realistic and\nhigh-quality motion synthesis of volumetric human heads, representing a\nsignificant advancement in the field of audio-driven 3D animation. Notably, our\napproach stands out in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the NPHM shape space. Our\nexperimental results substantiate the effectiveness of FaceTalk, consistently\nachieving superior and visually natural motion, encompassing diverse facial\nexpressions and styles, outperforming existing methods by 75% in perceptual\nuser study evaluation.\n","authors":["Shivangi Aneja","Justus Thies","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.08459v2.pdf","comment":"Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:\n  https://shivangi-aneja.github.io/projects/facetalk/"},{"id":"http://arxiv.org/abs/2403.11373v1","updated":"2024-03-17T23:44:20Z","published":"2024-03-17T23:44:20Z","title":"Reconstruct before Query: Continual Missing Modality Learning with\n  Decomposed Prompt Collaboration","summary":"  Pre-trained large multi-modal models (LMMs) exploit fine-tuning to adapt\ndiverse user applications. Nevertheless, fine-tuning may face challenges due to\ndeactivated sensors (e.g., cameras turned off for privacy or technical issues),\nyielding modality-incomplete data and leading to inconsistency in training data\nand the data for inference. Additionally, continuous training leads to\ncatastrophic forgetting, diluting the knowledge in pre-trained LMMs. To\novercome these challenges, we introduce a novel task, Continual Missing\nModality Learning (CMML), to investigate how models can generalize when data of\ncertain modalities is missing during continual fine-tuning. Our preliminary\nbenchmarks reveal that existing methods suffer from a significant performance\ndrop in CMML, even with the aid of advanced continual learning techniques.\nTherefore, we devise a framework termed Reconstruct before Query (RebQ). It\ndecomposes prompts into modality-specific ones and breaks them into components\nstored in pools accessible via a key-query mechanism, which facilitates\nParameterEfficient Fine-Tuning and enhances knowledge transferability for\nsubsequent tasks. Meanwhile, our RebQ leverages extensive multi-modal knowledge\nfrom pre-trained LMMs to reconstruct the data of missing modality.\nComprehensive experiments demonstrate that RebQ effectively reconstructs the\nmissing modality information and retains pre-trained knowledge. Specifically,\ncompared with the baseline, RebQ improves average precision from 20.00 to 50.92\nand decreases average forgetting from 75.95 to 8.56. Code and datasets are\navailable on https://github.com/Tree-Shu-Zhao/RebQ.pytorch\n","authors":["Shu Zhao","Xiaohan Zou","Tan Yu","Huijuan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.11373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11371v1","updated":"2024-03-17T23:29:41Z","published":"2024-03-17T23:29:41Z","title":"V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse\n  Weather Conditions","summary":"  Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception\nsystems have shown the significant success on 3D object detection. While these\nmodels perform well in the trained clean weather, they struggle in unseen\nadverse weather conditions with the real-world domain gap. In this paper, we\npropose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D\nobject detection on multi-agent perception system under adverse weather\nconditions. Not only in the clean weather does our research aim to ensure\nfavorable multi-agent performance, but also in the unseen adverse weather\nconditions by learning only on the clean weather data. To advance research in\nthis area, we have simulated the impact of three prevalent adverse weather\nconditions on two widely-used multi-agent datasets, resulting in the creation\nof two novel benchmark datasets: OPV2V-w and V2XSet-w.\n  To this end, we first introduce the Adaptive Weather Augmentation (AWA) to\nmimic the unseen adverse weather conditions, and then propose two alignments\nfor generalizable representation learning: Trust-region Weather-invariant\nAlignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensive\nexperimental results demonstrate that our V2X-DGW achieved improvements in the\nunseen adverse weather conditions.\n","authors":["Baolu Li","Jinlong Li","Xinyu Liu","Runsheng Xu","Zhengzhong Tu","Jiacheng Guo","Xiaopeng Li","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11370v1","updated":"2024-03-17T23:23:40Z","published":"2024-03-17T23:23:40Z","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic\n  Environments using Graph Neural Networks","summary":"  The assumption of a static environment is common in many geometric computer\nvision tasks like SLAM but limits their applicability in highly dynamic scenes.\nSince these tasks rely on identifying point correspondences between input\nimages within the static part of the environment, we propose a graph neural\nnetwork-based sparse feature matching network designed to perform robust\nmatching under challenging conditions while excluding keypoints on moving\nobjects. We employ a similar scheme of attentional aggregation over graph edges\nto enhance keypoint representations as state-of-the-art feature-matching\nnetworks but augment the graph with epipolar and temporal information and\nvastly reduce the number of graph edges. Furthermore, we introduce a\nself-supervised training scheme to extract pseudo labels for image pairs in\ndynamic environments from exclusively unprocessed visual-inertial data. A\nseries of experiments show the superior performance of our network as it\nexcludes keypoints on moving objects compared to state-of-the-art feature\nmatching networks while still achieving similar results regarding conventional\nmatching metrics. When integrated into a SLAM system, our network significantly\nimproves performance, especially in highly dynamic scenes.\n","authors":["Theresa Huber","Simon Schaefer","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.11370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00385v2","updated":"2024-03-17T23:23:22Z","published":"2023-04-30T04:40:32Z","title":"Cross-Shaped Windows Transformer with Self-supervised Pretraining for\n  Clinically Significant Prostate Cancer Detection in Bi-parametric MRI","summary":"  Biparametric magnetic resonance imaging (bpMRI) has demonstrated promising\nresults in prostate cancer (PCa) detection using convolutional neural networks\n(CNNs). Recently, transformers have achieved competitive performance compared\nto CNNs in computer vision. Large scale transformers need abundant annotated\ndata for training, which are difficult to obtain in medical imaging.\nSelf-supervised learning (SSL) utilizes unlabeled data to generate meaningful\nsemantic representations without the need for costly annotations, enhancing\nmodel performance on tasks with limited labeled data. We introduce a novel\nend-to-end Cross-Shaped windows (CSwin) transformer UNet model, CSwin UNet, to\ndetect clinically significant prostate cancer (csPCa) in prostate bi-parametric\nMR imaging (bpMRI) and demonstrate the effectiveness of our proposed\nself-supervised pre-training framework. Using a large prostate bpMRI dataset\nwith 1500 patients, we first pretrain CSwin transformer using multi-task\nself-supervised learning to improve data-efficiency and network\ngeneralizability. We then finetune using lesion annotations to perform csPCa\ndetection. Five-fold cross validation shows that self-supervised CSwin UNet\nachieves 0.888 AUC and 0.545 Average Precision (AP), significantly\noutperforming four comparable models (Swin UNETR, DynUNet, Attention UNet,\nUNet). Using a separate bpMRI dataset with 158 patients, we evaluate our method\nrobustness to external hold-out data. Self-supervised CSwin UNet achieves 0.79\nAUC and 0.45 AP, still outperforming all other comparable methods and\ndemonstrating good generalization to external data.\n","authors":["Yuheng Li","Jacob Wynne","Jing Wang","Richard L. J. Qiu","Justin Roper","Shaoyan Pan","Ashesh B. Jani","Tian Liu","Pretesh R. Patel","Hui Mao","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2305.00385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11367v1","updated":"2024-03-17T23:06:12Z","published":"2024-03-17T23:06:12Z","title":"3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual\n  ReLocalization","summary":"  This paper presents a novel system designed for 3D mapping and visual\nrelocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and\ncamera data to create accurate and visually plausible representations of the\nenvironment. By leveraging LiDAR data to initiate the training of the 3D\nGaussian Splatting map, our system constructs maps that are both detailed and\ngeometrically accurate. To mitigate excessive GPU memory usage and facilitate\nrapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.\nThis preparation makes our method well-suited for visual localization tasks,\nenabling efficient identification of correspondences between the query image\nand the rendered image from the Gaussian Splatting map via normalized\ncross-correlation (NCC). Additionally, we refine the camera pose of the query\nimage using feature-based matching and the Perspective-n-Point (PnP) technique.\nThe effectiveness, adaptability, and precision of our system are demonstrated\nthrough extensive evaluation on the KITTI360 dataset.\n","authors":["Peng Jiang","Gaurav Pandey","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.11367v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2210.06323v4","updated":"2024-03-17T22:58:03Z","published":"2022-10-12T15:42:40Z","title":"AISFormer: Amodal Instance Segmentation with Transformer","summary":"  Amodal Instance Segmentation (AIS) aims to segment the region of both visible\nand possible occluded parts of an object instance. While Mask R-CNN-based AIS\napproaches have shown promising results, they are unable to model high-level\nfeatures coherence due to the limited receptive field. The most recent\ntransformer-based models show impressive performance on vision tasks, even\nbetter than Convolution Neural Networks (CNN). In this work, we present\nAISFormer, an AIS framework, with a Transformer-based mask head. AISFormer\nexplicitly models the complex coherence between occluder, visible, amodal, and\ninvisible masks within an object's regions of interest by treating them as\nlearnable queries. Specifically, AISFormer contains four modules: (i) feature\nencoding: extract ROI and learn both short-range and long-range visual\nfeatures. (ii) mask transformer decoding: generate the occluder, visible, and\namodal mask query embeddings by a transformer decoder (iii) invisible mask\nembedding: model the coherence between the amodal and visible masks, and (iv)\nmask predicting: estimate output masks including occluder, visible, amodal and\ninvisible. We conduct extensive experiments and ablation studies on three\nchallenging benchmarks i.e. KINS, D2SA, and COCOA-cls to evaluate the\neffectiveness of AISFormer. The code is available at:\nhttps://github.com/UARK-AICV/AISFormer\n","authors":["Minh Tran","Khoa Vo","Kashu Yamazaki","Arthur Fernandes","Michael Kidd","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2210.06323v4.pdf","comment":"Accepted to BMVC2022"},{"id":"http://arxiv.org/abs/2403.11364v1","updated":"2024-03-17T22:49:07Z","published":"2024-03-17T22:49:07Z","title":"Creating Seamless 3D Maps Using Radiance Fields","summary":"  It is desirable to create 3D object models and 3D maps from 2D input images\nfor applications such as navigation, virtual tourism, and urban planning. The\ntraditional methods of creating 3D maps, (such as photogrammetry), require a\nlarge number of images and odometry. Additionally, traditional methods have\ndifficulty with reflective surfaces and specular reflections; windows and\nchrome in the scene can be problematic. Google Road View is a familiar\napplication, which uses traditional methods to fuse a collection of 2D input\nimages into the illusion of a 3D map. However, Google Road View does not create\nan actual 3D object model, only a collection of views. The objective of this\nwork is to create an actual 3D object model using updated techniques. Neural\nRadiance Fields (NeRF[1]) has emerged as a potential solution, offering the\ncapability to produce more precise and intricate 3D maps. Gaussian Splatting[4]\nis another contemporary technique. This investigation compares Neural Radiance\nFields to Gaussian Splatting, and describes some of their inner workings. Our\nprimary contribution is a method for improving the results of the 3D\nreconstructed models. Our results indicate that Gaussian Splatting was superior\nto the NeRF technique.\n","authors":["Sai Tarun Sathyan","Thomas B. Kinsman"],"pdf_url":"https://arxiv.org/pdf/2403.11364v1.pdf","comment":"10 pages with figures"},{"id":"http://arxiv.org/abs/2403.11340v1","updated":"2024-03-17T20:47:52Z","published":"2024-03-17T20:47:52Z","title":"StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining","summary":"  Hematoxylin and Eosin (H&E) staining is the most commonly used for disease\ndiagnosis and tumor recurrence tracking. Hematoxylin excels at highlighting\nnuclei, whereas eosin stains the cytoplasm. However, H&E stain lacks details\nfor differentiating different types of cells relevant to identifying the grade\nof the disease or response to specific treatment variations. Pathologists\nrequire special immunohistochemical (IHC) stains that highlight different cell\ntypes. These stains help in accurately identifying different regions of disease\ngrowth and their interactions with the cell's microenvironment. The advent of\ndeep learning models has made Image-to-Image (I2I) translation a key research\narea, reducing the need for expensive physical staining processes. Pix2Pix and\nCycleGAN are still the most commonly used methods for virtual staining\napplications. However, both suffer from hallucinations or staining\nirregularities when H&E stain has less discriminate information about the\nunderlying cells IHC needs to highlight (e.g.,CD3 lymphocytes). Diffusion\nmodels are currently the state-of-the-art models for image generation and\nconditional generation tasks. However, they require extensive and diverse\ndatasets (millions of samples) to converge, which is less feasible for virtual\nstaining applications.Inspired by the success of multitask deep learning models\nfor limited dataset size, we propose StainDiffuser, a novel multitask dual\ndiffusion architecture for virtual staining that converges under a limited\ntraining budget. StainDiffuser trains two diffusion processes simultaneously:\n(a) generation of cell-specific IHC stain from H&E and (b) H&E-based cell\nsegmentation using coarse segmentation only during training. Our results show\nthat StainDiffuser produces high-quality results for easier (CK8/18,epithelial\nmarker) and difficult stains(CD3, Lymphocytes).\n","authors":["Tushar Kataria","Beatrice Knudsen","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2403.11340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11338v1","updated":"2024-03-17T20:44:38Z","published":"2024-03-17T20:44:38Z","title":"Ensembling and Test Augmentation for Covid-19 Detection and Covid-19\n  Domain Adaptation from 3D CT-Scans","summary":"  Since the emergence of Covid-19 in late 2019, medical image analysis using\nartificial intelligence (AI) has emerged as a crucial research area,\nparticularly with the utility of CT-scan imaging for disease diagnosis. This\npaper contributes to the 4th COV19D competition, focusing on Covid-19 Detection\nand Covid-19 Domain Adaptation Challenges. Our approach centers on lung\nsegmentation and Covid-19 infection segmentation employing the recent CNN-based\nsegmentation architecture PDAtt-Unet, which simultaneously segments lung\nregions and infections. Departing from traditional methods, we concatenate the\ninput slice (grayscale) with segmented lung and infection, generating three\ninput channels akin to color channels. Additionally, we employ three 3D CNN\nbackbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and\n3D-Resnet-50 models to train Covid-19 recognition for both challenges.\nFurthermore, we explore ensemble approaches and testing augmentation to enhance\nperformance. Comparison with baseline results underscores the substantial\nefficiency of our approach, with a significant margin in terms of F1-score (14\n%). This study advances the field by presenting a comprehensive methodology for\naccurate Covid-19 detection and adaptation, leveraging cutting-edge AI\ntechniques in medical image analysis.\n","authors":["Fares Bougourzi","Feryal Windal Moula","Halim Benhabiles","Fadi Dornaika","Abdelmalik Taleb-Ahmed"],"pdf_url":"https://arxiv.org/pdf/2403.11338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11337v1","updated":"2024-03-17T20:36:43Z","published":"2024-03-17T20:36:43Z","title":"Enhancing Bandwidth Efficiency for Video Motion Transfer Applications\n  using Deep Learning Based Keypoint Prediction","summary":"  We propose a deep learning based novel prediction framework for enhanced\nbandwidth reduction in motion transfer enabled video applications such as video\nconferencing, virtual reality gaming and privacy preservation for patient\nhealth monitoring. To model complex motion, we use the First Order Motion Model\n(FOMM) that represents dynamic objects using learned keypoints along with their\nlocal affine transformations. Keypoints are extracted by a self-supervised\nkeypoint detector and organized in a time series corresponding to the video\nframes. Prediction of keypoints, to enable transmission using lower frames per\nsecond on the source device, is performed using a Variational Recurrent Neural\nNetwork (VRNN). The predicted keypoints are then synthesized to video frames\nusing an optical flow estimator and a generator network. This efficacy of\nleveraging keypoint based representations in conjunction with VRNN based\nprediction for both video animation and reconstruction is demonstrated on three\ndiverse datasets. For real-time applications, our results show the\neffectiveness of our proposed architecture by enabling up to 2x additional\nbandwidth reduction over existing keypoint based video motion transfer\nframeworks without significantly compromising video quality.\n","authors":["Xue Bai","Tasmiah Haque","Sumit Mohan","Yuliang Cai","Byungheon Jeong","Adam Halasz","Srinjoy Das"],"pdf_url":"https://arxiv.org/pdf/2403.11337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15851v2","updated":"2024-03-17T20:26:48Z","published":"2023-11-27T14:17:41Z","title":"Single-Model and Any-Modality for Video Object Tracking","summary":"  In the realm of video object tracking, auxiliary modalities such as depth,\nthermal, or event data have emerged as valuable assets to complement the RGB\ntrackers. In practice, most existing RGB trackers learn a single set of\nparameters to use them across datasets and applications. However, a similar\nsingle-model unification for multi-modality tracking presents several\nchallenges. These challenges stem from the inherent heterogeneity of inputs --\neach with modality-specific representations, the scarcity of multi-modal\ndatasets, and the absence of all the modalities at all times. In this work, we\nintroduce Un-Track, a Unified Tracker of a single set of parameters for any\nmodality. To handle any modality, our method learns their common latent space\nthrough low-rank factorization and reconstruction techniques. More importantly,\nwe use only the RGB-X pairs to learn the common latent space. This unique\nshared representation seamlessly binds all modalities together, enabling\neffective unification and accommodating any missing modality, all within a\nsingle transformer-based architecture. Our Un-Track achieves +8.1 absolute\nF-score gain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50)\nGFLOPs with +6.6M (over 93M) parameters, through a simple yet efficient\nprompting strategy. Extensive comparisons on five benchmark datasets with\ndifferent modalities show that Un-Track surpasses both SOTA unified trackers\nand modality-specific counterparts, validating our effectiveness and\npracticality. The source code is publicly available at\nhttps://github.com/Zongwei97/UnTrack.\n","authors":["Zongwei Wu","Jilai Zheng","Xiangxuan Ren","Florin-Alexandru Vasluianu","Chao Ma","Danda Pani Paudel","Luc Van Gool","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2311.15851v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2312.06630v3","updated":"2024-03-17T20:15:45Z","published":"2023-12-11T18:50:09Z","title":"TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance\n  Segmentation","summary":"  Training on large-scale datasets can boost the performance of video instance\nsegmentation while the annotated datasets for VIS are hard to scale up due to\nthe high labor cost. What we possess are numerous isolated filed-specific\ndatasets, thus, it is appealing to jointly train models across the aggregation\nof datasets to enhance data volume and diversity. However, due to the\nheterogeneity in category space, as mask precision increases with the data\nvolume, simply utilizing multiple datasets will dilute the attention of models\non different taxonomies. Thus, increasing the data scale and enriching taxonomy\nspace while improving classification precision is important. In this work, we\nanalyze that providing extra taxonomy information can help models concentrate\non specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset\nJoint Training for Video Instance Segmentation (TMT-VIS) to address this vital\nchallenge. Specifically, we design a two-stage taxonomy aggregation module that\nfirst compiles taxonomy information from input videos and then aggregates these\ntaxonomy priors into instance queries before the transformer decoder. We\nconduct extensive experimental evaluations on four popular and challenging\nbenchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Our\nmodel shows significant improvement over the baseline solutions, and sets new\nstate-of-the-art records on all benchmarks. These appealing and encouraging\nresults demonstrate the effectiveness and generality of our approach. The code\nis available at https://github.com/rkzheng99/TMT-VIS .\n","authors":["Rongkun Zheng","Lu Qi","Xi Chen","Yi Wang","Kun Wang","Yu Qiao","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.06630v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.11328v1","updated":"2024-03-17T20:14:57Z","published":"2024-03-17T20:14:57Z","title":"Domain-Guided Masked Autoencoders for Unique Player Identification","summary":"  Unique player identification is a fundamental module in vision-driven sports\nanalytics. Identifying players from broadcast videos can aid with various\ndownstream tasks such as player assessment, in-game analysis, and broadcast\nproduction. However, automatic detection of jersey numbers using deep features\nis challenging primarily due to: a) motion blur, b) low resolution video feed,\nand c) occlusions. With their recent success in various vision tasks, masked\nautoencoders (MAEs) have emerged as a superior alternative to conventional\nfeature extractors. However, most MAEs simply zero-out image patches either\nrandomly or focus on where to mask rather than how to mask. Motivated by human\nvision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to\nfacilitate robust feature extraction in the presence of motion blur for player\nidentification. We further introduce a new spatio-temporal network leveraging\nour novel d-MAE for unique player identification. We conduct experiments on\nthree large-scale sports datasets, including a curated baseball dataset, the\nSoccerNet dataset, and an in-house ice hockey dataset. We preprocess the\ndatasets using an upgraded keyframe identification (KfID) module by focusing on\nframes containing jersey numbers. Additionally, we propose a keyframe-fusion\ntechnique to augment keyframes, preserving spatial and temporal context. Our\nspatio-temporal network showcases significant improvements, surpassing the\ncurrent state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies,\nrespectively. Rigorous ablations highlight the effectiveness of our\ndomain-guided masking approach and the refined KfID module, resulting in\nperformance enhancements of 1.48% and 1.84% respectively, compared to original\narchitectures.\n","authors":["Bavesh Balaji","Jerrin Bright","Sirisha Rambhatla","Yuhao Chen","Alexander Wong","John Zelek","David A Clausi"],"pdf_url":"https://arxiv.org/pdf/2403.11328v1.pdf","comment":"Submitted to 21st International Conference on Robots and Vision\n  (CRV'24), Guelph, Ontario, Canada"},{"id":"http://arxiv.org/abs/2403.11324v1","updated":"2024-03-17T20:06:41Z","published":"2024-03-17T20:06:41Z","title":"GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering","summary":"  During the Gaussian Splatting optimization process, the scene's geometry can\ngradually deteriorate if its structure is not deliberately preserved,\nespecially in non-textured regions such as walls, ceilings, and furniture\nsurfaces. This degradation significantly affects the rendering quality of novel\nviews that deviate significantly from the viewpoints in the training data. To\nmitigate this issue, we propose a novel approach called GeoGaussian. Based on\nthe smoothly connected areas observed from point clouds, this method introduces\na novel pipeline to initialize thin Gaussians aligned with the surfaces, where\nthe characteristic can be transferred to new generations through a carefully\ndesigned densification strategy. Finally, the pipeline ensures that the scene's\ngeometry and texture are maintained through constrained optimization processes\nwith explicit geometry constraints. Benefiting from the proposed architecture,\nthe generative ability of 3D Gaussians is enhanced, especially in structured\nregions. Our proposed pipeline achieves state-of-the-art performance in novel\nview synthesis and geometric reconstruction, as evaluated qualitatively and\nquantitatively on public datasets.\n","authors":["Yanyan Li","Chenyu Lyu","Yan Di","Guangyao Zhai","Gim Hee Lee","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2403.11324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11317v1","updated":"2024-03-17T19:44:05Z","published":"2024-03-17T19:44:05Z","title":"Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches","summary":"  Two approaches have emerged to input images into large language models\n(LLMs). The first is to caption images into natural language. The second is to\nmap image feature embeddings into the domain of the LLM and pass the mapped\nembeddings directly to the LLM. The majority of recent few-shot multimodal work\nreports performance using architectures that employ variations of one of these\ntwo approaches. But they overlook an important comparison between them. We\ndesign a controlled and focused experiment to compare these two approaches to\nfew-shot visual question answering (VQA) with LLMs. Our findings indicate that\nfor Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to\nthe LLM embedding space does not guarantee improved performance over using\nimage captions. In the zero-shot regime, we find using textual image captions\nis better. In the few-shot regimes, how the in-context examples are selected\ndetermines which is better.\n","authors":["Igor Sterner","Weizhe Lin","Jinghong Chen","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2403.11317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11310v1","updated":"2024-03-17T19:10:07Z","published":"2024-03-17T19:10:07Z","title":"A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose\n  Estimation","summary":"  3D human pose data collected in controlled laboratory settings present\nchallenges for pose estimators that generalize across diverse scenarios. To\naddress this, domain generalization is employed. Current methodologies in\ndomain generalization for 3D human pose estimation typically utilize\nadversarial training to generate synthetic poses for training. Nonetheless,\nthese approaches exhibit several limitations. First, the lack of prior\ninformation about the target domain complicates the application of suitable\naugmentation through a single pose augmentor, affecting generalization on\ntarget domains. Moreover, adversarial training's discriminator tends to enforce\nsimilarity between source and synthesized poses, impeding the exploration of\nout-of-source distributions. Furthermore, the pose estimator's optimization is\nnot exposed to domain shifts, limiting its overall generalization ability.\n  To address these limitations, we propose a novel framework featuring two pose\naugmentors: the weak and the strong augmentors. Our framework employs\ndifferential strategies for generation and discrimination processes,\nfacilitating the preservation of knowledge related to source poses and the\nexploration of out-of-source distributions without prior information about\ntarget poses. Besides, we leverage meta-optimization to simulate domain shifts\nin the optimization process of the pose estimator, thereby improving its\ngeneralization ability. Our proposed approach significantly outperforms\nexisting methods, as demonstrated through comprehensive experiments on various\nbenchmark datasets.\n","authors":["Qucheng Peng","Ce Zheng","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11310v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11299v1","updated":"2024-03-17T18:42:38Z","published":"2024-03-17T18:42:38Z","title":"SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant","summary":"  Recent advancements in the vision-language model have shown notable\ngeneralization in vision-language tasks after visual instruction tuning.\nHowever, bridging the gap between the pre-trained vision encoder and the large\nlanguage models becomes the whole network's bottleneck. To improve\ncross-modality alignment, existing works usually consider more visual\ninstruction data covering a broader range of vision tasks to fine-tune the\nmodel for question-answering, which are costly to obtain. However, the image\ncontains rich contextual information that has been largely under-explored. This\npaper first attempts to harness this overlooked context within visual\ninstruction data, training the model to self-supervised `learning' how to ask\nhigh-quality questions. In this way, we introduce a novel framework named\nSQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA\nexhibits proficiency in generating flexible and meaningful image-related\nquestions while analyzing the visual clue and prior language knowledge,\nsignifying an advanced level of generalized visual understanding. Moreover,\nfine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent\nperformance improvement compared with traditional visual-instruction tuning\nmethods. This improvement highlights the efficacy of self-questioning\ntechniques in achieving a deeper and more nuanced comprehension of visual\ncontent across various contexts.\n","authors":["Guohao Sun","Can Qin","Jiamian Wang","Zeyuan Chen","Ran Xu","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2403.11299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06668v4","updated":"2024-03-17T18:29:53Z","published":"2023-08-13T02:59:36Z","title":"Large Language Models and Foundation Models in Smart Agriculture:\n  Basics, Opportunities, and Challenges","summary":"  The past decade has witnessed the rapid development and adoption of ML & DL\nmethodologies in agricultural systems, showcased by great successes in\nagricultural applications. However, these conventional ML/DL models have\ncertain limitations: they heavily rely on large, costly-to-acquire labeled\ndatasets for training, require specialized expertise for development and\nmaintenance, and are mostly tailored for specific tasks, thus lacking\ngeneralizability. Recently, large pre-trained models, also known as FMs, have\ndemonstrated remarkable successes in language, vision, and decision-making\ntasks across various domains. These models are trained on a large amount of\ndata from multiple domains and modalities. Once trained, they can accomplish\nversatile tasks with just minor fine-tuning and minimal task-specific labeled\ndata. Despite their proven effectiveness and huge potential, there has been\nlittle exploration of applying FMs to agriculture AI. Thus, this study aims to\nexplore the potential of FMs in the field of smart agriculture. In particular,\nconceptual tools and technical background are presented to help the\nunderstanding of the problem space and uncover new research directions. To this\nend, recent FMs in the general CS domain are reviewed, and the models are\ncategorized into four categories: language FMs, vision FMs, multimodal FMs, and\nreinforcement learning FMs. Then, the steps of developing agriculture FMs\n(AFMs) are outlined and potential applications in smart agriculture are\ndiscussed. Moreover, challenges and risks associated with developing AFMs are\ndiscussed, including model training, validation, and deployment. In summary,\nthe advancement of AI in agriculture is explored by introducing AFMs as a\npromising paradigm that can significantly mitigate the reliance on extensive\nlabeled datasets and enhance the efficiency, effectiveness, and generalization\nof agricultural AI systems.\n","authors":["Jiajia Li","Mingle Xu","Lirong Xiang","Dong Chen","Weichao Zhuang","Xunyuan Yin","Zhaojian Li"],"pdf_url":"https://arxiv.org/pdf/2308.06668v4.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.11295v1","updated":"2024-03-17T18:28:24Z","published":"2024-03-17T18:28:24Z","title":"Order-One Rolling Shutter Cameras","summary":"  Rolling shutter (RS) cameras dominate consumer and smartphone markets.\nSeveral methods for computing the absolute pose of RS cameras have appeared in\nthe last 20 years, but the relative pose problem has not been fully solved yet.\nWe provide a unified theory for the important class of order-one rolling\nshutter (RS$_1$) cameras. These cameras generalize the perspective projection\nto RS cameras, projecting a generic space point to exactly one image point via\na rational map. We introduce a new back-projection RS camera model,\ncharacterize RS$_1$ cameras, construct explicit parameterizations of such\ncameras, and determine the image of a space line. We classify all minimal\nproblems for solving the relative camera pose problem with linear RS$_1$\ncameras and discover new practical cases. Finally, we show how the theory can\nbe used to explain RS models previously used for absolute pose computation.\n","authors":["Marvin Anas Hahn","Kathlén Kohn","Orlando Marigliano","Tomas Pajdla"],"pdf_url":"https://arxiv.org/pdf/2403.11295v1.pdf","comment":"36 pages, 6 figures, 3 ancillary files"},{"id":"http://arxiv.org/abs/2403.11291v1","updated":"2024-03-17T18:06:06Z","published":"2024-03-17T18:06:06Z","title":"Advanced Knowledge Extraction of Physical Design Drawings, Translation\n  and conversion to CAD formats using Deep Learning","summary":"  The maintenance, archiving and usage of the design drawings is cumbersome in\nphysical form in different industries for longer period. It is hard to extract\ninformation by simple scanning of drawing sheets. Converting them to their\ndigital formats such as Computer-Aided Design (CAD), with needed knowledge\nextraction can solve this problem. The conversion of these machine drawings to\nits digital form is a crucial challenge which requires advanced techniques.\nThis research proposes an innovative methodology utilizing Deep Learning\nmethods. The approach employs object detection model, such as Yolov7, Faster\nR-CNN, to detect physical drawing objects present in the images followed by,\nedge detection algorithms such as canny filter to extract and refine the\nidentified lines from the drawing region and curve detection techniques to\ndetect circle. Also ornaments (complex shapes) within the drawings are\nextracted. To ensure comprehensive conversion, an Optical Character Recognition\n(OCR) tool is integrated to identify and extract the text elements from the\ndrawings. The extracted data which includes the lines, shapes and text is\nconsolidated and stored in a structured comma separated values(.csv) file\nformat. The accuracy and the efficiency of conversion is evaluated. Through\nthis, conversion can be automated to help organizations enhance their\nproductivity, facilitate seamless collaborations and preserve valuable design\ninformation in a digital format easily accessible. Overall, this study\ncontributes to the advancement of CAD conversions, providing accurate results\nfrom the translating process. Future research can focus on handling diverse\ndrawing types, enhanced accuracy in shape and line detection and extraction.\n","authors":["Jesher Joshua M","Ragav V","Syed Ibrahim S P"],"pdf_url":"https://arxiv.org/pdf/2403.11291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11284v1","updated":"2024-03-17T17:42:02Z","published":"2024-03-17T17:42:02Z","title":"Fast Personalized Text-to-Image Syntheses With Attention Injection","summary":"  Currently, personalized image generation methods mostly require considerable\ntime to finetune and often overfit the concept resulting in generated images\nthat are similar to custom concepts but difficult to edit by prompts. We\npropose an effective and fast approach that could balance the text-image\nconsistency and identity consistency of the generated image and reference\nimage. Our method can generate personalized images without any fine-tuning\nwhile maintaining the inherent text-to-image generation ability of diffusion\nmodels. Given a prompt and a reference image, we merge the custom concept into\ngenerated images by manipulating cross-attention and self-attention layers of\nthe original diffusion model to generate personalized images that match the\ntext description. Comprehensive experiments highlight the superiority of our\nmethod.\n","authors":["Yuxuan Zhang","Yiren Song","Jinpeng Yu","Han Pan","Zhongliang Jing"],"pdf_url":"https://arxiv.org/pdf/2403.11284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11273v1","updated":"2024-03-17T17:04:45Z","published":"2024-03-17T17:04:45Z","title":"BrightDreamer: Generic 3D Gaussian Generative Framework for Fast\n  Text-to-3D Synthesis","summary":"  Text-to-3D synthesis has recently seen intriguing advances by combining the\ntext-to-image models with 3D representation methods, e.g., Gaussian Splatting\n(GS), via Score Distillation Sampling (SDS). However, a hurdle of existing\nmethods is the low efficiency, per-prompt optimization for a single 3D object.\nTherefore, it is imperative for a paradigm shift from per-prompt optimization\nto one-stage generation for any unseen text prompts, which yet remains\nchallenging. A hurdle is how to directly generate a set of millions of 3D\nGaussians to represent a 3D object. This paper presents BrightDreamer, an\nend-to-end single-stage approach that can achieve generalizable and fast (77\nms) text-to-3D generation. Our key idea is to formulate the generation process\nas estimating the 3D deformation from an anchor shape with predefined\npositions. For this, we first propose a Text-guided Shape Deformation (TSD)\nnetwork to predict the deformed shape and its new positions, used as the\ncenters (one attribute) of 3D Gaussians. To estimate the other four attributes\n(i.e., scaling, rotation, opacity, and SH coefficient), we then design a novel\nText-guided Triplane Generator (TTG) to generate a triplane representation for\na 3D object. The center of each Gaussian enables us to transform the triplane\nfeature into the four attributes. The generated 3D Gaussians can be finally\nrendered at 705 frames per second. Extensive experiments demonstrate the\nsuperiority of our method over existing methods. Also, BrightDreamer possesses\na strong semantic understanding capability even for complex text prompts. The\nproject code is available at https://vlislab22.github.io/BrightDreamer.\n","authors":["Lutao Jiang","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04692v2","updated":"2024-03-17T16:59:25Z","published":"2024-03-07T17:41:37Z","title":"PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K\n  Text-to-Image Generation","summary":"  In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer\nmodel~(DiT) capable of directly generating images at 4K resolution.\nPixArt-\\Sigma represents a significant advancement over its predecessor,\nPixArt-\\alpha, offering images of markedly higher fidelity and improved\nalignment with text prompts. A key feature of PixArt-\\Sigma is its training\nefficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it\nevolves from the `weaker' baseline to a `stronger' model via incorporating\nhigher quality data, a process we term \"weak-to-strong training\". The\nadvancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data:\nPixArt-\\Sigma incorporates superior-quality image data, paired with more\nprecise and detailed image captions. (2) Efficient Token Compression: we\npropose a novel attention module within the DiT framework that compresses both\nkeys and values, significantly improving efficiency and facilitating\nultra-high-resolution image generation. Thanks to these improvements,\nPixArt-\\Sigma achieves superior image quality and user prompt adherence\ncapabilities with significantly smaller model size (0.6B parameters) than\nexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SD\nCascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K\nimages supports the creation of high-resolution posters and wallpapers,\nefficiently bolstering the production of high-quality visual content in\nindustries such as film and gaming.\n","authors":["Junsong Chen","Chongjian Ge","Enze Xie","Yue Wu","Lewei Yao","Xiaozhe Ren","Zhongdao Wang","Ping Luo","Huchuan Lu","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2403.04692v2.pdf","comment":"Project Page: https://pixart-alpha.github.io/PixArt-sigma-project/"},{"id":"http://arxiv.org/abs/2309.10556v2","updated":"2024-03-17T16:55:07Z","published":"2023-09-19T12:05:26Z","title":"Forgedit: Text Guided Image Editing via Learning and Forgetting","summary":"  Text-guided image editing on real or synthetic images, given only the\noriginal image itself and the target text prompt as inputs, is a very general\nand challenging task. It requires an editing model to estimate by itself which\npart of the image should be edited, and then perform either rigid or non-rigid\nediting while preserving the characteristics of original image. In this paper,\nwe design a novel text-guided image editing method, named as Forgedit. First,\nwe propose a vision-language joint optimization framework capable of\nreconstructing the original image in 30 seconds, much faster than previous SOTA\nand much less overfitting. Then we propose a novel vector projection mechanism\nin text embedding space of Diffusion Models, which is capable to control the\nidentity similarity and editing strength seperately. Finally, we discovered a\ngeneral property of UNet in Diffusion Models, i.e., Unet encoder learns space\nand structure, Unet decoder learns appearance and identity. With such a\nproperty, we design forgetting mechanisms to successfully tackle the fatal and\ninevitable overfitting issues when fine-tuning Diffusion Models on one image,\nthus significantly boosting the editing capability of Diffusion Models. Our\nmethod, Forgedit, built on Stable Diffusion, achieves new state-of-the-art\nresults on the challenging text-guided image editing benchmark: TEdBench,\nsurpassing the previous SOTA methods such as Imagic with Imagen, in terms of\nboth CLIP score and LPIPS score. Codes are available at\nhttps://github.com/witcherofresearch/Forgedit\n","authors":["Shiwen Zhang","Shuai Xiao","Weilin Huang"],"pdf_url":"https://arxiv.org/pdf/2309.10556v2.pdf","comment":"Codes are available at https://github.com/witcherofresearch/Forgedit"},{"id":"http://arxiv.org/abs/2403.11270v1","updated":"2024-03-17T16:48:46Z","published":"2024-03-17T16:48:46Z","title":"Bilateral Propagation Network for Depth Completion","summary":"  Depth completion aims to derive a dense depth map from sparse depth\nmeasurements with a synchronized color image. Current state-of-the-art (SOTA)\nmethods are predominantly propagation-based, which work as an iterative\nrefinement on the initial estimated dense depth. However, the initial depth\nestimations mostly result from direct applications of convolutional layers on\nthe sparse depth map. In this paper, we present a Bilateral Propagation Network\n(BP-Net), that propagates depth at the earliest stage to avoid directly\nconvolving on sparse data. Specifically, our approach propagates the target\ndepth from nearby depth measurements via a non-linear model, whose coefficients\nare generated through a multi-layer perceptron conditioned on both\n\\emph{radiometric difference} and \\emph{spatial distance}. By integrating\nbilateral propagation with multi-modal fusion and depth refinement in a\nmulti-scale framework, our BP-Net demonstrates outstanding performance on both\nindoor and outdoor scenes. It achieves SOTA on the NYUv2 dataset and ranks 1st\non the KITTI depth completion benchmark at the time of submission. Experimental\nresults not only show the effectiveness of bilateral propagation but also\nemphasize the significance of early-stage propagation in contrast to the\nrefinement stage. Our code and trained models will be available on the project\npage.\n","authors":["Jie Tang","Fei-Peng Tian","Boshi An","Jian Li","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2403.11270v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2305.12854v2","updated":"2024-03-17T16:34:31Z","published":"2023-05-22T09:27:17Z","title":"RDA-INR: Riemannian Diffeomorphic Autoencoding via Implicit Neural\n  Representations","summary":"  Diffeomorphic registration frameworks such as Large Deformation Diffeomorphic\nMetric Mapping (LDDMM) are used in computer graphics and the medical domain for\natlas building, statistical latent modeling, and pairwise and groupwise\nregistration. In recent years, researchers have developed neural network-based\napproaches regarding diffeomorphic registration to improve the accuracy and\ncomputational efficiency of traditional methods. In this work, we focus on a\nlimitation of neural network-based atlas building and statistical latent\nmodeling methods, namely that they either are (i) resolution dependent or (ii)\ndisregard any data/problem-specific geometry needed for proper mean-variance\nanalysis. In particular, we overcome this limitation by designing a novel\nencoder based on resolution-independent implicit neural representations. The\nencoder achieves resolution invariance for LDDMM-based statistical latent\nmodeling. Additionally, the encoder adds LDDMM Riemannian geometry to\nresolution-independent deep learning models for statistical latent modeling. We\nshowcase that the Riemannian geometry aspect improves latent modeling and is\nrequired for a proper mean-variance analysis. Furthermore, to showcase the\nbenefit of resolution independence for LDDMM-based data variability modeling,\nwe show that our approach outperforms another neural network-based LDDMM latent\ncode model. Our work paves a way to more research into how Riemannian geometry,\nshape/image analysis, and deep learning can be combined.\n","authors":["Sven Dummer","Nicola Strisciuglio","Christoph Brune"],"pdf_url":"https://arxiv.org/pdf/2305.12854v2.pdf","comment":"34 pages, 27 figures (including subfigures)"},{"id":"http://arxiv.org/abs/2403.11263v1","updated":"2024-03-17T16:25:25Z","published":"2024-03-17T16:25:25Z","title":"Stylized Face Sketch Extraction via Generative Prior with Limited Data","summary":"  Facial sketches are both a concise way of showing the identity of a person\nand a means to express artistic intention. While a few techniques have recently\nemerged that allow sketches to be extracted in different styles, they typically\nrely on a large amount of data that is difficult to obtain. Here, we propose\nStyleSketch, a method for extracting high-resolution stylized sketches from a\nface image. Using the rich semantics of the deep features from a pretrained\nStyleGAN, we are able to train a sketch generator with 16 pairs of face and the\ncorresponding sketch images. The sketch generator utilizes part-based losses\nwith two-stage learning for fast convergence during training for high-quality\nsketch extraction. Through a set of comparisons, we show that StyleSketch\noutperforms existing state-of-the-art sketch extraction methods and few-shot\nimage adaptation methods for the task of extracting high-resolution abstract\nface sketches. We further demonstrate the versatility of StyleSketch by\nextending its use to other domains and explore the possibility of semantic\nediting. The project page can be found in\nhttps://kwanyun.github.io/stylesketch_project.\n","authors":["Kwan Yun","Kwanggyoon Seo","Chang Wook Seo","Soyeon Yoon","Seongcheol Kim","Soohyun Ji","Amirsaman Ashtari","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.11263v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.11256v1","updated":"2024-03-17T16:19:40Z","published":"2024-03-17T16:19:40Z","title":"Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised\n  Domain Adaptation","summary":"  Source-free unsupervised domain adaptation (SFUDA) aims to enable the\nutilization of a pre-trained source model in an unlabeled target domain without\naccess to source data. Self-training is a way to solve SFUDA, where confident\ntarget samples are iteratively selected as pseudo-labeled samples to guide\ntarget model learning. However, prior heuristic noisy pseudo-label filtering\nmethods all involve introducing extra models, which are sensitive to model\nassumptions and may introduce additional errors or mislabeling. In this work,\nwe propose a method called Uncertainty-aware Pseudo-label-filtering Adaptation\n(UPA) to efficiently address this issue in a coarse-to-fine manner. Specially,\nwe first introduce a sample selection module named Adaptive Pseudo-label\nSelection (APS), which is responsible for filtering noisy pseudo labels. The\nAPS utilizes a simple sample uncertainty estimation method by aggregating\nknowledge from neighboring samples and confident samples are selected as clean\npseudo-labeled. Additionally, we incorporate Class-Aware Contrastive Learning\n(CACL) to mitigate the memorization of pseudo-label noise by learning robust\npair-wise representation supervised by pseudo labels. Through extensive\nexperiments conducted on three widely used benchmarks, we demonstrate that our\nproposed method achieves competitive performance on par with state-of-the-art\nSFUDA methods. Code is available at https://github.com/chenxi52/UPA.\n","authors":["Xi Chen","Haosen Yang","Huicong Zhang","Hongxun Yao","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11256v1.pdf","comment":"Neurocomputing 2024"},{"id":"http://arxiv.org/abs/2403.07773v3","updated":"2024-03-17T16:18:14Z","published":"2024-03-12T15:59:08Z","title":"SemCity: Semantic Scene Generation with Triplane Diffusion","summary":"  We present \"SemCity,\" a 3D diffusion model for semantic scene generation in\nreal-world outdoor environments. Most 3D diffusion models focus on generating a\nsingle object, synthetic indoor scenes, or synthetic outdoor scenes, while the\ngeneration of real-world outdoor scenes is rarely addressed. In this paper, we\nconcentrate on generating a real-outdoor scene through learning a diffusion\nmodel on a real-world outdoor dataset. In contrast to synthetic data,\nreal-outdoor datasets often contain more empty spaces due to sensor\nlimitations, causing challenges in learning real-outdoor distributions. To\naddress this issue, we exploit a triplane representation as a proxy form of\nscene distributions to be learned by our diffusion model. Furthermore, we\npropose a triplane manipulation that integrates seamlessly with our triplane\ndiffusion model. The manipulation improves our diffusion model's applicability\nin a variety of downstream tasks related to outdoor scene generation such as\nscene inpainting, scene outpainting, and semantic scene completion refinements.\nIn experimental results, we demonstrate that our triplane diffusion model shows\nmeaningful generation results compared with existing work in a real-outdoor\ndataset, SemanticKITTI. We also show our triplane manipulation facilitates\nseamlessly adding, removing, or modifying objects within a scene. Further, it\nalso enables the expansion of scenes toward a city-level scale. Finally, we\nevaluate our method on semantic scene completion refinements where our\ndiffusion model enhances predictions of semantic scene completion networks by\nlearning scene distribution. Our code is available at\nhttps://github.com/zoomin-lee/SemCity.\n","authors":["Jumin Lee","Sebin Lee","Changho Jo","Woobin Im","Juhyeong Seon","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.07773v3.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.06572v2","updated":"2024-03-17T15:58:32Z","published":"2023-11-11T13:52:59Z","title":"Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards\n  Fully Automated Radiation Oncology Treatments","summary":"  The field of Radiation Oncology is uniquely positioned to benefit from the\nuse of artificial intelligence to fully automate the creation of radiation\ntreatment plans for cancer therapy. This time-consuming and specialized task\ncombines patient imaging with organ and tumor segmentation to generate a 3D\nradiation dose distribution to meet clinical treatment goals, similar to\nvoxel-level dense prediction. In this work, we propose Swin UNETR++, that\ncontains a lightweight 3D Dual Cross-Attention (DCA) module to capture the\nintra and inter-volume relationships of each patient's unique anatomy, which\nfully convolutional neural networks lack. Our model was trained, validated, and\ntested on the Open Knowledge-Based Planning dataset. In addition to metrics of\nDose Score $\\overline{S_{\\text{Dose}}}$ and DVH Score\n$\\overline{S_{\\text{DVH}}}$ that quantitatively measure the difference between\nthe predicted and ground-truth 3D radiation dose distribution, we propose the\nqualitative metrics of average volume-wise acceptance rate\n$\\overline{R_{\\text{VA}}}$ and average patient-wise clinical acceptance rate\n$\\overline{R_{\\text{PA}}}$ to assess the clinical reliability of the\npredictions. Swin UNETR++ demonstrates near-state-of-the-art performance on\nvalidation and test dataset (validation: $\\overline{S_{\\text{DVH}}}$=1.492 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.649 Gy, $\\overline{R_{\\text{VA}}}$=88.58%,\n$\\overline{R_{\\text{PA}}}$=100.0%; test: $\\overline{S_{\\text{DVH}}}$=1.634 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.757 Gy, $\\overline{R_{\\text{VA}}}$=90.50%,\n$\\overline{R_{\\text{PA}}}$=98.0%), establishing a basis for future studies to\ntranslate 3D dose predictions into a deliverable treatment plan, facilitating\nfull automation.\n","authors":["Kuancheng Wang","Hai Siong Tan","Rafe Mcbeth"],"pdf_url":"https://arxiv.org/pdf/2311.06572v2.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 16 pages"},{"id":"http://arxiv.org/abs/2302.01089v3","updated":"2024-03-17T15:53:18Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v3.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2403.11251v1","updated":"2024-03-17T15:51:21Z","published":"2024-03-17T15:51:21Z","title":"NeoNeXt: Novel neural network operator and architecture based on the\n  patch-wise matrix multiplications","summary":"  Most of the computer vision architectures nowadays are built upon the\nwell-known foundation operations: fully-connected layers, convolutions and\nmulti-head self-attention blocks. In this paper we propose a novel foundation\noperation - NeoCell - which learns matrix patterns and performs patchwise\nmatrix multiplications with the input data. The main advantages of the proposed\noperator are (1) simple implementation without need in operations like im2col,\n(2) low computational complexity (especially for large matrices) and (3) simple\nand flexible implementation of up-/down-sampling. We validate NeoNeXt family of\nmodels based on this operation on ImageNet-1K classification task and show that\nthey achieve competitive quality.\n","authors":["Vladimir Korviakov","Denis Koposov"],"pdf_url":"https://arxiv.org/pdf/2403.11251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11249v1","updated":"2024-03-17T15:47:54Z","published":"2024-03-17T15:47:54Z","title":"YOLOv9 for Fracture Detection in Pediatric Wrist Trauma X-ray Images","summary":"  The introduction of YOLOv9, the latest version of the You Only Look Once\n(YOLO) series, has led to its widespread adoption across various scenarios.\nThis paper is the first to apply the YOLOv9 algorithm model to the fracture\ndetection task as computer-assisted diagnosis (CAD) to help radiologists and\nsurgeons to interpret X-ray images. Specifically, this paper trained the model\non the GRAZPEDWRI-DX dataset and extended the training set using data\naugmentation techniques to improve the model performance. Experimental results\ndemonstrate that compared to the mAP 50-95 of the current state-of-the-art\n(SOTA) model, the YOLOv9 model increased the value from 42.16% to 43.73%, with\nan improvement of 3.7%. The implementation code is publicly available at\nhttps://github.com/RuiyangJu/YOLOv9-Fracture-Detection.\n","authors":["Chun-Tse Chien","Rui-Yang Ju","Kuang-Yi Chou","Jen-Shiun Chiang"],"pdf_url":"https://arxiv.org/pdf/2403.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11247v1","updated":"2024-03-17T15:41:35Z","published":"2024-03-17T15:41:35Z","title":"Compact 3D Gaussian Splatting For Dense Visual SLAM","summary":"  Recent work has shown that 3D Gaussian-based SLAM enables high-quality\nreconstruction, accurate pose estimation, and real-time rendering of scenes.\nHowever, these approaches are built on a tremendous number of redundant 3D\nGaussian ellipsoids, leading to high memory and storage costs, and slow\ntraining speed. To address the limitation, we propose a compact 3D Gaussian\nSplatting SLAM system that reduces the number and the parameter size of\nGaussian ellipsoids. A sliding window-based masking strategy is first proposed\nto reduce the redundant ellipsoids. Then we observe that the covariance matrix\n(geometry) of most 3D Gaussian ellipsoids are extremely similar, which\nmotivates a novel geometry codebook to compress 3D Gaussian geometric\nattributes, i.e., the parameters. Robust and accurate pose estimation is\nachieved by a global bundle adjustment method with reprojection loss. Extensive\nexperiments demonstrate that our method achieves faster training and rendering\nspeed while maintaining the state-of-the-art (SOTA) quality of the scene\nrepresentation.\n","authors":["Tianchen Deng","Yaohui Chen","Leyan Zhang","Jianfei Yang","Shenghai Yuan","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11237v1","updated":"2024-03-17T14:52:05Z","published":"2024-03-17T14:52:05Z","title":"FORCE: Dataset and Method for Intuitive Physics Guided Human-object\n  Interaction","summary":"  Interactions between human and objects are influenced not only by the\nobject's pose and shape, but also by physical attributes such as object mass\nand surface friction. They introduce important motion nuances that are\nessential for diversity and realism. Despite advancements in recent\nkinematics-based methods, this aspect has been overlooked. Generating nuanced\nhuman motion presents two challenges. First, it is non-trivial to learn from\nmulti-modal human and object information derived from both the physical and\nnon-physical attributes. Second, there exists no dataset capturing nuanced\nhuman interactions with objects of varying physical properties, hampering model\ndevelopment. This work addresses the gap by introducing the FORCE model, a\nkinematic approach for synthesizing diverse, nuanced human-object interactions\nby modeling physical attributes. Our key insight is that human motion is\ndictated by the interrelation between the force exerted by the human and the\nperceived resistance. Guided by a novel intuitive physics encoding, the model\ncaptures the interplay between human force and resistance. Experiments also\ndemonstrate incorporating human force facilitates learning multi-class motion.\nAccompanying our model, we contribute the FORCE dataset. It features diverse,\ndifferent-styled motion through interactions with varying resistances.\n","authors":["Xiaohan Zhang","Bharat Lal Bhatnagar","Sebastian Starke","Ilya Petrov","Vladimir Guzov","Helisa Dhamo","Eduardo Pérez-Pellitero","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2403.11237v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2108.02759v2","updated":"2024-03-17T14:46:31Z","published":"2021-08-05T17:51:32Z","title":"Unifying Global-Local Representations in Salient Object Detection with\n  Transformer","summary":"  The fully convolutional network (FCN) has dominated salient object detection\nfor a long period. However, the locality of CNN requires the model deep enough\nto have a global receptive field and such a deep model always leads to the loss\nof local details. In this paper, we introduce a new attention-based encoder,\nvision transformer, into salient object detection to ensure the globalization\nof the representations from shallow to deep layers. With the global view in\nvery shallow layers, the transformer encoder preserves more local\nrepresentations to recover the spatial details in final saliency maps. Besides,\nas each layer can capture a global view of its previous layer, adjacent layers\ncan implicitly maximize the representation differences and minimize the\nredundant features, making that every output feature of transformer layers\ncontributes uniquely for final prediction. To decode features from the\ntransformer, we propose a simple yet effective deeply-transformed decoder. The\ndecoder densely decodes and upsamples the transformer features, generating the\nfinal saliency map with less noise injection. Experimental results demonstrate\nthat our method significantly outperforms other FCN-based and transformer-based\nmethods in five benchmarks by a large margin, with an average of 12.17%\nimprovement in terms of Mean Absolute Error (MAE). Code will be available at\nhttps://github.com/OliverRensu/GLSTR.\n","authors":["Sucheng Ren","Qiang Wen","Nanxuan Zhao","Guoqiang Han","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2108.02759v2.pdf","comment":"accepted by IEEE TETCI"},{"id":"http://arxiv.org/abs/2403.11234v1","updated":"2024-03-17T14:43:47Z","published":"2024-03-17T14:43:47Z","title":"Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class\n  Bias","summary":"  Domain adaptation is a critical task in machine learning that aims to improve\nmodel performance on a target domain by leveraging knowledge from a related\nsource domain. In this work, we introduce Universal Semi-Supervised Domain\nAdaptation (UniSSDA), a practical yet challenging setting where the target\ndomain is partially labeled, and the source and target label space may not\nstrictly match. UniSSDA is at the intersection of Universal Domain Adaptation\n(UniDA) and Semi-Supervised Domain Adaptation (SSDA): the UniDA setting does\nnot allow for fine-grained categorization of target private classes not\nrepresented in the source domain, while SSDA focuses on the restricted\nclosed-set setting where source and target label spaces match exactly. Existing\nUniDA and SSDA methods are susceptible to common-class bias in UniSSDA\nsettings, where models overfit to data distributions of classes common to both\ndomains at the expense of private classes. We propose a new prior-guided\npseudo-label refinement strategy to reduce the reinforcement of common-class\nbias due to pseudo-labeling, a common label propagation strategy in domain\nadaptation. We demonstrate the effectiveness of the proposed strategy on\nbenchmark datasets Office-Home, DomainNet, and VisDA. The proposed strategy\nattains the best performance across UniSSDA adaptation settings and establishes\na new baseline for UniSSDA.\n","authors":["Wenyu Zhang","Qingmu Liu","Felix Ong Wei Cong","Mohamed Ragab","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2403.11234v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11233v1","updated":"2024-03-17T14:42:05Z","published":"2024-03-17T14:42:05Z","title":"STAIR: Semantic-Targeted Active Implicit Reconstruction","summary":"  Many autonomous robotic applications require object-level understanding when\ndeployed. Actively reconstructing objects of interest, i.e. objects with\nspecific semantic meanings, is therefore relevant for a robot to perform\ndownstream tasks in an initially unknown environment. In this work, we propose\na novel framework for semantic-targeted active reconstruction using posed RGB-D\nmeasurements and 2D semantic labels as input. The key components of our\nframework are a semantic implicit neural representation and a compatible\nplanning utility function based on semantic rendering and uncertainty\nestimation, enabling adaptive view planning to target objects of interest. Our\nplanning approach achieves better reconstruction performance in terms of mesh\nand novel view rendering quality compared to implicit reconstruction baselines\nthat do not consider semantics for view planning. Our framework further\noutperforms a state-of-the-art semantic-targeted active reconstruction pipeline\nbased on explicit maps, justifying our choice of utilising implicit neural\nrepresentations to tackle semantic-targeted active reconstruction problems.\n","authors":["Liren Jin","Haofei Kuang","Yue Pan","Cyrill Stachniss","Marija Popović"],"pdf_url":"https://arxiv.org/pdf/2403.11233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11230v1","updated":"2024-03-17T14:34:51Z","published":"2024-03-17T14:34:51Z","title":"Simple 2D Convolutional Neural Network-based Approach for COVID-19\n  Detection","summary":"  This study explores the use of deep learning techniques for analyzing lung\nComputed Tomography (CT) images. Classic deep learning approaches face\nchallenges with varying slice counts and resolutions in CT images, a diversity\narising from the utilization of assorted scanning equipment. Typically,\npredictions are made on single slices which are then combined for a\ncomprehensive outcome. Yet, this method does not incorporate learning features\nspecific to each slice, leading to a compromise in effectiveness. To address\nthese challenges, we propose an advanced Spatial-Slice Feature Learning\n(SSFL++) framework specifically tailored for CT scans. It aims to filter out\nout-of-distribution (OOD) data within the entire CT scan, allowing us to select\nessential spatial-slice features for analysis by reducing data redundancy by\n70\\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS)\nmethod to enhance stability during training and inference phases, thereby\naccelerating convergence and enhancing overall performance. Remarkably, our\nexperiments reveal that our model achieves promising results with a simple\nEfficientNet-2D (E2D) model. The effectiveness of our approach is confirmed on\nthe COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop.\n","authors":["Chih-Chung Hsu","Chia-Ming Lee","Yang Fan Chiang","Yi-Shiuan Chou","Chih-Yu Jiang","Shen-Chieh Tai","Chi-Han Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.11230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11229v1","updated":"2024-03-17T14:30:56Z","published":"2024-03-17T14:30:56Z","title":"Concatenate, Fine-tuning, Re-training: A SAM-enabled Framework for\n  Semi-supervised 3D Medical Image Segmentation","summary":"  Segment Anything Model (SAM) fine-tuning has shown remarkable performance in\nmedical image segmentation in a fully supervised manner, but requires precise\nannotations. To reduce the annotation cost and maintain satisfactory\nperformance, in this work, we leverage the capabilities of SAM for establishing\nsemi-supervised medical image segmentation models. Rethinking the requirements\nof effectiveness, efficiency, and compatibility, we propose a three-stage\nframework, i.e., Concatenate, Fine-tuning, and Re-training (CFR). The current\nfine-tuning approaches mostly involve 2D slice-wise fine-tuning that disregards\nthe contextual information between adjacent slices. Our concatenation strategy\nmitigates the mismatch between natural and 3D medical images. The concatenated\nimages are then used for fine-tuning SAM, providing robust initialization\npseudo-labels. Afterwards, we train a 3D semi-supervised segmentation model\nwhile maintaining the same parameter size as the conventional segmenter such as\nV-Net. Our CFR framework is plug-and-play, and easily compatible with various\npopular semi-supervised methods. Extensive experiments validate that our CFR\nachieves significant improvements in both moderate annotation and scarce\nannotation across four datasets. In particular, CFR framework improves the Dice\nscore of Mean Teacher from 29.68% to 74.40% with only one labeled data of LA\ndataset.\n","authors":["Shumeng Li","Lei Qi","Qian Yu","Jing Huo","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2403.11229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09257v2","updated":"2024-03-17T14:14:28Z","published":"2024-03-14T10:30:43Z","title":"WSI-SAM: Multi-resolution Segment Anything Model (SAM) for\n  histopathology whole-slide images","summary":"  The Segment Anything Model (SAM) marks a significant advancement in\nsegmentation models, offering robust zero-shot abilities and dynamic prompting.\nHowever, existing medical SAMs are not suitable for the multi-scale nature of\nwhole-slide images (WSIs), restricting their effectiveness. To resolve this\ndrawback, we present WSI-SAM, enhancing SAM with precise object segmentation\ncapabilities for histopathology images using multi-resolution patches, while\npreserving its efficient, prompt-driven design, and zero-shot abilities. To\nfully exploit pretrained knowledge while minimizing training overhead, we keep\nSAM frozen, introducing only minimal extra parameters and computational\noverhead. In particular, we introduce High-Resolution (HR) token,\nLow-Resolution (LR) token and dual mask decoder. This decoder integrates the\noriginal SAM mask decoder with a lightweight fusion module that integrates\nfeatures at multiple scales. Instead of predicting a mask independently, we\nintegrate HR and LR token at intermediate layer to jointly learn features of\nthe same object across multiple resolutions. Experiments show that our WSI-SAM\noutperforms state-of-the-art SAM and its variants. In particular, our model\noutperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ\n(DCIS) segmentation tasks and breast cancer metastasis segmentation task\n(CAMELYON16 dataset). The code will be available at\nhttps://github.com/HongLiuuuuu/WSI-SAM.\n","authors":["Hong Liu","Haosen Yang","Paul J. van Diest","Josien P. W. Pluim","Mitko Veta"],"pdf_url":"https://arxiv.org/pdf/2403.09257v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2308.06378v3","updated":"2024-03-17T14:10:15Z","published":"2023-08-11T20:32:39Z","title":"DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System","summary":"  A key challenge in eXplainable Artificial Intelligence is the well-known\ntradeoff between the transparency of an algorithm (i.e., how easily a human can\ndirectly understand the algorithm, as opposed to receiving a post-hoc\nexplanation), and its accuracy. We report on the design of a new deep network\nthat achieves improved transparency without sacrificing accuracy. We design a\ndeep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy\nlogic and deep learning models and show that DCNFIS performs as accurately as\nexisting convolutional neural networks on four well-known datasets and 3 famous\narchitectures. Our performance comparison with available fuzzy methods show\nthat DCNFIS is now state-of-the-art fuzzy system and outperforms other shallow\nand deep fuzzy methods to the best of our knowledge. At the end, we exploit the\ntransparency of fuzzy logic by deriving explanations, in the form of saliency\nmaps, from the fuzzy rules encoded in the network to take benefit of fuzzy\nlogic upon regular deep learning methods. We investigate the properties of\nthese explanations in greater depth using the Fashion-MNIST dataset.\n","authors":["Mojtaba Yeganejou","Kimia Honari","Ryan Kluzinski","Scott Dick","Michael Lipsett","James Miller"],"pdf_url":"https://arxiv.org/pdf/2308.06378v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09034v2","updated":"2024-03-17T13:59:38Z","published":"2024-03-14T02:11:16Z","title":"rFaceNet: An End-to-End Network for Enhanced Physiological Signal\n  Extraction through Identity-Specific Facial Contours","summary":"  Remote photoplethysmography (rPPG) technique extracts blood volume pulse\n(BVP) signals from subtle pixel changes in video frames. This study introduces\nrFaceNet, an advanced rPPG method that enhances the extraction of facial BVP\nsignals with a focus on facial contours. rFaceNet integrates identity-specific\nfacial contour information and eliminates redundant data. It efficiently\nextracts facial contours from temporally normalized frame inputs through a\nTemporal Compressor Unit (TCU) and steers the model focus to relevant facial\nregions by using the Cross-Task Feature Combiner (CTFC). Through elaborate\ntraining, the quality and interpretability of facial physiological signals\nextracted by rFaceNet are greatly improved compared to previous methods.\nMoreover, our novel approach demonstrates superior performance than SOTA\nmethods in various heart rate estimation benchmarks.\n","authors":["Dali Zhu","Wenli Zhang","Hualin Zeng","Xiaohao Liu","Long Yang","Jiaqi Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.09034v2.pdf","comment":"under-review"},{"id":"http://arxiv.org/abs/2403.11222v1","updated":"2024-03-17T13:51:25Z","published":"2024-03-17T13:51:25Z","title":"SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream","summary":"  Spike cameras, leveraging spike-based integration sampling and high temporal\nresolution, offer distinct advantages over standard cameras. However, existing\napproaches reliant on spike cameras often assume optimal illumination, a\ncondition frequently unmet in real-world scenarios. To address this, we\nintroduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene\nrepresentation from spike camera data. Our approach leverages NeRF's multi-view\nconsistency to establish robust self-supervision, effectively eliminating\nerroneous measurements and uncovering coherent structures within exceedingly\nnoisy input amidst diverse real-world illumination scenarios. The framework\ncomprises two core elements: a spike generation model incorporating an\nintegrate-and-fire neuron layer and parameters accounting for non-idealities,\nsuch as threshold variation, and a spike rendering loss capable of generalizing\nacross varying illumination conditions. We describe how to effectively optimize\nneural radiance fields to render photorealistic novel views from the novel\ncontinuous spike stream, demonstrating advantages over other vision sensors in\ncertain scenes. Empirical evaluations conducted on both real and novel\nrealistically simulated sequences affirm the efficacy of our methodology. The\ndataset and source code are released at\nhttps://github.com/BIT-Vision/SpikeNeRF.\n","authors":["Lin Zhu","Kangmin Jia","Yifan Zhao","Yunshan Qi","Lizhi Wang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11222v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.03452v3","updated":"2024-03-17T13:47:00Z","published":"2024-03-06T04:36:43Z","title":"D4C glove-train: solving the RPM and Bongard-logo problem by\n  distributing and Circumscribing concepts","summary":"  This paper achieves significant progress in the field of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nproblems. We propose the D2C approach, which redefines conceptual boundaries in\nthese domains and bridges the gap between high-level concepts and their\nlow-dimensional representations. Based on this, we further introduce the D3C\nmethod that handles Bongard-Logo problems and significantly improves reasoning\naccuracy by estimating the distribution of image representations and measuring\ntheir Sinkhorn distance. To enhance computational efficiency, we introduce the\nD3C-cos variant, which provides an efficient and accurate solution for RPM\nproblems by constraining distribution distances. Additionally, we present\nLico-Net, a network that combines D3C and D3C-cos to achieve state-of-the-art\nperformance in both problem-solving and interpretability. Finally, we extend\nour approach to D4C, employing adversarial strategies to further refine\nconceptual boundaries and demonstrate notable improvements for both RPM and\nBongard-Logo problems. Overall, our contributions offer a new perspective and\npractical solutions to the field of abstract reasoning.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03452v3.pdf","comment":"16 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.11220v1","updated":"2024-03-17T13:43:10Z","published":"2024-03-17T13:43:10Z","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations","summary":"  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n","authors":["Yuwei Zhang","Yan Wu","Yanming Liu","Xinyue Peng"],"pdf_url":"https://arxiv.org/pdf/2403.11220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03379v2","updated":"2024-03-17T13:37:37Z","published":"2024-01-07T03:35:04Z","title":"Towards Effective Multiple-in-One Image Restoration: A Sequential and\n  Prompt Learning Strategy","summary":"  While single task image restoration (IR) has achieved significant successes,\nit remains a challenging issue to train a single model which can tackle\nmultiple IR tasks. In this work, we investigate in-depth the multiple-in-one\n(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO\nIR faces two pivotal challenges: the optimization of diverse objectives and the\nadaptation to multiple tasks. To tackle these challenges, we present two simple\nyet effective strategies. The first strategy, referred to as sequential\nlearning, attempts to address how to optimize the diverse objectives, which\nguides the network to incrementally learn individual IR tasks in a sequential\nmanner rather than mixing them together. The second strategy, i.e., prompt\nlearning, attempts to address how to adapt to the different IR tasks, which\nassists the network to understand the specific task and improves the\ngeneralization ability. By evaluating on 19 test sets, we demonstrate that the\nsequential and prompt learning strategies can significantly enhance the MiO\nperformance of commonly used CNN and Transformer backbones. Our experiments\nalso reveal that the two strategies can supplement each other to learn better\ndegradation representations and enhance the model robustness. It is expected\nthat our proposed MiO IR formulation and strategies could facilitate the\nresearch on how to train IR models with higher generalization capabilities.\n","authors":["Xiangtao Kong","Chao Dong","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09194v2","updated":"2024-03-17T13:28:32Z","published":"2024-03-14T09:07:31Z","title":"Intention-driven Ego-to-Exo Video Generation","summary":"  Ego-to-exo video generation refers to generating the corresponding exocentric\nvideo according to the egocentric video, providing valuable applications in\nAR/VR and embodied AI. Benefiting from advancements in diffusion model\ntechniques, notable progress has been achieved in video generation. However,\nexisting methods build upon the spatiotemporal consistency assumptions between\nadjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to\ndrastic changes in views. To this end, this paper proposes an Intention-Driven\nEgo-to-exo video generation framework (IDE) that leverages action intention\nconsisting of human movement and action description as view-independent\nrepresentation to guide video generation, preserving the consistency of content\nand motion. Specifically, the egocentric head trajectory is first estimated\nthrough multi-view stereo matching. Then, cross-view feature perception module\nis introduced to establish correspondences between exo- and ego- views, guiding\nthe trajectory transformation module to infer human full-body movement from the\nhead trajectory. Meanwhile, we present an action description unit that maps the\naction semantics into the feature space consistent with the exocentric image.\nFinally, the inferred human movement and high-level action descriptions jointly\nguide the generation of exocentric motion and interaction content (i.e.,\ncorresponding optical flow and occlusion maps) in the backward process of the\ndiffusion model, ultimately warping them into the corresponding exocentric\nvideo. We conduct extensive experiments on the relevant dataset with diverse\nexo-ego video pairs, and our IDE outperforms state-of-the-art models in both\nsubjective and objective assessments, demonstrating its efficacy in ego-to-exo\nvideo generation.\n","authors":["Hongchen Luo","Kai Zhu","Wei Zhai","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.09194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11211v1","updated":"2024-03-17T13:23:25Z","published":"2024-03-17T13:23:25Z","title":"RCdpia: A Renal Carcinoma Digital Pathology Image Annotation dataset\n  based on pathologists","summary":"  The annotation of digital pathological slide data for renal cell carcinoma is\nof paramount importance for correct diagnosis of artificial intelligence models\ndue to the heterogeneous nature of the tumor. This process not only facilitates\na deeper understanding of renal cell cancer heterogeneity but also aims to\nminimize noise in the data for more accurate studies. To enhance the\napplicability of the data, two pathologists were enlisted to meticulously\ncurate, screen, and label a kidney cancer pathology image dataset from The\nCancer Genome Atlas Program (TCGA) database. Subsequently, a Resnet model was\ndeveloped to validate the annotated dataset against an additional dataset from\nthe First Affiliated Hospital of Zhejiang University. Based on these results,\nwe have meticulously compiled the TCGA digital pathological dataset with\nindependent labeling of tumor regions and adjacent areas (RCdpia), which\nincludes 109 cases of kidney chromophobe cell carcinoma, 486 cases of kidney\nclear cell carcinoma, and 292 cases of kidney papillary cell carcinoma. This\ndataset is now publicly accessible at http://39.171.241.18:8888/RCdpia/.\nFurthermore, model analysis has revealed significant discrepancies in\npredictive outcomes when applying the same model to datasets from different\ncenters. Leveraging the RCdpia, we can now develop more precise digital\npathology artificial intelligence models for tasks such as normalization,\nclassification, and segmentation. These advancements underscore the potential\nfor more nuanced and accurate AI applications in the field of digital\npathology.\n","authors":["Qingrong Sun","Weixiang Zhong","Jie Zhou","Chong Lai","Xiaodong Teng","Maode Lai"],"pdf_url":"https://arxiv.org/pdf/2403.11211v1.pdf","comment":"8 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.11208v1","updated":"2024-03-17T13:17:25Z","published":"2024-03-17T13:17:25Z","title":"THOR: Text to Human-Object Interaction Diffusion via Relation\n  Intervention","summary":"  This paper addresses new methodologies to deal with the challenging task of\ngenerating dynamic Human-Object Interactions from textual descriptions\n(Text2HOI). While most existing works assume interactions with limited body\nparts or static objects, our task involves addressing the variation in human\nmotion, the diversity of object shapes, and the semantic vagueness of object\nmotion simultaneously. To tackle this, we propose a novel Text-guided\nHuman-Object Interaction diffusion model with Relation Intervention (THOR).\nTHOR is a cohesive diffusion model equipped with a relation intervention\nmechanism. In each diffusion step, we initiate text-guided human and object\nmotion and then leverage human-object relations to intervene in object motion.\nThis intervention enhances the spatial-temporal relations between humans and\nobjects, with human-centric interaction representation providing additional\nguidance for synthesizing consistent motion from text. To achieve more\nreasonable and realistic results, interaction losses is introduced at different\nlevels of motion granularity. Moreover, we construct Text-BEHAVE, a Text2HOI\ndataset that seamlessly integrates textual descriptions with the currently\nlargest publicly available 3D HOI dataset. Both quantitative and qualitative\nexperiments demonstrate the effectiveness of our proposed model.\n","authors":["Qianyang Wu","Ye Shi","Xiaoshui Huang","Jingyi Yu","Lan Xu","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11207v1","updated":"2024-03-17T13:15:22Z","published":"2024-03-17T13:15:22Z","title":"MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data","summary":"  Reconstructions of visual perception from brain activity have improved\ntremendously, but the practical utility of such methods has been limited. This\nis because such models are trained independently per subject where each subject\nrequires dozens of hours of expensive fMRI training data to attain high-quality\nresults. The present work showcases high-quality reconstructions using only 1\nhour of fMRI training data. We pretrain our model across 7 subjects and then\nfine-tune on minimal data from a new subject. Our novel functional alignment\nprocedure linearly maps all brain data to a shared-subject latent space,\nfollowed by a shared non-linear mapping to CLIP image space. We then map from\nCLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP\nlatents as inputs instead of text. This approach improves out-of-subject\ngeneralization with limited training data and also attains state-of-the-art\nimage retrieval and reconstruction metrics compared to single-subject\napproaches. MindEye2 demonstrates how accurate reconstructions of perception\nare possible from a single visit to the MRI facility. All code is available on\nGitHub.\n","authors":["Paul S. Scotti","Mihir Tripathy","Cesar Kadir Torrico Villanueva","Reese Kneeland","Tong Chen","Ashutosh Narang","Charan Santhirasegaran","Jonathan Xu","Thomas Naselaris","Kenneth A. Norman","Tanishq Mathew Abraham"],"pdf_url":"https://arxiv.org/pdf/2403.11207v1.pdf","comment":"Code at https://github.com/MedARC-AI/MindEyeV2/tree/main"},{"id":"http://arxiv.org/abs/2403.11197v1","updated":"2024-03-17T12:49:02Z","published":"2024-03-17T12:49:02Z","title":"TAG: Guidance-free Open-Vocabulary Semantic Segmentation","summary":"  Semantic segmentation is a crucial task in computer vision, where each pixel\nin an image is classified into a category. However, traditional methods face\nsignificant challenges, including the need for pixel-level annotations and\nextensive training. Furthermore, because supervised learning uses a limited set\nof predefined categories, models typically struggle with rare classes and\ncannot recognize new ones. Unsupervised and open-vocabulary segmentation,\nproposed to tackle these issues, faces challenges, including the inability to\nassign specific class labels to clusters and the necessity of user-provided\ntext queries for guidance. In this context, we propose a novel approach, TAG\nwhich achieves Training, Annotation, and Guidance-free open-vocabulary semantic\nsegmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment\nimages into meaningful categories without additional training or dense\nannotations. It retrieves class labels from an external database, providing\nflexibility to adapt to new scenarios. Our TAG achieves state-of-the-art\nresults on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation\nwithout given class names, i.e. improvement of +15.3 mIoU on PascalVOC. All\ncode and data will be released at https://github.com/Valkyrja3607/TAG.\n","authors":["Yasufumi Kawano","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2403.11197v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.11194v1","updated":"2024-03-17T12:40:49Z","published":"2024-03-17T12:40:49Z","title":"MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic\n  Segmentation","summary":"  Semantic segmentation is essential in computer vision for various\napplications, yet traditional approaches face significant challenges, including\nthe high cost of annotation and extensive training for supervised learning.\nAdditionally, due to the limited predefined categories in supervised learning,\nmodels typically struggle with infrequent classes and are unable to predict\nnovel classes. To address these limitations, we propose MaskDiffusion, an\ninnovative approach that leverages pretrained frozen Stable Diffusion to\nachieve open-vocabulary semantic segmentation without the need for additional\ntraining or annotation, leading to improved performance compared to similar\nmethods. We also demonstrate the superior performance of MaskDiffusion in\nhandling open vocabularies, including fine-grained and proper noun-based\ncategories, thus expanding the scope of segmentation applications. Overall, our\nMaskDiffusion shows significant qualitative and quantitative improvements in\ncontrast to other comparable unsupervised segmentation methods, i.e. on the\nPotsdam dataset (+10.5 mIoU compared to GEM) and COCO-Stuff (+14.8 mIoU\ncompared to DiffSeg). All code and data will be released at\nhttps://github.com/Valkyrja3607/MaskDiffusion.\n","authors":["Yasufumi Kawano","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2403.11194v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2403.11193v1","updated":"2024-03-17T12:40:46Z","published":"2024-03-17T12:40:46Z","title":"Neural Markov Random Field for Stereo Matching","summary":"  Stereo matching is a core task for many computer vision and robotics\napplications. Despite their dominance in traditional stereo methods, the\nhand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy\ncompared to end-to-end deep models. While deep learning representations have\ngreatly improved the unary terms of the MRF models, the overall accuracy is\nstill severely limited by the hand-crafted pairwise terms and message passing.\nTo address these issues, we propose a neural MRF model, where both potential\nfunctions and message passing are designed using data-driven neural networks.\nOur fully data-driven model is built on the foundation of variational inference\ntheory, to prevent convergence issues and retain stereo MRF's graph inductive\nbias. To make the inference tractable and scale well to high-resolution images,\nwe also propose a Disparity Proposal Network (DPN) to adaptively prune the\nsearch space of disparity. The proposed approach ranks $1^{st}$ on both KITTI\n2012 and 2015 leaderboards among all published methods while running faster\nthan 100 ms. This approach significantly outperforms prior global methods,\ne.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our\nmethod exhibits strong cross-domain generalization and can recover sharp edges.\nThe codes at https://github.com/aeolusguan/NMRF .\n","authors":["Tongfan Guan","Chen Wang","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11193v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11192v1","updated":"2024-03-17T12:38:58Z","published":"2024-03-17T12:38:58Z","title":"Self-Supervised Video Desmoking for Laparoscopic Surgery","summary":"  Due to the difficulty of collecting real paired data, most existing desmoking\nmethods train the models by synthesizing smoke, generalizing poorly to real\nsurgical scenarios. Although a few works have explored single-image real-world\ndesmoking in unpaired learning manners, they still encounter challenges in\nhandling dense smoke. In this work, we address these issues together by\nintroducing the self-supervised surgery video desmoking (SelfSVD). On the one\nhand, we observe that the frame captured before the activation of high-energy\ndevices is generally clear (named pre-smoke frame, PS frame), thus it can serve\nas supervision for other smoky frames, making real-world self-supervised video\ndesmoking practically feasible. On the other hand, in order to enhance the\ndesmoking performance, we further feed the valuable information from PS frame\ninto models, where a masking strategy and a regularization term are presented\nto avoid trivial solutions. In addition, we construct a real surgery video\ndataset for desmoking, which covers a variety of smoky scenes. Extensive\nexperiments on the dataset show that our SelfSVD can remove smoke more\neffectively and efficiently while recovering more photo-realistic details than\nthe state-of-the-art methods. The dataset, codes, and pre-trained models are\navailable at \\url{https://github.com/ZcsrenlongZ/SelfSVD}.\n","authors":["Renlong Wu","Zhilu Zhang","Shuohao Zhang","Longfei Gou","Haobin Chen","Lei Zhang","Hao Chen","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2403.11192v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2403.11189v1","updated":"2024-03-17T12:26:23Z","published":"2024-03-17T12:26:23Z","title":"Boosting Semi-Supervised Temporal Action Localization by Learning from\n  Non-Target Classes","summary":"  The crux of semi-supervised temporal action localization (SS-TAL) lies in\nexcavating valuable information from abundant unlabeled videos. However,\ncurrent approaches predominantly focus on building models that are robust to\nthe error-prone target class (i.e, the predicted class with the highest\nconfidence) while ignoring informative semantics within non-target classes.\nThis paper approaches SS-TAL from a novel perspective by advocating for\nlearning from non-target classes, transcending the conventional focus solely on\nthe target class. The proposed approach involves partitioning the label space\nof the predicted class distribution into distinct subspaces: target class,\npositive classes, negative classes, and ambiguous classes, aiming to mine both\npositive and negative semantics that are absent in the target class, while\nexcluding ambiguous classes. To this end, we first devise innovative strategies\nto adaptively select high-quality positive and negative classes from the label\nspace, by modeling both the confidence and rank of a class in relation to those\nof the target class. Then, we introduce novel positive and negative losses\ndesigned to guide the learning process, pushing predictions closer to positive\nclasses and away from negative classes. Finally, the positive and negative\nprocesses are integrated into a hybrid positive-negative learning framework,\nfacilitating the utilization of non-target classes in both labeled and\nunlabeled videos. Experimental results on THUMOS14 and ActivityNet v1.3\ndemonstrate the superiority of the proposed method over prior state-of-the-art\napproaches.\n","authors":["Kun Xia","Le Wang","Sanping Zhou","Gang Hua","Wei Tang"],"pdf_url":"https://arxiv.org/pdf/2403.11189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16861v2","updated":"2024-03-17T12:15:34Z","published":"2024-01-30T10:04:49Z","title":"Repositioning the Subject within Image","summary":"  Current image manipulation primarily centers on static manipulation, such as\nreplacing specific regions within an image or altering its overall style. In\nthis paper, we introduce an innovative dynamic manipulation task, subject\nrepositioning. This task involves relocating a user-specified subject to a\ndesired position while preserving the image's fidelity. Our research reveals\nthat the fundamental sub-tasks of subject repositioning, which include filling\nthe void left by the repositioned subject, reconstructing obscured portions of\nthe subject and blending the subject to be consistent with surrounding areas,\ncan be effectively reformulated as a unified, prompt-guided inpainting task.\nConsequently, we can employ a single diffusion generative model to address\nthese sub-tasks using various task prompts learned through our proposed task\ninversion technique. Additionally, we integrate pre-processing and\npost-processing techniques to further enhance the quality of subject\nrepositioning. These elements together form our SEgment-gEnerate-and-bLEnd\n(SEELE) framework. To assess SEELE's effectiveness in subject repositioning, we\nassemble a real-world subject repositioning dataset called ReS. Results of\nSEELE on ReS demonstrate its efficacy.\n","authors":["Yikai Wang","Chenjie Cao","Ke Fan","Qiaole Dong","Yifan Li","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2401.16861v2.pdf","comment":"Project page: https://yikai-wang.github.io/seele/. Dataset:\n  https://github.com/Yikai-Wang/ReS. Arxiv version uses small size images for\n  fast preview. Full size PDF is available at project page"},{"id":"http://arxiv.org/abs/2403.11186v1","updated":"2024-03-17T12:15:02Z","published":"2024-03-17T12:15:02Z","title":"NetTrack: Tracking Highly Dynamic Objects with a Net","summary":"  The complex dynamicity of open-world objects presents non-negligible\nchallenges for multi-object tracking (MOT), often manifested as severe\ndeformations, fast motion, and occlusions. Most methods that solely depend on\ncoarse-grained object cues, such as boxes and the overall appearance of the\nobject, are susceptible to degradation due to distorted internal relationships\nof dynamic objects. To address this problem, this work proposes NetTrack, an\nefficient, generic, and affordable tracking framework to introduce fine-grained\nlearning that is robust to dynamicity. Specifically, NetTrack constructs a\ndynamicity-aware association with a fine-grained Net, leveraging point-level\nvisual cues. Correspondingly, a fine-grained sampler and matching method have\nbeen incorporated. Furthermore, NetTrack learns object-text correspondence for\nfine-grained localization. To evaluate MOT in extremely dynamic open-world\nscenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits\nhigh dynamicity with diverse species and open-world scenarios. Comprehensive\nevaluation on BFT validates the effectiveness of fine-grained learning on\nobject dynamicity, and thorough transfer experiments on challenging open-world\nbenchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong\ngeneralization ability of NetTrack even without finetuning. Project page:\nhttps://george-zhuang.github.io/nettrack/.\n","authors":["Guangze Zheng","Shijie Lin","Haobo Zuo","Changhong Fu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.11186v1.pdf","comment":"Accepted by CVPR 2024"}],"Graphics":[{"id":"http://arxiv.org/abs/2312.08459v2","updated":"2024-03-17T23:45:01Z","published":"2023-12-13T19:01:07Z","title":"FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head\n  Models","summary":"  We introduce FaceTalk, a novel generative approach designed for synthesizing\nhigh-fidelity 3D motion sequences of talking human heads from input audio\nsignal. To capture the expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to couple speech signal\nwith the latent space of neural parametric head models to create high-fidelity,\ntemporally coherent motion sequences. We propose a new latent diffusion model\nfor this task, operating in the expression space of neural parametric head\nmodels, to synthesize audio-driven realistic head sequences. In the absence of\na dataset with corresponding NPHM expressions to audio, we optimize for these\ncorrespondences to produce a dataset of temporally-optimized NPHM expressions\nfit to audio-video recordings of people talking. To the best of our knowledge,\nthis is the first work to propose a generative approach for realistic and\nhigh-quality motion synthesis of volumetric human heads, representing a\nsignificant advancement in the field of audio-driven 3D animation. Notably, our\napproach stands out in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the NPHM shape space. Our\nexperimental results substantiate the effectiveness of FaceTalk, consistently\nachieving superior and visually natural motion, encompassing diverse facial\nexpressions and styles, outperforming existing methods by 75% in perceptual\nuser study evaluation.\n","authors":["Shivangi Aneja","Justus Thies","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.08459v2.pdf","comment":"Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:\n  https://shivangi-aneja.github.io/projects/facetalk/"},{"id":"http://arxiv.org/abs/2403.11367v1","updated":"2024-03-17T23:06:12Z","published":"2024-03-17T23:06:12Z","title":"3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual\n  ReLocalization","summary":"  This paper presents a novel system designed for 3D mapping and visual\nrelocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and\ncamera data to create accurate and visually plausible representations of the\nenvironment. By leveraging LiDAR data to initiate the training of the 3D\nGaussian Splatting map, our system constructs maps that are both detailed and\ngeometrically accurate. To mitigate excessive GPU memory usage and facilitate\nrapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.\nThis preparation makes our method well-suited for visual localization tasks,\nenabling efficient identification of correspondences between the query image\nand the rendered image from the Gaussian Splatting map via normalized\ncross-correlation (NCC). Additionally, we refine the camera pose of the query\nimage using feature-based matching and the Perspective-n-Point (PnP) technique.\nThe effectiveness, adaptability, and precision of our system are demonstrated\nthrough extensive evaluation on the KITTI360 dataset.\n","authors":["Peng Jiang","Gaurav Pandey","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.11367v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2305.12854v2","updated":"2024-03-17T16:34:31Z","published":"2023-05-22T09:27:17Z","title":"RDA-INR: Riemannian Diffeomorphic Autoencoding via Implicit Neural\n  Representations","summary":"  Diffeomorphic registration frameworks such as Large Deformation Diffeomorphic\nMetric Mapping (LDDMM) are used in computer graphics and the medical domain for\natlas building, statistical latent modeling, and pairwise and groupwise\nregistration. In recent years, researchers have developed neural network-based\napproaches regarding diffeomorphic registration to improve the accuracy and\ncomputational efficiency of traditional methods. In this work, we focus on a\nlimitation of neural network-based atlas building and statistical latent\nmodeling methods, namely that they either are (i) resolution dependent or (ii)\ndisregard any data/problem-specific geometry needed for proper mean-variance\nanalysis. In particular, we overcome this limitation by designing a novel\nencoder based on resolution-independent implicit neural representations. The\nencoder achieves resolution invariance for LDDMM-based statistical latent\nmodeling. Additionally, the encoder adds LDDMM Riemannian geometry to\nresolution-independent deep learning models for statistical latent modeling. We\nshowcase that the Riemannian geometry aspect improves latent modeling and is\nrequired for a proper mean-variance analysis. Furthermore, to showcase the\nbenefit of resolution independence for LDDMM-based data variability modeling,\nwe show that our approach outperforms another neural network-based LDDMM latent\ncode model. Our work paves a way to more research into how Riemannian geometry,\nshape/image analysis, and deep learning can be combined.\n","authors":["Sven Dummer","Nicola Strisciuglio","Christoph Brune"],"pdf_url":"https://arxiv.org/pdf/2305.12854v2.pdf","comment":"34 pages, 27 figures (including subfigures)"},{"id":"http://arxiv.org/abs/2403.11263v1","updated":"2024-03-17T16:25:25Z","published":"2024-03-17T16:25:25Z","title":"Stylized Face Sketch Extraction via Generative Prior with Limited Data","summary":"  Facial sketches are both a concise way of showing the identity of a person\nand a means to express artistic intention. While a few techniques have recently\nemerged that allow sketches to be extracted in different styles, they typically\nrely on a large amount of data that is difficult to obtain. Here, we propose\nStyleSketch, a method for extracting high-resolution stylized sketches from a\nface image. Using the rich semantics of the deep features from a pretrained\nStyleGAN, we are able to train a sketch generator with 16 pairs of face and the\ncorresponding sketch images. The sketch generator utilizes part-based losses\nwith two-stage learning for fast convergence during training for high-quality\nsketch extraction. Through a set of comparisons, we show that StyleSketch\noutperforms existing state-of-the-art sketch extraction methods and few-shot\nimage adaptation methods for the task of extracting high-resolution abstract\nface sketches. We further demonstrate the versatility of StyleSketch by\nextending its use to other domains and explore the possibility of semantic\nediting. The project page can be found in\nhttps://kwanyun.github.io/stylesketch_project.\n","authors":["Kwan Yun","Kwanggyoon Seo","Chang Wook Seo","Soyeon Yoon","Seongcheol Kim","Soohyun Ji","Amirsaman Ashtari","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.11263v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.11156v1","updated":"2024-03-17T09:40:51Z","published":"2024-03-17T09:40:51Z","title":"Journey into SPH Simulation: A Comprehensive Framework and Showcase","summary":"  This report presents the development and results of an advanced SPH (Smoothed\nParticle Hydrodynamics) simulation framework, designed for high fidelity fluid\ndynamics modeling. Our framework, accessible at\nhttps://github.com/jason-huang03/SPH_Project, integrates various SPH algorithms\nincluding WCSPH, PCISPH, and DFSPH, alongside techniques for rigid-fluid\ncoupling and high viscosity fluid simulations. Leveraging the computational\npower of CUDA and the versatility of Taichi, the framework excels in handling\nlarge-scale simulations with millions of particles. We demonstrate the\ncapability of our framework through a series of simulations showcasing\nrigid-fluid coupling, high viscosity fluids, and large-scale fluid dynamics.\nFurthermore, a detailed performance analysis reveals CUDA's superior efficiency\nacross different hardware platforms. This work is an exploraion into modern SPH\nsimulation techniques, showcasing their practical implementation and\ncapabilities.\n","authors":["Haofeng Huang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2403.11156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11141v1","updated":"2024-03-17T08:45:01Z","published":"2024-03-17T08:45:01Z","title":"The Simplex Projection: Lossless Visualization of 4D Compositional Data\n  on a 2D Canvas","summary":"  The simplex projection expands the capabilities of simplex plots (also known\nas ternary plots) to achieve a lossless visualization of 4D compositional data\non a 2D canvas. Previously, this was only possible for 3D compositional data.\nWe demonstrate how our approach can be applied to individual data points, point\nclouds, and continuous probability density functions on simplices. While we\nshowcase our visualization technique specifically for 4D compositional data, we\noffer rigorous proofs that support its extension to compositional data of any\n(finite) dimensionality.\n","authors":["Marvin Schmitt","Yuga Hikida","Stefan T Radev","Filip Sadlo","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2403.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11134v1","updated":"2024-03-17T07:57:08Z","published":"2024-03-17T07:57:08Z","title":"Recent Advances in 3D Gaussian Splatting","summary":"  The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the\nrendering speed of novel view synthesis. Unlike neural implicit representations\nlike Neural Radiance Fields (NeRF) that represent a 3D scene with position and\nviewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of\nGaussian ellipsoids to model the scene so that efficient rendering can be\naccomplished by rasterizing Gaussian ellipsoids into images. Apart from the\nfast rendering speed, the explicit representation of 3D Gaussian Splatting\nfacilitates editing tasks like dynamic reconstruction, geometry editing, and\nphysical simulation. Considering the rapid change and growing number of works\nin this field, we present a literature review of recent 3D Gaussian Splatting\nmethods, which can be roughly classified into 3D reconstruction, 3D editing,\nand other downstream applications by functionality. Traditional point-based\nrendering methods and the rendering formulation of 3D Gaussian Splatting are\nalso illustrated for a better understanding of this technique. This survey aims\nto help beginners get into this field quickly and provide experienced\nresearchers with a comprehensive overview, which can stimulate the future\ndevelopment of the 3D Gaussian Splatting representation.\n","authors":["Tong Wu","Yu-Jie Yuan","Ling-Xiao Zhang","Jie Yang","Yan-Pei Cao","Ling-Qi Yan","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.11134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13014v1","updated":"2024-03-17T17:42:20Z","published":"2024-03-17T17:42:20Z","title":"General Line Coordinates in 3D","summary":"  Interpretable interactive visual pattern discovery in lossless 3D\nvisualization is a promising way to advance machine learning. It enables end\nusers who are not data scientists to take control of the model development\nprocess as a self-service. It is conducted in 3D General Line Coordinates (GLC)\nvisualization space, which preserves all n-D information in 3D. This paper\npresents a system which combines three types of GLC: Shifted Paired Coordinates\n(SPC), Shifted Tripled Coordinates (STC), and General Line Coordinates-Linear\n(GLC-L) for interactive visual pattern discovery. A transition from 2-D\nvisualization to 3-D visualization allows for a more distinct visual pattern\nthan in 2-D and it also allows for finding the best data viewing positions,\nwhich are not available in 2-D. It enables in-depth visual analysis of various\nclass-specific data subsets comprehensible for end users in the original\ninterpretable attributes. Controlling model overgeneralization by end users is\nan additional benefit of this approach.\n","authors":["Joshua Martinez","Boris Kovalerchuk"],"pdf_url":"https://arxiv.org/pdf/2403.13014v1.pdf","comment":"8 pages, 25 figures"}]},"2024-03-19T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.06420v2","updated":"2024-03-19T17:52:09Z","published":"2024-03-11T04:13:26Z","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic\n  Manipulations With Large Language Models","summary":"  Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.\n","authors":["Liangliang Chen","Yutian Lei","Shiyu Jin","Ying Zhang","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12945v1","updated":"2024-03-19T17:48:38Z","published":"2024-03-19T17:48:38Z","title":"DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset","summary":"  The creation of large, diverse, high-quality robot manipulation datasets is\nan important stepping stone on the path toward more capable and robust robotic\nmanipulation policies. However, creating such datasets is challenging:\ncollecting robot manipulation data in diverse environments poses logistical and\nsafety challenges and requires substantial investments in hardware and human\nlabour. As a result, even the most general robot manipulation policies today\nare mostly trained on data collected in a small number of environments with\nlimited scene and task diversity. In this work, we introduce DROID (Distributed\nRobot Interaction Dataset), a diverse robot manipulation dataset with 76k\ndemonstration trajectories or 350 hours of interaction data, collected across\n564 scenes and 84 tasks by 50 data collectors in North America, Asia, and\nEurope over the course of 12 months. We demonstrate that training with DROID\nleads to policies with higher performance and improved generalization ability.\nWe open source the full dataset, policy learning code, and a detailed guide for\nreproducing our robot hardware setup.\n","authors":["Alexander Khazatsky","Karl Pertsch","Suraj Nair","Ashwin Balakrishna","Sudeep Dasari","Siddharth Karamcheti","Soroush Nasiriany","Mohan Kumar Srirama","Lawrence Yunliang Chen","Kirsty Ellis","Peter David Fagan","Joey Hejna","Masha Itkina","Marion Lepert","Yecheng Jason Ma","Patrick Tree Miller","Jimmy Wu","Suneel Belkhale","Shivin Dass","Huy Ha","Arhan Jain","Abraham Lee","Youngwoon Lee","Marius Memmel","Sungjae Park","Ilija Radosavovic","Kaiyuan Wang","Albert Zhan","Kevin Black","Cheng Chi","Kyle Beltran Hatch","Shan Lin","Jingpei Lu","Jean Mercat","Abdul Rehman","Pannag R Sanketi","Archit Sharma","Cody Simpson","Quan Vuong","Homer Rich Walke","Blake Wulfe","Ted Xiao","Jonathan Heewon Yang","Arefeh Yavary","Tony Z. Zhao","Christopher Agia","Rohan Baijal","Mateo Guaman Castro","Daphne Chen","Qiuyu Chen","Trinity Chung","Jaimyn Drake","Ethan Paul Foster","Jensen Gao","David Antonio Herrera","Minho Heo","Kyle Hsu","Jiaheng Hu","Donovon Jackson","Charlotte Le","Yunshuang Li","Kevin Lin","Roy Lin","Zehan Ma","Abhiram Maddukuri","Suvir Mirchandani","Daniel Morton","Tony Nguyen","Abigail O'Neill","Rosario Scalise","Derick Seale","Victor Son","Stephen Tian","Emi Tran","Andrew E. Wang","Yilin Wu","Annie Xie","Jingyun Yang","Patrick Yin","Yunchu Zhang","Osbert Bastani","Glen Berseth","Jeannette Bohg","Ken Goldberg","Abhinav Gupta","Abhishek Gupta","Dinesh Jayaraman","Joseph J Lim","Jitendra Malik","Roberto Martín-Martín","Subramanian Ramamoorthy","Dorsa Sadigh","Shuran Song","Jiajun Wu","Michael C. Yip","Yuke Zhu","Thomas Kollar","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12945v1.pdf","comment":"Project website: https://droid-dataset.github.io/"},{"id":"http://arxiv.org/abs/2403.12943v1","updated":"2024-03-19T17:47:37Z","published":"2024-03-19T17:47:37Z","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with\n  Cross-Attention Transformers","summary":"  While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io\n","authors":["Vidhi Jain","Maria Attarian","Nikhil J Joshi","Ayzaan Wahid","Danny Driess","Quan Vuong","Pannag R Sanketi","Pierre Sermanet","Stefan Welker","Christine Chan","Igor Gilitschenski","Yonatan Bisk","Debidatta Dwibedi"],"pdf_url":"https://arxiv.org/pdf/2403.12943v1.pdf","comment":"Robot learning: Imitation Learning, Robot Perception, Sensing &\n  Vision, Grasping & Manipulation"},{"id":"http://arxiv.org/abs/2403.12920v1","updated":"2024-03-19T17:23:44Z","published":"2024-03-19T17:23:44Z","title":"Semantic Layering in Room Segmentation via LLMs","summary":"  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs\n(SeLRoS), an advanced method for semantic room segmentation by integrating\nLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlike\nprevious approaches that solely focus on the geometric segmentation of indoor\nenvironments, our work enriches segmented maps with semantic data, including\nobject identification and spatial relationships, to enhance robotic navigation.\nBy leveraging LLMs, we provide a novel framework that interprets and organizes\ncomplex information about each segmented area, thereby improving the accuracy\nand contextual relevance of room segmentation. Furthermore, SeLRoS overcomes\nthe limitations of existing algorithms by using a semantic evaluation method to\naccurately distinguish true room divisions from those erroneously generated by\nfurniture and segmentation inaccuracies. The effectiveness of SeLRoS is\nverified through its application across 30 different 3D environments. Source\ncode and experiment videos for this work are available at:\nhttps://sites.google.com/view/selros.\n","authors":["Taehyeon Kim","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2403.12920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12910v1","updated":"2024-03-19T17:08:24Z","published":"2024-03-19T17:08:24Z","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","summary":"  Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.\n","authors":["Lucy Xiaoyang Shi","Zheyuan Hu","Tony Z. Zhao","Archit Sharma","Karl Pertsch","Jianlan Luo","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12910v1.pdf","comment":"Project website: https://yay-robot.github.io/"},{"id":"http://arxiv.org/abs/2403.11492v2","updated":"2024-03-19T17:04:35Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v2.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12892v1","updated":"2024-03-19T16:44:53Z","published":"2024-03-19T16:44:53Z","title":"Uoc luong kenh truyen trong he thong da robot su dung SDR","summary":"  This study focuses on developing an experimental system for estimating\ncommunication channels in a multi-robot mobile system using software-defined\nradio (SDR) devices. The system consists of two mobile robots programmed for\ntwo scenarios: one where the robot remains stationary and another where it\nfollows a predefined trajectory. Communication within the system is conducted\nthrough orthogonal frequency-division multiplexing (OFDM) to mitigate the\neffects of multipath propagation in indoor environments. The system's\nperformance is evaluated using the bit error rate (BER). Connections related to\nrobot motion and communication are implemented using Raspberry Pi 3 and BladeRF\nx115, respectively. The least squares (LS) technique is employed to estimate\nthe channel with a bit error rate of approximately 10^(-2).\n","authors":["Do Hai Son","Nguyen Huu Hung","Pham Duy Hung","Tran Thi Thuy Quynh"],"pdf_url":"https://arxiv.org/pdf/2403.12892v1.pdf","comment":"in Vietnamese language"},{"id":"http://arxiv.org/abs/2403.12891v1","updated":"2024-03-19T16:40:57Z","published":"2024-03-19T16:40:57Z","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across\n  Varied Bowl Configurations and Food Types","summary":"  In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.\n","authors":["Rui Liu","Amisha Bhaskar","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12876v1","updated":"2024-03-19T16:21:40Z","published":"2024-03-19T16:21:40Z","title":"LAVA: Long-horizon Visual Action based Food Acquisition","summary":"  Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals\nwith mobility impairments to regain autonomy in feeding themselves. The goal of\nRAF is to use a robot arm to acquire and transfer food to individuals from the\ntable. Existing RAF methods primarily focus on solid foods, leaving a gap in\nmanipulation strategies for semi-solid and deformable foods. This study\nintroduces Long-horizon Visual Action (LAVA) based food acquisition of liquid,\nsemisolid, and deformable foods. Long-horizon refers to the goal of \"clearing\nthe bowl\" by sequentially acquiring the food from the bowl. LAVA employs a\nhierarchical policy for long-horizon food acquisition tasks. The framework uses\nhigh-level policy to determine primitives by leveraging ScoopNet. At the\nmid-level, LAVA finds parameters for primitives using vision. To carry out\nsequential plans in the real world, LAVA delegates action execution which is\ndriven by Low-level policy that uses parameters received from mid-level policy\nand behavior cloning ensuring precise trajectory execution. We validate our\napproach on complex real-world acquisition trials involving granular, liquid,\nsemisolid, and deformable food types along with fruit chunks and soup\nacquisition. Across 46 bowls, LAVA acquires much more efficiently than\nbaselines with a success rate of 89 +/- 4% and generalizes across realistic\nplate variations such as different positions, varieties, and amount of food in\nthe bowl. Code, datasets, videos, and supplementary materials can be found on\nour website.\n","authors":["Amisha Bhaskar","Rui Liu","Vishnu D. Sharma","Guangyao Shi","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12876v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.12320v2","updated":"2024-03-19T16:18:21Z","published":"2023-10-18T20:44:16Z","title":"Asynchronous Distributed Smoothing and Mapping via On-Manifold Consensus\n  ADMM","summary":"  In this paper we present a fully distributed, asynchronous, and general\npurpose optimization algorithm for Consensus Simultaneous Localization and\nMapping (CSLAM). Multi-robot teams require that agents have timely and accurate\nsolutions to their state as well as the states of the other robots in the team.\nTo optimize this solution we develop a CSLAM back-end based on Consensus ADMM\ncalled MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed\nto tolerate failures of individual robots, asynchronous to tolerate\ncommunication delays and outages, and general purpose to handle any CSLAM\nproblem formulation. We demonstrate that MESA exhibits superior convergence\nrates and accuracy compare to existing state-of-the art CSLAM back-end\noptimizers.\n","authors":["Daniel McGann","Kyle Lassak","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2310.12320v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2306.06192v4","updated":"2024-03-19T16:16:16Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v4.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12865v1","updated":"2024-03-19T16:09:30Z","published":"2024-03-19T16:09:30Z","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for\n  Autonomous Flight in Complex and Dynamic Environments","summary":"  The role of a motion planner is pivotal in quadrotor applications, yet\nexisting methods often struggle to adapt to complex environments, limiting\ntheir ability to achieve fast, safe, and robust flight. In this letter, we\nintroduce a performance-enhanced quadrotor motion planner designed for\nautonomous flight in complex environments including dense obstacles, dynamic\nobstacles, and unknown disturbances. The global planner generates an initial\ntrajectory through kinodynamic path searching and refines it using B-spline\ntrajectory optimization. Subsequently, the local planner takes into account the\nquadrotor dynamics, estimated disturbance, global reference trajectory, control\ncost, time cost, and safety constraints to generate real-time control inputs,\nutilizing the framework of model predictive contouring control. Both\nsimulations and real-world experiments corroborate the heightened robustness,\nsafety, and speed of the proposed motion planner. Additionally, our motion\nplanner achieves flights at more than 6.8 m/s in a challenging and complex\nracing scenario.\n","authors":["Jiaxin Qiu","Qingchen Liu","Jiahu Qin","Dewang Cheng","Yawei Tian","Qichao Ma"],"pdf_url":"https://arxiv.org/pdf/2403.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12861v1","updated":"2024-03-19T16:05:51Z","published":"2024-03-19T16:05:51Z","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous\n  Deformable Manipulation","summary":"  Mastering dexterous robotic manipulation of deformable objects is vital for\novercoming the limitations of parallel grippers in real-world applications.\nCurrent trajectory optimisation approaches often struggle to solve such tasks\ndue to the large search space and the limited task information available from a\ncost function. In this work, we propose D-Cubed, a novel trajectory\noptimisation method using a latent diffusion model (LDM) trained from a\ntask-agnostic play dataset to solve dexterous deformable object manipulation\ntasks. D-Cubed learns a skill-latent space that encodes short-horizon actions\nin the play dataset using a VAE and trains a LDM to compose the skill latents\ninto a skill trajectory, representing a long-horizon action trajectory in the\ndataset. To optimise a trajectory for a target task, we introduce a novel\ngradient-free guided sampling method that employs the Cross-Entropy method\nwithin the reverse diffusion process. In particular, D-Cubed samples a small\nnumber of noisy skill trajectories using the LDM for exploration and evaluates\nthe trajectories in simulation. Then, D-Cubed selects the trajectory with the\nlowest cost for the subsequent reverse process. This effectively explores\npromising solution areas and optimises the sampled trajectories towards a\ntarget task throughout the reverse diffusion process. Through empirical\nevaluation on a public benchmark of dexterous deformable object manipulation\ntasks, we demonstrate that D-Cubed outperforms traditional trajectory\noptimisation and competitive baseline approaches by a significant margin. We\nfurther demonstrate that trajectories found by D-Cubed readily transfer to a\nreal-world LEAP hand on a folding task.\n","authors":["Jun Yamada","Shaohong Zhong","Jack Collins","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2403.12861v1.pdf","comment":"https://applied-ai-lab.github.io/D-cubed/"},{"id":"http://arxiv.org/abs/2403.12856v1","updated":"2024-03-19T16:01:25Z","published":"2024-03-19T16:01:25Z","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in\n  Map-based Path Planning","summary":"  In reinforcement learning (RL), exploiting environmental symmetries can\nsignificantly enhance efficiency, robustness, and performance. However,\nensuring that the deep RL policy and value networks are respectively\nequivariant and invariant to exploit these symmetries is a substantial\nchallenge. Related works try to design networks that are equivariant and\ninvariant by construction, limiting them to a very restricted library of\ncomponents, which in turn hampers the expressiveness of the networks. This\npaper proposes a method to construct equivariant policies and invariant value\nfunctions without specialized neural network components, which we term\nequivariant ensembles. We further add a regularization term for adding\ninductive bias during training. In a map-based path planning case study, we\nshow how equivariant ensembles and regularization benefit sample efficiency and\nperformance.\n","authors":["Mirco Theile","Hongpeng Cao","Marco Caccamo","Alberto L. Sangiovanni-Vincentelli"],"pdf_url":"https://arxiv.org/pdf/2403.12856v1.pdf","comment":"submitted for possible publication. A video can be found here:\n  https://youtu.be/L6NOdvU7n7s"},{"id":"http://arxiv.org/abs/2403.12853v1","updated":"2024-03-19T15:57:32Z","published":"2024-03-19T15:57:32Z","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform\n  Towards Ambient Intelligent Systems","summary":"  Realizing consumer-grade drones that are as useful as robot vacuums\nthroughout our homes or personal smartphones in our daily lives requires drones\nto sense, actuate, and respond to general scenarios that may arise. Towards\nthis vision, we propose RASP, a modular and reconfigurable sensing and\nactuation platform that allows drones to autonomously swap onboard sensors and\nactuators in only 25 seconds, allowing a single drone to quickly adapt to a\ndiverse range of tasks. RASP consists of a mechanical layer to physically swap\nsensor modules, an electrical layer to maintain power and communication lines\nto the sensor/actuator, and a software layer to maintain a common interface\nbetween the drone and any sensor module in our platform. Leveraging recent\nadvances in large language and visual language models, we further introduce the\narchitecture, implementation, and real-world deployments of a personal\nassistant system utilizing RASP. We demonstrate that RASP can enable a diverse\nrange of useful tasks in home, office, lab, and other indoor settings.\n","authors":["Minghui Zhao","Junxi Xia","Kaiyuan Hou","Yanchen Liu","Stephen Xia","Xiaofan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12842v1","updated":"2024-03-19T15:49:32Z","published":"2024-03-19T15:49:32Z","title":"The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical\n  Systems","summary":"  Hybrid systems are dynamical systems with continuous-time and discrete-time\ncomponents in their dynamics. When hybrid systems are defined on a principal\nbundle we are able to define two classes of impacts for the discrete-time\ntransition of the dynamics: interior impacts and exterior impacts. In this\npaper we define hybrid systems on principal bundles, study the underlying\ngeometry on the switching surface where impacts occur and we find conditions\nfor which both exterior and interior impacts are preserved by the mechanical\nconnection induced in the principal bundle.\n","authors":["William Clark","Leonardo Colombo","Anthony Bloch"],"pdf_url":"https://arxiv.org/pdf/2403.12842v1.pdf","comment":"6 pages. To be presented at a conference. Comments welcome"},{"id":"http://arxiv.org/abs/2403.12837v1","updated":"2024-03-19T15:42:46Z","published":"2024-03-19T15:42:46Z","title":"Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater\n  Environments","summary":"  Despite recent advances in semantic Simultaneous Localization and Mapping\n(SLAM) for terrestrial and aerial applications, underwater semantic SLAM\nremains an open and largely unaddressed research problem due to the unique\nsensing modalities and the object classes found underwater. This paper presents\nan object-based semantic SLAM method for underwater environments that can\nidentify, localize, classify, and map a wide variety of marine objects without\na priori knowledge of the object classes present in the scene. The method\nperforms unsupervised object segmentation and object-level feature aggregation,\nand then uses opti-acoustic sensor fusion for object localization.\nProbabilistic data association is used to determine observation to landmark\ncorrespondences. Given such correspondences, the method then jointly optimizes\nlandmark and vehicle position estimates. Indoor and outdoor underwater datasets\nwith a wide variety of objects and challenging acoustic and lighting conditions\nare collected for evaluation and made publicly available. Quantitative and\nqualitative results show the proposed method achieves reduced trajectory error\ncompared to baseline methods, and is able to obtain comparable map accuracy to\na baseline closed-set method that requires hand-labeled data of all objects in\nthe scene.\n","authors":["Kurran Singh","Jungseok Hong","Nicholas R. Rypkema","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2403.12837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12835v1","updated":"2024-03-19T15:41:39Z","published":"2024-03-19T15:41:39Z","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","summary":"  Traditional approaches in physics-based motion generation, centered around\nimitation learning and reward shaping, often struggle to adapt to new\nscenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical\nmethod that learns physically plausible interactions following open-vocabulary\ninstructions. Our approach begins by developing a set of atomic actions via a\nlow-level controller trained via imitation learning. Upon receiving an\nopen-vocabulary textual instruction, AnySkill employs a high-level policy that\nselects and integrates these atomic actions to maximize the CLIP similarity\nbetween the agent's rendered images and the text. An important feature of our\nmethod is the use of image-based rewards for the high-level policy, which\nallows the agent to learn interactions with objects without manual reward\nengineering. We demonstrate AnySkill's capability to generate realistic and\nnatural motion sequences in response to unseen instructions of varying lengths,\nmarking it the first method capable of open-vocabulary physical skill learning\nfor interactive humanoid agents.\n","authors":["Jieming Cui","Tengyu Liu","Nian Liu","Yaodong Yang","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02021v4","updated":"2024-03-19T15:05:05Z","published":"2022-09-05T15:41:13Z","title":"When Robotics Meets Wireless Communications: An Introductory Tutorial","summary":"  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles\n(UAVs) within the research community, industry, and society is growing fast.\nMany of these agents are nowadays equipped with communication systems that are,\nin some cases, essential to successfully achieve certain tasks. In this\ncontext, we have begun to witness the development of a new interdisciplinary\nresearch field at the intersection of robotics and communications. This\nresearch field has been boosted by the intention of integrating UAVs within the\n5G and 6G communication networks. This research will undoubtedly lead to many\nimportant applications in the near future. Nevertheless, one of the main\nobstacles to the development of this research area is that most researchers\naddress these problems by oversimplifying either the robotics or the\ncommunications aspect. This impedes the ability of reaching the full potential\nof this new interdisciplinary research area. In this tutorial, we present some\nof the modelling tools necessary to address problems involving both robotics\nand communication from an interdisciplinary perspective. As an illustrative\nexample of such problems, we focus in this tutorial on the issue of\ncommunication-aware trajectory planning.\n","authors":["Daniel Bonilla Licea","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2209.02021v4.pdf","comment":"35 pages, 192 references"},{"id":"http://arxiv.org/abs/2403.12798v1","updated":"2024-03-19T15:00:53Z","published":"2024-03-19T15:00:53Z","title":"Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A\n  Queueing-Theory-Based Efficiency Analysis","summary":"  In the era of digital commerce, the surge in online shopping and the\nexpectation for rapid delivery have placed unprecedented demands on warehouse\noperations. The traditional method of order fulfilment, where human order\npickers traverse large storage areas to pick items, has become a bottleneck,\nconsuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS)\noffer a solution by using robots to transport storage racks directly to\nhuman-operated picking stations, eliminating the need for pickers to travel.\nThis paper introduces combi-stations, a novel type of station that enables both\nitem picking and replenishment, as opposed to traditional separate stations. We\nanalyse the efficiency of combi-stations using queueing theory and demonstrate\ntheir potential to streamline warehouse operations. Our results suggest that\ncombi-stations can reduce the number of robots required for stability and\nsignificantly reduce order turnover time, indicating a promising direction for\nfuture warehouse automation.\n","authors":["Lin Xie","Sonja Otten"],"pdf_url":"https://arxiv.org/pdf/2403.12798v1.pdf","comment":"15 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1912.01782"},{"id":"http://arxiv.org/abs/2401.16205v2","updated":"2024-03-19T14:54:29Z","published":"2024-01-29T15:00:27Z","title":"CognitiveOS: Large Multimodal Model based System to Endow Any Type of\n  Robot with Generative AI","summary":"  This paper introduces CognitiveOS, the first operating system designed for\ncognitive robots capable of functioning across diverse robotic platforms.\nCognitiveOS is structured as a multi-agent system comprising modules built upon\na transformer architecture, facilitating communication through an internal\nmonologue format. These modules collectively empower the robot to tackle\nintricate real-world tasks. The paper delineates the operational principles of\nthe system along with descriptions of its nine distinct modules. The modular\ndesign endows the system with distinctive advantages over traditional\nend-to-end methodologies, notably in terms of adaptability and scalability. The\nsystem's modules are configurable, modifiable, or deactivatable depending on\nthe task requirements, while new modules can be seamlessly integrated. This\nsystem serves as a foundational resource for researchers and developers in the\ncognitive robotics domain, alleviating the burden of constructing a cognitive\nrobot system from scratch. Experimental findings demonstrate the system's\nadvanced task comprehension and adaptability across varied tasks, robotic\nplatforms, and module configurations, underscoring its potential for real-world\napplications. Moreover, in the category of Reasoning it outperformed\nCognitiveDog (by 15%) and RT2 (by 31%), achieving the highest to date rate of\n77%. We provide a code repository and dataset for the replication of\nCognitiveOS: link will be provided in camera-ready submission.\n","authors":["Artem Lykov","Mikhail Konenkov","Koffivi Fidèle Gbagbe","Mikhail Litvinov","Denis Davletshin","Aleksey Fedoseev","Miguel Altamirano Cabrera","Robinroy Peter","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2401.16205v2.pdf","comment":"The paper is submitted to the IEEE conference"},{"id":"http://arxiv.org/abs/2403.12761v1","updated":"2024-03-19T14:27:31Z","published":"2024-03-19T14:27:31Z","title":"BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs","summary":"  This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors.\n","authors":["Riccardo Andrea Izzo","Gianluca Bardaro","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2403.12761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04792v2","updated":"2024-03-19T14:13:53Z","published":"2023-08-09T08:31:05Z","title":"A Fast and Optimal Learning-based Path Planning Method for Planetary\n  Rovers","summary":"  Intelligent autonomous path planning is crucial to improve the exploration\nefficiency of planetary rovers. In this paper, we propose a learning-based\nmethod to quickly search for optimal paths in an elevation map, which is called\nNNPP. The NNPP model learns semantic information about start and goal\nlocations, as well as map representations, from numerous pre-annotated optimal\npath demonstrations, and produces a probabilistic distribution over each pixel\nrepresenting the likelihood of it belonging to an optimal path on the map. More\nspecifically, the paper computes the traversal cost for each grid cell from the\nslope, roughness and elevation difference obtained from the DEM. Subsequently,\nthe start and goal locations are encoded using a Gaussian distribution and\ndifferent location encoding parameters are analyzed for their effect on model\nperformance. After training, the NNPP model is able to perform path planning on\nnovel maps. Experiments show that the guidance field generated by the NNPP\nmodel can significantly reduce the search time for optimal paths under the same\nhardware conditions, and the advantage of NNPP increases with the scale of the\nmap.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie","Baoshi Cao"],"pdf_url":"https://arxiv.org/pdf/2308.04792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12725v1","updated":"2024-03-19T13:41:49Z","published":"2024-03-19T13:41:49Z","title":"Some geometric and topological data-driven methods in robot motion path\n  planning","summary":"  Motion path planning is an intrinsically geometric problem which is central\nfor design of robot systems. Since the early years of AI, robotics together\nwith computer vision have been the areas of computer science that drove its\ndevelopment. Many questions that arise, such as existence, optimality, and\ndiversity of motion paths in the configuration space that describes feasible\nrobot configurations, are of topological nature. The recent advances in\ntopological data analysis and related metric geometry, topology and\ncombinatorics have provided new tools to address these engineering tasks. We\nwill survey some questions, issues, recent work and promising directions in\ndata-driven geometric and topological methods with some emphasis on the use of\ndiscrete Morse theory.\n","authors":["Boris Goldfarb"],"pdf_url":"https://arxiv.org/pdf/2403.12725v1.pdf","comment":"21 pages, 6 figures, to appear in a book project on Topology,\n  Geometry and AI in the EMS Series in Industrial and Applied Mathematics,\n  edited by Michael Farber and Jes\\'us Gonz\\'alez"},{"id":"http://arxiv.org/abs/2403.12720v1","updated":"2024-03-19T13:29:44Z","published":"2024-03-19T13:29:44Z","title":"Shared Autonomy via Variable Impedance Control and Virtual Potential\n  Fields for Encoding Human Demonstration","summary":"  This article introduces a framework for complex human-robot collaboration\ntasks, such as the co-manufacturing of furniture. For these tasks, it is\nessential to encode tasks from human demonstration and reproduce these skills\nin a compliant and safe manner. Therefore, two key components are addressed in\nthis work: motion generation and shared autonomy. We propose a motion generator\nbased on a time-invariant potential field, capable of encoding wrench profiles,\ncomplex and closed-loop trajectories, and additionally incorporates obstacle\navoidance. Additionally, the paper addresses shared autonomy (SA) which enables\nsynergetic collaboration between human operators and robots by dynamically\nallocating authority. Variable impedance control (VIC) and force control are\nemployed, where impedance and wrench are adapted based on the human-robot\nautonomy factor derived from interaction forces. System passivity is ensured by\nan energy-tank based task passivation strategy. The framework's efficacy is\nvalidated through simulations and an experimental study employing a Franka\nEmika Research 3 robot.\n","authors":["Shail Jadav","Johannes Heidersberger","Christian Ott","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2403.12720v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.12686v1","updated":"2024-03-19T12:45:18Z","published":"2024-03-19T12:45:18Z","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and\n  mmWave Radar","summary":"  The perception of waterways based on human intent holds significant\nimportance for autonomous navigation and operations of Unmanned Surface\nVehicles (USVs) in water environments. Inspired by visual grounding, in this\npaper, we introduce WaterVG, the first visual grounding dataset designed for\nUSV-based waterway perception based on human intention prompts. WaterVG\nencompasses prompts describing multiple targets, with annotations at the\ninstance level including bounding boxes and masks. Notably, WaterVG includes\n11,568 samples with 34,950 referred targets, which integrates both visual and\nradar characteristics captured by monocular camera and millimeter-wave (mmWave)\nradar, enabling a finer granularity of text prompts. Furthermore, we propose a\nnovel multi-modal visual grounding model, Potamoi, which is a multi-modal and\nmulti-task model based on the one-stage paradigm with a designed Phased\nHeterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar\nWeighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA\nis a low-cost and efficient fusion module with a remarkably small parameter\ncount and FLOPs, elegantly aligning and fusing scenario context information\ncaptured by two sensors with linguistic features, which can effectively address\ntasks of referring expression comprehension and segmentation based on\nfine-grained prompts. Comprehensive experiments and evaluations have been\nconducted on WaterVG, where our Potamoi archives state-of-the-art performances\ncompared with counterparts.\n","authors":["Runwei Guan","Liye Jia","Fengyufan Yang","Shanliang Yao","Erick Purwanto","Xiaohui Zhu","Eng Gee Lim","Jeremy Smith","Ka Lok Man","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2403.12686v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.12685v1","updated":"2024-03-19T12:45:00Z","published":"2024-03-19T12:45:00Z","title":"Dynamic Manipulation of Deformable Objects using Imitation Learning with\n  Adaptation to Hardware Constraints","summary":"  Imitation Learning (IL) is a promising paradigm for learning dynamic\nmanipulation of deformable objects since it does not depend on\ndifficult-to-create accurate simulations of such objects. However, the\ntranslation of motions demonstrated by a human to a robot is a challenge for\nIL, due to differences in the embodiments and the robot's physical limits.\nThese limits are especially relevant in dynamic manipulation where high\nvelocities and accelerations are typical. To address this problem, we propose a\nframework that first maps a dynamic demonstration into a motion that respects\nthe robot's constraints using a constrained Dynamic Movement Primitive. Second,\nthe resulting object state is further optimized by quasi-static refinement\nmotions to optimize task performance metrics. This allows both efficiently\naltering the object state by dynamic motions and stable small-scale\nrefinements. We evaluate the framework in the challenging task of bag opening,\ndesigning the system BILBO: Bimanual dynamic manipulation using Imitation\nLearning for Bag Opening. Our results show that BILBO can successfully open a\nwide range of crumpled bags, using a demonstration with a single bag. See\nsupplementary material at https://sites.google.com/view/bilbo-bag.\n","authors":["Eric Hannus","Tran Nguyen Le","David Blanco-Mulero","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.12685v1.pdf","comment":"Submitted to 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024). 8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.12682v1","updated":"2024-03-19T12:36:51Z","published":"2024-03-19T12:36:51Z","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single\n  image and a NeRF model","summary":"  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera\npose of a given image, building on the Neural Radiance Fields (NeRF)\nformulation. IFFNeRF is specifically designed to operate in real-time and\neliminates the need for an initial pose guess that is proximate to the sought\nsolution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface\npoints from within the NeRF model. From these sampled points, we cast rays and\ndeduce the color for each ray through pixel-level view synthesis. The camera\npose can then be estimated as the solution to a Least Squares problem by\nselecting correspondences between the query image and the resulting bundle. We\nfacilitate this process through a learned attention mechanism, bridging the\nquery image embedding with the embedding of parameterized rays, thereby\nmatching rays pertinent to the image. Through synthetic and real evaluation\nsettings, we show that our method can improve the angular and translation error\naccuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing\nat 34fps on consumer hardware and not requiring the initial pose guess.\n","authors":["Matteo Bortolon","Theodore Tsesmelis","Stuart James","Fabio Poiesi","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2403.12682v1.pdf","comment":"Accepted ICRA 2024, Project page:\n  https://mbortolon97.github.io/iffnerf/"},{"id":"http://arxiv.org/abs/2403.12676v1","updated":"2024-03-19T12:21:23Z","published":"2024-03-19T12:21:23Z","title":"In-Hand Following of Deformable Linear Objects Using Dexterous Fingers\n  with Tactile Sensing","summary":"  Most research on deformable linear object (DLO) manipulation assumes rigid\ngrasping. However, beyond rigid grasping and re-grasping, in-hand following is\nalso an essential skill that humans use to dexterously manipulate DLOs, which\nrequires continuously changing the grasp point by in-hand sliding while holding\nthe DLO to prevent it from falling. Achieving such a skill is very challenging\nfor robots without using specially designed but not versatile end-effectors.\nPrevious works have attempted using generic parallel grippers, but their\nrobustness is unsatisfactory owing to the conflict between following and\nholding, which is hard to balance with a one-degree-of-freedom gripper. In this\nwork, inspired by how humans use fingers to follow DLOs, we explore the usage\nof a generic dexterous hand with tactile sensing to imitate human skills and\nachieve robust in-hand DLO following. To enable the hardware system to function\nin the real world, we develop a framework that includes Cartesian-space\narm-hand control, tactile-based in-hand 3-D DLO pose estimation, and\ntask-specific motion design. Experimental results demonstrate the significant\nsuperiority of our method over using parallel grippers, as well as its great\nrobustness, generalizability, and efficiency.\n","authors":["Mingrui Yu","Boyuan Liang","Xiang Zhang","Xinghao Zhu","Xiang Li","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2403.12676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v1","updated":"2024-03-19T12:11:57Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.07603v3","updated":"2024-03-19T11:17:00Z","published":"2024-01-15T11:20:34Z","title":"Multi-task real-robot data with gaze attention for dual-arm fine\n  manipulation","summary":"  In the field of robotic manipulation, deep imitation learning is recognized\nas a promising approach for acquiring manipulation skills. Additionally,\nlearning from diverse robot datasets is considered a viable method to achieve\nversatility and adaptability. In such research, by learning various tasks,\nrobots achieved generality across multiple objects. However, such multi-task\nrobot datasets have mainly focused on single-arm tasks that are relatively\nimprecise, not addressing the fine-grained object manipulation that robots are\nexpected to perform in the real world. This paper introduces a dataset of\ndiverse object manipulations that includes dual-arm tasks and/or tasks\nrequiring fine manipulation. To this end, we have generated dataset with 224k\nepisodes (150 hours, 1,104 language instructions) which includes dual-arm fine\ntasks such as bowl-moving, pencil-case opening or banana-peeling, and this data\nis publicly available. Additionally, this dataset includes visual attention\nsignals as well as dual-action labels, a signal that separates actions into a\nrobust reaching trajectory and precise interaction with objects, and language\ninstructions to achieve robust and precise object manipulation. We applied the\ndataset to our Dual-Action and Attention (DAA), a model designed for\nfine-grained dual arm manipulation tasks and robust against covariate shifts.\nThe model was tested with over 7k total trials in real robot manipulation\ntasks, demonstrating its capability in fine manipulation.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2401.07603v3.pdf","comment":"10 pages, The dataset is available at\n  https://sites.google.com/view/multi-task-fine"},{"id":"http://arxiv.org/abs/2310.12547v2","updated":"2024-03-19T11:09:12Z","published":"2023-10-19T07:54:30Z","title":"PGA: Personalizing Grasping Agents with Single Human-Robot Interaction","summary":"  Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that\ncomprehend and grasp objects based on natural language instructions. While the\nability to understand personal objects like my wallet facilitates more natural\ninteraction with human users, current LCRG systems only allow generic language\ninstructions, e.g., the black-colored wallet next to the laptop. To this end,\nwe introduce a task scenario GraspMine alongside a novel dataset aimed at\npinpointing and grasping personal objects given personal indicators via\nlearning from a single human-robot interaction, rather than a large labeled\ndataset. Our proposed method, Personalized Grasping Agent (PGA), addresses\nGraspMine by leveraging the unlabeled image data of the user's environment,\ncalled Reminiscence. Specifically, PGA acquires personal object information by\na user presenting a personal object with its associated indicator, followed by\nPGA inspecting the object by rotating it. Based on the acquired information,\nPGA pseudo-labels objects in the Reminiscence by our proposed label propagation\nalgorithm. Harnessing the information acquired from the interactions and the\npseudo-labeled objects in the Reminiscence, PGA adapts the object grounding\nmodel to grasp personal objects. This results in significant efficiency while\nprevious LCRG systems rely on resource-intensive human annotations --\nnecessitating hundreds of labeled data to learn my wallet. Moreover, PGA\noutperforms baseline methods across all metrics and even shows comparable\nperformance compared to the fully-supervised method, which learns from 9k\nannotated data samples. We further validate PGA's real-world applicability by\nemploying a physical robot to execute GrsapMine. Code and data are publicly\navailable at https://github.com/JHKim-snu/PGA.\n","authors":["Junghyun Kim","Gi-Cheon Kang","Jaein Kim","Seoyun Yang","Minjoon Jung","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12547v2.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2403.12631v1","updated":"2024-03-19T10:59:21Z","published":"2024-03-19T10:59:21Z","title":"PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic\n  Glove Applications","summary":"  Controlling hand exoskeletons to assist individuals with grasping tasks poses\na challenge due to the difficulty in understanding user intentions. We propose\nthat most daily grasping tasks during activities of daily living (ADL) can be\ndeduced by analyzing object geometries (simple and complex) from 3D point\nclouds. The study introduces PointGrasp, a real-time system designed for\nidentifying household scenes semantically, aiming to support and enhance\nassistance during ADL for tailored end-to-end grasping tasks. The system\ncomprises an RGB-D camera with an inertial measurement unit and a\nmicroprocessor integrated into a tendon-driven soft robotic glove. The RGB-D\ncamera processes 3D scenes at a rate exceeding 30 frames per second. The\nproposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple\nand 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies\nand pinpoints reachable objects. This system shows promise in end-to-end\nvision-driven robotic-assisted rehabilitation manual tasks.\n","authors":["Chen Hu","Shirui Lyu","Eojin Rho","Daekyum Kim","Shan Luo","Letizia Gionfrida"],"pdf_url":"https://arxiv.org/pdf/2403.12631v1.pdf","comment":"6 pages, 8 figures, conference"},{"id":"http://arxiv.org/abs/2203.09749v2","updated":"2024-03-19T10:56:12Z","published":"2022-03-18T05:17:00Z","title":"Goal-conditioned dual-action imitation learning for dexterous dual-arm\n  robot manipulation","summary":"  Long-horizon dexterous robot manipulation of deformable objects, such as\nbanana peeling, is a problematic task because of the difficulties in object\nmodeling and a lack of knowledge about stable and dexterous manipulation\nskills. This paper presents a goal-conditioned dual-action (GC-DA) deep\nimitation learning (DIL) approach that can learn dexterous manipulation skills\nusing human demonstration data. Previous DIL methods map the current sensory\ninput and reactive action, which often fails because of compounding errors in\nimitation learning caused by the recurrent computation of actions. The method\npredicts reactive action only when the precise manipulation of the target\nobject is required (local action) and generates the entire trajectory when\nprecise manipulation is not required (global action). This dual-action\nformulation effectively prevents compounding error in the imitation learning\nusing the trajectory-based global action while responding to unexpected changes\nin the target object during the reactive local action. The proposed method was\ntested in a real dual-arm robot and successfully accomplished the\nbanana-peeling task.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2203.09749v2.pdf","comment":"19 pages, published in Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2403.12607v1","updated":"2024-03-19T10:18:03Z","published":"2024-03-19T10:18:03Z","title":"Looking for the Human in HRI Teaching: User-Centered Course Design for\n  Tech-Savvy Students","summary":"  Top-down, user-centered thinking is not typically a strength of all students,\nespecially tech-savvy computer science-related ones. We propose Human-Robot\nInteraction (HRI) introductory courses as a highly suitable opportunity to\nfoster these important skills since the HRI discipline includes a focus on\nhumans as users. Our HRI course therefore contains elements like scenario-based\ndesign of laboratory projects, discussing and merging ideas and other\nself-empowerment techniques. Participants describe, implement and present\neveryday scenarios using Pepper robots and our customized open-source visual\nprogramming tool. We observe that students obtain a good grasp of the taught\ntopics and improve their user-centered thinking skills.\n","authors":["Tobias Doernbach"],"pdf_url":"https://arxiv.org/pdf/2403.12607v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.12589v1","updated":"2024-03-19T09:48:18Z","published":"2024-03-19T09:48:18Z","title":"FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting","summary":"  Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.\n","authors":["Clément Gaspard","Grégoire Passault","Mélodie Daniel","Olivier Ly"],"pdf_url":"https://arxiv.org/pdf/2403.12589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09786v2","updated":"2024-03-19T09:12:31Z","published":"2023-12-15T13:40:34Z","title":"Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro\n  Aerial Vehicle in Cluttered Environments","summary":"  Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown\nenvironments requires accurate sensors for Global Navigation Satellite System\n(GNSS)-denied localization and obstacle avoidance. Such a requirement limits\nthe usage of cheap and micro-scale vehicles with constrained payload capacity\nif industrial-grade reliability and precision are required. This paper\ninvestigates the possibility of offloading the necessity to carry heavy sensors\nto another member of the UAV team while preserving the desired capability of\nthe smaller robot intended for exploring narrow passages. A novel cooperative\nguidance framework offloading the sensing requirements from a minimalistic\nsecondary UAV to a superior primary UAV is proposed. The primary UAV constructs\na dense occupancy map of the environment and plans collision-free paths for\nboth UAVs to ensure reaching the desired secondary UAV's goals even in areas\nnot accessible by the bigger robot. The primary UAV guides the secondary UAV to\nfollow the planned path while tracking the UAV using Light Detection and\nRanging (LiDAR)-based relative localization. The proposed approach was verified\nin real-world experiments with a heterogeneous team of a 3D LiDAR-equipped\nprimary UAV and a micro-scale camera-equipped secondary UAV moving autonomously\nthrough unknown cluttered GNSS-denied environments with the proposed framework\nrunning fully on board the UAVs.\n","authors":["Václav Pritzl","Matouš Vrba","Yurii Stasinchuk","Vít Krátký","Jiří Horyna","Petr Štěpán","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2312.09786v2.pdf","comment":"8 pages, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.11370v2","updated":"2024-03-19T09:12:00Z","published":"2024-03-17T23:23:40Z","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic\n  Environments using Graph Neural Networks","summary":"  The assumption of a static environment is common in many geometric computer\nvision tasks like SLAM but limits their applicability in highly dynamic scenes.\nSince these tasks rely on identifying point correspondences between input\nimages within the static part of the environment, we propose a graph neural\nnetwork-based sparse feature matching network designed to perform robust\nmatching under challenging conditions while excluding keypoints on moving\nobjects. We employ a similar scheme of attentional aggregation over graph edges\nto enhance keypoint representations as state-of-the-art feature-matching\nnetworks but augment the graph with epipolar and temporal information and\nvastly reduce the number of graph edges. Furthermore, we introduce a\nself-supervised training scheme to extract pseudo labels for image pairs in\ndynamic environments from exclusively unprocessed visual-inertial data. A\nseries of experiments show the superior performance of our network as it\nexcludes keypoints on moving objects compared to state-of-the-art feature\nmatching networks while still achieving similar results regarding conventional\nmatching metrics. When integrated into a SLAM system, our network significantly\nimproves performance, especially in highly dynamic scenes.\n","authors":["Theresa Huber","Simon Schaefer","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.11370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12552v1","updated":"2024-03-19T08:54:52Z","published":"2024-03-19T08:54:52Z","title":"M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for\n  Autonomous Driving","summary":"  End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.\n","authors":["Dongyang Xu","Haokun Li","Qingfan Wang","Ziying Song","Lei Chen","Hanming Deng"],"pdf_url":"https://arxiv.org/pdf/2403.12552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09007v2","updated":"2024-03-19T08:28:47Z","published":"2023-09-16T14:33:47Z","title":"MonoForce: Self-supervised Learning of Physics-aware Model for\n  Predicting Robot-terrain Interaction","summary":"  While autonomous navigation of mobile robots on rigid terrain is a\nwell-explored problem, navigating on deformable terrain such as tall grass or\nbushes remains a challenge. To address it, we introduce an explainable,\nphysics-aware and end-to-end differentiable model which predicts the outcome of\nrobot-terrain interaction from camera images, both on rigid and non-rigid\nterrain. The proposed MonoForce model consists of a black-box module which\npredicts robot-terrain interaction forces from onboard cameras, followed by a\nwhite-box module, which transforms these forces and a control signals into\npredicted trajectories, using only the laws of classical mechanics. The\ndifferentiable white-box module allows backpropagating the predicted trajectory\nerrors into the black-box module, serving as a self-supervised loss that\nmeasures consistency between the predicted forces and ground-truth trajectories\nof the robot. Experimental evaluation on a public dataset and our data has\nshown that while the prediction capabilities are comparable to state-of-the-art\nalgorithms on rigid terrain, MonoForce shows superior accuracy on non-rigid\nterrain such as tall grass or bushes. To facilitate the reproducibility of our\nresults, we release both the code and datasets.\n","authors":["Ruslan Agishev","Karel Zimmermann","Vladimír Kubelka","Martin Pecka","Tomáš Svoboda"],"pdf_url":"https://arxiv.org/pdf/2309.09007v2.pdf","comment":"8 pages, IROS-2024 submission"},{"id":"http://arxiv.org/abs/2403.12538v1","updated":"2024-03-19T08:25:42Z","published":"2024-03-19T08:25:42Z","title":"Multi-View Active Sensing for Human-Robot Interaction via Hierarchically\n  Connected Tree","summary":"  Comprehensive perception of human beings is the prerequisite to ensure the\nsafety of human-robot interaction. Currently, prevailing visual sensing\napproach typically involves a single static camera, resulting in a restricted\nand occluded field of view. In our work, we develop an active vision system\nusing multiple cameras to dynamically capture multi-source RGB-D data. An\nintegrated human sensing strategy based on a hierarchically connected tree\nstructure is proposed to fuse localized visual information. Constituting the\ntree model are the nodes representing keypoints and the edges representing\nkeyparts, which are consistently interconnected to preserve the structural\nconstraints during multi-source fusion. Utilizing RGB-D data and HRNet, the 3D\npositions of keypoints are analytically estimated, and their presence is\ninferred through a sliding widow of confidence scores. Subsequently, the point\nclouds of reliable keyparts are extracted by drawing occlusion-resistant masks,\nenabling fine registration between data clouds and cylindrical model following\nthe hierarchical order. Experimental results demonstrate that our method\nenhances keypart recognition recall from 69.20% to 90.10%, compared to\nemploying a single static camera. Furthermore, in overcoming challenges related\nto localized and occluded perception, the robotic arm's obstacle avoidance\ncapabilities are effectively improved.\n","authors":["Yuanjiong Ying","Xian Huang","Wei Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12535v1","updated":"2024-03-19T08:19:53Z","published":"2024-03-19T08:19:53Z","title":"High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided\n  Densification and Regularized Optimization","summary":"  We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that\nprovides metrically accurate pose tracking and visually realistic\nreconstruction. To this end, we first propose a Gaussian densification strategy\nbased on the rendering loss to map unobserved areas and refine reobserved\nareas. Second, we introduce extra regularization parameters to alleviate the\nforgetting problem in the continuous mapping problem, where parameters tend to\noverfit the latest frame and result in decreasing rendering quality for\nprevious frames. Both mapping and tracking are performed with Gaussian\nparameters by minimizing re-rendering loss in a differentiable way. Compared to\nrecent neural and concurrently developed gaussian splatting RGBD SLAM\nbaselines, our method achieves state-of-the-art results on the synthetic\ndataset Replica and competitive results on the real-world dataset TUM.\n","authors":["Shuo Sun","Malcolm Mielle","Achim J. Lilienthal","Martin Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12535v1.pdf","comment":"submitted to IROS24"},{"id":"http://arxiv.org/abs/2403.12533v1","updated":"2024-03-19T08:09:44Z","published":"2024-03-19T08:09:44Z","title":"To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions","summary":"  How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.\n","authors":["Daniel Tanneberg","Felix Ocker","Stephan Hasler","Joerg Deigmoeller","Anna Belardinelli","Chao Wang","Heiko Wersing","Bernhard Sendhoff","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2403.12533v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.12504v1","updated":"2024-03-19T07:11:00Z","published":"2024-03-19T07:11:00Z","title":"TON-VIO: Online Time Offset Modeling Networks for Robust Temporal\n  Alignment in High Dynamic Motion VIO","summary":"  Temporal misalignment (time offset) between sensors is common in low cost\nvisual-inertial odometry (VIO) systems. Such temporal misalignment introduces\ninconsistent constraints for state estimation, leading to a significant\npositioning drift especially in high dynamic motion scenarios. In this article,\nwe focus on online temporal calibration to reduce the positioning drift caused\nby the time offset for high dynamic motion VIO. For the time offset observation\nmodel, most existing methods rely on accurate state estimation or stable visual\ntracking. For the prediction model, current methods oversimplify the time\noffset as a constant value with white Gaussian noise. However, these ideal\nconditions are seldom satisfied in real high dynamic scenarios, resulting in\nthe poor performance. In this paper, we introduce online time offset modeling\nnetworks (TON) to enhance real-time temporal calibration. TON improves the\naccuracy of time offset observation and prediction modeling. Specifically, for\nobservation modeling, we propose feature velocity observation networks to\nenhance velocity computation for features in unstable visual tracking\nconditions. For prediction modeling, we present time offset prediction networks\nto learn its evolution pattern. To highlight the effectiveness of our method,\nwe integrate the proposed TON into both optimization-based and filter-based VIO\nsystems. Simulation and real-world experiments are conducted to demonstrate the\nenhanced performance of our approach. Additionally, to contribute to the VIO\ncommunity, we will open-source the code of our method on:\nhttps://github.com/Franky-X/FVON-TPN.\n","authors":["Chaoran Xiong","Guoqing Liu","Qi Wu","Songpengcheng Xia","Tong Hua","Kehui Ma","Zhen Sun","Yan Xiang","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2403.12504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12502v1","updated":"2024-03-19T07:10:04Z","published":"2024-03-19T07:10:04Z","title":"Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by\n  Human Finger","summary":"  Under-actuated robot grippers as a pervasive tool of robots have become a\nconsiderable research focus. Despite their simplicity of mechanical design and\ncontrol strategy, they suffer from poor versatility and weak adaptability,\nmaking widespread applications limited. To better relieve relevant research\ngaps, we present a novel 3-finger linkage-based gripper that realizes\nretractable and reconfigurable multi-mode grasps driven by a single motor.\nFirstly, inspired by the changes that occurred in the contact surface with a\nhuman finger moving, we artfully design a slider-slide rail mechanism as the\nphalanx to achieve retraction of each finger, allowing for better performance\nin the enveloping grasping mode. Secondly, a reconfigurable structure is\nconstructed to broaden the grasping range of objects' dimensions for the\nproposed gripper. By adjusting the configuration and gesture of each finger,\nthe gripper can achieve five grasping modes. Thirdly, the proposed gripper is\njust actuated by a single motor, yet it can be capable of grasping and\nreconfiguring simultaneously. Finally, various experiments on grasps of\nslender, thin, and large-volume objects are implemented to evaluate the\nperformance of the proposed gripper in practical scenarios, which demonstrates\nthe excellent grasping capabilities of the gripper.\n","authors":["Jihao Li","Tingbo Liao","Hassen Nigatu","Haotian Guo","Guodong Lu","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12502v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.12471v1","updated":"2024-03-19T06:07:01Z","published":"2024-03-19T06:07:01Z","title":"Theoretical Modeling and Bio-inspired Trajectory Optimization of A\n  Multiple-locomotion Origami Robot","summary":"  Recent research on mobile robots has focused on increasing their adaptability\nto unpredictable and unstructured environments using soft materials and\nstructures. However, the determination of key design parameters and control\nover these compliant robots are predominantly iterated through experiments,\nlacking a solid theoretical foundation. To improve their efficiency, this paper\naims to provide mathematics modeling over two locomotion, crawling and\nswimming. Specifically, a dynamic model is first devised to reveal the\ninfluence of the contact surfaces' frictional coefficients on displacements in\ndifferent motion phases. Besides, a swimming kinematics model is provided using\ncoordinate transformation, based on which, we further develop an algorithm that\nsystematically plans human-like swimming gaits, with maximum thrust obtained.\nThe proposed algorithm is highly generalizable and has the potential to be\napplied in other soft robots with multiple joints. Simulation experiments have\nbeen conducted to illustrate the effectiveness of the proposed modeling.\n","authors":["Keqi Zhu","Haotian Guo","Wei Yu","Hassen Nigatu","Tong Li","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12471v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.12465v1","updated":"2024-03-19T05:46:20Z","published":"2024-03-19T05:46:20Z","title":"Diagrammatic Instructions to Specify Spatial Objectives and Constraints\n  with Applications to Mobile Base Placement","summary":"  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach\nfor human operators to specify objectives and constraints that are related to\nspatial regions in the working environment. Human operators are enabled to\nsketch out regions directly on camera images that correspond to the objectives\nand constraints. These sketches are projected to 3D spatial coordinates, and\ncontinuous Spatial Instruction Maps (SIMs) are learned upon them. These maps\ncan then be integrated into optimization problems for tasks of robots. In\nparticular, we demonstrate how Spatial Diagrammatic Instructions can be applied\nto solve the Base Placement Problem of mobile manipulators, which concerns the\nbest place to put the manipulator to facilitate a certain task. Human operators\ncan specify, via sketch, spatial regions of interest for a manipulation task\nand permissible regions for the mobile manipulator to be at. Then, an\noptimization problem that maximizes the manipulator's reachability, or\ncoverage, over the designated regions of interest while remaining in the\npermissible regions is solved. We provide extensive empirical evaluations, and\nshow that our formulation of Spatial Instruction Maps provides accurate\nrepresentations of user-specified diagrammatic instructions. Furthermore, we\ndemonstrate that our diagrammatic approach to the Mobile Base Placement Problem\nenables higher quality solutions and faster run-time.\n","authors":["Qilin Sun","Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2403.12465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08460v2","updated":"2024-03-19T05:25:20Z","published":"2024-03-13T12:20:20Z","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model","summary":"  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n","authors":["Ruibin Zhang","Donglai Xue","Yuhan Wang","Ruixu Geng","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08460v2.pdf","comment":"8 pages, 6 figures, submitted to RA-L"},{"id":"http://arxiv.org/abs/2403.12449v1","updated":"2024-03-19T05:18:47Z","published":"2024-03-19T05:18:47Z","title":"Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter","summary":"  In this paper, we propose a novel method for plane clustering specialized in\ncluttered scenes using an RGB-D camera and validate its effectiveness through\nrobot grasping experiments. Unlike existing methods, which focus on large-scale\nindoor structures, our approach -- Multi-Object RANSAC emphasizes cluttered\nenvironments that contain a wide range of objects with different scales. It\nenhances plane segmentation by generating subplanes in Deep Plane Clustering\n(DPC) module, which are then merged with the final planes by post-processing.\nDPC rearranges the point cloud by voting layers to make subplane clusters,\ntrained in a self-supervised manner using pseudo-labels generated from RANSAC.\nMulti-Object RANSAC demonstrates superior plane instance segmentation\nperformances over other recent RANSAC applications. We conducted an experiment\non robot suction-based grasping, comparing our method with vision-based\ngrasping network and RANSAC applications. The results from this real-world\nscenario showed its remarkable performance surpassing the baseline methods,\nhighlighting its potential for advanced scene understanding and manipulation.\n","authors":["Seunghyeon Lim","Youngjae Yoo","Jun Ki Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12449v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.14590v3","updated":"2024-03-19T04:21:24Z","published":"2023-09-26T00:45:04Z","title":"HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition\n  under Spatiotemporal Variations","summary":"  Place recognition is crucial for robot localization and loop closure in\nsimultaneous localization and mapping (SLAM). Light Detection and Ranging\n(LiDAR), known for its robust sensing capabilities and measurement consistency\neven in varying illumination conditions, has become pivotal in various fields,\nsurpassing traditional imaging sensors in certain applications. Among various\ntypes of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning\npatterns have recently been utilized in robotics applications. Some LiDARs\nprovide additional measurements such as reflectivity, Near Infrared (NIR), and\nvelocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these\nadvances, there is a lack of comprehensive datasets reflecting the broad\nspectrum of LiDAR configurations for place recognition. To tackle this issue,\nour paper proposes the HeLiPR dataset, curated especially for place recognition\nwith heterogeneous LiDARs, embodying spatiotemporal variations. To the best of\nour knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset\nsupporting inter-LiDAR place recognition with both non-repetitive and spinning\nLiDARs, accommodating different field of view (FOV)s and varying numbers of\nrays. The dataset covers diverse environments, from urban cityscapes to\nhigh-dynamic freeways, over a month, enhancing adaptability and robustness\nacross scenarios. Notably, HeLiPR includes trajectories parallel to MulRan\nsequences, making it valuable for research in heterogeneous LiDAR place\nrecognition and long-term studies. The dataset is accessible at\nhttps://sites.google.com/view/heliprdataset.\n","authors":["Minwoo Jung","Wooseong Yang","Dongjae Lee","Hyeonjae Gil","Giseop Kim","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2309.14590v3.pdf","comment":"11 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.12421v1","updated":"2024-03-19T04:08:18Z","published":"2024-03-19T04:08:18Z","title":"UniDexFPM: Universal Dexterous Functional Pre-grasp Manipulation Via\n  Diffusion Policy","summary":"  Objects in the real world are often not naturally positioned for functional\ngrasping, which usually requires repositioning and reorientation before they\ncan be grasped, a process known as pre-grasp manipulation. However, effective\nlearning of universal dexterous functional pre-grasp manipulation necessitates\nprecise control over relative position, relative orientation, and contact\nbetween the hand and object, while generalizing to diverse dynamic scenarios\nwith varying objects and goal poses. We address the challenge by using\nteacher-student learning. We propose a novel mutual reward that incentivizes\nagents to jointly optimize three key criteria. Furthermore, we introduce a\npipeline that leverages a mixture-of-experts strategy to learn diverse\nmanipulation policies, followed by a diffusion policy to capture complex action\ndistributions from these experts. Our method achieves a success rate of 72.6%\nacross 30+ object categories encompassing 1400+ objects and 10k+ goal poses.\nNotably, our method relies solely on object pose information for universal\ndexterous functional pre-grasp manipulation by using extrinsic dexterity and\nadjusting from feedback. Additional experiments under noisy object pose\nobservation showcase the robustness of our method and its potential for\nreal-world applications. The demonstrations can be viewed at\nhttps://unidexfpm.github.io.\n","authors":["Tianhao Wu","Yunchong Gan","Mingdong Wu","Jingbo Cheng","Yaodong Yang","Yixin Zhu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12420v1","updated":"2024-03-19T04:07:45Z","published":"2024-03-19T04:07:45Z","title":"Bin Packing Optimization via Deep Reinforcement Learning","summary":"  The Bin Packing Problem (BPP) has attracted enthusiastic research interest\nrecently, owing to widespread applications in logistics and warehousing\nenvironments. It is truly essential to optimize the bin packing to enable more\nobjects to be packed into boxes. Object packing order and placement strategy\nare the two crucial optimization objectives of the BPP. However, existing\noptimization methods for BPP, such as the genetic algorithm (GA), emerge as the\nmain issues in highly computational cost and relatively low accuracy, making it\ndifficult to implement in realistic scenarios. To well relieve the research\ngaps, we present a novel optimization methodology of two-dimensional (2D)-BPP\nand three-dimensional (3D)-BPP for objects with regular shapes via deep\nreinforcement learning (DRL), maximizing the space utilization and minimizing\nthe usage number of boxes. First, an end-to-end DRL neural network constructed\nby a modified Pointer Network consisting of an encoder, a decoder and an\nattention module is proposed to achieve the optimal object packing order.\nSecond, conforming to the top-down operation mode, the placement strategy based\non a height map is used to arrange the ordered objects in the boxes, preventing\nthe objects from colliding with boxes and other objects in boxes. Third, the\nreward and loss functions are defined as the indicators of the compactness,\npyramid, and usage number of boxes to conduct the training of the DRL neural\nnetwork based on an on-policy actor-critic framework. Finally, a series of\nexperiments are implemented to compare our method with conventional packing\nmethods, from which we conclude that our method outperforms these packing\nmethods in both packing accuracy and efficiency.\n","authors":["Baoying Wang","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11461v2","updated":"2024-03-19T03:32:15Z","published":"2024-03-18T04:26:52Z","title":"VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation","summary":"  In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a\nnovel method designed to enhance 3D manipulation capabilities through\naction-aware view rendering. VIHE autoregressively refines actions in multiple\nstages by conditioning on rendered views posed from action predictions in the\nearlier stages. These virtual in-hand views provide a strong inductive bias for\neffectively recognizing the correct pose for the hand, especially for\nchallenging high-precision tasks such as peg insertion. On 18 manipulation\ntasks in RLBench simulated environments, VIHE achieves a new state-of-the-art,\nwith a 12% absolute improvement, increasing from 65% to 77% over the existing\nstate-of-the-art model using 100 demonstrations per task. In real-world\nscenarios, VIHE can learn manipulation tasks with just a handful of\ndemonstrations, highlighting its practical utility. Videos and code\nimplementation can be found at our project site: https://vihe-3d.github.io.\n","authors":["Weiyao Wang","Yutian Lei","Shiyu Jin","Gregory D. Hager","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09297v2","updated":"2024-03-19T03:25:50Z","published":"2023-09-17T15:14:01Z","title":"Chasing Day and Night: Towards Robust and Efficient All-Day Object\n  Detection Guided by an Event Camera","summary":"  The ability to detect objects in all lighting (i.e., normal-, over-, and\nunder-exposed) conditions is crucial for real-world applications, such as\nself-driving.Traditional RGB-based detectors often fail under such varying\nlighting conditions.Therefore, recent works utilize novel event cameras to\nsupplement or guide the RGB modality; however, these methods typically adopt\nasymmetric network structures that rely predominantly on the RGB modality,\nresulting in limited robustness for all-day detection. In this paper, we\npropose EOLO, a novel object detection framework that achieves robust and\nefficient all-day detection by fusing both RGB and event modalities. Our EOLO\nframework is built based on a lightweight spiking neural network (SNN) to\nefficiently leverage the asynchronous property of events. Buttressed by it, we\nfirst introduce an Event Temporal Attention (ETA) module to learn the high\ntemporal information from events while preserving crucial edge information.\nSecondly, as different modalities exhibit varying levels of importance under\ndiverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion\n(SREF) module to effectively fuse RGB-Event features without relying on a\nspecific modality, thus ensuring a balanced and adaptive fusion for all-day\ndetection. In addition, to compensate for the lack of paired RGB-Event datasets\nfor all-day training and evaluation, we propose an event synthesis approach\nbased on the randomized optical flow that allows for directly generating the\nevent frame from a single exposure image. We further build two new datasets,\nE-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC.\nExtensive experiments demonstrate that our EOLO outperforms the\nstate-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in\nall lighting conditions.Our code and datasets will be available at\nhttps://vlislab22.github.io/EOLO/\n","authors":["Jiahang Cao","Xu Zheng","Yuanhuiyi Lyu","Jiaxu Wang","Renjing Xu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.09297v2.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.12396v1","updated":"2024-03-19T03:09:24Z","published":"2024-03-19T03:09:24Z","title":"OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation","summary":"  This paper studies a new open-set problem, the open-vocabulary category-level\nobject pose and size estimation. Given human text descriptions of arbitrary\nnovel object categories, the robot agent seeks to predict the position,\norientation, and size of the target object in the observed scene image. To\nenable such generalizability, we first introduce OO3D-9D, a large-scale\nphotorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the\nlargest and most diverse dataset in the field of category-level object pose and\nsize estimation. It includes additional annotations for the symmetry axis of\neach category, which help resolve symmetric ambiguity. Apart from the\nlarge-scale dataset, we find another key to enabling such generalizability is\nleveraging the strong prior knowledge in pre-trained visual-language foundation\nmodels. We then propose a framework built on pre-trained DinoV2 and\ntext-to-image stable diffusion models to infer the normalized object coordinate\nspace (NOCS) maps of the target instances. This framework fully leverages the\nvisual semantic prior from DinoV2 and the aligned visual and language knowledge\nwithin the text-to-image diffusion model, which enables generalization to\nvarious text descriptions of novel categories. Comprehensive quantitative and\nqualitative experiments demonstrate that the proposed open-vocabulary method,\ntrained on our large-scale synthesized data, significantly outperforms the\nbaseline and can effectively generalize to real-world images of unseen\ncategories. The project page is at https://ov9d.github.io.\n","authors":["Junhao Cai","Yisheng He","Weihao Yuan","Siyu Zhu","Zilong Dong","Liefeng Bo","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.12396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10554v2","updated":"2024-03-19T03:05:53Z","published":"2024-03-13T04:40:53Z","title":"Safe Planning through Incremental Decomposition of Signal Temporal Logic\n  Specifications","summary":"  Trajectory planning is a critical process that enables autonomous systems to\nsafely navigate complex environments. Signal temporal logic (STL)\nspecifications are an effective way to encode complex temporally extended\nobjectives for trajectory planning in cyber-physical systems (CPS). However,\nplanning from these specifications using existing techniques scale\nexponentially with the number of nested operators and the horizon of\nspecification. Additionally, performance is exacerbated at runtime due to\nlimited computational budgets and compounding modeling errors. Decomposing a\ncomplex specification into smaller subtasks and incrementally planning for them\ncan remedy these issues. In this work, we present a way to decompose STL\nrequirements temporally to improve planning efficiency and performance. The key\ninsight in our work is to encode all specifications as a set of reachability\nand invariance constraints and scheduling these constraints sequentially at\nruntime. Our proposed technique outperforms the state-of-the-art trajectory\nsynthesis techniques for both linear and non linear dynamical systems.\n","authors":["Parv Kapoor","Eunsuk Kang","Romulo Meira-Goes"],"pdf_url":"https://arxiv.org/pdf/2403.10554v2.pdf","comment":"Accepted to Nasa Formal Methods (NFM) 2024"},{"id":"http://arxiv.org/abs/2403.11852v2","updated":"2024-03-19T02:46:55Z","published":"2024-03-18T15:02:46Z","title":"Reinforcement Learning with Latent State Inference for Autonomous\n  On-ramp Merging under Observation Delay","summary":"  This paper presents a novel approach to address the challenging problem of\nautonomous on-ramp merging, where a self-driving vehicle needs to seamlessly\nintegrate into a flow of vehicles on a multi-lane highway. We introduce the\nLane-keeping, Lane-changing with Latent-state Inference and Safety Controller\n(L3IS) agent, designed to perform the on-ramp merging task safely without\ncomprehensive knowledge about surrounding vehicles' intents or driving styles.\nWe also present an augmentation of this agent called AL3IS that accounts for\nobservation delays, allowing the agent to make more robust decisions in\nreal-world environments with vehicle-to-vehicle (V2V) communication delays. By\nmodeling the unobservable aspects of the environment through latent states,\nsuch as other drivers' intents, our approach enhances the agent's ability to\nadapt to dynamic traffic conditions, optimize merging maneuvers, and ensure\nsafe interactions with other vehicles. We demonstrate the effectiveness of our\nmethod through extensive simulations generated from real traffic data and\ncompare its performance with existing approaches. L3IS shows a 99.90% success\nrate in a challenging on-ramp merging case generated from the real US Highway\n101 data. We further perform a sensitivity analysis on AL3IS to evaluate its\nrobustness against varying observation delays, which demonstrates an acceptable\nperformance of 93.84% success rate in 1-second V2V communication delay.\n","authors":["Amin Tabrizian","Zhitong Huang","Peng Wei"],"pdf_url":"https://arxiv.org/pdf/2403.11852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12377v1","updated":"2024-03-19T02:40:51Z","published":"2024-03-19T02:40:51Z","title":"Online Multi-Agent Pickup and Delivery with Task Deadlines","summary":"  Managing delivery deadlines in automated warehouses and factories is crucial\nfor maintaining customer satisfaction and ensuring seamless production. This\nstudy introduces the problem of online multi-agent pickup and delivery with\ntask deadlines (MAPD-D), which is an advanced variant of the online MAPD\nproblem incorporating delivery deadlines. MAPD-D presents a dynamic\ndeadline-driven approach that includes task deadlines, with tasks being added\nat any time (online), thus challenging conventional MAPD frameworks. To tackle\nMAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP).\nThe D-TP algorithm is designed to calculate pickup deadlines and assign tasks\nwhile balancing execution cost and deadline proximity. Additionally, we\nintroduce the D-TP with task swaps (D-TPTS) method to further reduce task\ntardiness, enhancing flexibility and efficiency via task-swapping strategies.\nNumerical experiments were conducted in simulated warehouse environments to\nshowcase the effectiveness of the proposed methods. Both D-TP and D-TPTS\ndemonstrate significant reductions in task tardiness compared to existing\nmethods, thereby contributing to efficient operations in automated warehouses\nand factories with delivery deadlines.\n","authors":["Hiroya Makino","Seigo Ito"],"pdf_url":"https://arxiv.org/pdf/2403.12377v1.pdf","comment":"6 pages, 2 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2310.07968v3","updated":"2024-03-19T01:32:19Z","published":"2023-10-12T01:17:56Z","title":"Think, Act, and Ask: Open-World Interactive Personalized Robot\n  Navigation","summary":"  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards\nopen-vocabulary objects in unknown environments. The existing works of ZSON\nmainly focus on following individual instructions to find generic object\nclasses, neglecting the utilization of natural language interaction and the\ncomplexities of identifying user-specific objects. To address these\nlimitations, we introduce Zero-shot Interactive Personalized Object Navigation\n(ZIPON), where robots need to navigate to personalized goal objects while\nengaging in conversations with users. To solve ZIPON, we propose a new\nframework termed Open-woRld Interactive persOnalized Navigation (ORION), which\nuses Large Language Models (LLMs) to make sequential decisions to manipulate\ndifferent modules for perception, navigation and communication. Experimental\nresults show that the performance of interactive agents that can leverage user\nfeedback exhibits significant improvement. However, obtaining a good balance\nbetween task completion and the efficiency of navigation and interaction\nremains challenging for all methods. We further provide more findings on the\nimpact of diverse user feedback forms on the agents' performance.\n","authors":["Yinpei Dai","Run Peng","Sikai Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2310.07968v3.pdf","comment":"Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:\n  https://github.com/sled-group/navchat"},{"id":"http://arxiv.org/abs/2305.19157v2","updated":"2024-03-19T01:09:34Z","published":"2023-05-30T15:58:56Z","title":"Sensor Fault Detection and Compensation with Performance Prescription\n  for Robotic Manipulators","summary":"  This paper focuses on sensor fault detection and compensation for robotic\nmanipulators. The proposed method features a new adaptive observer and a new\nterminal sliding mode control law established on a second-order integral\nsliding surface. The method enables sensor fault detection without the need to\nknow the bounds on fault value and/or its derivative. It also enables fast and\nfixed-time fault-tolerant control whose performance can be prescribed\nbeforehand by defining funnel bounds on the tracking error. The ultimate\nboundedness of the estimation errors for the proposed observer and the\nfixed-time stability of the control system are shown using Lyapunov stability\nanalysis. The effectiveness of the proposed method is verified using numerical\nsimulations on two different robotic manipulators, and the results are compared\nwith existing methods. Our results demonstrate performance gains obtained by\nthe proposed method compared to the existing results.\n","authors":["S. Mohammadreza Ebrahimi","Farid Norouzi","Hossein Dastres","Reza Faieghi","Mehdi Naderi","Milad Malekzadeh"],"pdf_url":"https://arxiv.org/pdf/2305.19157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09509v2","updated":"2024-03-19T00:05:21Z","published":"2023-06-15T21:06:54Z","title":"Granger-Causal Hierarchical Skill Discovery","summary":"  Reinforcement Learning (RL) has demonstrated promising results in learning\npolicies for complex tasks, but it often suffers from low sample efficiency and\nlimited transferability. Hierarchical RL (HRL) methods aim to address the\ndifficulty of learning long-horizon tasks by decomposing policies into skills,\nabstracting states, and reusing skills in new tasks. However, many HRL methods\nrequire some initial task success to discover useful skills, which\nparadoxically may be very unlikely without access to useful skills. On the\nother hand, reward-free HRL methods often need to learn far too many skills to\nachieve proper coverage in high-dimensional domains. In contrast, we introduce\nthe Chain of Interaction Skills (COInS) algorithm, which focuses on\ncontrollability in factored domains to identify a small number of task-agnostic\nskills that still permit a high degree of control. COInS uses learned detectors\nto identify interactions between state factors and then trains a chain of\nskills to control each of these factors successively. We evaluate COInS on a\nrobotic pushing task with obstacles-a challenging domain where other RL and HRL\nmethods fall short. We also demonstrate the transferability of skills learned\nby COInS, using variants of Breakout, a common RL benchmark, and show 2-3x\nimprovement in both sample efficiency and final performance compared to\nstandard RL baselines.\n","authors":["Caleb Chuck","Kevin Black","Aditya Arjun","Yuke Zhu","Scott Niekum"],"pdf_url":"https://arxiv.org/pdf/2306.09509v2.pdf","comment":"Accepted TMLR 2024"},{"id":"http://arxiv.org/abs/2403.13208v1","updated":"2024-03-19T23:57:55Z","published":"2024-03-19T23:57:55Z","title":"CaDRE: Controllable and Diverse Generation of Safety-Critical Driving\n  Scenarios using Real-World Trajectories","summary":"  Simulation is an indispensable tool in the development and testing of\nautonomous vehicles (AVs), offering an efficient and safe alternative to road\ntesting by allowing the exploration of a wide range of scenarios. Despite its\nadvantages, a significant challenge within simulation-based testing is the\ngeneration of safety-critical scenarios, which are essential to ensure that AVs\ncan handle rare but potentially fatal situations. This paper addresses this\nchallenge by introducing a novel generative framework, CaDRE, which is\nspecifically designed for generating diverse and controllable safety-critical\nscenarios using real-world trajectories. Our approach optimizes for both the\nquality and diversity of scenarios by employing a unique formulation and\nalgorithm that integrates real-world data, domain knowledge, and black-box\noptimization techniques. We validate the effectiveness of our framework through\nextensive testing in three representative types of traffic scenarios. The\nresults demonstrate superior performance in generating diverse and high-quality\nscenarios with greater sample efficiency than existing reinforcement learning\nand sampling-based methods.\n","authors":["Peide Huang","Wenhao Ding","Jonathan Francis","Bingqing Chen","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13198v1","updated":"2024-03-19T23:18:40Z","published":"2024-03-19T23:18:40Z","title":"Towards Robots That Know When They Need Help: Affordance-Based\n  Uncertainty for Large Language Model Planners","summary":"  Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in consumer robotics where LLM hallucinations may\nresult in robots confidently executing plans that are contrary to user goals,\nrelying more frequently on human assistance, or preventing the robot from\nasking for help at all. In this work, we present LAP, a novel approach for\nutilizing off-the-shelf LLM's, alongside scene and object Affordances, in\nrobotic Planners that minimize harmful hallucinations and know when to ask for\nhelp. Our key finding is that calculating and leveraging a scene affordance\nscore, a measure of whether a given action is possible in the provided scene,\nhelps to mitigate hallucinations in LLM predictions and better align the LLM's\nconfidence measure with the probability of success. We specifically propose and\ntest three different affordance scores, which can be used independently or in\ntandem to improve performance across different use cases. The most successful\nof these individual scores involves prompting an LLM to determine if a given\naction is possible and safe in the given scene and uses the LLM's response to\ncompute the score. Through experiments in both simulation and the real world,\non tasks with a variety of ambiguities, we show that LAP significantly\nincreases success rate and decreases the amount of human intervention required\nrelative to prior art. For example, in our real-world testing paradigm, LAP\ndecreases the human help rate of previous methods by over 33% at a success rate\nof 70%.\n","authors":["James F. Mullen Jr.","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.13198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13188v1","updated":"2024-03-19T22:57:03Z","published":"2024-03-19T22:57:03Z","title":"Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation","summary":"  LiDAR semantic segmentation frameworks predominantly leverage geometry-based\nfeatures to differentiate objects within a scan. While these methods excel in\nscenarios with clear boundaries and distinct shapes, their performance declines\nin environments where boundaries are blurred, particularly in off-road\ncontexts. To address this, recent strides in 3D segmentation algorithms have\nfocused on harnessing raw LiDAR intensity measurements to improve prediction\naccuracy. Despite these efforts, current learning-based models struggle to\ncorrelate the intricate connections between raw intensity and factors such as\ndistance, incidence angle, material reflectivity, and atmospheric conditions.\nBuilding upon our prior work, this paper delves into the advantages of\nemploying calibrated intensity (also referred to as reflectivity) within\nlearning-based LiDAR semantic segmentation frameworks. We initially establish\nthat incorporating reflectivity as an input enhances the existing LiDAR\nsemantic segmentation model. Furthermore, we present findings that enable the\nmodel to learn to calibrate intensity can boost its performance. Through\nextensive experimentation on the off-road dataset Rellis-3D, we demonstrate\nnotable improvements. Specifically, converting intensity to reflectivity\nresults in a 4% increase in mean Intersection over Union (mIoU) when compared\nto using raw intensity in Off-road scenarios. Additionally, we also investigate\nthe possible benefits of using calibrated intensity in semantic segmentation in\nurban environments (SemanticKITTI) and cross-sensor domain adaptation.\n","authors":["Kasi Viswanath","Peng Jiang","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.13188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13177v1","updated":"2024-03-19T22:06:37Z","published":"2024-03-19T22:06:37Z","title":"User-customizable Shared Control for Fine Teleoperation via Virtual\n  Reality","summary":"  Shared control can ease and enhance a human operator's ability to teleoperate\nrobots, particularly for intricate tasks demanding fine control over multiple\ndegrees of freedom. However, the arbitration process dictating how much\nautonomous assistance to administer in shared control can confuse novice\noperators and impede their understanding of the robot's behavior. To overcome\nthese adverse side-effects, we propose a novel formulation of shared control\nthat enables operators to tailor the arbitration to their unique capabilities\nand preferences. Unlike prior approaches to customizable shared control where\nusers could indirectly modify the latent parameters of the arbitration function\nby issuing a feedback command, we instead make these parameters observable and\ndirectly editable via a virtual reality (VR) interface. We present our\nuser-customizable shared control method for a teleoperation task in SE(3),\nknown as the buzz wire game. A user study is conducted with participants\nteleoperating a robotic arm in VR to complete the game. The experiment spanned\ntwo weeks per subject to investigate longitudinal trends. Our findings reveal\nthat users allowed to interactively tune the arbitration parameters across\ntrials generalize well to adaptations in the task, exhibiting improvements in\nprecision and fluency over direct teleoperation and conventional shared\ncontrol.\n","authors":["Rui Luo","Mark Zolotas","Drake Moore","Taskin Padir"],"pdf_url":"https://arxiv.org/pdf/2403.13177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13170v1","updated":"2024-03-19T21:49:26Z","published":"2024-03-19T21:49:26Z","title":"On Designing Consistent Covariance Recovery from a Deep Learning Visual\n  Odometry Engine","summary":"  Deep learning techniques have significantly advanced in providing accurate\nvisual odometry solutions by leveraging large datasets. However, generating\nuncertainty estimates for these methods remains a challenge. Traditional sensor\nfusion approaches in a Bayesian framework are well-established, but deep\nlearning techniques with millions of parameters lack efficient methods for\nuncertainty estimation.\n  This paper addresses the issue of uncertainty estimation for pre-trained\ndeep-learning models in monocular visual odometry. We propose formulating a\nfactor graph on an implicit layer of the deep learning network to recover\nrelative covariance estimates, which allows us to determine the covariance of\nthe Visual Odometry (VO) solution. We showcase the consistency of the deep\nlearning engine's covariance approximation with an empirical analysis of the\ncovariance model on the EUROC datasets to demonstrate the correctness of our\nformulation.\n","authors":["Jagatpreet Singh Nir","Dennis Giaya","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2403.13170v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13147v1","updated":"2024-03-19T20:50:20Z","published":"2024-03-19T20:50:20Z","title":"Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand\n  Orthosis for Stroke","summary":"  We propose MetaEMG, a meta-learning approach for fast adaptation in intent\ninferral on a robotic hand orthosis for stroke. One key challenge in machine\nlearning for assistive and rehabilitative robotics with disabled-bodied\nsubjects is the difficulty of collecting labeled training data. Muscle tone and\nspasticity often vary significantly among stroke subjects, and hand function\ncan even change across different use sessions of the device for the same\nsubject. We investigate the use of meta-learning to mitigate the burden of data\ncollection needed to adapt high-capacity neural networks to a new session or\nsubject. Our experiments on real clinical data collected from five stroke\nsubjects show that MetaEMG can improve the intent inferral accuracy with a\nsmall session- or subject-specific dataset and very few fine-tuning epochs. To\nthe best of our knowledge, we are the first to formulate intent inferral on\nstroke subjects as a meta-learning problem and demonstrate fast adaptation to a\nnew session or subject for controlling a robotic hand orthosis with EMG\nsignals.\n","authors":["Pedro Leandro La Rotta","Jingxi Xu","Ava Chen","Lauren Winterbottom","Wenxi Chen","Dawn Nilsen","Joel Stein","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2403.13147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13144v1","updated":"2024-03-19T20:41:06Z","published":"2024-03-19T20:41:06Z","title":"Interactive Robot-Environment Self-Calibration via Compliant Exploratory\n  Actions","summary":"  Calibrating robots into their workspaces is crucial for manipulation tasks.\nExisting calibration techniques often rely on sensors external to the robot\n(cameras, laser scanners, etc.) or specialized tools. This reliance complicates\nthe calibration process and increases the costs and time requirements.\nFurthermore, the associated setup and measurement procedures require\nsignificant human intervention, which makes them more challenging to operate.\nUsing the built-in force-torque sensors, which are nowadays a default component\nin collaborative robots, this work proposes a self-calibration framework where\nrobot-environmental spatial relations are automatically estimated through\ncompliant exploratory actions by the robot itself. The self-calibration\napproach converges, verifies its own accuracy, and terminates upon completion,\nautonomously purely through interactive exploration of the environment's\ngeometries. Extensive experiments validate the effectiveness of our\nself-calibration approach in accurately establishing the robot-environment\nspatial relationships without the need for additional sensing equipment or any\nhuman intervention.\n","authors":["Podshara Chanrungmaneekul","Kejia Ren","Joshua T. Grace","Aaron M. Dollar","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13132v1","updated":"2024-03-19T20:04:35Z","published":"2024-03-19T20:04:35Z","title":"Wearable Roller Rings to Enable Robot Dexterous In-Hand Manipulation\n  through Active Surfaces","summary":"  In-hand manipulation is a crucial ability for reorienting and repositioning\nobjects within grasps. The main challenges are not only the complexity in the\ncomputational models, but also the risks of grasp instability caused by active\nfinger motions, such as rolling, sliding, breaking, and remaking contacts.\nBased on the idea of manipulation without lifting a finger, this paper presents\nthe development of Roller Rings (RR), a modular robotic attachment with active\nsurfaces that is wearable by both robot and human hands. By installing and\nangling the RRs on grasping systems, such that their spatial motions are not\nco-linear, we derive a general differential motion model for the object\nactuated by the active surfaces. Our motion model shows that complete in-hand\nmanipulation skill sets can be provided by as few as only 2 RRs through\nnon-holonomic object motions, while more RRs can enable enhanced manipulation\ndexterity with fewer motion constraints. Through extensive experiments, we wear\nRRs on both a robot hand and a human hand to evaluate their manipulation\ncapabilities, and show that the RRs can be employed to manipulate arbitrary\nobject shapes to provide dexterous in-hand manipulation.\n","authors":["Hayden Webb","Podshara Chanrungmaneekul","Shenli Yuan","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13129v1","updated":"2024-03-19T19:58:54Z","published":"2024-03-19T19:58:54Z","title":"Better Call SAL: Towards Learning to Segment Anything in Lidar","summary":"  We propose $\\texttt{SAL}$ ($\\texttt{S}$egment $\\texttt{A}$nything in\n$\\texttt{L}$idar) method consisting of a text-promptable zero-shot model for\nsegmenting and classifying any object in Lidar, and a pseudo-labeling engine\nthat facilitates model training without manual supervision. While the\nestablished paradigm for $\\textit{Lidar Panoptic Segmentation}$ (LPS) relies on\nmanual supervision for a handful of object classes defined a priori, we utilize\n2D vision foundation models to generate 3D supervision \"for free\". Our\npseudo-labels consist of instance masks and corresponding CLIP tokens, which we\nlift to Lidar using calibrated multi-modal data. By training our model on these\nlabels, we distill the 2D foundation models into our Lidar $\\texttt{SAL}$\nmodel. Even without manual labels, our model achieves $91\\%$ in terms of\nclass-agnostic segmentation and $44\\%$ in terms of zero-shot LPS of the fully\nsupervised state-of-the-art. Furthermore, we outperform several baselines that\ndo not distill but only lift image features to 3D. More importantly, we\ndemonstrate that $\\texttt{SAL}$ supports arbitrary class prompts, can be easily\nextended to new datasets, and shows significant potential to improve with\nincreasing amounts of self-labeled data.\n","authors":["Aljoša Ošep","Tim Meinhardt","Francesco Ferroni","Neehar Peri","Deva Ramanan","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13124v1","updated":"2024-03-19T19:54:32Z","published":"2024-03-19T19:54:32Z","title":"Cooperative Modular Manipulation with Numerous Cable-Driven Robots for\n  Assistive Construction and Gap Crossing","summary":"  Soldiers in the field often need to cross negative obstacles, such as rivers\nor canyons, to reach goals or safety. Military gap crossing involves on-site\ntemporary bridges construction. However, this procedure is conducted with\ndangerous, time and labor intensive operations, and specialized machinery. We\nenvision a scalable robotic solution inspired by advancements in\nforce-controlled and Cable Driven Parallel Robots (CDPRs); this solution can\naddress the challenges inherent in this transportation problem, achieving fast,\nefficient, and safe deployment and field operations. We introduce the embodied\nvision in Co3MaNDR, a solution to the military gap crossing problem, a\ndistributed robot consisting of several modules simultaneously pulling on a\ncentral payload, controlling the cables' tensions to achieve complex\nobjectives, such as precise trajectory tracking or force amplification.\nHardware experiments demonstrate teleoperation of a payload, trajectory\nfollowing, and the sensing and amplification of operators' applied physical\nforces during slow operations. An operator was shown to manipulate a 27.2 kg\n(60 lb) payload with an average force utilization of 14.5\\% of its weight.\nResults indicate that the system can be scaled up to heavier payloads without\ncompromising performance or introducing superfluous complexity. This research\nlays a foundation to expand CDPR technology to uncoordinated and unstable\nmobile platforms in unknown environments.\n","authors":["Kevin Murphy","Joao C. V. Soares","Justin K. Yim","Dustin Nottage","Ahmet Soylemezoglu","Joao Ramos"],"pdf_url":"https://arxiv.org/pdf/2403.13124v1.pdf","comment":"8 pages, 9 figures. Submit to IROS 2024"},{"id":"http://arxiv.org/abs/2305.03001v2","updated":"2024-03-19T19:40:44Z","published":"2023-05-04T17:19:47Z","title":"OSDaR23: Open Sensor Data for Rail 2023","summary":"  To achieve a driverless train operation on mainline railways, actual and\npotential obstacles for the train's driveway must be detected automatically by\nappropriate sensor systems. Machine learning algorithms have proven to be\npowerful tools for this task during the last years. However, these algorithms\nrequire large amounts of high-quality annotated data containing\nrailway-specific objects as training data. Unfortunately, all of the publicly\navailable datasets that tackle this requirement are restricted in some way.\nTherefore, this paper presents OSDaR23, a multi-sensor dataset of 45\nsubsequences acquired in Hamburg, Germany, in September 2021, that was created\nto foster driverless train operation on mainline railways. The sensor setup\nconsists of multiple calibrated and synchronized infrared (IR) and visual (RGB)\ncameras, lidars, a radar, and position and acceleration sensors mounted on the\nfront of a rail vehicle. In addition to the raw data, the dataset contains\n204091 polyline, polygonal, rectangle, and cuboid annotations in total for 20\ndifferent object classes. It is the first publicly available multi-sensor\ndataset annotated with a variety of object classes that are relevant for the\nrailway context. OSDaR23, available at data.fid-move.de/dataset/osdar23, can\nalso be used for tasks beyond collision prediction, which are listed in this\npaper.\n","authors":["Rustam Tagiew","Martin Köppel","Karsten Schwalbe","Patrick Denzler","Philipp Neumaier","Tobias Klockau","Martin Boekhoff","Pavel Klasek","Roman Tilly"],"pdf_url":"https://arxiv.org/pdf/2305.03001v2.pdf","comment":"7 pages, 11 images, 5 tables"},{"id":"http://arxiv.org/abs/2311.00112v2","updated":"2024-03-19T19:37:22Z","published":"2023-10-31T19:39:44Z","title":"Hierarchical Optimization-based Control for Whole-body Loco-manipulation\n  of Heavy Objects","summary":"  In recent years, the field of legged robotics has seen growing interest in\nenhancing the capabilities of these robots through the integration of\narticulated robotic arms. However, achieving successful loco-manipulation,\nespecially involving interaction with heavy objects, is far from\nstraightforward, as object manipulation can introduce substantial disturbances\nthat impact the robot's locomotion. This paper presents a novel framework for\nlegged loco-manipulation that considers whole-body coordination through a\nhierarchical optimization-based control framework. First, an online\nmanipulation planner computes the manipulation forces and manipulated object\ntask-based reference trajectory. Then, pose optimization aligns the robot's\ntrajectory with kinematic constraints. The resultant robot reference trajectory\nis executed via a linear MPC controller incorporating the desired manipulation\nforces into its prediction model. Our approach has been validated in simulation\nand hardware experiments, highlighting the necessity of whole-body optimization\ncompared to the baseline locomotion MPC when interacting with heavy objects.\nExperimental results with Unitree Aliengo, equipped with a custom-made robotic\narm, showcase its ability to lift and carry an 8kg payload and manipulate\ndoors.\n","authors":["Alberto Rigo","Muqun Hu","Satyandra K. Gupta","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.00112v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.09442v2","updated":"2024-03-19T19:11:32Z","published":"2023-10-13T23:23:39Z","title":"Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC","summary":"  In the context of legged robots, adaptive behavior involves adaptive\nbalancing and adaptive swing foot reflection. While adaptive balancing\ncounteracts perturbations to the robot, adaptive swing foot reflection helps\nthe robot to navigate intricate terrains without foot entrapment. In this\npaper, we manage to bring both aspects of adaptive behavior to quadruped\nlocomotion by combining RL and MPC while improving the robustness and agility\nof blind legged locomotion. This integration leverages MPC's strength in\npredictive capabilities and RL's adeptness in drawing from past experiences.\nUnlike traditional locomotion controls that separate stance foot control and\nswing foot trajectory, our innovative approach unifies them, addressing their\nlack of synchronization. At the heart of our contribution is the synthesis of\nstance foot control with swing foot reflection, improving agility and\nrobustness in locomotion with adaptive behavior. A hallmark of our approach is\nrobust blind stair climbing through swing foot reflection. Moreover, we\nintentionally designed the learning module as a general plugin for different\nrobot platforms. We trained the policy and implemented our approach on the\nUnitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s,\na peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably,\nthis framework also allows the robot to maintain stable locomotion while\nbearing an unexpected load of 10 kg, or 83\\% of its body mass. We further\ndemonstrate the generalizability and robustness of the same policy where it\nrealizes zero-shot transfer to different robot platforms like Go1 and AlienGo\nrobots for load carrying. Code is made available for the use of the research\ncommunity at https://github.com/DRCL-USC/RL_augmented_MPC.git\n","authors":["Yiyu Chen","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.09442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13093v1","updated":"2024-03-19T18:42:22Z","published":"2024-03-19T18:42:22Z","title":"Graph Neural Network-based Multi-agent Reinforcement Learning for\n  Resilient Distributed Coordination of Multi-Robot Systems","summary":"  Existing multi-agent coordination techniques are often fragile and vulnerable\nto anomalies such as agent attrition and communication disturbances, which are\nquite common in the real-world deployment of systems like field robotics. To\nbetter prepare these systems for the real world, we present a graph neural\nnetwork (GNN)-based multi-agent reinforcement learning (MARL) method for\nresilient distributed coordination of a multi-robot system. Our method,\nMulti-Agent Graph Embedding-based Coordination (MAGEC), is trained using\nmulti-agent proximal policy optimization (PPO) and enables distributed\ncoordination around global objectives under agent attrition, partial\nobservability, and limited or disturbed communications. We use a multi-robot\npatrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator\nand then compare its performance with prior coordination approaches. Results\ndemonstrate that MAGEC outperforms existing methods in several experiments\ninvolving agent attrition and communication disturbance, and provides\ncompetitive results in scenarios without such anomalies.\n","authors":["Anthony Goeckner","Yueyuan Sui","Nicolas Martinet","Xinliang Li","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13093v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13090v1","updated":"2024-03-19T18:38:50Z","published":"2024-03-19T18:38:50Z","title":"Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in\n  Robot Manipulators: A Self-Improving Online Training Framework","summary":"  The evolution and growing automation of collaborative robots introduce more\ncomplexity and unpredictability to systems, highlighting the crucial need for\nrobot's adaptability and flexibility to address the increasing complexities of\ntheir environment. In typical industrial production scenarios, robots are often\nrequired to be re-programmed when facing a more demanding task or even a few\nchanges in workspace conditions. To increase productivity, efficiency and\nreduce human effort in the design process, this paper explores the potential of\nusing digital twin combined with Reinforcement Learning (RL) to enable robots\nto generate self-improving collision-free trajectories in real time. The\ndigital twin, acting as a virtual counterpart of the physical system, serves as\na 'forward run' for monitoring, controlling, and optimizing the physical system\nin a safe and cost-effective manner. The physical system sends data to\nsynchronize the digital system through the video feeds from cameras, which\nallows the virtual robot to update its observation and policy based on real\nscenarios. The bidirectional communication between digital and physical systems\nprovides a promising platform for hardware-in-the-loop RL training through\ntrial and error until the robot successfully adapts to its new environment. The\nproposed online training framework is demonstrated on the Unfactory Xarm5\ncollaborative robot, where the robot end-effector aims to reach the target\nposition while avoiding obstacles. The experiment suggest that proposed\nframework is capable of performing policy online training, and that there\nremains significant room for improvement.\n","authors":["Yuzhu Sun","Mien Van","Stephen McIlvanna","Nguyen Minh Nhat","Kabirat Olayemi","Jack Close","Seán McLoone"],"pdf_url":"https://arxiv.org/pdf/2403.13090v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13085v1","updated":"2024-03-19T18:32:10Z","published":"2024-03-19T18:32:10Z","title":"Subgoal Diffuser: Coarse-to-fine Subgoal Generation to Guide Model\n  Predictive Control for Robot Manipulation","summary":"  Manipulation of articulated and deformable objects can be difficult due to\ntheir compliant and under-actuated nature. Unexpected disturbances can cause\nthe object to deviate from a predicted state, making it necessary to use\nModel-Predictive Control (MPC) methods to plan motion. However, these methods\nneed a short planning horizon to be practical. Thus, MPC is ill-suited for\nlong-horizon manipulation tasks due to local minima. In this paper, we present\na diffusion-based method that guides an MPC method to accomplish long-horizon\nmanipulation tasks by dynamically specifying sequences of subgoals for the MPC\nto follow. Our method, called Subgoal Diffuser, generates subgoals in a\ncoarse-to-fine manner, producing sparse subgoals when the task is easily\naccomplished by MPC and more dense subgoals when the MPC method needs more\nguidance. The density of subgoals is determined dynamically based on a learned\nestimate of reachability, and subgoals are distributed to focus on challenging\nparts of the task. We evaluate our method on two robot manipulation tasks and\nfind it improves the planning performance of an MPC method, and also\noutperforms prior diffusion-based methods.\n","authors":["Zixuan Huang","Yating Lin","Fan Yang","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2403.13085v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11788v2","updated":"2024-03-19T18:20:59Z","published":"2024-03-18T13:46:32Z","title":"Locomotion Generation for a Rat Robot based on Environmental Changes via\n  Reinforcement Learning","summary":"  This research focuses on developing reinforcement learning approaches for the\nlocomotion generation of small-size quadruped robots. The rat robot NeRmo is\nemployed as the experimental platform. Due to the constrained volume,\nsmall-size quadruped robots typically possess fewer and weaker sensors,\nresulting in difficulty in accurately perceiving and responding to\nenvironmental changes. In this context, insufficient and imprecise feedback\ndata from sensors makes it difficult to generate adaptive locomotion based on\nreinforcement learning. To overcome these challenges, this paper proposes a\nnovel reinforcement learning approach that focuses on extracting effective\nperceptual information to enhance the environmental adaptability of small-size\nquadruped robots. According to the frequency of a robot's gait stride, key\ninformation of sensor data is analyzed utilizing sinusoidal functions derived\nfrom Fourier transform results. Additionally, a multifunctional reward\nmechanism is proposed to generate adaptive locomotion in different tasks.\nExtensive simulations are conducted to assess the effectiveness of the proposed\nreinforcement learning approach in generating rat robot locomotion in various\nenvironments. The experiment results illustrate the capability of the proposed\napproach to maintain stable locomotion of a rat robot across different\nterrains, including ramps, stairs, and spiral stairs.\n","authors":["Xinhui Shan","Yuhong Huang","Zhenshan Bing","Zitao Zhang","Xiangtong Yao","Kai Huang","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.11788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13079v1","updated":"2024-03-19T18:15:35Z","published":"2024-03-19T18:15:35Z","title":"Current-Based Impedance Control for Interacting with Mobile Manipulators","summary":"  As robots shift from industrial to human-centered spaces, adopting mobile\nmanipulators, which expand workspace capabilities, becomes crucial. In these\nsettings, seamless interaction with humans necessitates compliant control. Two\ncommon methods for safe interaction, admittance, and impedance control, require\nforce or torque sensors, often absent in lower-cost or lightweight robots. This\npaper presents an adaption of impedance control that can be used on\ncurrent-controlled robots without the use of force or torque sensors and its\napplication for compliant control of a mobile manipulator. A calibration method\nis designed that enables estimation of the actuators' current/torque ratios and\nfrictions, used by the adapted impedance controller, and that can handle model\nerrors. The calibration method and the performance of the designed controller\nare experimentally validated using the Kinova GEN3 Lite arm. Results show that\nthe calibration method is consistent and that the designed controller for the\narm is compliant while also being able to track targets with five-millimeter\nprecision when no interaction is present. Additionally, this paper presents two\noperational modes for interacting with the mobile manipulator: one for guiding\nthe robot around the workspace through interacting with the arm and another for\nexecuting a tracking task, both maintaining compliance to external forces.\nThese operational modes were tested in real-world experiments, affirming their\npractical applicability and effectiveness.\n","authors":["Jelmer de Wolde","Luzia Knoedler","Gianluca Garofalo","Javier Alonso-Mora"],"pdf_url":"https://arxiv.org/pdf/2403.13079v1.pdf","comment":"8 pages, 13 figures, under review for IROS 2024"},{"id":"http://arxiv.org/abs/2310.00481v2","updated":"2024-03-19T18:05:18Z","published":"2023-09-30T20:26:00Z","title":"LANCAR: Leveraging Language for Context-Aware Robot Locomotion in\n  Unstructured Environments","summary":"  Navigating robots through unstructured terrains is challenging, primarily due\nto the dynamic environmental changes. While humans adeptly navigate such\nterrains by using context from their observations, creating a similar\ncontext-aware navigation system for robots is difficult. The essence of the\nissue lies in the acquisition and interpretation of contextual information, a\ntask complicated by the inherent ambiguity of human language. In this work, we\nintroduce LANCAR, which addresses this issue by combining a context translator\nwith reinforcement learning (RL) agents for context-aware locomotion. LANCAR\nallows robots to comprehend contextual information through Large Language\nModels (LLMs) sourced from human observers and convert this information into\nactionable contextual embeddings. These embeddings, combined with the robot's\nsensor data, provide a complete input for the RL agent's policy network. We\nprovide an extensive evaluation of LANCAR under different levels of contextual\nambiguity and compare with alternative methods. The experimental results\nshowcase the superior generalizability and adaptability across different\nterrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward\nover the best alternatives, highlighting its potential to enhance robotic\nnavigation in unstructured environments. More details and experiment videos\ncould be found in http://raaslab.org/projects/LLM_Context_Estimation/.\n","authors":["Chak Lam Shek","Xiyang Wu","Wesley A. Suttle","Carl Busart","Erin Zaroukian","Dinesh Manocha","Pratap Tokekar","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2310.00481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13042v1","updated":"2024-03-19T17:57:09Z","published":"2024-03-19T17:57:09Z","title":"TAPTR: Tracking Any Point with Transformers as Detection","summary":"  In this paper, we propose a simple and strong framework for Tracking Any\nPoint with TRansformers (TAPTR). Based on the observation that point tracking\nbears a great resemblance to object detection and tracking, we borrow designs\nfrom DETR-like algorithms to address the task of TAP. In the proposed\nframework, in each video frame, each tracking point is represented as a point\nquery, which consists of a positional part and a content part. As in DETR, each\nquery (its position and content feature) is naturally updated layer by layer.\nIts visibility is predicted by its updated content feature. Queries belonging\nto the same tracking point can exchange information through self-attention\nalong the temporal dimension. As all such operations are well-designed in\nDETR-like algorithms, the model is conceptually very simple. We also adopt some\nuseful designs such as cost volume from optical flow models and develop simple\ndesigns to provide long temporal information while mitigating the feature\ndrifting issue. Our framework demonstrates strong performance with\nstate-of-the-art performance on various TAP datasets with faster inference\nspeed.\n","authors":["Hongyang Li","Hao Zhang","Shilong Liu","Zhaoyang Zeng","Tianhe Ren","Feng Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13042v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.12965v1","updated":"2024-03-19T17:59:52Z","published":"2024-03-19T17:59:52Z","title":"Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence\n  Alignment","summary":"  This paper introduces a novel framework for virtual try-on, termed\nWear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable\nsolution. Besides generating high-fidelity results, our method supports users\nto precisely manipulate the wearing style. To achieve this goal, we first\nconstruct a strong pipeline for standard virtual try-on, supporting\nsingle/multiple garment try-on and model-to-model settings in complicated\nscenarios. To make it manipulable, we propose sparse correspondence alignment\nwhich involves point-based control to guide the generation for specific\nlocations. With this design, Wear-Any-Way gets state-of-the-art performance for\nthe standard setting and provides a novel interaction form for customizing the\nwearing style. For instance, it supports users to drag the sleeve to make it\nrolled up, drag the coat to make it open, and utilize clicks to control the\nstyle of tuck, etc. Wear-Any-Way enables more liberated and flexible\nexpressions of the attires, holding profound implications in the fashion\nindustry.\n","authors":["Mengting Chen","Xi Chen","Zhonghua Zhai","Chen Ju","Xuewen Hong","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.12965v1.pdf","comment":"Project Page: https://mengtingchen.github.io/wear-any-way-page/"},{"id":"http://arxiv.org/abs/2403.12966v1","updated":"2024-03-19T17:59:52Z","published":"2024-03-19T17:59:52Z","title":"Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language\n  Models","summary":"  In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot\n","authors":["Zuyan Liu","Yuhao Dong","Yongming Rao","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2403.12966v1.pdf","comment":"Project Page: https://sites.google.com/view/chain-of-spot/"},{"id":"http://arxiv.org/abs/2403.12964v1","updated":"2024-03-19T17:59:39Z","published":"2024-03-19T17:59:39Z","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language\n  Models","summary":"  Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2403.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12963v1","updated":"2024-03-19T17:59:33Z","published":"2024-03-19T17:59:33Z","title":"FouriScale: A Frequency Perspective on Training-Free High-Resolution\n  Image Synthesis","summary":"  In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale.\n","authors":["Linjiang Huang","Rongyao Fang","Aiping Zhang","Guanglu Song","Si Liu","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.12963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12962v1","updated":"2024-03-19T17:59:18Z","published":"2024-03-19T17:59:18Z","title":"FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation","summary":"  The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods.\n","authors":["Shuai Yang","Yifan Zhou","Ziwei Liu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.12962v1.pdf","comment":"CVPR 24, Code: https://github.com/williamyang1991/FRESCO, Project:\n  https://www.mmlab-ntu.com/project/fresco/"},{"id":"http://arxiv.org/abs/2403.12961v1","updated":"2024-03-19T17:59:09Z","published":"2024-03-19T17:59:09Z","title":"TexTile: A Differentiable Metric for Texture Tileability","summary":"  We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.\n","authors":["Carlos Rodriguez-Pardo","Dan Casas","Elena Garces","Jorge Lopez-Moreno"],"pdf_url":"https://arxiv.org/pdf/2403.12961v1.pdf","comment":"CVPR 2024. Project page: https://mslab.es/projects/TexTile/"},{"id":"http://arxiv.org/abs/2403.12960v1","updated":"2024-03-19T17:58:04Z","published":"2024-03-19T17:58:04Z","title":"FaceXFormer: A Unified Transformer for Facial Analysis","summary":"  In this work, we introduce FaceXformer, an end-to-end unified transformer\nmodel for a comprehensive range of facial analysis tasks such as face parsing,\nlandmark detection, head pose estimation, attributes recognition, and\nestimation of age, gender, race, and landmarks visibility. Conventional methods\nin face analysis have often relied on task-specific designs and preprocessing\ntechniques, which limit their approach to a unified architecture. Unlike these\nconventional methods, our FaceXformer leverages a transformer-based\nencoder-decoder architecture where each task is treated as a learnable token,\nenabling the integration of multiple tasks within a single framework. Moreover,\nwe propose a parameter-efficient decoder, FaceX, which jointly processes face\nand task tokens, thereby learning generalized and robust face representations\nacross different tasks. To the best of our knowledge, this is the first work to\npropose a single model capable of handling all these facial analysis tasks\nusing transformers. We conducted a comprehensive analysis of effective\nbackbones for unified face task processing and evaluated different task queries\nand the synergy between them. We conduct experiments against state-of-the-art\nspecialized models and previous multi-task models in both intra-dataset and\ncross-dataset evaluations across multiple benchmarks. Additionally, our model\neffectively handles images \"in-the-wild,\" demonstrating its robustness and\ngeneralizability across eight different tasks, all while maintaining the\nreal-time performance of 37 FPS.\n","authors":["Kartik Narayan","Vibashan VS","Rama Chellappa","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.12960v1.pdf","comment":"Project page: https://kartik-3004.github.io/facexformer_web/"},{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.12957v1","updated":"2024-03-19T17:57:52Z","published":"2024-03-19T17:57:52Z","title":"GVGEN: Text-to-3D Generation with Volumetric Representation","summary":"  In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed ($\\sim$7\nseconds), effectively striking a balance between quality and efficiency.\n","authors":["Xianglong He","Junyi Chen","Sida Peng","Di Huang","Yangguang Li","Xiaoshui Huang","Chun Yuan","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2403.12957v1.pdf","comment":"project page: https://gvgen.github.io/"},{"id":"http://arxiv.org/abs/2403.12953v1","updated":"2024-03-19T17:55:22Z","published":"2024-03-19T17:55:22Z","title":"FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation","summary":"  In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models\n","authors":["Rajeev Yasarla","Manish Kumar Singh","Hong Cai","Yunxiao Shi","Jisoo Jeong","Yinhao Zhu","Shizhong Han","Risheek Garrepalli","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2403.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12952v1","updated":"2024-03-19T17:54:34Z","published":"2024-03-19T17:54:34Z","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models","summary":"  Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.\n","authors":["Elaine Sui","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.12952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14918v2","updated":"2024-03-19T17:53:39Z","published":"2023-11-25T03:33:36Z","title":"Resolution- and Stimulus-agnostic Super-Resolution of Ultra-High-Field\n  Functional MRI: Application to Visual Studies","summary":"  High-resolution fMRI provides a window into the brain's mesoscale\norganization. Yet, higher spatial resolution increases scan times, to\ncompensate for the low signal and contrast-to-noise ratio. This work introduces\na deep learning-based 3D super-resolution (SR) method for fMRI. By\nincorporating a resolution-agnostic image augmentation framework, our method\nadapts to varying voxel sizes without retraining. We apply this innovative\ntechnique to localize fine-scale motion-selective sites in the early visual\nareas. Detection of these sites typically requires a resolution higher than 1\nmm isotropic, whereas here, we visualize them based on lower resolution (2-3mm\nisotropic) fMRI data. Remarkably, the super-resolved fMRI is able to recover\nhigh-frequency detail of the interdigitated organization of these sites\n(relative to the color-selective sites), even with training data sourced from\ndifferent subjects and experimental paradigms -- including non-visual\nresting-state fMRI, underscoring its robustness and versatility. Quantitative\nand qualitative results indicate that our method has the potential to enhance\nthe spatial resolution of fMRI, leading to a drastic reduction in acquisition\ntime.\n","authors":["Hongwei Bran Li","Matthew S. Rosen","Shahin Nasr","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2311.14918v2.pdf","comment":"ISBI2024 final version"},{"id":"http://arxiv.org/abs/2311.10081v2","updated":"2024-03-19T17:51:45Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v2.pdf","comment":"CVPR 2024. The feedback datasets are released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2402.03908v2","updated":"2024-03-19T17:41:04Z","published":"2024-02-06T11:21:58Z","title":"EscherNet: A Generative Model for Scalable View Synthesis","summary":"  We introduce EscherNet, a multi-view conditioned diffusion model for view\nsynthesis. EscherNet learns implicit and generative 3D representations coupled\nwith a specialised camera positional encoding, allowing precise and continuous\nrelative control of the camera transformation between an arbitrary number of\nreference and target views. EscherNet offers exceptional generality,\nflexibility, and scalability in view synthesis -- it can generate more than 100\nconsistent target views simultaneously on a single consumer-grade GPU, despite\nbeing trained with a fixed number of 3 reference views to 3 target views. As a\nresult, EscherNet not only addresses zero-shot novel view synthesis, but also\nnaturally unifies single- and multi-image 3D reconstruction, combining these\ndiverse tasks into a single, cohesive framework. Our extensive experiments\ndemonstrate that EscherNet achieves state-of-the-art performance in multiple\nbenchmarks, even when compared to methods specifically tailored for each\nindividual problem. This remarkable versatility opens up new directions for\ndesigning scalable neural architectures for 3D vision. Project page:\nhttps://kxhit.github.io/EscherNet.\n","authors":["Xin Kong","Shikun Liu","Xiaoyang Lyu","Marwan Taher","Xiaojuan Qi","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2402.03908v2.pdf","comment":"CVPR2024 Project Page: https://kxhit.github.io/EscherNet"},{"id":"http://arxiv.org/abs/2403.12935v1","updated":"2024-03-19T17:37:18Z","published":"2024-03-19T17:37:18Z","title":"Segment Anything for comprehensive analysis of grapevine cluster\n  architecture and berry properties","summary":"  Grape cluster architecture and compactness are complex traits influencing\ndisease susceptibility, fruit quality, and yield. Evaluation methods for these\ntraits include visual scoring, manual methodologies, and computer vision, with\nthe latter being the most scalable approach. Most of the existing computer\nvision approaches for processing cluster images often rely on conventional\nsegmentation or machine learning with extensive training and limited\ngeneralization. The Segment Anything Model (SAM), a novel foundation model\ntrained on a massive image dataset, enables automated object segmentation\nwithout additional training. This study demonstrates out-of-the-box SAM's high\naccuracy in identifying individual berries in 2D cluster images. Using this\nmodel, we managed to segment approximately 3,500 cluster images, generating\nover 150,000 berry masks, each linked with spatial coordinates within their\nclusters. The correlation between human-identified berries and SAM predictions\nwas very strong (Pearson r2=0.96). Although the visible berry count in images\ntypically underestimates the actual cluster berry count due to visibility\nissues, we demonstrated that this discrepancy could be adjusted using a linear\nregression model (adjusted R2=0.87). We emphasized the critical importance of\nthe angle at which the cluster is imaged, noting its substantial effect on\nberry counts and architecture. We proposed different approaches in which berry\nlocation information facilitated the calculation of complex features related to\ncluster architecture and compactness. Finally, we discussed SAM's potential\nintegration into currently available pipelines for image generation and\nprocessing in vineyard conditions.\n","authors":["Efrain Torres-Lomas","Jimena Lado-Jimena","Guillermo Garcia-Zamora","Luis Diaz-Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.12935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12933v1","updated":"2024-03-19T17:36:28Z","published":"2024-03-19T17:36:28Z","title":"Zero-Reference Low-Light Enhancement via Physical Quadruple Priors","summary":"  Understanding illumination and reducing the need for supervision pose a\nsignificant challenge in low-light enhancement. Current approaches are highly\nsensitive to data usage during training and illumination-specific\nhyper-parameters, limiting their ability to handle unseen scenarios. In this\npaper, we propose a new zero-reference low-light enhancement framework\ntrainable solely with normal light images. To accomplish this, we devise an\nillumination-invariant prior inspired by the theory of physical light transfer.\nThis prior serves as the bridge between normal and low-light images. Then, we\ndevelop a prior-to-image framework trained without low-light data. During\ntesting, this framework is able to restore our illumination-invariant prior\nback to images, automatically achieving low-light enhancement. Within this\nframework, we leverage a pretrained generative diffusion model for model\nability, introduce a bypass decoder to handle detail distortion, as well as\noffer a lightweight version for practicality. Extensive experiments demonstrate\nour framework's superiority in various scenarios as well as good\ninterpretability, robustness, and efficiency. Code is available on our project\nhomepage: http://daooshee.github.io/QuadPrior-Website/\n","authors":["Wenjing Wang","Huan Yang","Jianlong Fu","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12933v1.pdf","comment":"Accepted by CVPR-2024"},{"id":"http://arxiv.org/abs/2306.02960v2","updated":"2024-03-19T17:35:51Z","published":"2023-06-05T15:26:02Z","title":"Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical\n  Flow Estimation","summary":"  In the field of robotics, event-based cameras are emerging as a promising\nlow-power alternative to traditional frame-based cameras for capturing\nhigh-speed motion and high dynamic range scenes. This is due to their sparse\nand asynchronous event outputs. Spiking Neural Networks (SNNs) with their\nasynchronous event-driven compute, show great potential for extracting the\nspatio-temporal features from these event streams. In contrast, the standard\nAnalog Neural Networks (ANNs) fail to process event data effectively. However,\ntraining SNNs is difficult due to additional trainable parameters (thresholds\nand leaks), vanishing spikes at deeper layers, and a non-differentiable binary\nactivation function. Furthermore, an additional data structure, membrane\npotential, responsible for keeping track of temporal information, must be\nfetched and updated at every timestep in SNNs. To overcome these challenges, we\npropose a novel SNN-ANN hybrid architecture that combines the strengths of\nboth. Specifically, we leverage the asynchronous compute capabilities of SNN\nlayers to effectively extract the input temporal information. Concurrently, the\nANN layers facilitate training and efficient hardware deployment on traditional\nmachine learning hardware such as GPUs. We provide extensive experimental\nanalysis for assigning each layer to be spiking or analog, leading to a network\nconfiguration optimized for performance and ease of training. We evaluate our\nhybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle\nStereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid\nSNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)\nwith 22% lower energy consumption compared to Full-SNN, and 48% lower AEE\ncompared to Full-ANN, while maintaining comparable energy usage.\n","authors":["Shubham Negi","Deepika Sharma","Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2306.02960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12931v1","updated":"2024-03-19T17:34:27Z","published":"2024-03-19T17:34:27Z","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by\n  Self-Cooperative Diffusion GANs","summary":"  We introduce YOSO, a novel generative model designed for rapid, scalable, and\nhigh-fidelity one-step image synthesis. This is achieved by integrating the\ndiffusion process with GANs. Specifically, we smooth the distribution by the\ndenoising generator itself, performing self-cooperative learning. We show that\nour method can serve as a one-step generation model training from scratch with\ncompetitive performance. Moreover, we show that our method can be extended to\nfinetune pre-trained text-to-image diffusion for high-quality one-step\ntext-to-image synthesis even with LoRA fine-tuning. In particular, we provide\nthe first diffusion transformer that can generate images in one step trained on\n512 resolution, with the capability of adapting to 1024 resolution without\nexplicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.\n","authors":["Yihong Luo","Xiaolong Chen","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12931v1.pdf","comment":"Early version"},{"id":"http://arxiv.org/abs/2403.12922v1","updated":"2024-03-19T17:27:55Z","published":"2024-03-19T17:27:55Z","title":"Contextual AD Narration with Interleaved Multimodal Sequence","summary":"  The Audio Description (AD) task aims to generate descriptions of visual\nelements for visually impaired individuals to help them access long-form video\ncontents, like movie. With video feature, text, character bank and context\ninformation as inputs, the generated ADs are able to correspond to the\ncharacters by name and provide reasonable, contextual descriptions to help\naudience understand the storyline of movie. To achieve this goal, we propose to\nleverage pre-trained foundation models through a simple and unified framework\nto generate ADs with interleaved multimodal sequence as input, termed as\nUni-AD. To enhance the alignment of features across various modalities with\nfiner granularity, we introduce a simple and lightweight module that maps video\nfeatures into the textual feature space. Moreover, we also propose a\ncharacter-refinement module to provide more precise information by identifying\nthe main characters who play more significant role in the video context. With\nthese unique designs, we further incorporate contextual information and a\ncontrastive loss into our architecture to generate more smooth and contextual\nADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve\nstate-of-the-art performance on AD generation, which demonstrates the\neffectiveness of our approach. Code will be available at\nhttps://github.com/MCG-NJU/Uni-AD.\n","authors":["Hanlin Wang","Zhan Tong","Kecheng Zheng","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12920v1","updated":"2024-03-19T17:23:44Z","published":"2024-03-19T17:23:44Z","title":"Semantic Layering in Room Segmentation via LLMs","summary":"  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs\n(SeLRoS), an advanced method for semantic room segmentation by integrating\nLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlike\nprevious approaches that solely focus on the geometric segmentation of indoor\nenvironments, our work enriches segmented maps with semantic data, including\nobject identification and spatial relationships, to enhance robotic navigation.\nBy leveraging LLMs, we provide a novel framework that interprets and organizes\ncomplex information about each segmented area, thereby improving the accuracy\nand contextual relevance of room segmentation. Furthermore, SeLRoS overcomes\nthe limitations of existing algorithms by using a semantic evaluation method to\naccurately distinguish true room divisions from those erroneously generated by\nfurniture and segmentation inaccuracies. The effectiveness of SeLRoS is\nverified through its application across 30 different 3D environments. Source\ncode and experiment videos for this work are available at:\nhttps://sites.google.com/view/selros.\n","authors":["Taehyeon Kim","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2403.12920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11942v2","updated":"2024-03-19T17:20:59Z","published":"2024-03-18T16:36:54Z","title":"Exploring Facial Expression Recognition through Semi-Supervised\n  Pretraining and Temporal Modeling","summary":"  Facial Expression Recognition (FER) plays a crucial role in computer vision\nand finds extensive applications across various fields. This paper aims to\npresent our approach for the upcoming 6th Affective Behavior Analysis\nin-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial\nexpression recognition task, The limited size of the FER dataset poses a\nchallenge to the expression recognition model's generalization ability,\nresulting in subpar recognition performance. To address this problem, we employ\na semi-supervised learning technique to generate expression category\npseudo-labels for unlabeled face data. At the same time, we uniformly sampled\nthe labeled facial expression samples and implemented a debiased feedback\nlearning strategy to address the problem of category imbalance in the dataset\nand the possible data bias in semi-supervised learning. Moreover, to further\ncompensate for the limitation and bias of features obtained only from static\nimages, we introduced a Temporal Encoder to learn and capture temporal\nrelationships between neighbouring expression image features. In the 6th ABAW\ncompetition, our method achieved outstanding results on the official validation\nset, a result that fully confirms the effectiveness and competitiveness of our\nproposed method.\n","authors":["Jun Yu","Zhihong Wei","Zhongpeng Cai","Gongpeng Zhao","Zerui Zhang","Yongqi Wang","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.11942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15619v2","updated":"2024-03-19T17:17:50Z","published":"2023-11-27T08:32:28Z","title":"Align before Adapt: Leveraging Entity-to-Region Alignments for\n  Generalizable Video Action Recognition","summary":"  Large-scale visual-language pre-trained models have achieved significant\nsuccess in various video tasks. However, most existing methods follow an \"adapt\nthen align\" paradigm, which adapts pre-trained image encoders to model\nvideo-level representations and utilizes one-hot or text embedding of the\naction labels for supervision. This paradigm overlooks the challenge of mapping\nfrom static images to complicated activity concepts. In this paper, we propose\na novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video\nrepresentation learning, we exploit the entity-to-region alignments for each\nframe. The alignments are fulfilled by matching the region-aware image\nembeddings to an offline-constructed text corpus. With the aligned entities, we\nfeed their text embeddings to a transformer-based video adapter as the queries,\nwhich can help extract the semantics of the most important entities from a\nvideo to a vector. This paradigm reuses the visual-language alignment of VLP\nduring adaptation and tries to explain an action by the underlying entities.\nThis helps understand actions by bridging the gap with complex activity\nsemantics, particularly when facing unfamiliar or unseen categories. ALT\ndemonstrates competitive performance while maintaining remarkably low\ncomputational costs. In fully supervised experiments, it achieves 88.1% top-1\naccuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the\nprevious state-of-the-art methods in both zero-shot and few-shot experiments,\nemphasizing its superior generalizability across various learning scenarios.\n","authors":["Yifei Chen","Dapeng Chen","Ruijin Liu","Sai Zhou","Wenyuan Xue","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2311.15619v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12915v1","updated":"2024-03-19T17:12:58Z","published":"2024-03-19T17:12:58Z","title":"Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model","summary":"  We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed\nfor ultra-high-resolution image synthesis. PDM utilizes a pyramid latent\nrepresentation, providing a broader design space that enables more flexible,\nstructured, and efficient perceptual compression which enable AutoEncoder and\nNetwork of Diffusion to equip branches and deeper layers. To enhance PDM's\ncapabilities for generative tasks, we propose the integration of\nSpatial-Channel Attention and Res-Skip Connection, along with the utilization\nof Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and\nAutoEncoder. In summary, PDM achieves the synthesis of images with a 2K\nresolution for the first time, demonstrated on two new datasets comprising\nimages of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believe\nthat this work offers an alternative approach to designing scalable image\ngenerative models, while also providing incremental reinforcement for existing\nframeworks.\n","authors":["Jiajie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12915v1.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2401.07931v2","updated":"2024-03-19T17:07:40Z","published":"2024-01-15T19:47:14Z","title":"Vertical Federated Image Segmentation","summary":"  With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.\n","authors":["Paul K. Mandal","Cole Leo"],"pdf_url":"https://arxiv.org/pdf/2401.07931v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.11232v2","updated":"2024-03-19T17:05:57Z","published":"2023-12-18T14:30:54Z","title":"Self-Supervised Learning for Image Super-Resolution and Deblurring","summary":"  Self-supervised methods have recently proved to be nearly as effective as\nsupervised methods in various imaging inverse problems, paving the way for\nlearning-based methods in scientific and medical imaging applications where\nground truth data is hard or expensive to obtain. This is the case in magnetic\nresonance imaging and computed tomography. These methods critically rely on\ninvariance to translations and/or rotations of the image distribution to learn\nfrom incomplete measurement data alone. However, existing approaches fail to\nobtain competitive performances in the problems of image super-resolution and\ndeblurring, which play a key role in most imaging systems. In this work, we\nshow that invariance to translations and rotations is insufficient to learn\nfrom measurements that only contain low-frequency information. Instead, we\npropose a new self-supervised approach that leverages the fact that many image\ndistributions are approximately scale-invariant, and that enables recovering\nhigh-frequency information lost in the measurement process. We demonstrate\nthroughout a series of experiments on real datasets that the proposed method\noutperforms other self-supervised approaches, and obtains performances on par\nwith fully supervised learning.\n","authors":["Jérémy Scanvic","Mike Davies","Patrice Abry","Julián Tachella"],"pdf_url":"https://arxiv.org/pdf/2312.11232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11492v2","updated":"2024-03-19T17:04:35Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v2.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12906v1","updated":"2024-03-19T17:02:07Z","published":"2024-03-19T17:02:07Z","title":"TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation","summary":"  Texturing 3D humans with semantic UV maps remains a challenge due to the\ndifficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D\nadvancements in supervising multi-view renderings using large text-to-image\n(T2I) models, issues persist with generation speed, text consistency, and\ntexture quality, resulting in data scarcity among existing datasets. We present\nTexDreamer, the first zero-shot multimodal high-fidelity 3D human texture\ngeneration model. Utilizing an efficient texture adaptation finetuning\nstrategy, we adapt large T2I model to a semantic UV structure while preserving\nits original generalization capability. Leveraging a novel feature translator\nmodule, the trained model is capable of generating high-fidelity 3D human\ntextures from either text or image within seconds. Furthermore, we introduce\nArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024)\n3D human texture dataset which contains 50k high-fidelity textures with text\ndescriptions.\n","authors":["Yufei Liu","Junwei Zhu","Junshu Tang","Shijie Zhang","Jiangning Zhang","Weijian Cao","Chengjie Wang","Yunsheng Wu","Dongjin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12906v1.pdf","comment":"Project Page: https://ggxxii.github.io/texdreamer/"},{"id":"http://arxiv.org/abs/2401.00420v2","updated":"2024-03-19T16:56:53Z","published":"2023-12-31T08:06:53Z","title":"SynCDR : Training Cross Domain Retrieval Models with Synthetic Data","summary":"  In cross-domain retrieval, a model is required to identify images from the\nsame semantic category across two visual domains. For instance, given a sketch\nof an object, a model needs to retrieve a real image of it from an online\nstore's catalog. A standard approach for such a problem is learning a feature\nspace of images where Euclidean distances reflect similarity. Even without\nhuman annotations, which may be expensive to acquire, prior methods function\nreasonably well using unlabeled images for training. Our problem constraint\ntakes this further to scenarios where the two domains do not necessarily share\nany common categories in training data. This can occur when the two domains in\nquestion come from different versions of some biometric sensor recording\nidentities of different people. We posit a simple solution, which is to\ngenerate synthetic data to fill in these missing category examples across\ndomains. This, we do via category preserving translation of images from one\nvisual domain to another. We compare approaches specifically trained for this\ntranslation for a pair of domains, as well as those that can use large-scale\npre-trained text-to-image diffusion models via prompts, and find that the\nlatter can generate better replacement synthetic data, leading to more accurate\ncross-domain retrieval models. Our best SynCDR model can outperform prior art\nby up to 15\\%. Code for our work is available at\nhttps://github.com/samarth4149/SynCDR .\n","authors":["Samarth Mishra","Carlos D. Castillo","Hongcheng Wang","Kate Saenko","Venkatesh Saligrama"],"pdf_url":"https://arxiv.org/pdf/2401.00420v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2403.12895v1","updated":"2024-03-19T16:48:40Z","published":"2024-03-19T16:48:40Z","title":"mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document\n  Understanding","summary":"  Structure information is critical for understanding the semantics of\ntext-rich images, such as documents, tables, and charts. Existing Multimodal\nLarge Language Models (MLLMs) for Visual Document Understanding are equipped\nwith text recognition ability but lack general structure understanding\nabilities for text-rich document images. In this work, we emphasize the\nimportance of structure information in Visual Document Understanding and\npropose the Unified Structure Learning to boost the performance of MLLMs. Our\nUnified Structure Learning comprises structure-aware parsing tasks and\nmulti-grained text localization tasks across 5 domains: document, webpage,\ntable, chart, and natural image. To better encode structure information, we\ndesign a simple and effective vision-to-text module H-Reducer, which can not\nonly maintain the layout information but also reduce the length of visual\nfeatures by merging horizontal adjacent patches through convolution, enabling\nthe LLM to understand high-resolution images more efficiently. Furthermore, by\nconstructing structure-aware text sequences and multi-grained pairs of texts\nand bounding boxes for publicly available text-rich images, we build a\ncomprehensive training set DocStruct4M to support structure learning. Finally,\nwe construct a small but high-quality reasoning tuning dataset DocReason25K to\ntrigger the detailed explanation ability in the document domain. Our model\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document\nunderstanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM\nby more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are\npublicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.\n","authors":["Anwen Hu","Haiyang Xu","Jiabo Ye","Ming Yan","Liang Zhang","Bo Zhang","Chen Li","Ji Zhang","Qin Jin","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12895v1.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.12894v1","updated":"2024-03-19T16:46:29Z","published":"2024-03-19T16:46:29Z","title":"MEDBind: Unifying Language and Multimodal Medical Data Embeddings","summary":"  Medical vision-language pretraining models (VLPM) have achieved remarkable\nprogress in fusing chest X-rays (CXR) with clinical texts, introducing\nimage-text data binding approaches that enable zero-shot learning and\ndownstream clinical tasks. However, the current landscape lacks the holistic\nintegration of additional medical modalities, such as electrocardiograms (ECG).\nWe present MEDBind (Medical Electronic patient recorD), which learns joint\nembeddings across CXR, ECG, and medical text. Using text data as the central\nanchor, MEDBind features tri-modality binding, delivering competitive\nperformance in top-K retrieval, zero-shot, and few-shot benchmarks against\nestablished VLPM, and the ability for CXR-to-ECG zero-shot classification and\nretrieval. This seamless integration is achieved through combination of\ncontrastive loss on modality-text pairs with our proposed contrastive loss\nfunction, Edge-Modality Contrastive Loss, fostering a cohesive embedding space\nfor CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve\ndownstream tasks by directly integrating CXR and ECG embeddings into a\nlarge-language model for multimodal prompt tuning.\n","authors":["Yuan Gao","Sangwook Kim","David E Austin","Chris McIntosh"],"pdf_url":"https://arxiv.org/pdf/2403.12894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12032v2","updated":"2024-03-19T16:45:22Z","published":"2024-03-18T17:59:09Z","title":"Generic 3D Diffusion Adapter Using Controlled Multi-View Editing","summary":"  Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.\n","authors":["Hansheng Chen","Ruoxi Shi","Yulin Liu","Bokui Shen","Jiayuan Gu","Gordon Wetzstein","Hao Su","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2403.12032v2.pdf","comment":"V2 note: Fix missing acknowledgements. Project page:\n  https://lakonik.github.io/mvedit"},{"id":"http://arxiv.org/abs/2403.12891v1","updated":"2024-03-19T16:40:57Z","published":"2024-03-19T16:40:57Z","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across\n  Varied Bowl Configurations and Food Types","summary":"  In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.\n","authors":["Rui Liu","Amisha Bhaskar","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01838v2","updated":"2024-03-19T16:40:25Z","published":"2023-04-04T14:44:06Z","title":"BugNIST - a Large Volumetric Dataset for Object Detection under Domain\n  Shift","summary":"  Domain shift significantly influences the performance of deep learning\nalgorithms, particularly for object detection within volumetric 3D images.\nAnnotated training data is essential for deep learning-based object detection.\nHowever, annotating densely packed objects is time-consuming and costly.\nInstead, we suggest training models on individually scanned objects, causing a\ndomain shift between training and detection data. To address this challenge, we\nintroduce the BugNIST dataset, comprising 9154 micro-CT volumes of 12 bug types\nand 388 volumes of tightly packed bug mixtures. This dataset is characterized\nby having objects with the same appearance in the source and target domain,\nwhich is uncommon for other benchmark datasets for domain shift. During\ntraining, individual bug volumes labeled by class are utilized, while testing\nemploys mixtures with center point annotations and bug type labels. Together\nwith the dataset, we provide a baseline detection analysis, aiming at advancing\nthe field of 3D object detection methods.\n","authors":["Patrick Møller Jensen","Vedrana Andersen Dahl","Carsten Gundlach","Rebecca Engberg","Hans Martin Kjer","Anders Bjorholm Dahl"],"pdf_url":"https://arxiv.org/pdf/2304.01838v2.pdf","comment":"20 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.09611v2","updated":"2024-03-19T16:37:13Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03094v2","updated":"2024-03-19T16:34:28Z","published":"2024-02-05T15:25:32Z","title":"Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object\n  Detector","summary":"  This paper studies the challenging cross-domain few-shot object detection\n(CD-FSOD), aiming to develop an accurate object detector for novel domains with\nminimal labeled examples. While transformer-based open-set detectors, such as\nDE-ViT, show promise in traditional few-shot object detection, their\ngeneralization to CD-FSOD remains unclear: 1) can such open-set detection\nmethods easily generalize to CD-FSOD? 2) If not, how can models be enhanced\nwhen facing huge domain gaps? To answer the first question, we employ measures\nincluding style, inter-class variance (ICV), and indefinable boundaries (IB) to\nunderstand the domain gap. Based on these measures, we establish a new\nbenchmark named CD-FSOD to evaluate object detection methods, revealing that\nmost of the current approaches fail to generalize across domains. Technically,\nwe observe that the performance decline is associated with our proposed\nmeasures: style, ICV, and IB. Consequently, we propose several novel modules to\naddress these issues. First, the learnable instance features align initial\nfixed instances with target categories, enhancing feature distinctiveness.\nSecond, the instance reweighting module assigns higher importance to\nhigh-quality instances with slight IB. Third, the domain prompter encourages\nfeatures resilient to different styles by synthesizing imaginary domains\nwithout altering semantic contents. These techniques collectively contribute to\nthe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),\nsignificantly improving upon the base DE-ViT. Experimental results validate the\nefficacy of our model. All datasets, codes, and models will be released to the\ncommunity.\n","authors":["Yuqian Fu","Yu Wang","Yixuan Pan","Lian Huai","Xingyu Qiu","Zeyu Shangguan","Tong Liu","Yanwei Fu","Luc Van Gool","Xingqun Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.03094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12886v1","updated":"2024-03-19T16:33:26Z","published":"2024-03-19T16:33:26Z","title":"EmoVOCA: Speech-Driven Emotional 3D Talking Heads","summary":"  The domain of 3D talking head generation has witnessed significant progress\nin recent years. A notable challenge in this field consists in blending\nspeech-related motions with expression dynamics, which is primarily caused by\nthe lack of comprehensive 3D datasets that combine diversity in spoken\nsentences with a variety of facial expressions. Whereas literature works\nattempted to exploit 2D video data and parametric 3D models as a workaround,\nthese still show limitations when jointly modeling the two motions. In this\nwork, we address this problem from a different perspective, and propose an\ninnovative data-driven technique that we used for creating a synthetic dataset,\ncalled EmoVOCA, obtained by combining a collection of inexpressive 3D talking\nheads and a set of 3D expressive sequences. To demonstrate the advantages of\nthis approach, and the quality of the dataset, we then designed and trained an\nemotional 3D talking head generator that accepts a 3D face, an audio file, an\nemotion label, and an intensity value as inputs, and learns to animate the\naudio-synchronized lip movements with expressive traits of the face.\nComprehensive experiments, both quantitative and qualitative, using our data\nand generator evidence superior ability in synthesizing convincing animations,\nwhen compared with the best performing methods in the literature. Our code and\npre-trained model will be made available.\n","authors":["Federico Nocentini","Claudio Ferrari","Stefano Berretti"],"pdf_url":"https://arxiv.org/pdf/2403.12886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12884v1","updated":"2024-03-19T16:31:30Z","published":"2024-03-19T16:31:30Z","title":"HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning","summary":"  Recent advances in visual reasoning (VR), particularly with the aid of Large\nVision-Language Models (VLMs), show promise but require access to large-scale\ndatasets and face challenges such as high computational costs and limited\ngeneralization capabilities. Compositional visual reasoning approaches have\nemerged as effective strategies; however, they heavily rely on the commonsense\nknowledge encoded in Large Language Models (LLMs) to perform planning,\nreasoning, or both, without considering the effect of their decisions on the\nvisual reasoning process, which can lead to errors or failed procedures. To\naddress these challenges, we introduce HYDRA, a multi-stage dynamic\ncompositional visual reasoning framework designed for reliable and\nincrementally progressive general reasoning. HYDRA integrates three essential\nmodules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive\ncontroller, and a reasoner. The planner and reasoner modules utilize an LLM to\ngenerate instruction samples and executable code from the selected instruction,\nrespectively, while the RL agent dynamically interacts with these modules,\nmaking high-level decisions on selection of the best instruction sample given\ninformation from the historical state stored through a feedback loop. This\nadaptable design enables HYDRA to adjust its actions based on previous feedback\nreceived during the reasoning process, leading to more reliable reasoning\noutputs and ultimately enhancing its overall effectiveness. Our framework\ndemonstrates state-of-the-art performance in various VR tasks on four different\nwidely-used datasets.\n","authors":["Fucai Ke","Zhixi Cai","Simindokht Jahangard","Weiqing Wang","Pari Delir Haghighi","Hamid Rezatofighi"],"pdf_url":"https://arxiv.org/pdf/2403.12884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12883v1","updated":"2024-03-19T16:29:59Z","published":"2024-03-19T16:29:59Z","title":"Confusing Pair Correction Based on Category Prototype for Domain\n  Adaptation under Noisy Environments","summary":"  In this paper, we address unsupervised domain adaptation under noisy\nenvironments, which is more challenging and practical than traditional domain\nadaptation. In this scenario, the model is prone to overfitting noisy labels,\nresulting in a more pronounced domain shift and a notable decline in the\noverall model performance. Previous methods employed prototype methods for\ndomain adaptation on robust feature spaces. However, these approaches struggle\nto effectively classify classes with similar features under noisy environments.\nTo address this issue, we propose a new method to detect and correct confusing\nclass pair. We first divide classes into easy and hard classes based on the\nsmall loss criterion. We then leverage the top-2 predictions for each sample\nafter aligning the source and target domain to find the confusing pair in the\nhard classes. We apply label correction to the noisy samples within the\nconfusing pair. With the proposed label correction method, we can train our\nmodel with more accurate labels. Extensive experiments confirm the\neffectiveness of our method and demonstrate its favorable performance compared\nwith existing state-of-the-art methods. Our codes are publicly available at\nhttps://github.com/Hehxcf/CPC/.\n","authors":["Churan Zhi","Junbao Zhuo","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12883v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2403.12870v1","updated":"2024-03-19T16:15:08Z","published":"2024-03-19T16:15:08Z","title":"PoNQ: a Neural QEM-based Mesh Representation","summary":"  Although polygon meshes have been a standard representation in geometry\nprocessing, their irregular and combinatorial nature hinders their suitability\nfor learning-based applications. In this work, we introduce a novel learnable\nmesh representation through a set of local 3D sample Points and their\nassociated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,\nwhich we denote PoNQ. A global mesh is directly derived from PoNQ by\nefficiently leveraging the knowledge of the local quadric errors. Besides\nmarking the first use of QEM within a neural shape representation, our\ncontribution guarantees both topological and geometrical properties by ensuring\nthat a PoNQ mesh does not self-intersect and is always the boundary of a\nvolume. Notably, our representation does not rely on a regular grid, is\nsupervised directly by the target surface alone, and also handles open surfaces\nwith boundaries and/or sharp features. We demonstrate the efficacy of PoNQ\nthrough a learning-based mesh prediction from SDF grids and show that our\nmethod surpasses recent state-of-the-art techniques in terms of both surface\nand edge-based metrics.\n","authors":["Nissim Maruani","Maks Ovsjanikov","Pierre Alliez","Mathieu Desbrun"],"pdf_url":"https://arxiv.org/pdf/2403.12870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18451v2","updated":"2024-03-19T16:09:37Z","published":"2024-02-28T16:24:08Z","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation","summary":"  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yinzhe Wu","Yang Nan","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03799v2","updated":"2024-03-19T16:08:37Z","published":"2023-12-06T14:58:03Z","title":"Low-power, Continuous Remote Behavioral Localization with Event Cameras","summary":"  Researchers in natural science need reliable methods for quantifying animal\nbehavior. Recently, numerous computer vision methods emerged to automate the\nprocess. However, observing wild species at remote locations remains a\nchallenging task due to difficult lighting conditions and constraints on power\nsupply and data storage. Event cameras offer unique advantages for\nbattery-dependent remote monitoring due to their low power consumption and high\ndynamic range capabilities. We use this novel sensor to quantify a behavior in\nChinstrap penguins called ecstatic display. We formulate the problem as a\ntemporal action detection task, determining the start and end times of the\nbehavior. For this purpose, we recorded a colony of breeding penguins in\nAntarctica for several weeks and labeled event data on 16 nests. The developed\nmethod consists of a generator of candidate time intervals (proposals) and a\nclassifier of the actions within them. The experiments show that the event\ncameras' natural response to motion is effective for continuous behavior\nmonitoring and detection, reaching a mean average precision (mAP) of 58% (which\nincreases to 63% in good weather conditions). The results also demonstrate the\nrobustness against various lighting conditions contained in the challenging\ndataset. The low-power capabilities of the event camera allow it to record\nsignificantly longer than with a conventional camera. This work pioneers the\nuse of event cameras for remote wildlife observation, opening new\ninterdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/\n","authors":["Friedhelm Hamann","Suman Ghosh","Ignacio Juarez Martinez","Tom Hart","Alex Kacelnik","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2312.03799v2.pdf","comment":"13 pages, 8 figures, 12 tables, Project page:\n  https://tub-rip.github.io/eventpenguins/"},{"id":"http://arxiv.org/abs/2203.07738v4","updated":"2024-03-19T16:03:09Z","published":"2022-03-15T09:13:35Z","title":"GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning","summary":"  Few-shot learning (FSL), purposing to resolve the problem of data-scarce, has\nattracted considerable attention in recent years. A popular FSL framework\ncontains two phases: (i) the pre-train phase employs the base data to train a\nCNN-based feature extractor. (ii) the meta-test phase applies the frozen\nfeature extractor to novel data (novel data has different categories from base\ndata) and designs a classifier for recognition. To correct few-shot data\ndistribution, researchers propose Semi-Supervised Few-Shot Learning (SSFSL) by\nintroducing unlabeled data. Although SSFSL has been proved to achieve\noutstanding performances in the FSL community, there still exists a fundamental\nproblem: the pre-trained feature extractor can not adapt to the novel data\nflawlessly due to the cross-category setting. Usually, large amounts of noises\nare introduced to the novel feature. We dub it as Feature-Extractor-Maladaptive\n(FEM) problem. To tackle FEM, we make two efforts in this paper. First, we\npropose a novel label prediction method, Isolated Graph Learning (IGL). IGL\nintroduces the Laplacian operator to encode the raw data to graph space, which\nhelps reduce the dependence on features when classifying, and then project\ngraph representation to label space for prediction. The key point is that: IGL\ncan weaken the negative influence of noise from the feature representation\nperspective, and is also flexible to independently complete training and\ntesting procedures, which is suitable for SSFSL. Second, we propose Graph\nCo-Training (GCT) to tackle this challenge from a multi-modal fusion\nperspective by extending the proposed IGL to the co-training framework. GCT is\na semi-supervised method that exploits the unlabeled samples with two modal\nfeatures to crossly strengthen the IGL classifier.\n","authors":["Rui Xu","Lei Xing","Shuai Shao","Lifei Zhao","Baodi Liu","Weifeng Liu","Yicong Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.07738v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12852v1","updated":"2024-03-19T15:57:04Z","published":"2024-03-19T15:57:04Z","title":"Generative Enhancement for 3D Medical Images","summary":"  The limited availability of 3D medical image datasets, due to privacy\nconcerns and high collection or annotation costs, poses significant challenges\nin the field of medical imaging. While a promising alternative is the use of\nsynthesized medical data, there are few solutions for realistic 3D medical\nimage synthesis due to difficulties in backbone design and fewer 3D training\nsamples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel\ngenerative approach to the synthesis of 3D medical images and the enhancement\nof existing datasets using conditional diffusion models. Our method begins with\na 2D slice, noted as the informed slice to serve the patient prior, and\npropagates the generation process using a 3D segmentation mask. By decomposing\nthe 3D medical images into masks and patient prior information, GEM-3D offers a\nflexible yet effective solution for generating versatile 3D images from\nexisting datasets. GEM-3D can enable dataset enhancement by combining informed\nslice selection and generation at random positions, along with editable mask\nvolumes to introduce large variations in diffusion sampling. Moreover, as the\ninformed slice contains patient-wise information, GEM-3D can also facilitate\ncounterfactual image synthesis and dataset-level de-enhancement with desired\ncontrol. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D\nis capable of synthesizing high-quality 3D medical images with volumetric\nconsistency, offering a straightforward solution for dataset enhancement during\ninference. The code is available at https://github.com/HKU-MedAI/GEM-3D.\n","authors":["Lingting Zhu","Noel Codella","Dongdong Chen","Zhenchao Jin","Lu Yuan","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12852v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12848v1","updated":"2024-03-19T15:54:48Z","published":"2024-03-19T15:54:48Z","title":"Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape\n  Generation","summary":"  Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Early works typically\nemploy shape retrieval based frameworks which naturally suffer from limited\nshape diversity. Recent progresses have been made in shape generation with\npowerful generative models, such as diffusion models, which increases the shape\nfidelity. However, these approaches separately treat 3D shape generation and\nlayout generation. The synthesized scenes are usually hampered by layout\ncollision, which implies that the scene-level fidelity is still under-explored.\nIn this paper, we aim at generating realistic and reasonable 3D scenes from\nscene graph. To enrich the representation capability of the given scene graph\ninputs, large language model is utilized to explicitly aggregate the global\ngraph features with local relationship features. With a unified graph\nconvolution network (GCN), graph features are extracted from scene graphs\nupdated via joint layout-shape distribution. During scene generation, an\nIoU-based regularization loss is introduced to constrain the predicted 3D\nlayouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D\nscene synthesis, especially in terms of scene-level fidelity. The source code\nwill be released after publication.\n","authors":["Yao Wei","Martin Renqiang Min","George Vosselman","Li Erran Li","Michael Ying Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02317v3","updated":"2024-03-19T15:48:17Z","published":"2024-01-04T15:34:44Z","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","summary":"  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.\n","authors":["Yiran Song","Qianyu Zhou","Xiangtai Li","Deng-Ping Fan","Xuequan Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02317v3.pdf","comment":"Code:https://github.com/zongzi13545329/BA-SAM"},{"id":"http://arxiv.org/abs/2403.12839v1","updated":"2024-03-19T15:45:54Z","published":"2024-03-19T15:45:54Z","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene\n  Rendering","summary":"  Neural radiance fields~(NeRF) have recently been applied to render\nlarge-scale scenes. However, their limited model capacity typically results in\nblurred rendering results. Existing large-scale NeRFs primarily address this\nlimitation by partitioning the scene into blocks, which are subsequently\nhandled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and\nprocessed independently, lead to inconsistencies in geometry and appearance\nacross the scene. Consequently, the rendering quality fails to exhibit\nsignificant improvement despite the expansion of model capacity. In this work,\nwe present global-guided focal neural radiance field (GF-NeRF) that achieves\nhigh-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a\ntwo-stage (Global and Focal) architecture and a global-guided training\nstrategy. The global stage obtains a continuous representation of the entire\nscene while the focal stage decomposes the scene into multiple blocks and\nfurther processes them with distinct sub-encoders. Leveraging this two-stage\narchitecture, sub-encoders only need fine-tuning based on the global encoder,\nthus reducing training complexity in the focal stage while maintaining\nscene-wide consistency. Spatial information and error information from the\nglobal stage also benefit the sub-encoders to focus on crucial areas and\neffectively capture more details of large-scale scenes. Notably, our approach\ndoes not rely on any prior knowledge about the target scene, attributing\nGF-NeRF adaptable to various large-scale scene types, including street-view and\naerial-view scenes. We demonstrate that our method achieves high-fidelity,\nnatural rendering results on various types of large-scale datasets. Our project\npage: https://shaomq2187.github.io/GF-NeRF/\n","authors":["Mingqi Shao","Feng Xiong","Hang Zhang","Shuang Yang","Mu Xu","Wei Bian","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12835v1","updated":"2024-03-19T15:41:39Z","published":"2024-03-19T15:41:39Z","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","summary":"  Traditional approaches in physics-based motion generation, centered around\nimitation learning and reward shaping, often struggle to adapt to new\nscenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical\nmethod that learns physically plausible interactions following open-vocabulary\ninstructions. Our approach begins by developing a set of atomic actions via a\nlow-level controller trained via imitation learning. Upon receiving an\nopen-vocabulary textual instruction, AnySkill employs a high-level policy that\nselects and integrates these atomic actions to maximize the CLIP similarity\nbetween the agent's rendered images and the text. An important feature of our\nmethod is the use of image-based rewards for the high-level policy, which\nallows the agent to learn interactions with objects without manual reward\nengineering. We demonstrate AnySkill's capability to generate realistic and\nnatural motion sequences in response to unseen instructions of varying lengths,\nmarking it the first method capable of open-vocabulary physical skill learning\nfor interactive humanoid agents.\n","authors":["Jieming Cui","Tengyu Liu","Nian Liu","Yaodong Yang","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12834v1","updated":"2024-03-19T15:41:16Z","published":"2024-03-19T15:41:16Z","title":"Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation","summary":"  Traditionally, segmentation algorithms require dense annotations for\ntraining, demanding significant annotation efforts, particularly within the 3D\nmedical imaging field. Scribble-supervised learning emerges as a possible\nsolution to this challenge, promising a reduction in annotation efforts when\ncreating large-scale datasets. Recently, a plethora of methods for optimized\nlearning from scribbles have been proposed, but have so far failed to position\nscribble annotation as a beneficial alternative. We relate this shortcoming to\ntwo major issues: 1) the complex nature of many methods which deeply ties them\nto the underlying segmentation model, thus preventing a migration to more\npowerful state-of-the-art models as the field progresses and 2) the lack of a\nsystematic evaluation to validate consistent performance across the broader\nmedical domain, resulting in a lack of trust when applying these methods to new\nsegmentation problems. To address these issues, we propose a comprehensive\nscribble supervision benchmark consisting of seven datasets covering a diverse\nset of anatomies and pathologies imaged with varying modalities. We furthermore\npropose the systematic use of partial losses, i.e. losses that are only\ncomputed on annotated voxels. Contrary to most existing methods, these losses\ncan be seamlessly integrated into state-of-the-art segmentation methods,\nenabling them to learn from scribble annotations while preserving their\noriginal loss formulations. Our evaluation using nnU-Net reveals that while\nmost existing methods suffer from a lack of generalization, the proposed\napproach consistently delivers state-of-the-art performance. Thanks to its\nsimplicity, our approach presents an embarrassingly simple yet effective\nsolution to the challenges of scribble supervision. Source code as well as our\nextensive scribble benchmarking suite will be made publicly available upon\npublication.\n","authors":["Karol Gotkowski","Carsten Lüth","Paul F. Jäger","Sebastian Ziegler","Lars Krämer","Stefan Denner","Shuhan Xiao","Nico Disch","Klaus H. Maier-Hein","Fabian Isensee"],"pdf_url":"https://arxiv.org/pdf/2403.12834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02934v3","updated":"2024-03-19T15:33:17Z","published":"2023-12-05T18:05:14Z","title":"WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera\n  Driving Scene Generation","summary":"  Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.\n","authors":["Jiachen Lu","Ze Huang","Zeyu Yang","Jiahui Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.02934v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12816v1","updated":"2024-03-19T15:15:19Z","published":"2024-03-19T15:15:19Z","title":"Re-identification from histopathology images","summary":"  In numerous studies, deep learning algorithms have proven their potential for\nthe analysis of histopathology images, for example, for revealing the subtypes\nof tumors or the primary origin of metastases. These models require large\ndatasets for training, which must be anonymized to prevent possible patient\nidentity leaks. This study demonstrates that even relatively simple deep\nlearning algorithms can re-identify patients in large histopathology datasets\nwith substantial accuracy. We evaluated our algorithms on two TCIA datasets\nincluding lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).\nWe also demonstrate the algorithm's performance on an in-house dataset of\nmeningioma tissue. We predicted the source patient of a slide with F1 scores of\n50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31\n% on our meningioma dataset. Based on our findings, we formulated a risk\nassessment scheme to estimate the risk to the patient's privacy prior to\npublication.\n","authors":["Jonathan Ganz","Jonas Ammeling","Samir Jabari","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2403.12816v1.pdf","comment":"20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12806v1","updated":"2024-03-19T15:07:08Z","published":"2024-03-19T15:07:08Z","title":"VisualCritic: Making LMMs Perceive Visual Quality Like Humans","summary":"  At present, large multimodal models (LMMs) have exhibited impressive\ngeneralization capabilities in understanding and generating visual signals.\nHowever, they currently still lack sufficient capability to perceive low-level\nvisual quality akin to human perception. Can LMMs achieve this and show the\nsame degree of generalization in this regard? If so, not only could the\nversatility of LMMs be further enhanced, but also the challenge of poor\ncross-dataset performance in the field of visual quality assessment could be\naddressed. In this paper, we explore this question and provide the answer\n\"Yes!\". As the result of this initial exploration, we present VisualCritic, the\nfirst LMM for broad-spectrum image subjective quality assessment. VisualCritic\ncan be used across diverse data right out of box, without any requirements of\ndataset-specific adaptation operations like conventional specialist models. As\nan instruction-following LMM, VisualCritic enables new capabilities of (1)\nquantitatively measuring the perceptual quality of given images in terms of\ntheir Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other\nnumerical indicators, (2) qualitatively evaluating visual quality and providing\nexplainable descriptions, (3) discerning whether a given image is AI-generated\nor photographic. Extensive experiments demonstrate the efficacy of VisualCritic\nby comparing it with other open-source LMMs and conventional specialist models\nover both AI-generated and photographic images.\n","authors":["Zhipeng Huang","Zhizheng Zhang","Yiting Lu","Zheng-Jun Zha","Zhibo Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.12806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02778v2","updated":"2024-03-19T15:05:22Z","published":"2023-11-05T21:46:12Z","title":"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis","summary":"  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n","authors":["Xuqian Ren","Wenjia Wang","Dingding Cai","Tuuli Tuominen","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2311.02778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12803v1","updated":"2024-03-19T15:04:35Z","published":"2024-03-19T15:04:35Z","title":"DreamDA: Generative Data Augmentation with Diffusion Models","summary":"  The acquisition of large-scale, high-quality data is a resource-intensive and\ntime-consuming endeavor. Compared to conventional Data Augmentation (DA)\ntechniques (e.g. cropping and rotation), exploiting prevailing diffusion models\nfor data generation has received scant attention in classification tasks.\nExisting generative DA methods either inadequately bridge the domain gap\nbetween real-world and synthesized images, or inherently suffer from a lack of\ndiversity. To solve these issues, this paper proposes a new\nclassification-oriented framework DreamDA, which enables data synthesis and\nlabel generation by way of diffusion models. DreamDA generates diverse samples\nthat adhere to the original data distribution by considering training images in\nthe original data as seeds and perturbing their reverse diffusion process. In\naddition, since the labels of the generated data may not align with the labels\nof their corresponding seed images, we introduce a self-training paradigm for\ngenerating pseudo labels and training classifiers using the synthesized data.\nExtensive experiments across four tasks and five datasets demonstrate\nconsistent improvements over strong baselines, revealing the efficacy of\nDreamDA in synthesizing high-quality and diverse images with accurate labels.\nOur code will be available at https://github.com/yunxiangfu2001/DreamDA.\n","authors":["Yunxiang Fu","Chaoqi Chen","Yu Qiao","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12803v1.pdf","comment":"14 pages, 8 tables, 3 figures"},{"id":"http://arxiv.org/abs/2211.14823v3","updated":"2024-03-19T15:02:04Z","published":"2022-11-27T13:31:00Z","title":"3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer\n  Avenue","summary":"  This paper studies how to flexibly integrate reconstructed 3D models into\npractical 3D modeling pipelines such as 3D scene creation and rendering. Due to\nthe technical difficulty, one can only obtain rough 3D models (R3DMs) for most\nreal objects using existing 3D reconstruction techniques. As a result,\nphysically-based rendering (PBR) would render low-quality images or videos for\nscenes that are constructed by R3DMs. One promising solution would be\nrepresenting real-world objects as Neural Fields such as NeRFs, which are able\nto generate photo-realistic renderings of an object under desired viewpoints.\nHowever, a drawback is that the synthesized views through Neural Fields\nRendering (NFR) cannot reflect the simulated lighting details on R3DMs in PBR\npipelines, especially when object interactions in the 3D scene creation cause\nlocal shadows. To solve this dilemma, we propose a lighting transfer network\n(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.\nLighTNet reasons about a simplified image composition model, remedies the\nuneven surface issue caused by R3DMs, and is empowered by several\nperceptual-motivated constraints and a new Lab angle loss which enhances the\ncontrast between lighting strength and colors. Comparisons demonstrate that\nLighTNet is superior in synthesizing impressive lighting, and is promising in\npushing NFR further in practical 3D modeling workflows.\n","authors":["Bowen Cai","Yujie Li","Yuqin Liang","Rongfei Jia","Binqiang Zhao","Mingming Gong","Huan Fu"],"pdf_url":"https://arxiv.org/pdf/2211.14823v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI), project page:\n  http://3d-front-future.github.io/LighTNet"},{"id":"http://arxiv.org/abs/2403.12801v1","updated":"2024-03-19T15:01:19Z","published":"2024-03-19T15:01:19Z","title":"RelationVLM: Making Large Vision-Language Models Understand Visual\n  Relations","summary":"  The development of Large Vision-Language Models (LVLMs) is striving to catch\nup with the success of Large Language Models (LLMs), yet it faces more\nchallenges to be resolved. Very recent works enable LVLMs to localize\nobject-level visual contents and ground text to them. Nonetheless, current\nLVLMs still struggle to precisely understand visual relations due to the lack\nof relevant data. In this work, we present RelationVLM, a large vision-language\nmodel capable of comprehending various levels and types of relations whether\nacross multiple images or within a video. Specifically, we devise a multi-stage\nrelation-aware training scheme and a series of corresponding data configuration\nstrategies to bestow RelationVLM with the capabilities of understanding\nsemantic relations, temporal associations and geometric transforms. Extensive\ncase studies and quantitative evaluations show RelationVLM has strong\ncapability in understanding such relations and emerges impressive in-context\ncapability of reasoning from few-shot examples by comparison. This work fosters\nthe advancements of LVLMs by enabling them to support a wider range of\ndownstream applications toward artificial general intelligence.\n","authors":["Zhipeng Huang","Zhizheng Zhang","Zheng-Jun Zha","Yan Lu","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.12801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12800v1","updated":"2024-03-19T15:01:18Z","published":"2024-03-19T15:01:18Z","title":"Learning Neural Volumetric Pose Features for Camera Localization","summary":"  We introduce a novel neural volumetric pose feature, termed PoseMap, designed\nto enhance camera localization by encapsulating the information between images\nand the associated camera poses. Our framework leverages an Absolute Pose\nRegression (APR) architecture, together with an augmented NeRF module. This\nintegration not only facilitates the generation of novel views to enrich the\ntraining dataset but also enables the learning of effective pose features.\nAdditionally, we extend our architecture for self-supervised online alignment,\nallowing our method to be used and fine-tuned for unlabelled images within a\nunified framework. Experiments demonstrate that our method achieves 14.28% and\n20.51% performance gain on average in indoor and outdoor benchmark scenes,\noutperforming existing APR methods with state-of-the-art accuracy.\n","authors":["Jingyu Lin","Jiaqi Gu","Bojian Wu","Lubin Fan","Renjie Chen","Ligang Liu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12800v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.10521v2","updated":"2024-03-19T14:54:14Z","published":"2024-03-15T17:59:53Z","title":"P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap\n  Priors","summary":"  Autonomous vehicles are gradually entering city roads today, with the help of\nhigh-definition maps (HDMaps). However, the reliance on HDMaps prevents\nautonomous vehicles from stepping into regions without this expensive digital\ninfrastructure. This fact drives many researchers to study online HDMap\ngeneration algorithms, but the performance of these algorithms at far regions\nis still unsatisfying. We present P-MapNet, in which the letter P highlights\nthe fact that we focus on incorporating map priors to improve model\nperformance. Specifically, we exploit priors in both SDMap and HDMap. On one\nhand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an\nadditional conditioning branch. Despite the misalignment challenge, our\nattention-based architecture adaptively attends to relevant SDMap skeletons and\nsignificantly improves performance. On the other hand, we exploit a masked\nautoencoder to capture the prior distribution of HDMap, which can serve as a\nrefinement module to mitigate occlusions and artifacts. We benchmark on the\nnuScenes and Argoverse2 datasets. Through comprehensive experiments, we show\nthat: (1) our SDMap prior can improve online map generation performance, using\nboth rasterized (by up to $+18.73$ $\\rm mIoU$) and vectorized (by up to $+8.50$\n$\\rm mAP$) output representations. (2) our HDMap prior can improve map\nperceptual metrics by up to $6.34\\%$. (3) P-MapNet can be switched into\ndifferent inference modes that covers different regions of the\naccuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solution\nthat brings larger improvements on longer ranges. Codes and models are publicly\navailable at https://jike5.github.io/P-MapNet.\n","authors":["Zhou Jiang","Zhenxin Zhu","Pengfei Li","Huan-ang Gao","Tianyuan Yuan","Yongliang Shi","Hang Zhao","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.10521v2.pdf","comment":"Code: https://jike5.github.io/P-MapNet"},{"id":"http://arxiv.org/abs/2403.12037v2","updated":"2024-03-19T14:52:28Z","published":"2024-03-18T17:59:42Z","title":"MineDreamer: Learning to Follow Instructions via Chain-of-Imagination\n  for Simulated-World Control","summary":"  It is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.\n","authors":["Enshen Zhou","Yiran Qin","Zhenfei Yin","Yuzhou Huang","Ruimao Zhang","Lu Sheng","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2403.12037v2.pdf","comment":"Project page: https://sites.google.com/view/minedreamer/main"},{"id":"http://arxiv.org/abs/2403.12787v1","updated":"2024-03-19T14:51:01Z","published":"2024-03-19T14:51:01Z","title":"DDSB: An Unsupervised and Training-free Method for Phase Detection in\n  Echocardiography","summary":"  Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is\nkey for cardiac function assessment through echocardiography. However,\ntraditional methods face several limitations: they require extensive amounts of\ndata, extensive annotations by medical experts, significant training resources,\nand often lack robustness. Addressing these challenges, we proposed an\nunsupervised and training-free method, our novel approach leverages\nunsupervised segmentation to enhance fault tolerance against segmentation\ninaccuracies. By identifying anchor points and analyzing directional\ndeformation, we effectively reduce dependence on the accuracy of initial\nsegmentation images and enhance fault tolerance, all while improving\nrobustness. Tested on Echo-dynamic and CAMUS datasets, our method achieves\ncomparable accuracy to learning-based models without their associated\ndrawbacks. The code is available at https://github.com/MRUIL/DDSB\n","authors":["Zhenyu Bu","Yang Liu","Jiayu Huo","Jingjing Peng","Kaini Wang","Guangquan Zhou","Rachel Sparks","Prokar Dasgupta","Alejandro Granados","Sebastien Ourselin"],"pdf_url":"https://arxiv.org/pdf/2403.12787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12784v1","updated":"2024-03-19T14:50:13Z","published":"2024-03-19T14:50:13Z","title":"Total Disentanglement of Font Images into Style and Character Class\n  Features","summary":"  In this paper, we demonstrate a total disentanglement of font images. Total\ndisentanglement is a neural network-based method for decomposing each font\nimage nonlinearly and completely into its style and content (i.e., character\nclass) features. It uses a simple but careful training procedure to extract the\ncommon style feature from all `A'-`Z' images in the same font and the common\ncontent feature from all `A' (or another class) images in different fonts.\nThese disentangled features guarantee the reconstruction of the original font\nimage. Various experiments have been conducted to understand the performance of\ntotal disentanglement. First, it is demonstrated that total disentanglement is\nachievable with very high accuracy; this is experimental proof of the\nlong-standing open question, ``Does `A'-ness exist?'' Hofstadter (1985).\nSecond, it is demonstrated that the disentangled features produced by total\ndisentanglement apply to a variety of tasks, including font recognition,\ncharacter recognition, and one-shot font image generation.\n","authors":["Daichi Haraguchi","Wataru Shimoda","Kota Yamaguchi","Seiichi Uchida"],"pdf_url":"https://arxiv.org/pdf/2403.12784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12778v1","updated":"2024-03-19T14:45:17Z","published":"2024-03-19T14:45:17Z","title":"ViTGaze: Gaze Following with Interaction Features in Vision Transformers","summary":"  Gaze following aims to interpret human-scene interactions by predicting the\nperson's focal point of gaze. Prevailing approaches often use multi-modality\ninputs, most of which adopt a two-stage framework. Hence their performance\nhighly depends on the previous prediction accuracy. Others use a\nsingle-modality approach with complex decoders, increasing network\ncomputational load. Inspired by the remarkable success of pre-trained plain\nVision Transformers (ViTs), we introduce a novel single-modality gaze following\nframework, ViTGaze. In contrast to previous methods, ViTGaze creates a brand\nnew gaze following framework based mainly on powerful encoders (dec. param.\nless than 1%). Our principal insight lies in that the inter-token interactions\nwithin self-attention can be transferred to interactions between humans and\nscenes. Leveraging this presumption, we formulate a framework consisting of a\n4D interaction encoder and a 2D spatial guidance module to extract human-scene\ninteraction information from self-attention maps. Furthermore, our\ninvestigation reveals that ViT with self-supervised pre-training exhibits an\nenhanced ability to extract correlated information. A large number of\nexperiments have been conducted to demonstrate the performance of the proposed\nmethod. Our method achieves state-of-the-art (SOTA) performance among all\nsingle-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and\nvery comparable performance against multi-modality methods with 59% number of\nparameters less.\n","authors":["Yuehao Song","Xinggang Wang","Jingfeng Yao","Wenyu Liu","Jinglin Zhang","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2403.12778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12777v1","updated":"2024-03-19T14:44:54Z","published":"2024-03-19T14:44:54Z","title":"Discover and Mitigate Multiple Biased Subgroups in Image Classifiers","summary":"  Machine learning models can perform well on in-distribution data but often\nfail on biased subgroups that are underrepresented in the training data,\nhindering the robustness of models for reliable applications. Such subgroups\nare typically unknown due to the absence of subgroup labels. Discovering biased\nsubgroups is the key to understanding models' failure modes and further\nimproving models' robustness. Most previous works of subgroup discovery make an\nimplicit assumption that models only underperform on a single biased subgroup,\nwhich does not hold on in-the-wild data where multiple biased subgroups exist.\n  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),\na novel method to address a more challenging but also more practical problem of\ndiscovering multiple biased subgroups in image classifiers. Our approach\ndecomposes the image features into multiple components that represent multiple\nsubgroups. This decomposition is achieved via a bilinear dimension reduction\nmethod, Partial Least Square (PLS), guided by useful supervision from the image\nclassifier. We further interpret the semantic meaning of each subgroup\ncomponent by generating natural language descriptions using vision-language\nfoundation models. Finally, DIM mitigates multiple biased subgroups\nsimultaneously via two strategies, including the data- and model-centric\nstrategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate\nthe effectiveness of DIM in discovering and mitigating multiple biased\nsubgroups. Furthermore, DIM uncovers the failure modes of the classifier on\nHard ImageNet, showcasing its broader applicability to understanding model bias\nin image classifiers. The code is available at\nhttps://github.com/ZhangAIPI/DIM.\n","authors":["Zeliang Zhang","Mingqian Feng","Zhiheng Li","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12770v1","updated":"2024-03-19T14:34:44Z","published":"2024-03-19T14:34:44Z","title":"Multispectral Image Restoration by Generalized Opponent Transformation\n  Total Variation","summary":"  Multispectral images (MSI) contain light information in different wavelengths\nof objects, which convey spectral-spatial information and help improve the\nperformance of various image processing tasks. Numerous techniques have been\ncreated to extend the application of total variation regularization in\nrestoring multispectral images, for example, based on channel coupling and\nadaptive total variation regularization. The primary contribution of this paper\nis to propose and develop a new multispectral total variation regularization in\na generalized opponent transformation domain instead of the original\nmultispectral image domain. Here opponent transformations for multispectral\nimages are generalized from a well-known opponent transformation for color\nimages. We will explore the properties of generalized opponent transformation\ntotal variation (GOTTV) regularization and the corresponding optimization\nformula for multispectral image restoration. To evaluate the effectiveness of\nthe new GOTTV method, we provide numerical examples that showcase its superior\nperformance compared to existing multispectral image total variation methods,\nusing criteria such as MPSNR and MSSIM.\n","authors":["Zhantao Ma","Michael K. Ng"],"pdf_url":"https://arxiv.org/pdf/2403.12770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12767v1","updated":"2024-03-19T14:32:21Z","published":"2024-03-19T14:32:21Z","title":"Inter- and intra-uncertainty based feature aggregation model for\n  semi-supervised histopathology image segmentation","summary":"  Acquiring pixel-level annotations is often limited in applications such as\nhistology studies that require domain expertise. Various semi-supervised\nlearning approaches have been developed to work with limited ground truth\nannotations, such as the popular teacher-student models. However, hierarchical\nprediction uncertainty within the student model (intra-uncertainty) and image\nprediction uncertainty (inter-uncertainty) have not been fully utilized by\nexisting methods. To address these issues, we first propose a novel inter- and\nintra-uncertainty regularization method to measure and constrain both inter-\nand intra-inconsistencies in the teacher-student architecture. We also propose\na new two-stage network with pseudo-mask guided feature aggregation (PG-FANet)\nas the segmentation model. The two-stage structure complements with the\nuncertainty regularization strategy to avoid introducing extra modules in\nsolving uncertainties and the aggregation mechanisms enable multi-scale and\nmulti-stage feature integration. Comprehensive experimental results over the\nMoNuSeg and CRAG datasets show that our PG-FANet outperforms other\nstate-of-the-art methods and our semi-supervised learning framework yields\ncompetitive performance with a limited amount of labeled data.\n","authors":["Qiangguo Jin","Hui Cui","Changming Sun","Yang Song","Jiangbin Zheng","Leilei Cao","Leyi Wei","Ran Su"],"pdf_url":"https://arxiv.org/pdf/2403.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12760v1","updated":"2024-03-19T14:27:24Z","published":"2024-03-19T14:27:24Z","title":"WaveFace: Authentic Face Restoration with Efficient Frequency Recovery","summary":"  Although diffusion models are rising as a powerful solution for blind face\nrestoration, they are criticized for two problems: 1) slow training and\ninference speed, and 2) failure in preserving identity and recovering\nfine-grained facial details. In this work, we propose WaveFace to solve the\nproblems in the frequency domain, where low- and high-frequency components\ndecomposed by wavelet transformation are considered individually to maximize\nauthenticity as well as efficiency. The diffusion model is applied to recover\nthe low-frequency component only, which presents general information of the\noriginal image but 1/16 in size. To preserve the original identity, the\ngeneration is conditioned on the low-frequency component of low-quality images\nat each denoising step. Meanwhile, high-frequency components at multiple\ndecomposition levels are handled by a unified network, which recovers complex\nfacial details in a single step. Evaluations on four benchmark datasets show\nthat: 1) WaveFace outperforms state-of-the-art methods in authenticity,\nespecially in terms of identity preservation, and 2) authentic images are\nrestored with the efficiency 10x faster than existing diffusion model-based BFR\nmethods.\n","authors":["Yunqi Miao","Jiankang Deng","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2403.12760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16226v2","updated":"2024-03-19T14:17:54Z","published":"2023-10-24T22:41:14Z","title":"TiC-CLIP: Continual Training of CLIP Models","summary":"  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to \\emph{continually} train these models. This problem is\nexacerbated by the lack of any large scale continual learning benchmarks or\nbaselines. We introduce the first set of web-scale Time-Continual (TiC)\nbenchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and\nTiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped\nimage-text pairs spanning 9 years (2014--2022). We first use our benchmarks to\ncurate various \\emph{dynamic} evaluations to measure temporal robustness of\nexisting models. We show OpenAI's CLIP (trained on data up to 2020) loses\n$\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021--2022\ncompared with more recently trained models in OpenCLIP repository. We then\nstudy how to efficiently train models on time-continuous data. We demonstrate\nthat a simple rehearsal-based approach that continues training from the last\ncheckpoint and replays old data reduces compute by $2.5\\times$ when compared to\nthe standard practice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.\n","authors":["Saurabh Garg","Mehrdad Farajtabar","Hadi Pouransari","Raviteja Vemulapalli","Sachin Mehta","Oncel Tuzel","Vaishaal Shankar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2310.16226v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12748v1","updated":"2024-03-19T14:11:26Z","published":"2024-03-19T14:11:26Z","title":"Building Brain Tumor Segmentation Networks with User-Assisted Filter\n  Estimation and Selection","summary":"  Brain tumor image segmentation is a challenging research topic in which\ndeep-learning models have presented the best results. However, the traditional\nway of training those models from many pre-annotated images leaves several\nunanswered questions. Hence methodologies, such as Feature Learning from Image\nMarkers (FLIM), have involved an expert in the learning loop to reduce human\neffort in data annotation and build models sufficiently deep for a given\nproblem. FLIM has been successfully used to create encoders, estimating the\nfilters of all convolutional layers from patches centered at marker voxels. In\nthis work, we present Multi-Step (MS) FLIM - a user-assisted approach to\nestimating and selecting the most relevant filters from multiple FLIM\nexecutions. MS-FLIM is used only for the first convolutional layer, and the\nresults already indicate improvement over FLIM. For evaluation, we build a\nsimple U-shaped encoder-decoder network, named sU-Net, for glioblastoma\nsegmentation using T1Gd and FLAIR MRI scans, varying the encoder's training\nmethod, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared\nthese sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two\ndatasets. The results show that the sU-Net based on MS-FLIM outperforms the\nother training methods and achieves effectiveness within the standard\ndeviations of the SOTA models.\n","authors":["Matheus A. Cerqueira","Flávia Sprenger","Bernardo C. A. Teixeira","Alexandre X. Falcão"],"pdf_url":"https://arxiv.org/pdf/2403.12748v1.pdf","comment":"10 pages, 5 figures, 2 tables, 24 references, manuscript of\n  conference paper"},{"id":"http://arxiv.org/abs/2401.10191v3","updated":"2024-03-19T14:09:31Z","published":"2024-01-18T18:25:29Z","title":"Divide and not forget: Ensemble of selectively trained experts in\n  Continual Learning","summary":"  Class-incremental learning is becoming more popular as it helps models widen\ntheir applicability while not forgetting what they already know. A trend in\nthis area is to use a mixture-of-expert technique, where different models work\ntogether to solve the task. However, the experts are usually trained all at\nonce using whole task data, which makes them all prone to forgetting and\nincreasing computational burden. To address this limitation, we introduce a\nnovel approach named SEED. SEED selects only one, the most optimal expert for a\nconsidered task, and uses data from this task to fine-tune only this expert.\nFor this purpose, each expert represents each class with a Gaussian\ndistribution, and the optimal expert is selected based on the similarity of\nthose distributions. Consequently, SEED increases diversity and heterogeneity\nwithin the experts while maintaining the high stability of this ensemble\nmethod. The extensive experiments demonstrate that SEED achieves\nstate-of-the-art performance in exemplar-free settings across various\nscenarios, showing the potential of expert diversification through data in\ncontinual learning.\n","authors":["Grzegorz Rypeść","Sebastian Cygert","Valeriya Khan","Tomasz Trzciński","Bartosz Zieliński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2401.10191v3.pdf","comment":"Accepted for ICLR 2024 (main track), code is available at:\n  https://github.com/grypesc/SEED"},{"id":"http://arxiv.org/abs/2403.11956v2","updated":"2024-03-19T14:03:22Z","published":"2024-03-18T16:52:49Z","title":"Subjective-Aligned Dateset and Metric for Text-to-Video Quality\n  Assessment","summary":"  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n","authors":["Tengchuan Kou","Xiaohong Liu","Zicheng Zhang","Chunyi Li","Haoning Wu","Xiongkuo Min","Guangtao Zhai","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12743v1","updated":"2024-03-19T14:02:13Z","published":"2024-03-19T14:02:13Z","title":"Towards Controllable Face Generation with Semantic Latent Diffusion\n  Models","summary":"  Semantic Image Synthesis (SIS) is among the most popular and effective\ntechniques in the field of face generation and editing, thanks to its good\ngeneration quality and the versatility is brings along. Recent works attempted\nto go beyond the standard GAN-based framework, and started to explore Diffusion\nModels (DMs) for this task as these stand out with respect to GANs in terms of\nboth quality and diversity. On the other hand, DMs lack in fine-grained\ncontrollability and reproducibility. To address that, in this paper we propose\na SIS framework based on a novel Latent Diffusion Model architecture for human\nface generation and editing that is both able to reproduce and manipulate a\nreal reference image and generate diversity-driven results. The proposed system\nutilizes both SPADE normalization and cross-attention layers to merge shape and\nstyle information and, by doing so, allows for a precise control over each of\nthe semantic parts of the human face. This was not possible with previous\nmethods in the state of the art. Finally, we performed an extensive set of\nexperiments to prove that our model surpasses current state of the art, both\nqualitatively and quantitatively.\n","authors":["Alex Ergasti","Claudio Ferrari","Tomaso Fontanini","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2403.12743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03094v3","updated":"2024-03-19T13:56:07Z","published":"2023-04-06T14:22:02Z","title":"PopulAtion Parameter Averaging (PAPA)","summary":"  Ensemble methods combine the predictions of multiple models to improve\nperformance, but they require significantly higher computation costs at\ninference time. To avoid these costs, multiple neural networks can be combined\ninto one by averaging their weights. However, this usually performs\nsignificantly worse than ensembling. Weight averaging is only beneficial when\ndifferent enough to benefit from combining them, but similar enough to average\nwell. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a\nmethod that combines the generality of ensembling with the efficiency of weight\naveraging. PAPA leverages a population of diverse models (trained on different\ndata orders, augmentations, and regularizations) while slowly pushing the\nweights of the networks toward the population average of the weights. We also\npropose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather\nthan continuously; all methods increase generalization, but PAPA tends to\nperform best. PAPA reduces the performance gap between averaging and\nensembling, increasing the average accuracy of a population of models by up to\n0.8% on CIFAR-10, 1.9% on CIFAR-100, and 1.6% on ImageNet when compared to\ntraining independent (non-averaged) models.\n","authors":["Alexia Jolicoeur-Martineau","Emy Gervais","Kilian Fatras","Yan Zhang","Simon Lacoste-Julien"],"pdf_url":"https://arxiv.org/pdf/2304.03094v3.pdf","comment":"Blog post: https://ajolicoeur.wordpress.com/papa/, Code:\n  https://github.com/SamsungSAILMontreal/PAPA, TMLR journal publication:\n  https://openreview.net/forum?id=cPDVjsOytS"},{"id":"http://arxiv.org/abs/2403.12736v1","updated":"2024-03-19T13:53:37Z","published":"2024-03-19T13:53:37Z","title":"Towards Multimodal In-Context Learning for Vision & Language Models","summary":"  Inspired by the emergence of Large Language Models (LLMs) that can truly\nunderstand human language, significant progress has been made in aligning\nother, non-language, modalities to be `understandable' by an LLM, primarily via\nconverting their samples into a sequence of embedded language-like tokens\ndirectly fed into the LLM (decoder) input stream. However, so far limited\nattention has been given to transferring (and evaluating) one of the core LLM\ncapabilities to the emerging VLMs, namely the In-Context Learning (ICL)\nability, or in other words to guide VLMs to desired target downstream tasks or\noutput structure using in-context image+text demonstrations. In this work, we\ndive deeper into analyzing the capabilities of some of the state-of-the-art\nVLMs to follow ICL instructions, discovering them to be somewhat lacking. We\ndiscover that even models that underwent large-scale mixed modality\npre-training and were implicitly guided to make use of interleaved image and\ntext information (intended to consume helpful context from multiple images)\nunder-perform when prompted with few-shot (ICL) demonstrations, likely due to\ntheir lack of `direct' ICL instruction tuning. To test this conjecture, we\npropose a simple, yet surprisingly effective, strategy of extending a common\nVLM alignment framework with ICL support, methodology, and curriculum. We\nexplore, analyze, and provide insights into effective data mixes, leading up to\na significant 21.03% (and 11.3% on average) ICL performance boost over the\nstrongest VLM baselines and a variety of ICL benchmarks. We also contribute new\nbenchmarks for ICL evaluation in VLMs and discuss their advantages over the\nprior art.\n","authors":["Sivan Doveh","Shaked Perek","M. Jehanzeb Mirza","Amit Alfassy","Assaf Arbelle","Shimon Ullman","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2403.12736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12728v1","updated":"2024-03-19T13:43:27Z","published":"2024-03-19T13:43:27Z","title":"Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and\n  Pose Estimation","summary":"  Fully-supervised category-level pose estimation aims to determine the 6-DoF\nposes of unseen instances from known categories, requiring expensive mannual\nlabeling costs. Recently, various self-supervised category-level pose\nestimation methods have been proposed to reduce the requirement of the\nannotated datasets. However, most methods rely on synthetic data or 3D CAD\nmodel for self-supervised training, and they are typically limited to\naddressing single-object pose problems without considering multi-objective\ntasks or shape reconstruction. To overcome these challenges and limitations, we\nintroduce a diffusion-driven self-supervised network for multi-object shape\nreconstruction and categorical pose estimation, only leveraging the shape\npriors. Specifically, to capture the SE(3)-equivariant pose features and 3D\nscale-invariant shape information, we present a Prior-Aware Pyramid 3D Point\nTransformer in our network. This module adopts a point convolutional layer with\nradial-kernels for pose-aware learning and a 3D scale-invariant graph\nconvolution layer for object-level shape representation, respectively.\nFurthermore, we introduce a pretrain-to-refine self-supervised training\nparadigm to train our network. It enables proposed network to capture the\nassociations between shape priors and observations, addressing the challenge of\nintra-class shape variations by utilising the diffusion mechanism. Extensive\nexperiments conducted on four public datasets and a self-built dataset\ndemonstrate that our method significantly outperforms state-of-the-art\nself-supervised category-level baselines and even surpasses some\nfully-supervised instance-level and category-level methods.\n","authors":["Jingtao Sun","Yaonan Wang","Mingtao Feng","Chao Ding","Mike Zheng Shou","Ajmal Saeed Mian"],"pdf_url":"https://arxiv.org/pdf/2403.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12722v1","updated":"2024-03-19T13:39:05Z","published":"2024-03-19T13:39:05Z","title":"HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting","summary":"  Holistic understanding of urban scenes based on RGB images is a challenging\nyet important problem. It encompasses understanding both the geometry and\nappearance to enable novel view synthesis, parsing semantic labels, and\ntracking moving objects. Despite considerable progress, existing approaches\noften focus on specific aspects of this task and require additional inputs such\nas LiDAR scans or manually annotated 3D bounding boxes. In this paper, we\nintroduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic\nurban scene understanding. Our main idea involves the joint optimization of\ngeometry, appearance, semantics, and motion using a combination of static and\ndynamic 3D Gaussians, where moving object poses are regularized via physical\nconstraints. Our approach offers the ability to render new viewpoints in\nreal-time, yielding 2D and 3D semantic information with high accuracy, and\nreconstruct dynamic scenes, even in scenarios where 3D bounding box detection\nare highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2\ndemonstrate the effectiveness of our approach.\n","authors":["Hongyu Zhou","Jiahao Shao","Lu Xu","Dongfeng Bai","Weichao Qiu","Bingbing Liu","Yue Wang","Andreas Geiger","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.12722v1.pdf","comment":"Our project page is at https://xdimlab.github.io/hugs_website"},{"id":"http://arxiv.org/abs/2307.00574v4","updated":"2024-03-19T13:35:37Z","published":"2023-07-02T13:57:45Z","title":"Bidirectional Temporal Diffusion Model for Temporally Consistent Human\n  Animation","summary":"  We introduce a method to generate temporally coherent human animation from a\nsingle image, a video, or a random noise. This problem has been formulated as\nmodeling of an auto-regressive generation, i.e., to regress past frames to\ndecode future frames. However, such unidirectional generation is highly prone\nto motion drifting over time, generating unrealistic human animation with\nsignificant artifacts such as appearance distortion. We claim that\nbidirectional temporal modeling enforces temporal coherence on a generative\nnetwork by largely suppressing the motion ambiguity of human appearance. To\nprove our claim, we design a novel human animation framework using a denoising\ndiffusion model: a neural network learns to generate the image of a person by\ndenoising temporal Gaussian noises whose intermediate results are\ncross-conditioned bidirectionally between consecutive frames. In the\nexperiments, our method demonstrates strong performance compared to existing\nunidirectional approaches with realistic temporal coherence\n","authors":["Tserendorj Adiya","Jae Shin Yoon","Jungeun Lee","Sanghun Kim","Hwasup Lim"],"pdf_url":"https://arxiv.org/pdf/2307.00574v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v2","updated":"2024-03-19T13:28:27Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuhene","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v2.pdf","comment":"Project Page (Code and Data):\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2403.12712v1","updated":"2024-03-19T13:19:41Z","published":"2024-03-19T13:19:41Z","title":"Addressing Source Scale Bias via Image Warping for Domain Adaptation","summary":"  In visual recognition, scale bias is a key challenge due to the imbalance of\nobject and image size distribution inherent in real scene datasets.\nConventional solutions involve injecting scale invariance priors, oversampling\nthe dataset at different scales during training, or adjusting scale at\ninference. While these strategies mitigate scale bias to some extent, their\nability to adapt across diverse datasets is limited. Besides, they increase\ncomputational load during training and latency during inference. In this work,\nwe use adaptive attentional processing -- oversampling salient object regions\nby warping images in-place during training. Discovering that shifting the\nsource scale distribution improves backbone features, we developed a\ninstance-level warping guidance aimed at object region sampling to mitigate\nsource scale bias in domain adaptation. Our approach improves adaptation across\ngeographies, lighting and weather conditions, is agnostic to the task, domain\nadaptation algorithm, saliency guidance, and underlying model architecture.\nHighlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7\nmAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear\n$\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC. Our\napproach adds minimal memory during training and has no additional latency at\ninference time. Please see Appendix for more results and analysis.\n","authors":["Shen Zheng","Anurag Ghosh","Srinivasa G. Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2403.12712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12710v1","updated":"2024-03-19T13:17:26Z","published":"2024-03-19T13:17:26Z","title":"Selective, Interpretable, and Motion Consistent Privacy Attribute\n  Obfuscation for Action Recognition","summary":"  Concerns for the privacy of individuals captured in public imagery have led\nto privacy-preserving action recognition. Existing approaches often suffer from\nissues arising through obfuscation being applied globally and a lack of\ninterpretability. Global obfuscation hides privacy sensitive regions, but also\ncontextual regions important for action recognition. Lack of interpretability\nerodes trust in these new technologies. We highlight the limitations of current\nparadigms and propose a solution: Human selected privacy templates that yield\ninterpretability by design, an obfuscation scheme that selectively hides\nattributes and also induces temporal consistency, which is important in action\nrecognition. Our approach is architecture agnostic and directly modifies input\nimagery, while existing approaches generally require architecture training. Our\napproach offers more flexibility, as no retraining is required, and outperforms\nalternatives on three widely used datasets.\n","authors":["Filip Ilic","He Zhao","Thomas Pock","Richard P. Wildes"],"pdf_url":"https://arxiv.org/pdf/2403.12710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12707v1","updated":"2024-03-19T13:09:19Z","published":"2024-03-19T13:09:19Z","title":"Selective Domain-Invariant Feature for Generalizable Deepfake Detection","summary":"  With diverse presentation forgery methods emerging continually, detecting the\nauthenticity of images has drawn growing attention. Although existing methods\nhave achieved impressive accuracy in training dataset detection, they still\nperform poorly in the unseen domain and suffer from forgery of irrelevant\ninformation such as background and identity, affecting generalizability. To\nsolve this problem, we proposed a novel framework Selective Domain-Invariant\nFeature (SDIF), which reduces the sensitivity to face forgery by fusing content\nfeatures and styles. Specifically, we first use a Farthest-Point Sampling (FPS)\ntraining strategy to construct a task-relevant style sample representation\nspace for fusing with content features. Then, we propose a dynamic feature\nextraction module to generate features with diverse styles to improve the\nperformance and effectiveness of the feature extractor. Finally, a domain\nseparation strategy is used to retain domain-related features to help\ndistinguish between real and fake faces. Both qualitative and quantitative\nresults in existing benchmarks and proposals demonstrate the effectiveness of\nour approach.\n","authors":["Yingxin Lai","Guoqing Yang Yifan He","Zhiming Luo","Shaozi Li"],"pdf_url":"https://arxiv.org/pdf/2403.12707v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.12706v1","updated":"2024-03-19T13:08:54Z","published":"2024-03-19T13:08:54Z","title":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation","summary":"  We present AnimateDiff-Lightning for lightning-fast video generation. Our\nmodel uses progressive adversarial diffusion distillation to achieve new\nstate-of-the-art in few-step video generation. We discuss our modifications to\nadapt it for the video modality. Furthermore, we propose to simultaneously\ndistill the probability flow of multiple base diffusion models, resulting in a\nsingle distilled motion module with broader style compatibility. We are pleased\nto release our distilled AnimateDiff-Lightning model for the community's use.\n","authors":["Shanchuan Lin","Xiao Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11220v2","updated":"2024-03-19T13:02:10Z","published":"2024-03-17T13:43:10Z","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations","summary":"  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n","authors":["Yuwei Zhang","Yan Wu","Yanming Liu","Xinyue Peng"],"pdf_url":"https://arxiv.org/pdf/2403.11220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12702v1","updated":"2024-03-19T13:01:57Z","published":"2024-03-19T13:01:57Z","title":"Learning Cross-view Visual Geo-localization without Ground Truth","summary":"  Cross-View Geo-Localization (CVGL) involves determining the geographical\nlocation of a query image by matching it with a corresponding GPS-tagged\nreference image. Current state-of-the-art methods predominantly rely on\ntraining models with labeled paired images, incurring substantial annotation\ncosts and training burdens. In this study, we investigate the adaptation of\nfrozen models for CVGL without requiring ground truth pair labels. We observe\nthat training on unlabeled cross-view images presents significant challenges,\nincluding the need to establish relationships within unlabeled data and\nreconcile view discrepancies between uncertain queries and references. To\naddress these challenges, we propose a self-supervised learning framework to\ntrain a learnable adapter for a frozen Foundation Model (FM). This adapter is\ndesigned to map feature distributions from diverse views into a uniform space\nusing unlabeled data exclusively. To establish relationships within unlabeled\ndata, we introduce an Expectation-Maximization-based Pseudo-labeling module,\nwhich iteratively estimates associations between cross-view features and\noptimizes the adapter. To maintain the robustness of the FM's representation,\nwe incorporate an information consistency module with a reconstruction loss,\nensuring that adapted features retain strong discriminative ability across\nviews. Experimental results demonstrate that our proposed method achieves\nsignificant improvements over vanilla FMs and competitive accuracy compared to\nsupervised methods, while necessitating fewer training parameters and relying\nsolely on unlabeled data. Evaluation of our adaptation for task-specific models\nfurther highlights its broad applicability.\n","authors":["Haoyuan Li","Chang Xu","Wen Yang","Huai Yu","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2403.12702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12695v1","updated":"2024-03-19T12:52:38Z","published":"2024-03-19T12:52:38Z","title":"Federated Semi-supervised Learning for Medical Image Segmentation with\n  intra-client and inter-client Consistency","summary":"  Medical image segmentation plays a vital role in clinic disease diagnosis and\nmedical image analysis. However, labeling medical images for segmentation task\nis tough due to the indispensable domain expertise of radiologists.\nFurthermore, considering the privacy and sensitivity of medical images, it is\nimpractical to build a centralized segmentation dataset from different medical\ninstitutions. Federated learning aims to train a shared model of isolated\nclients without local data exchange which aligns well with the scarcity and\nprivacy characteristics of medical data. To solve the problem of labeling hard,\nmany advanced semi-supervised methods have been proposed in a centralized data\nsetting. As for federated learning, how to conduct semi-supervised learning\nunder this distributed scenario is worth investigating. In this work, we\npropose a novel federated semi-supervised learning framework for medical image\nsegmentation. The intra-client and inter-client consistency learning are\nintroduced to smooth predictions at the data level and avoid confirmation bias\nof local models. They are achieved with the assistance of a Variational\nAutoencoder (VAE) trained collaboratively by clients. The added VAE model plays\nthree roles: 1) extracting latent low-dimensional features of all labeled and\nunlabeled data; 2) performing a novel type of data augmentation in calculating\nintra-client consistency loss; 3) utilizing the generative ability of itself to\nconduct inter-client consistency distillation. The proposed framework is\ncompared with other federated semi-supervised or self-supervised learning\nmethods. The experimental results illustrate that our method outperforms the\nstate-of-the-art method while avoiding a lot of computation and communication\noverhead.\n","authors":["Yubin Zheng","Peng Tang","Tianjie Ju","Weidong Qiu","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2403.12695v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2403.12693v1","updated":"2024-03-19T12:51:39Z","published":"2024-03-19T12:51:39Z","title":"As Firm As Their Foundations: Can open-sourced foundation models be used\n  to create adversarial examples for downstream tasks?","summary":"  Foundation models pre-trained on web-scale vision-language data, such as\nCLIP, are widely used as cornerstones of powerful machine learning systems.\nWhile pre-training offers clear advantages for downstream learning, it also\nendows downstream models with shared adversarial vulnerabilities that can be\neasily identified through the open-sourced foundation model. In this work, we\nexpose such vulnerabilities in CLIP's downstream models and show that\nfoundation models can serve as a basis for attacking their downstream systems.\nIn particular, we propose a simple yet effective adversarial attack strategy\ntermed Patch Representation Misalignment (PRM). Solely based on open-sourced\nCLIP vision encoders, this method produces adversaries that simultaneously fool\nmore than 20 downstream models spanning 4 common vision-language tasks\n(semantic segmentation, object detection, image captioning and visual\nquestion-answering). Our findings highlight the concerning safety risks\nintroduced by the extensive usage of public foundational models in the\ndevelopment of downstream systems, calling for extra caution in these\nscenarios.\n","authors":["Anjun Hu","Jindong Gu","Francesco Pinto","Konstantinos Kamnitsas","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2403.12693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12687v1","updated":"2024-03-19T12:45:52Z","published":"2024-03-19T12:45:52Z","title":"Audio-Visual Compound Expression Recognition Method based on Late\n  Modality Fusion and Rule-based Decision","summary":"  This paper presents the results of the SUN team for the Compound Expressions\nRecognition Challenge of the 6th ABAW Competition. We propose a novel\naudio-visual method for compound expression recognition. Our method relies on\nemotion recognition models that fuse modalities at the emotion probability\nlevel, while decisions regarding the prediction of compound expressions are\nbased on predefined rules. Notably, our method does not use any training data\nspecific to the target task. The method is evaluated in multi-corpus training\nand cross-corpus validation setups. Our findings from the challenge demonstrate\nthat the proposed method can potentially form a basis for development of\nintelligent tools for annotating audio-visual data in the context of human's\nbasic and compound emotions. The source code is publicly available.\n","authors":["Elena Ryumina","Maxim Markitantov","Dmitry Ryumin","Heysem Kaya","Alexey Karpov"],"pdf_url":"https://arxiv.org/pdf/2403.12687v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.12686v1","updated":"2024-03-19T12:45:18Z","published":"2024-03-19T12:45:18Z","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and\n  mmWave Radar","summary":"  The perception of waterways based on human intent holds significant\nimportance for autonomous navigation and operations of Unmanned Surface\nVehicles (USVs) in water environments. Inspired by visual grounding, in this\npaper, we introduce WaterVG, the first visual grounding dataset designed for\nUSV-based waterway perception based on human intention prompts. WaterVG\nencompasses prompts describing multiple targets, with annotations at the\ninstance level including bounding boxes and masks. Notably, WaterVG includes\n11,568 samples with 34,950 referred targets, which integrates both visual and\nradar characteristics captured by monocular camera and millimeter-wave (mmWave)\nradar, enabling a finer granularity of text prompts. Furthermore, we propose a\nnovel multi-modal visual grounding model, Potamoi, which is a multi-modal and\nmulti-task model based on the one-stage paradigm with a designed Phased\nHeterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar\nWeighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA\nis a low-cost and efficient fusion module with a remarkably small parameter\ncount and FLOPs, elegantly aligning and fusing scenario context information\ncaptured by two sensors with linguistic features, which can effectively address\ntasks of referring expression comprehension and segmentation based on\nfine-grained prompts. Comprehensive experiments and evaluations have been\nconducted on WaterVG, where our Potamoi archives state-of-the-art performances\ncompared with counterparts.\n","authors":["Runwei Guan","Liye Jia","Fengyufan Yang","Shanliang Yao","Erick Purwanto","Xiaohui Zhu","Eng Gee Lim","Jeremy Smith","Ka Lok Man","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2403.12686v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.12682v1","updated":"2024-03-19T12:36:51Z","published":"2024-03-19T12:36:51Z","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single\n  image and a NeRF model","summary":"  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera\npose of a given image, building on the Neural Radiance Fields (NeRF)\nformulation. IFFNeRF is specifically designed to operate in real-time and\neliminates the need for an initial pose guess that is proximate to the sought\nsolution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface\npoints from within the NeRF model. From these sampled points, we cast rays and\ndeduce the color for each ray through pixel-level view synthesis. The camera\npose can then be estimated as the solution to a Least Squares problem by\nselecting correspondences between the query image and the resulting bundle. We\nfacilitate this process through a learned attention mechanism, bridging the\nquery image embedding with the embedding of parameterized rays, thereby\nmatching rays pertinent to the image. Through synthetic and real evaluation\nsettings, we show that our method can improve the angular and translation error\naccuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing\nat 34fps on consumer hardware and not requiring the initial pose guess.\n","authors":["Matteo Bortolon","Theodore Tsesmelis","Stuart James","Fabio Poiesi","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2403.12682v1.pdf","comment":"Accepted ICRA 2024, Project page:\n  https://mbortolon97.github.io/iffnerf/"},{"id":"http://arxiv.org/abs/2308.13812v2","updated":"2024-03-19T12:29:54Z","published":"2023-08-26T08:31:48Z","title":"Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs","summary":"  Text-to-video (T2V) synthesis has gained increasing attention in the\ncommunity, in which the recently emerged diffusion models (DMs) have\npromisingly shown stronger performance than the past approaches. While existing\nstate-of-the-art DMs are competent to achieve high-resolution video generation,\nthey may largely suffer from key limitations (e.g., action occurrence\ndisorders, crude video motions) with respect to the intricate temporal dynamics\nmodeling, one of the crux of video synthesis. In this work, we investigate\nstrengthening the awareness of video dynamics for DMs, for high-quality T2V\ngeneration. Inspired by human intuition, we design an innovative dynamic scene\nmanager (dubbed as Dysen) module, which includes (step-1) extracting from input\ntext the key actions with proper time-order arrangement, (step-2) transforming\nthe action schedules into the dynamic scene graph (DSG) representations, and\n(step-3) enriching the scenes in the DSG with sufficient and reasonable\ndetails. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via\nin-context learning, Dysen realizes (nearly) human-level temporal dynamics\nunderstanding. Finally, the resulting video DSG with rich action scene details\nis encoded as fine-grained spatio-temporal features, integrated into the\nbackbone T2V DM for video generating. Experiments on popular T2V datasets\nsuggest that our Dysen-VDM consistently outperforms prior arts with significant\nmargins, especially in scenarios with complex actions. Codes at\nhttps://haofei.vip/Dysen-VDM\n","authors":["Hao Fei","Shengqiong Wu","Wei Ji","Hanwang Zhang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2308.13812v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2307.01187v4","updated":"2024-03-19T12:27:37Z","published":"2023-07-03T17:52:44Z","title":"SAMAug: Point Prompt Augmentation for Segment Anything Model","summary":"  This paper introduces SAMAug, a novel visual point augmentation method for\nthe Segment Anything Model (SAM) that enhances interactive image segmentation\nperformance. SAMAug generates augmented point prompts to provide more\ninformation about the user's intention to SAM. Starting with an initial point\nprompt, SAM produces an initial mask, which is then fed into our proposed\nSAMAug to generate augmented point prompts. By incorporating these extra\npoints, SAM can generate augmented segmentation masks based on both the\naugmented point prompts and the initial prompt, resulting in improved\nsegmentation performance. We conducted evaluations using four different point\naugmentation strategies: random sampling, sampling based on maximum difference\nentropy, maximum distance, and saliency. Experiment results on the COCO,\nFundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's\nsegmentation results, especially using the maximum distance and saliency.\nSAMAug demonstrates the potential of visual prompt augmentation for computer\nvision. Codes of SAMAug are available at github.com/yhydhx/SAMAug\n","authors":["Haixing Dai","Chong Ma","Zhiling Yan","Zhengliang Liu","Enze Shi","Yiwei Li","Peng Shu","Xiaozheng Wei","Lin Zhao","Zihao Wu","Fang Zeng","Dajiang Zhu","Wei Liu","Quanzheng Li","Lichao Sun","Shu Zhang Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2307.01187v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07269v2","updated":"2024-03-19T12:27:33Z","published":"2023-08-14T16:52:42Z","title":"EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models","summary":"  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.\n","authors":["Peng Wang","Ningyu Zhang","Bozhong Tian","Zekun Xi","Yunzhi Yao","Ziwen Xu","Mengru Wang","Shengyu Mao","Xiaohan Wang","Siyuan Cheng","Kangwei Liu","Yuansheng Ni","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.07269v2.pdf","comment":"Code: https://github.com/zjunlp/EasyEdit HF Demo:\n  https://huggingface.co/spaces/zjunlp/EasyEdit Video:\n  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit"},{"id":"http://arxiv.org/abs/2403.12670v1","updated":"2024-03-19T12:11:57Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2309.07439v2","updated":"2024-03-19T11:51:38Z","published":"2023-09-14T05:45:40Z","title":"DePT: Decoupled Prompt Tuning","summary":"  This work breaks through the Base-New Tradeoff (BNT)dilemma in prompt tuning,\ni.e., the better the tuned model generalizes to the base (or target) task, the\nworse it generalizes to new tasks, and vice versa. Specifically, through an\nin-depth analysis of the learned features of the base and new tasks, we observe\nthat the BNT stems from a channel bias issue, i.e., the vast majority of\nfeature channels are occupied by base-specific knowledge, resulting in the\ncollapse of taskshared knowledge important to new tasks. To address this, we\npropose the Decoupled Prompt Tuning (DePT) framework, which decouples\nbase-specific knowledge from feature channels into an isolated feature space\nduring prompt tuning, so as to maximally preserve task-shared knowledge in the\noriginal feature space for achieving better zero-shot generalization on new\ntasks. Importantly, our DePT is orthogonal to existing prompt tuning methods,\nhence it can improve all of them. Extensive experiments on 11 datasets show the\nstrong flexibility and effectiveness of DePT. Our code and pretrained models\nare available at https://github.com/Koorye/DePT.\n","authors":["Ji Zhang","Shihan Wu","Lianli Gao","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2309.07439v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.12658v1","updated":"2024-03-19T11:48:35Z","published":"2024-03-19T11:48:35Z","title":"Tuning-Free Image Customization with Image and Text Guidance","summary":"  Despite significant advancements in image customization with diffusion\nmodels, current methods still have several limitations: 1) unintended changes\nin non-target areas when regenerating the entire image; 2) guidance solely by a\nreference image or text descriptions; and 3) time-consuming fine-tuning, which\nlimits their practical application. In response, we introduce a tuning-free\nframework for simultaneous text-image-guided image customization, enabling\nprecise editing of specific image regions within seconds. Our approach\npreserves the semantic features of the reference image subject while allowing\nmodification of detailed attributes based on text descriptions. To achieve\nthis, we propose an innovative attention blending strategy that blends\nself-attention features in the UNet decoder during the denoising process. To\nour knowledge, this is the first tuning-free method that concurrently utilizes\ntext and image guidance for image customization in specific regions. Our\napproach outperforms previous methods in both human and quantitative\nevaluations, providing an efficient solution for various practical\napplications, such as image synthesis, design, and creative photography.\n","authors":["Pengzhi Li","Qiang Nie","Ying Chen","Xi Jiang","Kai Wu","Yuhuan Lin","Yong Liu","Jinlong Peng","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.12658v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.10904v2","updated":"2024-03-19T11:37:28Z","published":"2024-03-16T11:38:58Z","title":"Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of\n  Complex Physical Systems","summary":"  Data-driven modeling of complex physical systems is receiving a growing\namount of attention in the simulation and machine learning communities. Since\nmost physical simulations are based on compute-intensive, iterative\nimplementations of differential equation systems, a (partial) replacement with\nlearned, 1-step inference models has the potential for significant speedups in\na wide range of application areas. In this context, we present a novel\nbenchmark for the evaluation of 1-step generative learning models in terms of\nspeed and physical correctness. Our Urban Sound Propagation benchmark is based\non the physically complex and practically relevant, yet intuitively easy to\ngrasp task of modeling the 2d propagation of waves from a sound source in an\nurban environment. We provide a dataset with 100k samples, where each sample\nconsists of pairs of real 2d building maps drawn from OpenStreetmap, a\nparameterized sound source, and a simulated ground truth sound propagation for\nthe given scene. The dataset provides four different simulation tasks with\nincreasing complexity regarding reflection, diffraction and source variance. A\nfirst baseline evaluation of common generative U-Net, GAN and Diffusion models\nshows, that while these models are very well capable of modeling sound\npropagations in simple cases, the approximation of sub-systems represented by\nhigher order equations systematically fails. Information about the dataset,\ndownload instructions and source codes are provided on our website:\nhttps://www.urban-sound-data.org.\n","authors":["Martin Spitznagel","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.10904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11831v2","updated":"2024-03-19T11:31:44Z","published":"2024-03-18T14:43:04Z","title":"BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting","summary":"  While neural rendering has demonstrated impressive capabilities in 3D scene\nreconstruction and novel view synthesis, it heavily relies on high-quality\nsharp images and accurate camera poses. Numerous approaches have been proposed\nto train Neural Radiance Fields (NeRF) with motion-blurred images, commonly\nencountered in real-world scenarios such as low-light or long-exposure\nconditions. However, the implicit representation of NeRF struggles to\naccurately recover intricate details from severely motion-blurred images and\ncannot achieve real-time rendering. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction and real-time\nrendering by explicitly optimizing point clouds as Gaussian spheres.\n  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle\nAdjusted Deblur Gaussian Splatting), which leverages explicit Gaussian\nrepresentation and handles severe motion-blurred images with inaccurate camera\nposes to achieve high-quality scene reconstruction. Our method models the\nphysical image formation process of motion-blurred images and jointly learns\nthe parameters of Gaussians while recovering camera motion trajectories during\nexposure time.\n  In our experiments, we demonstrate that BAD-Gaussians not only achieves\nsuperior rendering quality compared to previous state-of-the-art deblur neural\nrendering methods on both synthetic and real datasets but also enables\nreal-time rendering capabilities.\n  Our project page and source code is available at\nhttps://lingzhezhao.github.io/BAD-Gaussians/\n","authors":["Lingzhe Zhao","Peng Wang","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11831v2.pdf","comment":"Project Page and Source Code:\n  https://lingzhezhao.github.io/BAD-Gaussians/"},{"id":"http://arxiv.org/abs/2403.10942v2","updated":"2024-03-19T11:28:12Z","published":"2024-03-16T14:58:58Z","title":"ScanTalk: 3D Talking Heads from Unregistered Scans","summary":"  Speech-driven 3D talking heads generation has emerged as a significant area\nof interest among researchers, presenting numerous challenges. Existing methods\nare constrained by animating faces with fixed topologies, wherein point-wise\ncorrespondence is established, and the number and order of points remains\nconsistent across all identities the model can animate. In this work, we\npresent ScanTalk, a novel framework capable of animating 3D faces in arbitrary\ntopologies including scanned data. Our approach relies on the DiffusionNet\narchitecture to overcome the fixed topology constraint, offering promising\navenues for more flexible and realistic 3D animations. By leveraging the power\nof DiffusionNet, ScanTalk not only adapts to diverse facial structures but also\nmaintains fidelity when dealing with scanned data, thereby enhancing the\nauthenticity and versatility of generated 3D talking heads. Through\ncomprehensive comparisons with state-of-the-art methods, we validate the\nefficacy of our approach, demonstrating its capacity to generate realistic\ntalking heads comparable to existing techniques. While our primary objective is\nto develop a generic method free from topological constraints, all\nstate-of-the-art methodologies are bound by such limitations. Code for\nreproducing our results, and the pre-trained model will be made available.\n","authors":["Federico Nocentini","Thomas Besnier","Claudio Ferrari","Sylvain Arguillere","Stefano Berretti","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2403.10942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12547v2","updated":"2024-03-19T11:09:12Z","published":"2023-10-19T07:54:30Z","title":"PGA: Personalizing Grasping Agents with Single Human-Robot Interaction","summary":"  Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that\ncomprehend and grasp objects based on natural language instructions. While the\nability to understand personal objects like my wallet facilitates more natural\ninteraction with human users, current LCRG systems only allow generic language\ninstructions, e.g., the black-colored wallet next to the laptop. To this end,\nwe introduce a task scenario GraspMine alongside a novel dataset aimed at\npinpointing and grasping personal objects given personal indicators via\nlearning from a single human-robot interaction, rather than a large labeled\ndataset. Our proposed method, Personalized Grasping Agent (PGA), addresses\nGraspMine by leveraging the unlabeled image data of the user's environment,\ncalled Reminiscence. Specifically, PGA acquires personal object information by\na user presenting a personal object with its associated indicator, followed by\nPGA inspecting the object by rotating it. Based on the acquired information,\nPGA pseudo-labels objects in the Reminiscence by our proposed label propagation\nalgorithm. Harnessing the information acquired from the interactions and the\npseudo-labeled objects in the Reminiscence, PGA adapts the object grounding\nmodel to grasp personal objects. This results in significant efficiency while\nprevious LCRG systems rely on resource-intensive human annotations --\nnecessitating hundreds of labeled data to learn my wallet. Moreover, PGA\noutperforms baseline methods across all metrics and even shows comparable\nperformance compared to the fully-supervised method, which learns from 9k\nannotated data samples. We further validate PGA's real-world applicability by\nemploying a physical robot to execute GrsapMine. Code and data are publicly\navailable at https://github.com/JHKim-snu/PGA.\n","authors":["Junghyun Kim","Gi-Cheon Kang","Jaein Kim","Seoyun Yang","Minjoon Jung","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12547v2.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2312.13316v3","updated":"2024-03-19T11:01:35Z","published":"2023-12-20T11:00:54Z","title":"ECAMP: Entity-centered Context-aware Medical Vision Language\n  Pre-training","summary":"  Despite significant advancements in medical vision-language pre-training,\nexisting methods have largely overlooked the inherent entity-specific context\nwithin radiology reports and the complex cross-modality contextual\nrelationships between text and images. To close this gap, we propose a novel\nEntity-centered Context-aware Medical Vision-language Pre-training (ECAMP)\nframework, which is designed to enable a more entity-centered and\ncontext-sensitive interpretation of medical data. Utilizing the recent powerful\nlarge language model, we distill entity-centered context from medical reports,\nwhich enables ECAMP to gain more effective supervision from the text modality.\nBy further pre-training our model with carefully designed entity-aware,\ncontext-enhanced masked language modeling and context-guided super-resolution\ntasks, ECAMP significantly refines the interplay between text and image\nmodalities, leading to an enhanced ability to extract entity-centered\ncontextual features. Besides, our proposed multi-scale context fusion design\nalso improves the semantic integration of both coarse and fine-level image\nrepresentations, prompting better performance for multi-scale downstream\napplications. Combining these components leads to significant performance leaps\nover current state-of-the-art methods and establishes a new standard for\ncross-modality learning in medical imaging, whose effectiveness is demonstrated\nby our extensive experiments on various tasks including classification,\nsegmentation, and detection across several public datasets. Code and models are\navailable at https://github.com/ToniChopp/ECAMP.\n","authors":["Rongsheng Wang","Qingsong Yao","Haoran Lai","Zhiyang He","Xiaodong Tao","Zihang Jiang","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.13316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09749v2","updated":"2024-03-19T10:56:12Z","published":"2022-03-18T05:17:00Z","title":"Goal-conditioned dual-action imitation learning for dexterous dual-arm\n  robot manipulation","summary":"  Long-horizon dexterous robot manipulation of deformable objects, such as\nbanana peeling, is a problematic task because of the difficulties in object\nmodeling and a lack of knowledge about stable and dexterous manipulation\nskills. This paper presents a goal-conditioned dual-action (GC-DA) deep\nimitation learning (DIL) approach that can learn dexterous manipulation skills\nusing human demonstration data. Previous DIL methods map the current sensory\ninput and reactive action, which often fails because of compounding errors in\nimitation learning caused by the recurrent computation of actions. The method\npredicts reactive action only when the precise manipulation of the target\nobject is required (local action) and generates the entire trajectory when\nprecise manipulation is not required (global action). This dual-action\nformulation effectively prevents compounding error in the imitation learning\nusing the trajectory-based global action while responding to unexpected changes\nin the target object during the reactive local action. The proposed method was\ntested in a real dual-arm robot and successfully accomplished the\nbanana-peeling task.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2203.09749v2.pdf","comment":"19 pages, published in Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2310.10325v2","updated":"2024-03-19T09:54:41Z","published":"2023-10-16T12:08:35Z","title":"Towards image compression with perfect realism at ultra-low bitrates","summary":"  Image codecs are typically optimized to trade-off bitrate \\vs distortion\nmetrics. At low bitrates, this leads to compression artefacts which are easily\nperceptible, even when training with perceptual or adversarial losses. To\nimprove image quality and remove dependency on the bitrate, we propose to\ndecode with iterative diffusion models. We condition the decoding process on a\nvector-quantized image representation, as well as a global image description to\nprovide additional context. We dub our model PerCo for 'perceptual\ncompression', and compare it to state-of-the-art codecs at rates from 0.1 down\nto 0.003 bits per pixel. The latter rate is more than an order of magnitude\nsmaller than those considered in most prior work, compressing a 512x768 Kodak\nimage with less than 153 bytes. Despite this ultra-low bitrate, our approach\nmaintains the ability to reconstruct realistic images. We find that our model\nleads to reconstructions with state-of-the-art visual quality as measured by\nFID and KID. As predicted by rate-distortion-perception theory, visual quality\nis less dependent on the bitrate than previous methods.\n","authors":["Marlène Careil","Matthew J. Muckley","Jakob Verbeek","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2310.10325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08255v2","updated":"2024-03-19T09:49:01Z","published":"2023-12-13T16:18:40Z","title":"OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep\n  Learning Methods","summary":"  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n","authors":["Mikhail Kulyabin","Aleksei Zhdanov","Anastasia Nikiforova","Andrey Stepichev","Anna Kuznetsova","Mikhail Ronkin","Vasilii Borisov","Alexander Bogachev","Sergey Korotkich","Paul A Constable","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2312.08255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12585v1","updated":"2024-03-19T09:47:08Z","published":"2024-03-19T09:47:08Z","title":"LASPA: Latent Spatial Alignment for Fast Training-free Single Image\n  Editing","summary":"  We present a novel, training-free approach for textual editing of real images\nusing diffusion models. Unlike prior methods that rely on computationally\nexpensive finetuning, our approach leverages LAtent SPatial Alignment (LASPA)\nto efficiently preserve image details. We demonstrate how the diffusion process\nis amenable to spatial guidance using a reference image, leading to\nsemantically coherent edits. This eliminates the need for complex optimization\nand costly model finetuning, resulting in significantly faster editing compared\nto previous methods. Additionally, our method avoids the storage requirements\nassociated with large finetuned models. These advantages make our approach\nparticularly well-suited for editing on mobile devices and applications\ndemanding rapid response times. While simple and fast, our method achieves\n62-71\\% preference in a user-study and significantly better model-based editing\nstrength and image preservation scores.\n","authors":["Yazeed Alharbi","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2403.12585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12580v1","updated":"2024-03-19T09:44:41Z","published":"2024-03-19T09:44:41Z","title":"Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile\n  Industrial Anomaly Detection","summary":"  Industrial anomaly detection (IAD) has garnered significant attention and\nexperienced rapid development. However, the recent development of IAD approach\nhas encountered certain difficulties due to dataset limitations. On the one\nhand, most of the state-of-the-art methods have achieved saturation (over 99%\nin AUROC) on mainstream datasets such as MVTec, and the differences of methods\ncannot be well distinguished, leading to a significant gap between public\ndatasets and actual application scenarios. On the other hand, the research on\nvarious new practical anomaly detection settings is limited by the scale of the\ndataset, posing a risk of overfitting in evaluation results. Therefore, we\npropose a large-scale, Real-world, and multi-view Industrial Anomaly Detection\ndataset, named Real-IAD, which contains 150K high-resolution images of 30\ndifferent objects, an order of magnitude larger than existing datasets. It has\na larger range of defect area and ratio proportions, making it more challenging\nthan previous datasets. To make the dataset closer to real application\nscenarios, we adopted a multi-view shooting method and proposed sample-level\nevaluation metrics. In addition, beyond the general unsupervised anomaly\ndetection setting, we propose a new setting for Fully Unsupervised Industrial\nAnomaly Detection (FUIAD) based on the observation that the yield rate in\nindustrial production is usually greater than 60%, which has more practical\napplication value. Finally, we report the results of popular IAD methods on the\nReal-IAD dataset, providing a highly challenging benchmark to promote the\ndevelopment of the IAD field.\n","authors":["Chengjie Wang","Wenbing Zhu","Bin-Bin Gao","Zhenye Gan","Jianning Zhang","Zhihao Gu","Shuguang Qian","Mingang Chen","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.12580v1.pdf","comment":"It is accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.12574v1","updated":"2024-03-19T09:34:11Z","published":"2024-03-19T09:34:11Z","title":"EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based\n  Detection with Recurrent Spiking Neural Networks","summary":"  Event cameras, with their high dynamic range and temporal resolution, are\nideally suited for object detection, especially under scenarios with motion\nblur and challenging lighting conditions. However, while most existing\napproaches prioritize optimizing spatiotemporal representations with advanced\ndetection backbones and early aggregation functions, the crucial issue of\nadaptive event sampling remains largely unaddressed. Spiking Neural Networks\n(SNNs), which operate on an event-driven paradigm through sparse spike\ncommunication, emerge as a natural fit for addressing this challenge. In this\nstudy, we discover that the neural dynamics of spiking neurons align closely\nwith the behavior of an ideal temporal event sampler. Motivated by this\ninsight, we propose a novel adaptive sampling module that leverages recurrent\nconvolutional SNNs enhanced with temporal memory, facilitating a fully\nend-to-end learnable framework for event-based detection. Additionally, we\nintroduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to\nregulate potential distribution and address performance degradation encountered\nin spike-based sampling modules. Through rigorous testing on neuromorphic\ndatasets for event-based detection, our approach demonstrably surpasses\nexisting state-of-the-art spike-based methods, achieving superior performance\nwith significantly fewer parameters and time steps. For instance, our method\nachieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\%\nfewer parameters and three time steps. Moreover, the applicability and\neffectiveness of our adaptive sampling methodology extend beyond SNNs, as\ndemonstrated through further validation on conventional non-spiking detection\nmodels.\n","authors":["Ziming Wang","Ziling Wang","Huaning Li","Lang Qin","Runhao Jiang","De Ma","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12573v1","updated":"2024-03-19T09:33:07Z","published":"2024-03-19T09:33:07Z","title":"Lifting Multi-View Detection and Tracking to the Bird's Eye View","summary":"  Taking advantage of multi-view aggregation presents a promising solution to\ntackle challenges such as occlusion and missed detection in multi-object\ntracking and detection. Recent advancements in multi-view detection and 3D\nobject recognition have significantly improved performance by strategically\nprojecting all views onto the ground plane and conducting detection analysis\nfrom a Bird's Eye View. In this paper, we compare modern lifting methods, both\nparameter-free and parameterized, to multi-view aggregation. Additionally, we\npresent an architecture that aggregates the features of multiple times steps to\nlearn robust detection and combines appearance- and motion-based cues for\ntracking. Most current tracking approaches either focus on pedestrians or\nvehicles. In our work, we combine both branches and add new challenges to\nmulti-view detection with cross-scene setups. Our method generalizes to three\npublic datasets across two domains: (1) pedestrian: Wildtrack and MultiviewX,\nand (2) roadside perception: Synthehicle, achieving state-of-the-art\nperformance in detection and tracking. https://github.com/tteepe/TrackTacular\n","authors":["Torben Teepe","Philipp Wolters","Johannes Gilg","Fabian Herzog","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2403.12573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12572v1","updated":"2024-03-19T09:30:56Z","published":"2024-03-19T09:30:56Z","title":"Compound Expression Recognition via Multi Model Ensemble","summary":"  Compound Expression Recognition (CER) plays a crucial role in interpersonal\ninteractions. Due to the existence of Compound Expressions , human emotional\nexpressions are complex, requiring consideration of both local and global\nfacial expressions to make judgments. In this paper, to address this issue, we\npropose a solution based on ensemble learning methods for Compound Expression\nRecognition. Specifically, our task is classification, where we train three\nexpression classification models based on convolutional networks, Vision\nTransformers, and multi-scale local attention networks. Then, through model\nensemble using late fusion, we merge the outputs of multiple models to predict\nthe final result. Our method achieves high accuracy on RAF-DB and is able to\nrecognize expressions through zero-shot on certain portions of C-EXPR-DB.\n","authors":["Jun Yu","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12570v1","updated":"2024-03-19T09:28:19Z","published":"2024-03-19T09:28:19Z","title":"Adapting Visual-Language Models for Generalizable Anomaly Detection in\n  Medical Images","summary":"  Recent advancements in large-scale visual-language pre-trained models have\nled to significant progress in zero-/few-shot anomaly detection within natural\nimage domains. However, the substantial domain divergence between natural and\nmedical images limits the effectiveness of these methodologies in medical\nanomaly detection. This paper introduces a novel lightweight multi-level\nadaptation and comparison framework to repurpose the CLIP model for medical\nanomaly detection. Our approach integrates multiple residual adapters into the\npre-trained visual encoder, enabling a stepwise enhancement of visual features\nacross different levels. This multi-level adaptation is guided by multi-level,\npixel-wise visual-language feature alignment loss functions, which recalibrate\nthe model's focus from object semantics in natural imagery to anomaly\nidentification in medical images. The adapted features exhibit improved\ngeneralization across various medical data types, even in zero-shot scenarios\nwhere the model encounters unseen medical modalities and anatomical regions\nduring training. Our experiments on medical anomaly detection benchmarks\ndemonstrate that our method significantly surpasses current state-of-the-art\nmodels, with an average AUC improvement of 6.24% and 7.33% for anomaly\nclassification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot\nand few-shot settings, respectively. Source code is available at:\nhttps://github.com/MediaBrain-SJTU/MVFA-AD\n","authors":["Chaoqin Huang","Aofan Jiang","Jinghao Feng","Ya Zhang","Xinchao Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12570v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12559v1","updated":"2024-03-19T09:14:52Z","published":"2024-03-19T09:14:52Z","title":"Confidence Self-Calibration for Multi-Label Class-Incremental Learning","summary":"  The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL)\narises when only the new classes are labeled during training, while past and\nfuture labels remain unavailable. This issue leads to a proliferation of\nfalse-positive errors due to erroneously high confidence multi-label\npredictions, exacerbating catastrophic forgetting within the disjoint label\nspace. In this paper, we aim to refine multi-label confidence calibration in\nMLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for\nlabel relationship calibration, we introduce a class-incremental graph\nconvolutional network that bridges the isolated label spaces by constructing\nlearnable, dynamically extended label relationship graph. Then, for confidence\ncalibration, we present a max-entropy regularization for each multi-label\nincrement, facilitating confidence self-calibration through the penalization of\nover-confident output distributions. Our approach attains new state-of-the-art\nresults in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the\ncalibration of label confidences confirmed through our methodology.\n","authors":["Kaile Du","Yifan Zhou","Fan Lyu","Yuyang Li","Chen Lu","Guangcan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06731v2","updated":"2024-03-19T09:13:22Z","published":"2023-12-11T09:44:41Z","title":"Genixer: Empowering Multimodal Large Language Models as a Powerful Data\n  Generator","summary":"  Instruction tuning data is essential for training the Multimodal Large\nLanguage Models (MLLMs). However, the creation of high-quality instruction\ntuning data presents significant challenges. Prior methods that depended on\nGPT-4 for data generation were not only costly but also lacked satisfactory\nperformance in complex tasks (i.e., grounding-based reasoning tasks). To\naddress these issues, we developed an innovative data generation pipeline,\nGenixer, to generate various high-quality instruction tuning data, including\nnine representative tasks, e.g., Common VQA, REC, REG, and PointQ.\nSpecifically, Genixer provides a unified solution with four key steps for\nalleviating the difficulty of data generation: (i) instruction data collection,\n(ii) instruction template design, (iii) empowering MLLM, and (iv) data\ngeneration and filtering. Subsequently, the superior qualitative results of our\nGenixer demonstrate that current MLLMs have a strong potential to evolve into\npowerful data generators. Additionally, to validate the efficacy of generated\ndata quantitatively, we add the instruction tuning data produced by Genixer\ninto the training of two representative MLLMs and observe the consistent\nimprovements on various VQA tasks and multimodal benchmarks.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.06731v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2312.10103v2","updated":"2024-03-19T09:13:19Z","published":"2023-12-15T02:54:31Z","title":"GSVA: Generalized Segmentation via Multimodal Large Language Models","summary":"  Generalized Referring Expression Segmentation (GRES) extends the scope of\nclassic RES to refer to multiple objects in one expression or identify the\nempty targets absent in the image. GRES poses challenges in modeling the\ncomplex spatial relationships of the instances in the image and identifying\nnon-existing referents. Multimodal Large Language Models (MLLMs) have recently\nshown tremendous progress in these complicated vision-language tasks.\nConnecting Large Language Models (LLMs) and vision models, MLLMs are proficient\nin understanding contexts with visual inputs. Among them, LISA, as a\nrepresentative, adopts a special [SEG] token to prompt a segmentation mask\ndecoder, e.g., SAM, to enable MLLMs in the RES task. However, existing\nsolutions to GRES remain unsatisfactory since current segmentation MLLMs cannot\ncorrectly handle the cases where users might reference multiple subjects in a\nsingular prompt or provide descriptions incongruent with any image target. In\nthis paper, we propose Generalized Segmentation Vision Assistant (GSVA) to\naddress this gap. Specifically, GSVA reuses the [SEG] token to prompt the\nsegmentation model towards supporting multiple mask references simultaneously\nand innovatively learns to generate a [REJ] token to reject the null targets\nexplicitly. Experiments validate GSVA's efficacy in resolving the GRES issue,\nmarking a notable enhancement and setting a new record on the GRES benchmark\ngRefCOCO dataset. GSVA also proves effective across various classic referring\nsegmentation and comprehension tasks.\n","authors":["Zhuofan Xia","Dongchen Han","Yizeng Han","Xuran Pan","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.10103v2.pdf","comment":"Accepted by CVPR2024 (19 pages, 9 figures, 11 tables)"},{"id":"http://arxiv.org/abs/2403.11370v2","updated":"2024-03-19T09:12:00Z","published":"2024-03-17T23:23:40Z","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic\n  Environments using Graph Neural Networks","summary":"  The assumption of a static environment is common in many geometric computer\nvision tasks like SLAM but limits their applicability in highly dynamic scenes.\nSince these tasks rely on identifying point correspondences between input\nimages within the static part of the environment, we propose a graph neural\nnetwork-based sparse feature matching network designed to perform robust\nmatching under challenging conditions while excluding keypoints on moving\nobjects. We employ a similar scheme of attentional aggregation over graph edges\nto enhance keypoint representations as state-of-the-art feature-matching\nnetworks but augment the graph with epipolar and temporal information and\nvastly reduce the number of graph edges. Furthermore, we introduce a\nself-supervised training scheme to extract pseudo labels for image pairs in\ndynamic environments from exclusively unprocessed visual-inertial data. A\nseries of experiments show the superior performance of our network as it\nexcludes keypoints on moving objects compared to state-of-the-art feature\nmatching networks while still achieving similar results regarding conventional\nmatching metrics. When integrated into a SLAM system, our network significantly\nimproves performance, especially in highly dynamic scenes.\n","authors":["Theresa Huber","Simon Schaefer","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.11370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16607v2","updated":"2024-03-19T08:58:17Z","published":"2024-02-26T14:40:15Z","title":"GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos","summary":"  In this paper, we present a novel method that facilitates the creation of\nvivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation\nlies in addressing the intricate challenges of delivering high-fidelity human\nbody reconstructions and aligning 3D Gaussians with human skin surfaces\naccurately. The key contributions of this paper are twofold. Firstly, we\nintroduce a pose refinement technique to improve hand and foot pose accuracy by\naligning normal maps and silhouettes. Precise pose is crucial for correct shape\nand appearance reconstruction. Secondly, we address the problems of unbalanced\naggregation and initialization bias that previously diminished the quality of\n3D Gaussian avatars, through a novel surface-guided re-initialization method\nthat ensures accurate alignment of 3D Gaussian points with avatar surfaces.\nExperimental results demonstrate that our proposed method achieves\nhigh-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive\nexperimental analyses validate the performance qualitatively and\nquantitatively, demonstrating that it achieves state-of-the-art performance in\nphoto-realistic novel view synthesis while offering fine-grained control over\nthe human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.\n","authors":["Xinqi Liu","Chenming Wu","Jialun Liu","Xing Liu","Jinbo Wu","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10103v2","updated":"2024-03-19T08:56:44Z","published":"2024-03-15T08:48:37Z","title":"DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video","summary":"  Recent advancements in dynamic neural radiance field methods have yielded\nremarkable outcomes. However, these approaches rely on the assumption of sharp\ninput images. When faced with motion blur, existing dynamic NeRF methods often\nstruggle to generate high-quality novel views. In this paper, we propose\nDyBluRF, a dynamic radiance field approach that synthesizes sharp novel views\nfrom a monocular video affected by motion blur. To account for motion blur in\ninput images, we simultaneously capture the camera trajectory and object\nDiscrete Cosine Transform (DCT) trajectories within the scene. Additionally, we\nemploy a global cross-time rendering approach to ensure consistent temporal\ncoherence across the entire scene. We curate a dataset comprising diverse\ndynamic scenes that are specifically tailored for our task. Experimental\nresults on our dataset demonstrate that our method outperforms existing\napproaches in generating sharp novel views from motion-blurred inputs while\nmaintaining spatial-temporal consistency of the scene.\n","authors":["Huiqiang Sun","Xingyi Li","Liao Shen","Xinyi Ye","Ke Xian","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2403.10103v2.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://huiqiang-sun.github.io/dyblurf/"},{"id":"http://arxiv.org/abs/2403.12552v1","updated":"2024-03-19T08:54:52Z","published":"2024-03-19T08:54:52Z","title":"M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for\n  Autonomous Driving","summary":"  End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.\n","authors":["Dongyang Xu","Haokun Li","Qingfan Wang","Ziying Song","Lei Chen","Hanming Deng"],"pdf_url":"https://arxiv.org/pdf/2403.12552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10787v2","updated":"2024-03-19T08:51:51Z","published":"2023-09-19T17:35:16Z","title":"AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual\n  Representation Models","summary":"  Audio-visual representation learning aims to develop systems with human-like\nperception by utilizing correlation between auditory and visual information.\nHowever, current models often focus on a limited set of tasks, and\ngeneralization abilities of learned representations are unclear. To this end,\nwe propose the AV-SUPERB benchmark that enables general-purpose evaluation of\nunimodal audio/visual and bimodal fusion representations on 7 datasets covering\n5 audio-visual tasks in speech and audio processing. We evaluate 5 recent\nself-supervised models and show that none of these models generalize to all\ntasks, emphasizing the need for future study on improving universal model\nperformance. In addition, we show that representations may be improved with\nintermediate-task fine-tuning and audio event classification with AudioSet\nserves as a strong intermediate task. We release our benchmark with evaluation\ncode and a model submission platform to encourage further research in\naudio-visual learning.\n","authors":["Yuan Tseng","Layne Berry","Yi-Ting Chen","I-Hsiang Chiu","Hsuan-Hao Lin","Max Liu","Puyuan Peng","Yi-Jen Shih","Hung-Yu Wang","Haibin Wu","Po-Yao Huang","Chun-Mao Lai","Shang-Wen Li","David Harwath","Yu Tsao","Shinji Watanabe","Abdelrahman Mohamed","Chi-Luen Feng","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.10787v2.pdf","comment":"Accepted to ICASSP 2024; Evaluation Code:\n  https://github.com/roger-tseng/av-superb Submission Platform:\n  https://av.superbbenchmark.org"},{"id":"http://arxiv.org/abs/2403.12550v1","updated":"2024-03-19T08:49:48Z","published":"2024-03-19T08:49:48Z","title":"RGBD GS-ICP SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) with dense representation plays\na key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)\napplications. Recent advancements in dense representation SLAM have highlighted\nthe potential of leveraging neural scene representation and 3D Gaussian\nrepresentation for high-fidelity spatial representation. In this paper, we\npropose a novel dense representation SLAM approach with a fusion of Generalized\nIterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast\nto existing methods, we utilize a single Gaussian map for both tracking and\nmapping, resulting in mutual benefits. Through the exchange of covariances\nbetween tracking and mapping processes with scale alignment techniques, we\nminimize redundant computations and achieve an efficient system. Additionally,\nwe enhance tracking accuracy and mapping quality through our keyframe selection\nmethods. Experimental results demonstrate the effectiveness of our approach,\nshowing an incredibly fast speed up to 107 FPS (for the entire system) and\nsuperior quality of the reconstructed map.\n","authors":["Seongbo Ha","Jiung Yeon","Hyeonwoo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18639v3","updated":"2024-03-19T08:45:53Z","published":"2023-10-28T08:48:44Z","title":"Towards Plastic and Stable Exemplar-Free Incremental Learning: A\n  Dual-Learner Framework with Cumulative Parameter Averaging","summary":"  The dilemma between plasticity and stability presents a significant challenge\nin Incremental Learning (IL), especially in the exemplar-free scenario where\naccessing old-task samples is strictly prohibited during the learning of a new\ntask. A straightforward solution to this issue is learning and storing an\nindependent model for each task, known as Single Task Learning (STL). Despite\nthe linear growth in model storage with the number of tasks in STL, we\nempirically discover that averaging these model parameters can potentially\npreserve knowledge across all tasks. Inspired by this observation, we propose a\nDual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA\nemploys a dual-learner design: a plastic learner focused on acquiring new-task\nknowledge and a stable learner responsible for accumulating all learned\nknowledge. The knowledge from the plastic learner is transferred to the stable\nlearner via cumulative parameter averaging. Additionally, several task-specific\nclassifiers work in cooperation with the stable learner to yield the final\nprediction. Specifically, when learning a new task, these modules are updated\nin a cyclic manner: i) the plastic learner is initially optimized using a\nself-supervised loss besides the supervised loss to enhance the feature\nextraction robustness; ii) the stable learner is then updated with respect to\nthe plastic learner in a cumulative parameter averaging manner to maintain its\ntask-wise generalization; iii) the task-specific classifier is accordingly\noptimized to align with the stable learner. Experimental results on CIFAR-100\nand Tiny-ImageNet show that DLCPA outperforms several state-of-the-art\nexemplar-free baselines in both Task-IL and Class-IL settings.\n","authors":["Wenju Sun","Qingyong Li","Wen Wang","Yangli-ao Geng"],"pdf_url":"https://arxiv.org/pdf/2310.18639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17200v3","updated":"2024-03-19T08:41:00Z","published":"2024-02-27T04:37:04Z","title":"Enhancing Quality of Compressed Images by Mitigating Enhancement Bias\n  Towards Compression Domain","summary":"  Existing quality enhancement methods for compressed images focus on aligning\nthe enhancement domain with the raw domain to yield realistic images. However,\nthese methods exhibit a pervasive enhancement bias towards the compression\ndomain, inadvertently regarding it as more realistic than the raw domain. This\nbias makes enhanced images closely resemble their compressed counterparts, thus\ndegrading their perceptual quality. In this paper, we propose a simple yet\neffective method to mitigate this bias and enhance the quality of compressed\nimages. Our method employs a conditional discriminator with the compressed\nimage as a key condition, and then incorporates a domain-divergence\nregularization to actively distance the enhancement domain from the compression\ndomain. Through this dual strategy, our method enables the discrimination\nagainst the compression domain, and brings the enhancement domain closer to the\nraw domain. Comprehensive quality evaluations confirm the superiority of our\nmethod over other state-of-the-art methods without incurring inference\noverheads.\n","authors":["Qunliang Xing","Mai Xu","Shengxi Li","Xin Deng","Meisong Zheng","Huaida Liu","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17200v3.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12543v1","updated":"2024-03-19T08:40:19Z","published":"2024-03-19T08:40:19Z","title":"HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free\n  Matching","summary":"  Deep learning-based image matching methods play a crucial role in computer\nvision, yet they often suffer from substantial computational demands. To tackle\nthis challenge, we present HCPM, an efficient and detector-free local\nfeature-matching method that employs hierarchical pruning to optimize the\nmatching pipeline. In contrast to recent detector-free methods that depend on\nan exhaustive set of coarse-level candidates for matching, HCPM selectively\nconcentrates on a concise subset of informative candidates, resulting in fewer\ncomputational candidates and enhanced matching efficiency. The method comprises\na self-pruning stage for selecting reliable candidates and an\ninteractive-pruning stage that identifies correlated patches at the coarse\nlevel. Our results reveal that HCPM significantly surpasses existing methods in\nterms of speed while maintaining high accuracy. The source code will be made\navailable upon publication.\n","authors":["Ying Chen","Yong Liu","Kai Wu","Qiang Nie","Shang Xu","Huifang Ma","Bing Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04962v2","updated":"2024-03-19T08:28:46Z","published":"2023-04-11T04:12:31Z","title":"Mask-Based Modeling for Neural Radiance Fields","summary":"  Most Neural Radiance Fields (NeRFs) exhibit limited generalization\ncapabilities, which restrict their applicability in representing multiple\nscenes using a single model. To address this problem, existing generalizable\nNeRF methods simply condition the model on image features. These methods still\nstruggle to learn precise global representations over diverse scenes since they\nlack an effective mechanism for interacting among different points and views.\nIn this work, we unveil that 3D implicit representation learning can be\nsignificantly improved by mask-based modeling. Specifically, we propose masked\nray and view modeling for generalizable NeRF (MRVM-NeRF), which is a\nself-supervised pretraining target to predict complete scene representations\nfrom partially masked features along each ray. With this pretraining target,\nMRVM-NeRF enables better use of correlations across different points and views\nas the geometry priors, which thereby strengthens the capability of capturing\nintricate details within the scenes and boosts the generalization capability\nacross different scenes. Extensive experiments demonstrate the effectiveness of\nour proposed MRVM-NeRF on both synthetic and real-world datasets, qualitatively\nand quantitatively. Besides, we also conduct experiments to show the\ncompatibility of our proposed method with various backbones and its superiority\nunder few-shot cases.\n","authors":["Ganlin Yang","Guoqiang Wei","Zhizheng Zhang","Yan Lu","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2304.04962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12537v1","updated":"2024-03-19T08:23:12Z","published":"2024-03-19T08:23:12Z","title":"Prompt-Guided Adaptive Model Transformation for Whole Slide Image\n  Classification","summary":"  Multiple instance learning (MIL) has emerged as a popular method for\nclassifying histopathology whole slide images (WSIs). Existing approaches\ntypically rely on frozen pre-trained models to extract instance features,\nneglecting the substantial domain shift between pre-training natural and\nhistopathological images. To address this issue, we propose PAMT, a novel\nPrompt-guided Adaptive Model Transformation framework that enhances MIL\nclassification performance by seamlessly adapting pre-trained models to the\nspecific characteristics of histopathology data. To capture the intricate\nhistopathology distribution, we introduce Representative Patch Sampling (RPS)\nand Prototypical Visual Prompt (PVP) to reform the input data, building a\ncompact while informative representation. Furthermore, to narrow the domain\ngap, we introduce Adaptive Model Transformation (AMT) that integrates adapter\nblocks within the feature extraction pipeline, enabling the pre-trained models\nto learn domain-specific features. We rigorously evaluate our approach on two\npublicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial\nimprovements across various MIL models. Our findings affirm the potential of\nPAMT to set a new benchmark in WSI classification, underscoring the value of a\ntargeted reprogramming approach.\n","authors":["Yi Lin","Zhengjie Zhu","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.12537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01166v2","updated":"2024-03-19T08:22:42Z","published":"2024-02-02T06:20:44Z","title":"A Comprehensive Survey on 3D Content Generation","summary":"  Recent years have witnessed remarkable advances in artificial intelligence\ngenerated content(AIGC), with diverse input modalities, e.g., text, image,\nvideo, audio and 3D. The 3D is the most close visual modality to real-world 3D\nenvironment and carries enormous knowledge. The 3D content generation shows\nboth academic and practical values while also presenting formidable technical\nchallenges. This review aims to consolidate developments within the burgeoning\ndomain of 3D content generation. Specifically, a new taxonomy is proposed that\ncategorizes existing approaches into three types: 3D native generative methods,\n2D prior-based 3D generative methods, and hybrid 3D generative methods. The\nsurvey covers approximately 60 papers spanning the major techniques. Besides,\nwe discuss limitations of current 3D content generation techniques, and point\nout open challenges as well as promising directions for future work.\nAccompanied with this survey, we have established a project website where the\nresources on 3D content generation research are provided. The project page is\navailable at https://github.com/hitcslj/Awesome-AIGC-3D.\n","authors":["Jian Liu","Xiaoshui Huang","Tianyu Huang","Lu Chen","Yuenan Hou","Shixiang Tang","Ziwei Liu","Wanli Ouyang","Wangmeng Zuo","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01166v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2403.12536v1","updated":"2024-03-19T08:21:54Z","published":"2024-03-19T08:21:54Z","title":"Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping\n  with Multi-maps","summary":"  In this paper, we introduce Vox-Fusion++, a multi-maps-based robust dense\ntracking and mapping system that seamlessly fuses neural implicit\nrepresentations with traditional volumetric fusion techniques. Building upon\nthe concept of implicit mapping and positioning systems, our approach extends\nits applicability to real-world scenarios. Our system employs a voxel-based\nneural implicit surface representation, enabling efficient encoding and\noptimization of the scene within each voxel. To handle diverse environments\nwithout prior knowledge, we incorporate an octree-based structure for scene\ndivision and dynamic expansion. To achieve real-time performance, we propose a\nhigh-performance multi-process framework. This ensures the system's suitability\nfor applications with stringent time constraints. Additionally, we adopt the\nidea of multi-maps to handle large-scale scenes, and leverage loop detection\nand hierarchical pose optimization strategies to reduce long-term pose drift\nand remove duplicate geometry. Through comprehensive evaluations, we\ndemonstrate that our method outperforms previous methods in terms of\nreconstruction quality and accuracy across various scenarios. We also show that\nour Vox-Fusion++ can be used in augmented reality and collaborative mapping\napplications. Our source code will be publicly available at\n\\url{https://github.com/zju3dv/Vox-Fusion_Plus_Plus}\n","authors":["Hongjia Zhai","Hai Li","Xingrui Yang","Gan Huang","Yuhang Ming","Hujun Bao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12536v1.pdf","comment":"14 pages. arXiv admin note: text overlap with arXiv:2210.15858"},{"id":"http://arxiv.org/abs/2403.11077v2","updated":"2024-03-19T08:21:02Z","published":"2024-03-17T04:02:39Z","title":"Zippo: Zipping Color and Transparency Distributions into a Single\n  Diffusion Model","summary":"  Beyond the superiority of the text-to-image diffusion model in generating\nhigh-quality images, recent studies have attempted to uncover its potential for\nadapting the learned semantic knowledge to visual perception tasks. In this\nwork, instead of translating a generative diffusion model into a visual\nperception model, we explore to retain the generative ability with the\nperceptive adaptation. To accomplish this, we present Zippo, a unified\nframework for zipping the color and transparency distributions into a single\ndiffusion model by expanding the diffusion latent into a joint representation\nof RGB images and alpha mattes. By alternatively selecting one modality as the\ncondition and then applying the diffusion process to the counterpart modality,\nZippo is capable of generating RGB images from alpha mattes and predicting\ntransparency from input images. In addition to single-modality prediction, we\npropose a modality-aware noise reassignment strategy to further empower Zippo\nwith jointly generating RGB images and its corresponding alpha mattes under the\ntext guidance. Our experiments showcase Zippo's ability of efficient\ntext-conditioned transparent image generation and present plausible results of\nMatte-to-RGB and RGB-to-Matte translation.\n","authors":["Kangyang Xie","Binbin Yang","Hao Chen","Meng Wang","Cheng Zou","Hui Xue","Ming Yang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2403.11077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12535v1","updated":"2024-03-19T08:19:53Z","published":"2024-03-19T08:19:53Z","title":"High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided\n  Densification and Regularized Optimization","summary":"  We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that\nprovides metrically accurate pose tracking and visually realistic\nreconstruction. To this end, we first propose a Gaussian densification strategy\nbased on the rendering loss to map unobserved areas and refine reobserved\nareas. Second, we introduce extra regularization parameters to alleviate the\nforgetting problem in the continuous mapping problem, where parameters tend to\noverfit the latest frame and result in decreasing rendering quality for\nprevious frames. Both mapping and tracking are performed with Gaussian\nparameters by minimizing re-rendering loss in a differentiable way. Compared to\nrecent neural and concurrently developed gaussian splatting RGBD SLAM\nbaselines, our method achieves state-of-the-art results on the synthetic\ndataset Replica and competitive results on the real-world dataset TUM.\n","authors":["Shuo Sun","Malcolm Mielle","Achim J. Lilienthal","Martin Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12535v1.pdf","comment":"submitted to IROS24"},{"id":"http://arxiv.org/abs/2403.12534v1","updated":"2024-03-19T08:15:53Z","published":"2024-03-19T08:15:53Z","title":"ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation\n  for Event-based Action Recognition and More","summary":"  Event cameras have recently been shown beneficial for practical vision tasks,\nsuch as action recognition, thanks to their high temporal resolution, power\nefficiency, and reduced privacy concerns. However, current research is hindered\nby 1) the difficulty in processing events because of their prolonged duration\nand dynamic actions with complex and ambiguous semantics and 2) the redundant\naction depiction of the event frame representation with fixed stacks. We find\nlanguage naturally conveys abundant semantic information, rendering it\nstunningly superior in reducing semantic uncertainty. In light of this, we\npropose ExACT, a novel approach that, for the first time, tackles event-based\naction recognition from a cross-modal conceptualizing perspective. Our ExACT\nbrings two technical contributions. Firstly, we propose an adaptive\nfine-grained event (AFE) representation to adaptively filter out the repeated\nevents for the stationary objects while preserving dynamic ones. This subtly\nenhances the performance of ExACT without extra computational cost. Then, we\npropose a conceptual reasoning-based uncertainty estimation module, which\nsimulates the recognition process to enrich the semantic representation. In\nparticular, conceptual reasoning builds the temporal relation based on the\naction semantics, and uncertainty estimation tackles the semantic uncertainty\nof actions based on the distributional representation. Experiments show that\nour ExACT achieves superior recognition accuracy of 94.83%(+2.23%),\n90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.\n","authors":["Jiazhou Zhou","Xu Zheng","Yuanhuiyi Lyu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12534v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.12532v1","updated":"2024-03-19T08:09:27Z","published":"2024-03-19T08:09:27Z","title":"UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind\n  Them All","summary":"  We present UniBind, a flexible and efficient approach that learns a unified\nrepresentation space for seven diverse modalities -- images, text, audio, point\ncloud, thermal, video, and event data. Existing works, eg., ImageBind, treat\nthe image as the central modality and build an image-centered representation\nspace; however, the space may be sub-optimal as it leads to an unbalanced\nrepresentation space among all modalities. Moreover, the category names are\ndirectly used to extract text embeddings for the downstream tasks, making it\nhardly possible to represent the semantics of multi-modal data. The\n'out-of-the-box' insight of our UniBind is to make the alignment center\nmodality-agnostic and further learn a unified and balanced representation\nspace, empowered by the large language models (LLMs). UniBind is superior in\nits flexible application to all CLIP-style models and delivers remarkable\nperformance boosts. To make this possible, we 1) construct a knowledge base of\ntext embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build\nLLM-augmented class-wise embedding center on top of the knowledge base and\nencoded visual embeddings; 3) align all the embeddings to the LLM-augmented\nembedding center via contrastive learning to achieve a unified and balanced\nrepresentation space. UniBind shows strong zero-shot recognition performance\ngains over prior arts by an average of 6.36%. Finally, we achieve new\nstate-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal\nfine-tuning setting while reducing 90% of the learnable parameters.\n","authors":["Yuanhuiyi Lyu","Xu Zheng","Jiazhou Zhou","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12532v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.12530v1","updated":"2024-03-19T08:08:12Z","published":"2024-03-19T08:08:12Z","title":"PCT: Perspective Cue Training Framework for Multi-Camera BEV\n  Segmentation","summary":"  Generating annotations for bird's-eye-view (BEV) segmentation presents\nsignificant challenges due to the scenes' complexity and the high manual\nannotation cost. In this work, we address these challenges by leveraging the\nabundance of unlabeled data available. We propose the Perspective Cue Training\n(PCT) framework, a novel training framework that utilizes pseudo-labels\ngenerated from unlabeled perspective images using publicly available semantic\nsegmentation models trained on large street-view datasets. PCT applies a\nperspective view task head to the image encoder shared with the BEV\nsegmentation head, effectively utilizing the unlabeled data to be trained with\nthe generated pseudo-labels. Since image encoders are present in nearly all\ncamera-based BEV segmentation architectures, PCT is flexible and applicable to\nvarious existing BEV architectures. PCT can be applied to various settings\nwhere unlabeled data is available. In this paper, we applied PCT for\nsemi-supervised learning (SSL) and unsupervised domain adaptation (UDA).\nAdditionally, we introduce strong input perturbation through Camera Dropout\n(CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are\ncrucial for enhancing SSL capabilities using our teacher-student framework. Our\ncomprehensive approach is simple and flexible but yields significant\nimprovements over various baselines for SSL and UDA, achieving competitive\nperformances even against the current state-of-the-art.\n","authors":["Haruya Ishikawa","Takumi Iida","Yoshinori Konishi","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2403.12530v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.03526v2","updated":"2024-03-19T08:03:07Z","published":"2023-12-06T14:40:05Z","title":"On the Diversity and Realism of Distilled Dataset: An Efficient Dataset\n  Distillation Paradigm","summary":"  Contemporary machine learning requires training large neural networks on\nmassive datasets and thus faces the challenges of high computational demands.\nDataset distillation, as a recent emerging strategy, aims to compress\nreal-world datasets for efficient training. However, this line of research\ncurrently struggle with large-scale and high-resolution datasets, hindering its\npracticality and feasibility. To this end, we re-examine the existing dataset\ndistillation methods and identify three properties required for large-scale\nreal-world applications, namely, realism, diversity, and efficiency. As a\nremedy, we propose RDED, a novel computationally-efficient yet effective data\ndistillation paradigm, to enable both diversity and realism of the distilled\ndata. Extensive empirical results over various neural architectures and\ndatasets demonstrate the advancement of RDED: we can distill the full\nImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,\nachieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU\n(while the SOTA only achieves 21% but requires 6 hours).\n","authors":["Peng Sun","Bei Shi","Daiwei Yu","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2312.03526v2.pdf","comment":"17 pages, 20 figures"},{"id":"http://arxiv.org/abs/2305.08473v2","updated":"2024-03-19T07:59:52Z","published":"2023-05-15T09:24:48Z","title":"Shared and Private Information Learning in Multimodal Sentiment Analysis\n  with Deep Modal Alignment and Self-supervised Multi-Task Learning","summary":"  Designing an effective representation learning method for multimodal\nsentiment analysis tasks is a crucial research direction. The challenge lies in\nlearning both shared and private information in a complete modal\nrepresentation, which is difficult with uniform multimodal labels and a raw\nfeature fusion approach. In this work, we propose a deep modal shared\ninformation learning module based on the covariance matrix to capture the\nshared information between modalities. Additionally, we use a label generation\nmodule based on a self-supervised learning strategy to capture the private\ninformation of the modalities. Our module is plug-and-play in multimodal tasks,\nand by changing the parameterization, it can adjust the information exchange\nrelationship between the modes and learn the private or shared information\nbetween the specified modes. We also employ a multi-task learning strategy to\nhelp the model focus its attention on the modal differentiation training data.\nWe provide a detailed formulation derivation and feasibility proof for the\ndesign of the deep modal shared information learning module. We conduct\nextensive experiments on three common multimodal sentiment analysis baseline\ndatasets, and the experimental results validate the reliability of our model.\nFurthermore, we explore more combinatorial techniques for the use of the\nmodule. Our approach outperforms current state-of-the-art methods on most of\nthe metrics of the three public datasets.\n","authors":["Songning Lai","Jiakang Li","Guinan Guo","Xifeng Hu","Yulong Li","Yuan Tan","Zichen Song","Yutong Liu","Zhaoxia Ren","Chun Wan","Danmin Miao","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2305.08473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10089v2","updated":"2024-03-19T07:58:17Z","published":"2024-03-15T08:05:16Z","title":"Approximation and bounding techniques for the Fisher-Rao distances","summary":"  The Fisher-Rao distance between two probability distributions of a\nstatistical model is defined as the Riemannian geodesic distance induced by the\nFisher information metric. In order to calculate the Fisher-Rao distance in\nclosed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and\n(2) to integrate the Fisher length element along those geodesics. We consider\nseveral numerically robust approximation and bounding techniques for the\nFisher-Rao distances: First, we report generic upper bounds on Fisher-Rao\ndistances based on closed-form 1D Fisher-Rao distances of submodels. Second, we\ndescribe several generic approximation schemes depending on whether the\nFisher-Rao geodesics or pregeodesics are available in closed-form or not. In\nparticular, we obtain a generic method to guarantee an arbitrarily small\nadditive error on the approximation provided that Fisher-Rao pregeodesics and\ntight lower and upper bounds are available. Third, we consider the case of\nFisher metrics being Hessian metrics, and report generic tight upper bounds on\nthe Fisher-Rao distances using techniques of information geometry.\nUniparametric and biparametric statistical models always have Fisher Hessian\nmetrics, and in general a simple test allows to check whether the Fisher\ninformation matrix yields a Hessian metric or not. Fourth, we consider\nelliptical distribution families and show how to apply the above techniques to\nthese models. We also propose two new distances based either on the Fisher-Rao\nlengths of curves serving as proxies of Fisher-Rao geodesics, or based on the\nBirkhoff/Hilbert projective cone distance. Last, we consider an alternative\ngroup-theoretic approach for statistical transformation models based on the\nnotion of maximal invariant which yields insights on the structures of the\nFisher-Rao distance formula which may be used fruitfully in applications.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2403.10089v2.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2403.12519v1","updated":"2024-03-19T07:42:57Z","published":"2024-03-19T07:42:57Z","title":"Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language\n  Recognition","summary":"  Skeleton-aware sign language recognition (SLR) has gained popularity due to\nits ability to remain unaffected by background information and its lower\ncomputational requirements. Current methods utilize spatial graph modules and\ntemporal modules to capture spatial and temporal features, respectively.\nHowever, their spatial graph modules are typically built on fixed graph\nstructures such as graph convolutional networks or a single learnable graph,\nwhich only partially explore joint relationships. Additionally, a simple\ntemporal convolution kernel is used to capture temporal information, which may\nnot fully capture the complex movement patterns of different signers. To\novercome these limitations, we propose a new spatial architecture consisting of\ntwo concurrent branches, which build input-sensitive joint relationships and\nincorporates specific domain knowledge for recognition, respectively. These two\nbranches are followed by an aggregation process to distinguishe important joint\nconnections. We then propose a new temporal module to model multi-scale\ntemporal information to capture complex human dynamics. Our method achieves\nstate-of-the-art accuracy compared to previous skeleton-aware methods on four\nlarge-scale SLR benchmarks. Moreover, our method demonstrates superior accuracy\ncompared to RGB-based methods in most cases while requiring much fewer\ncomputational resources, bringing better accuracy-computation trade-off. Code\nis available at https://github.com/hulianyuyy/DSTA-SLR.\n","authors":["Lianyu Hu","Liqing Gao","Zekang Liu","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2403.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14817v2","updated":"2024-03-19T07:29:20Z","published":"2024-02-22T18:59:56Z","title":"Cameras as Rays: Pose Estimation via Ray Diffusion","summary":"  Estimating camera poses is a fundamental task for 3D reconstruction and\nremains challenging given sparsely sampled views (<10). In contrast to existing\napproaches that pursue top-down prediction of global parametrizations of camera\nextrinsics, we propose a distributed representation of camera pose that treats\na camera as a bundle of rays. This representation allows for a tight coupling\nwith spatial image features improving pose precision. We observe that this\nrepresentation is naturally suited for set-level transformers and develop a\nregression-based approach that maps image patches to corresponding rays. To\ncapture the inherent uncertainties in sparse-view pose inference, we adapt this\napproach to learn a denoising diffusion model which allows us to sample\nplausible modes while improving performance. Our proposed methods, both\nregression- and diffusion-based, demonstrate state-of-the-art performance on\ncamera pose estimation on CO3D while generalizing to unseen object categories\nand in-the-wild captures.\n","authors":["Jason Y. Zhang","Amy Lin","Moneish Kumar","Tzu-Hsuan Yang","Deva Ramanan","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2402.14817v2.pdf","comment":"In ICLR 2024 (oral). v2: updated references. Project webpage:\n  https://jasonyzhang.com/RayDiffusion"},{"id":"http://arxiv.org/abs/2403.12510v1","updated":"2024-03-19T07:24:54Z","published":"2024-03-19T07:24:54Z","title":"Generalized Consistency Trajectory Models for Image Manipulation","summary":"  Diffusion-based generative models excel in unconditional generation, as well\nas on applied tasks such as image editing and restoration. The success of\ndiffusion models lies in the iterative nature of diffusion: diffusion breaks\ndown the complex process of mapping noise to data into a sequence of simple\ndenoising tasks. Moreover, we are able to exert fine-grained control over the\ngeneration process by injecting guidance terms into each denoising step.\nHowever, the iterative process is also computationally intensive, often taking\nfrom tens up to thousands of function evaluations. Although consistency\ntrajectory models (CTMs) enable traversal between any time points along the\nprobability flow ODE (PFODE) and score inference with a single function\nevaluation, CTMs only allow translation from Gaussian noise to data. Thus, this\nwork aims to unlock the full potential of CTMs by proposing generalized CTMs\n(GCTMs), which translate between arbitrary distributions via ODEs. We discuss\nthe design space of GCTMs and demonstrate their efficacy in various image\nmanipulation tasks such as image-to-image translation, restoration, and\nediting. Code: \\url{https://github.com/1202kbs/GCTM}\n","authors":["Beomsu Kim","Jaemin Kim","Jeongsol Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12505v1","updated":"2024-03-19T07:11:53Z","published":"2024-03-19T07:11:53Z","title":"Semantics, Distortion, and Style Matter: Towards Source-free UDA for\n  Panoramic Segmentation","summary":"  This paper addresses an interesting yet challenging problem -- source-free\nunsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic\nsegmentation -- given only a pinhole image-trained model (i.e., source) and\nunlabeled panoramic images (i.e., target). Tackling this problem is nontrivial\ndue to the semantic mismatches, style discrepancies, and inevitable distortion\nof panoramic images. To this end, we propose a novel method that utilizes\nTangent Projection (TP) as it has less distortion and meanwhile slits the\nequirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.\nBoth projections are shown effective in extracting knowledge from the source\nmodel. However, the distinct projection discrepancies between source and target\ndomains impede the direct knowledge transfer; thus, we propose a panoramic\nprototype adaptation module (PPAM) to integrate panoramic prototypes from the\nextracted knowledge for adaptation. We then impose the loss constraints on both\npredictions and prototypes and propose a cross-dual attention module (CDAM) at\nthe feature level to better align the spatial and channel characteristics\nacross the domains and projections. Both knowledge extraction and transfer\nprocesses are synchronously updated to reach the best performance. Extensive\nexperiments on the synthetic and real-world benchmarks, including outdoor and\nindoor scenarios, demonstrate that our method achieves significantly better\nperformance than prior SFUDA methods for pinhole-to-panoramic adaptation.\n","authors":["Xu Zheng","Pengyuan Zhou","Athanasios Vasilakos","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12505v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12009v2","updated":"2024-03-19T07:11:28Z","published":"2024-03-18T17:47:39Z","title":"Leveraging Spatial and Semantic Feature Extraction for Skin Cancer\n  Diagnosis with Capsule Networks and Graph Neural Networks","summary":"  In the realm of skin lesion image classification, the intricate spatial and\nsemantic features pose significant challenges for conventional Convolutional\nNeural Network (CNN)-based methodologies. These challenges are compounded by\nthe imbalanced nature of skin lesion datasets, which hampers the ability of\nmodels to learn minority class features effectively. Despite augmentation\nstrategies, such as those using Generative Adversarial Networks (GANs),\nprevious attempts have not fully addressed these complexities. This study\nintroduces an innovative approach by integrating Graph Neural Networks (GNNs)\nwith Capsule Networks to enhance classification performance. GNNs, known for\ntheir proficiency in handling graph-structured data, offer an advanced\nmechanism for capturing complex patterns and relationships beyond the\ncapabilities of traditional CNNs. Capsule Networks further contribute by\nproviding superior recognition of spatial hierarchies within images. Our\nresearch focuses on evaluating and enhancing the Tiny Pyramid Vision GNN (Tiny\nPyramid ViG) architecture by incorporating it with a Capsule Network. This\nhybrid model was applied to the MNIST:HAM10000 dataset, a comprehensive skin\nlesion dataset designed for benchmarking classification models. After 75 epochs\nof training, our model achieved a significant accuracy improvement, reaching\n89.23% and 95.52%, surpassing established benchmarks such as GoogLeNet\n(83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-B7\n(92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA\n(93.47%) on the same dataset. This outcome underscores the potential of our\napproach in overcoming the inherent challenges of skin lesion classification,\ncontributing to the advancement of image-based diagnosis in dermatology.\n","authors":["K. P. Santoso","R. V. H. Ginardi","R. A. Sastrowardoyo","F. A. Madany"],"pdf_url":"https://arxiv.org/pdf/2403.12009v2.pdf","comment":"This is the first version of our paper, we gladly expect feedback and\n  corrections if there is any mistake within our paper"},{"id":"http://arxiv.org/abs/2403.07487v3","updated":"2024-03-19T07:05:37Z","published":"2024-03-12T10:25:29Z","title":"Motion Mamba: Efficient and Long Sequence Motion Generation with\n  Hierarchical and Bidirectional Selective SSM","summary":"  Human motion generation stands as a significant pursuit in generative\ncomputer vision, while achieving long-sequence and efficient motion generation\nremains challenging. Recent advancements in state space models (SSMs), notably\nMamba, have showcased considerable promise in long sequence modeling with an\nefficient hardware-aware design, which appears to be a promising direction to\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\ngeneration faces hurdles since the lack of a specialized design architecture to\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\nsimple and efficient approach that presents the pioneering motion generation\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\n(HTM) block to process temporal data by ensemble varying numbers of isolated\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\nblock to bidirectionally process latent poses, to enhance accurate motion\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\ncompared to the previous best diffusion-based method, which demonstrates strong\ncapabilities of high-quality long sequence motion modeling and real-time human\nmotion generation. See project website\nhttps://steve-zeyu-zhang.github.io/MotionMamba/\n","authors":["Zeyu Zhang","Akide Liu","Ian Reid","Richard Hartley","Bohan Zhuang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12494v1","updated":"2024-03-19T07:02:08Z","published":"2024-03-19T07:02:08Z","title":"Task-Customized Mixture of Adapters for General Image Fusion","summary":"  General image fusion aims at integrating important information from\nmulti-source images. However, due to the significant cross-task gap, the\nrespective fusion mechanism varies considerably in practice, resulting in\nlimited performance across subtasks. To handle this problem, we propose a novel\ntask-customized mixture of adapters (TC-MoA) for general image fusion,\nadaptively prompting various fusion tasks in a unified model. We borrow the\ninsight from the mixture of experts (MoE), taking the experts as efficient\ntuning adapters to prompt a pre-trained foundation model. These adapters are\nshared across different tasks and constrained by mutual information\nregularization, ensuring compatibility with different tasks while\ncomplementarity for multi-source images. The task-specific routing networks\ncustomize these adapters to extract task-specific information from different\nsources with dynamic dominant intensity, performing adaptive visual feature\nprompt fusion. Notably, our TC-MoA controls the dominant intensity bias for\ndifferent fusion tasks, successfully unifying multiple fusion tasks in a single\nmodel. Extensive experiments show that TC-MoA outperforms the competing\napproaches in learning commonalities while retaining compatibility for general\nimage fusion (multi-modal, multi-exposure, and multi-focus), and also\ndemonstrating striking controllability on more generalization experiments. The\ncode is available at https://github.com/YangSun22/TC-MoA .\n","authors":["Pengfei Zhu","Yang Sun","Bing Cao","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2403.12494v1.pdf","comment":"19 pages, 17 figures, CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12493v1","updated":"2024-03-19T07:02:06Z","published":"2024-03-19T07:02:06Z","title":"A Trainable Feature Extractor Module for Deep Neural Networks and\n  Scanpath Classification","summary":"  Scanpath classification is an area in eye tracking research with possible\napplications in medicine, manufacturing as well as training systems for\nstudents in various domains. In this paper we propose a trainable feature\nextraction module for deep neural networks. The purpose of this module is to\ntransform a scanpath into a feature vector which is directly useable for the\ndeep neural network architecture. Based on the backpropagated error of the deep\nneural network, the feature extraction module adapts its parameters to improve\nthe classification performance. Therefore, our feature extraction module is\njointly trainable with the deep neural network. The motivation to this feature\nextraction module is based on classical histogram-based approaches which\nusually compute distributions over a scanpath. We evaluated our module on three\npublic datasets and compared it to the state of the art approaches.\n","authors":["Wolfgang Fuhl"],"pdf_url":"https://arxiv.org/pdf/2403.12493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12488v1","updated":"2024-03-19T06:54:33Z","published":"2024-03-19T06:54:33Z","title":"DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of\n  MLLM","summary":"  We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot\nobject detection ability of multimodal large language models (MLLMs), such as\nGPT-4V and Gemini. Our approach consists of a detection prompting toolkit\ninspired by high-precision detection priors and a new Chain-of-Thought to\nimplement these prompts. Specifically, the prompts in the toolkit are designed\nto guide the MLLM to focus on regional information (e.g., zooming in), read\ncoordinates according to measure standards (e.g., overlaying rulers and\ncompasses), and infer from the contextual information (e.g., overlaying scene\ngraphs). Building upon these tools, the new detection chain-of-thought can\nautomatically decompose the task into simple subtasks, diagnose the\npredictions, and plan for progressive box refinements. The effectiveness of our\nframework is demonstrated across a spectrum of detection tasks, especially hard\ncases. Compared to existing state-of-the-art methods, GPT-4V with our\nDetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS\nCOCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val\nset for zero-shot referring expression comprehension, +14.5% AP on D-cube\ndescribe object detection FULL setting.\n","authors":["Yixuan Wu","Yizhou Wang","Shixiang Tang","Wenhao Wu","Tong He","Wanli Ouyang","Jian Wu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2403.12488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08985v3","updated":"2024-03-19T06:50:17Z","published":"2023-12-14T14:31:40Z","title":"OMG: Towards Open-vocabulary Motion Generation via Mixture of\n  Controllers","summary":"  We have recently seen tremendous progress in realistic text-to-motion\ngeneration. Yet, the existing methods often fail or produce implausible motions\nwith unseen text inputs, which limits the applications. In this paper, we\npresent OMG, a novel framework, which enables compelling motion generation from\nzero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the\npretrain-then-finetune paradigm into the text-to-motion generation. At the\npre-training stage, our model improves the generation ability by learning the\nrich out-of-domain inherent motion traits. To this end, we scale up a large\nunconditional diffusion model up to 1B parameters, so as to utilize the massive\nunlabeled motion data up to over 20M motion instances. At the subsequent\nfine-tuning stage, we introduce motion ControlNet, which incorporates text\nprompts as conditioning information, through a trainable copy of the\npre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.\nMoC block adaptively recognizes various ranges of the sub-motions with a\ncross-attention mechanism and processes them separately with the\ntext-token-specific experts. Such a design effectively aligns the CLIP token\nembeddings of text prompts to various ranges of compact and expressive motion\nfeatures. Extensive experiments demonstrate that our OMG achieves significant\nimprovements over the state-of-the-art methods on zero-shot text-to-motion\ngeneration. Project page: https://tr3e.github.io/omg-page.\n","authors":["Han Liang","Jiacheng Bao","Ruichi Zhang","Sihan Ren","Yuecheng Xu","Sibei Yang","Xin Chen","Jingyi Yu","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2312.08985v3.pdf","comment":"accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12483v1","updated":"2024-03-19T06:40:06Z","published":"2024-03-19T06:40:06Z","title":"A Hybrid Transformer-Sequencer approach for Age and Gender\n  classification from in-wild facial images","summary":"  The advancements in computer vision and image processing techniques have led\nto emergence of new application in the domain of visual surveillance, targeted\nadvertisement, content-based searching, and human-computer interaction etc. Out\nof the various techniques in computer vision, face analysis, in particular, has\ngained much attention. Several previous studies have tried to explore different\napplications of facial feature processing for a variety of tasks, including age\nand gender classification. However, despite several previous studies having\nexplored the problem, the age and gender classification of in-wild human faces\nis still far from the achieving the desired levels of accuracy required for\nreal-world applications. This paper, therefore, attempts to bridge this gap by\nproposing a hybrid model that combines self-attention and BiLSTM approaches for\nage and gender classification problems. The proposed models performance is\ncompared with several state-of-the-art model proposed so far. An improvement of\napproximately 10percent and 6percent over the state-of-the-art implementations\nfor age and gender classification, respectively, are noted for the proposed\nmodel. The proposed model is thus found to achieve superior performance and is\nfound to provide a more generalized learning. The model can, therefore, be\napplied as a core classification component in various image processing and\ncomputer vision problems.\n","authors":["Aakash Singh","Vivek Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2403.12483v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2308.09591v3","updated":"2024-03-19T06:37:32Z","published":"2023-08-18T14:38:31Z","title":"O$^2$-Recon: Completing 3D Reconstruction of Occluded Objects in the\n  Scene with a Pre-trained 2D Diffusion Model","summary":"  Occlusion is a common issue in 3D reconstruction from RGB-D videos, often\nblocking the complete reconstruction of objects and presenting an ongoing\nproblem. In this paper, we propose a novel framework, empowered by a 2D\ndiffusion-based in-painting model, to reconstruct complete surfaces for the\nhidden parts of objects. Specifically, we utilize a pre-trained diffusion model\nto fill in the hidden areas of 2D images. Then we use these in-painted images\nto optimize a neural implicit surface representation for each instance for 3D\nreconstruction. Since creating the in-painting masks needed for this process is\ntricky, we adopt a human-in-the-loop strategy that involves very little human\nengagement to generate high-quality masks. Moreover, some parts of objects can\nbe totally hidden because the videos are usually shot from limited\nperspectives. To ensure recovering these invisible areas, we develop a cascaded\nnetwork architecture for predicting signed distance field, making use of\ndifferent frequency bands of positional encoding and maintaining overall\nsmoothness. Besides the commonly used rendering loss, Eikonal loss, and\nsilhouette loss, we adopt a CLIP-based semantic consistency loss to guide the\nsurface from unseen camera angles. Experiments on ScanNet scenes show that our\nproposed framework achieves state-of-the-art accuracy and completeness in\nobject-level reconstruction from scene-level RGB-D videos. Code:\nhttps://github.com/THU-LYJ-Lab/O2-Recon.\n","authors":["Yubin Hu","Sheng Ye","Wang Zhao","Matthieu Lin","Yuze He","Yu-Hui Wen","Ying He","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2308.09591v3.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2403.12481v1","updated":"2024-03-19T06:36:42Z","published":"2024-03-19T06:36:42Z","title":"TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer","summary":"  Detecting fake news has received a lot of attention. Many previous methods\nconcatenate independently encoded unimodal data, ignoring the benefits of\nintegrated multimodal information. Also, the absence of specialized feature\nextraction for text and images further limits these methods. This paper\nintroduces an end-to-end model called TT-BLIP that applies the bootstrapping\nlanguage-image pretraining for unified vision-language understanding and\ngeneration (BLIP) for three types of information: BERT and\nBLIP\\textsubscript{Txt} for text, ResNet and BLIP\\textsubscript{Img} for\nimages, and bidirectional BLIP encoders for multimodal information. The\nMultimodal Tri-Transformer fuses tri-modal features using three types of\nmulti-head attention mechanisms, ensuring integrated modalities for enhanced\nrepresentations and improved multimodal data analysis. The experiments are\nperformed using two fake news datasets, Weibo and Gossipcop. The results\nindicate TT-BLIP outperforms the state-of-the-art models.\n","authors":["Eunjee Choi","Jong-Kook Kim"],"pdf_url":"https://arxiv.org/pdf/2403.12481v1.pdf","comment":"8 pages, submitted to conference"},{"id":"http://arxiv.org/abs/2311.16117v2","updated":"2024-03-19T06:27:18Z","published":"2023-10-03T15:45:50Z","title":"Predicated Diffusion: Predicate Logic-Based Attention Guidance for\n  Text-to-Image Diffusion Models","summary":"  Diffusion models have achieved remarkable results in generating high-quality,\ndiverse, and creative images. However, when it comes to text-based image\ngeneration, they often fail to capture the intended meaning presented in the\ntext. For instance, a specified object may not be generated, an unnecessary\nobject may be generated, and an adjective may alter objects it was not intended\nto modify. Moreover, we found that relationships indicating possession between\nobjects are often overlooked. While users' intentions in text are diverse,\nexisting methods tend to specialize in only some aspects of these. In this\npaper, we propose Predicated Diffusion, a unified framework to express users'\nintentions. We consider that the root of the above issues lies in the text\nencoder, which often focuses only on individual words and neglects the logical\nrelationships between them. The proposed method does not solely rely on the\ntext encoder, but instead, represents the intended meaning in the text as\npropositions using predicate logic and treats the pixels in the attention maps\nas the fuzzy predicates. This enables us to obtain a differentiable loss\nfunction that makes the image fulfill the proposition by minimizing it. When\ncompared to several existing methods, we demonstrated that Predicated Diffusion\ncan generate images that are more faithful to various text prompts, as verified\nby human evaluators and pretrained image-text models.\n","authors":["Kota Sueyoshi","Takashi Matsubara"],"pdf_url":"https://arxiv.org/pdf/2311.16117v2.pdf","comment":"20 pages, 16 figures, 6 tables, ~500 images, ~30MB"},{"id":"http://arxiv.org/abs/2403.12473v1","updated":"2024-03-19T06:18:25Z","published":"2024-03-19T06:18:25Z","title":"PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human\n  Mesh Recovery","summary":"  With the recent advancements in single-image-based human mesh recovery, there\nis a growing interest in enhancing its performance in certain extreme\nscenarios, such as occlusion, while maintaining overall model accuracy.\nAlthough obtaining accurately annotated 3D human poses under occlusion is\nchallenging, there is still a wealth of rich and precise 2D pose annotations\nthat can be leveraged. However, existing works mostly focus on directly\nleveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, we\npresent PostoMETRO($\\textbf{Pos}$e $\\textbf{to}$ken enhanced $\\textbf{ME}$sh\n$\\textbf{TR}$ansf$\\textbf{O}$rmer), which integrates occlusion-resilient 2D\npose representation into transformers in a token-wise manner. Utilizing a\nspecialized pose tokenizer, we efficiently condense 2D pose data to a compact\nsequence of pose tokens and feed them to the transformer together with the\nimage tokens. This process not only ensures a rich depiction of texture from\nthe image but also fosters a robust integration of pose and image information.\nSubsequently, these combined tokens are queried by vertex and joint tokens to\ndecode 3D coordinates of mesh vertices and human joints. Facilitated by the\nrobust pose token representation and the effective combination, we are able to\nproduce more precise 3D coordinates, even under extreme scenarios like\nocclusion. Experiments on both standard and occlusion-specific benchmarks\ndemonstrate the effectiveness of PostoMETRO. Qualitative results further\nillustrate the clarity of how 2D pose can help 3D reconstruction. Code will be\nmade available.\n","authors":["Wendi Yang","Zihang Jiang","Shang Zhao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12470v1","updated":"2024-03-19T06:01:11Z","published":"2024-03-19T06:01:11Z","title":"SC-Diff: 3D Shape Completion with Latent Diffusion Models","summary":"  This paper introduces a 3D shape completion approach using a 3D latent\ndiffusion model optimized for completing shapes, represented as Truncated\nSigned Distance Functions (TSDFs), from partial 3D scans. Our method combines\nimage-based conditioning through cross-attention and spatial conditioning\nthrough the integration of 3D features from captured partial scans. This dual\nguidance enables high-fidelity, realistic shape completions at superior\nresolutions. At the core of our approach is the compression of 3D data into a\nlow-dimensional latent space using an auto-encoder inspired by 2D latent\ndiffusion models. This compression facilitates the processing of\nhigher-resolution shapes and allows us to apply our model across multiple\nobject classes, a significant improvement over other existing diffusion-based\nshape completion methods, which often require a separate diffusion model for\neach class. We validated our approach against two common benchmarks in the\nfield of shape completion, demonstrating competitive performance in terms of\naccuracy and realism and performing on par with state-of-the-art methods\ndespite operating at a higher resolution with a single model for all object\nclasses. We present a comprehensive evaluation of our model, showcasing its\nefficacy in handling diverse shape completion challenges, even on unseen object\nclasses. The code will be released upon acceptance.\n","authors":["Juan D. Galvis","Xingxing Zuo","Simon Schaefer","Stefan Leutengger"],"pdf_url":"https://arxiv.org/pdf/2403.12470v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.06025v3","updated":"2024-03-19T05:58:51Z","published":"2024-03-09T22:25:14Z","title":"CarbonNet: How Computer Vision Plays a Role in Climate Change?\n  Application: Learning Geomechanics from Subsurface Geometry of CCS to\n  Mitigate Global Warming","summary":"  We introduce a new approach using computer vision to predict the land surface\ndisplacement from subsurface geometry images for Carbon Capture and\nSequestration (CCS). CCS has been proved to be a key component for a carbon\nneutral society. However, scientists see there are challenges along the way\nincluding the high computational cost due to the large model scale and\nlimitations to generalize a pre-trained model with complex physics. We tackle\nthose challenges by training models directly from the subsurface geometry\nimages. The goal is to understand the respons of land surface displacement due\nto carbon injection and utilize our trained models to inform decision making in\nCCS projects.\n  We implement multiple models (CNN, ResNet, and ResNetUNet) for static\nmechanics problem, which is a image prediction problem. Next, we use the LSTM\nand transformer for transient mechanics scenario, which is a video prediction\nproblem. It shows ResNetUNet outperforms the others thanks to its architecture\nin static mechanics problem, and LSTM shows comparable performance to\ntransformer in transient problem. This report proceeds by outlining our dataset\nin detail followed by model descriptions in method section. Result and\ndiscussion state the key learning, observations, and conclusion with future\nwork rounds out the paper.\n","authors":["Wei Chen","Yunan Li","Yuan Tian"],"pdf_url":"https://arxiv.org/pdf/2403.06025v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12466v1","updated":"2024-03-19T05:50:48Z","published":"2024-03-19T05:50:48Z","title":"Few-shot Object Localization","summary":"  Existing few-shot object counting tasks primarily focus on quantifying the\nnumber of objects in an image, neglecting precise positional information. To\nbridge this research gap, this paper introduces the novel task of Few-Shot\nObject Localization (FSOL), which aims to provide accurate object positional\ninformation. This task achieves generalized object localization by leveraging a\nsmall number of labeled support samples to query the positional information of\nobjects within corresponding images. To advance this research field, we propose\nan innovative high-performance baseline model. Our model integrates a dual-path\nfeature augmentation module to enhance shape association and gradient\ndifferences between supports and query images, alongside a self-query module\ndesigned to explore the association between feature maps and query images.\nExperimental results demonstrate a significant performance improvement of our\napproach in the FSOL task, establishing an efficient benchmark for further\nresearch.\n","authors":["Yunhan Ren","Bo Li","Chengyang Zhang","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11616v2","updated":"2024-03-19T05:49:31Z","published":"2024-03-18T09:47:41Z","title":"Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level\n  Perception","summary":"  For training a video-based action recognition model that accepts multi-view\nvideo, annotating frame-level labels is tedious and difficult. However, it is\nrelatively easy to annotate sequence-level labels. This kind of coarse\nannotations are called as weak labels. However, training a multi-view\nvideo-based action recognition model with weak labels for frame-level\nperception is challenging. In this paper, we propose a novel learning\nframework, where the weak labels are first used to train a multi-view\nvideo-based base model, which is subsequently used for downstream frame-level\nperception tasks. The base model is trained to obtain individual latent\nembeddings for each view in the multi-view input. For training the model using\nthe weak labels, we propose a novel latent loss function. We also propose a\nmodel that uses the view-specific latent embeddings for downstream frame-level\naction recognition and detection tasks. The proposed framework is evaluated\nusing the MM Office dataset by comparing several baseline algorithms. The\nresults show that the proposed base model is effectively trained using weak\nlabels and the latent embeddings help the downstream models improve accuracy.\n","authors":["Vijay John","Yasutomo Kawanishi"],"pdf_url":"https://arxiv.org/pdf/2403.11616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03135v5","updated":"2024-03-19T05:30:50Z","published":"2023-08-06T15:05:42Z","title":"EventBind: Learning a Unified Representation to Bind Them All for\n  Event-based Open-world Understanding","summary":"  In this paper, we propose EventBind, a novel and effective framework that\nunleashes the potential of vision-language models (VLMs) for event-based\nrecognition to compensate for the lack of large-scale event-based datasets. In\nparticular, due to the distinct modality gap with the image-text data and the\nlack of large-scale datasets, learning a common representation space for\nimages, texts, and events is non-trivial.Intuitively, we need to address two\nkey challenges: 1) how to generalize CLIP's visual encoder to event data while\nfully leveraging events' unique properties, e.g., sparsity and high temporal\nresolution; 2) how to effectively align the multi-modal embeddings, i.e.,\nimage, text, and events. Accordingly, we first introduce a novel event encoder\nthat subtly models the temporal information from events and meanwhile,\ngenerates event prompts for modality bridging. We then design a text encoder\nthat generates content prompts and utilizes hybrid text prompts to enhance\nEventBind's generalization ability across diverse datasets.With the proposed\nevent encoder, text encoder, and image encoder, a novel Hierarchical Triple\nContrastive Alignment (HTCA) module is introduced to jointly optimize the\ncorrelation and enable efficient knowledge transfer among the three modalities.\nWe evaluate various settings, including fine-tuning and few-shot on three\nbenchmarks, and our EventBind achieves new state-of-the-art accuracy compared\nwith the previous methods, such as on N-Caltech101 (+5.34% and +1.70%) and\nN-Imagenet (+5.65% and +1.99%) with fine-tuning and 20-shot settings,\nrespectively. Moreover, our EventBind can be flexibly extended to the event\nretrieval task using text or image queries, showing plausible performance. Our\nproject code will be made publicly available.\n","authors":["Jiazhou Zhou","Xu Zheng","Yuanhuiyi Lyu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.03135v5.pdf","comment":"Conference version with supplementary"},{"id":"http://arxiv.org/abs/2403.12459v1","updated":"2024-03-19T05:30:50Z","published":"2024-03-19T05:30:50Z","title":"Non-negative Contrastive Learning","summary":"  Deep representations have shown promising performance when transferred to\ndownstream tasks in a black-box manner. Yet, their inherent lack of\ninterpretability remains a significant challenge, as these features are often\nopaque to human understanding. In this paper, we propose Non-negative\nContrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization\n(NMF) aimed at deriving interpretable features. The power of NCL lies in its\nenforcement of non-negativity constraints on features, reminiscent of NMF's\ncapability to extract features that align closely with sample clusters. NCL not\nonly aligns mathematically well with an NMF objective but also preserves NMF's\ninterpretability attributes, resulting in a more sparse and disentangled\nrepresentation compared to standard contrastive learning (CL). Theoretically,\nwe establish guarantees on the identifiability and downstream generalization of\nNCL. Empirically, we show that these advantages enable NCL to outperform CL\nsignificantly on feature disentanglement, feature selection, as well as\ndownstream classification tasks. At last, we show that NCL can be easily\nextended to other learning scenarios and benefit supervised learning as well.\nCode is available at https://github.com/PKU-ML/non_neg.\n","authors":["Yifei Wang","Qi Zhang","Yaoyu Guo","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12459v1.pdf","comment":"22 pages. Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12457v1","updated":"2024-03-19T05:27:52Z","published":"2024-03-19T05:27:52Z","title":"Privacy-Preserving Face Recognition Using Trainable Feature Subtraction","summary":"  The widespread adoption of face recognition has led to increasing privacy\nconcerns, as unauthorized access to face images can expose sensitive personal\ninformation. This paper explores face image protection against viewing and\nrecovery attacks. Inspired by image compression, we propose creating a visually\nuninformative face image through feature subtraction between an original face\nand its model-produced regeneration. Recognizable identity features within the\nimage are encouraged by co-training a recognition model on its high-dimensional\nfeature representation. To enhance privacy, the high-dimensional representation\nis crafted through random channel shuffling, resulting in randomized\nrecognizable images devoid of attacker-leverageable texture details. We distill\nour methodologies into a novel privacy-preserving face recognition method,\nMinusFace. Experiments demonstrate its high recognition accuracy and effective\nprivacy protection. Its code is available at https://github.com/Tencent/TFace.\n","authors":["Yuxi Mi","Zhizhou Zhong","Yuge Huang","Jiazhen Ji","Jianqing Xu","Jun Wang","Shaoming Wang","Shouhong Ding","Shuigeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12457v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12455v1","updated":"2024-03-19T05:27:04Z","published":"2024-03-19T05:27:04Z","title":"CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation","summary":"  Open-vocabulary video instance segmentation strives to segment and track\ninstances belonging to an open set of categories in a video. The\nvision-language model Contrastive Language-Image Pre-training (CLIP) has shown\nstrong zero-shot classification ability in image-level open-vocabulary task. In\nthis paper, we propose a simple encoder-decoder network, called CLIP-VIS, to\nadapt CLIP for open-vocabulary video instance segmentation. Our CLIP-VIS adopts\nfrozen CLIP image encoder and introduces three modules, including\nclass-agnostic mask generation, temporal topK-enhanced matching, and weighted\nopen-vocabulary classification. Given a set of initial queries, class-agnostic\nmask generation employs a transformer decoder to predict query masks and\ncorresponding object scores and mask IoU scores. Then, temporal topK-enhanced\nmatching performs query matching across frames by using K mostly matched\nframes. Finally, weighted open-vocabulary classification first generates query\nvisual features with mask pooling, and second performs weighted classification\nusing object scores and mask IoU scores. Our CLIP-VIS does not require the\nannotations of instance categories and identities. The experiments are\nperformed on various video instance segmentation datasets, which demonstrate\nthe effectiveness of our proposed method, especially on novel categories. When\nusing ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of\n32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg\nby 11.0% and 24.0% respectively. We will release the source code and models at\nhttps://github.com/zwq456/CLIP-VIS.git.\n","authors":["Wenqi Zhu","Jiale Cao","Jin Xie","Shuangming Yang","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2403.12455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08460v2","updated":"2024-03-19T05:25:20Z","published":"2024-03-13T12:20:20Z","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model","summary":"  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n","authors":["Ruibin Zhang","Donglai Xue","Yuhan Wang","Ruixu Geng","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08460v2.pdf","comment":"8 pages, 6 figures, submitted to RA-L"},{"id":"http://arxiv.org/abs/2403.12450v1","updated":"2024-03-19T05:21:12Z","published":"2024-03-19T05:21:12Z","title":"Intention Action Anticipation Model with Guide-Feedback Loop Mechanism","summary":"  Anticipating human intention from videos has broad applications, such as\nautomatic driving, robot assistive technology, and virtual reality. This study\naddresses the problem of intention action anticipation using egocentric video\nsequences to estimate actions that indicate human intention. We propose a\nHierarchical Complete-Recent (HCR) information fusion model that makes full use\nof the features of the entire video sequence (i.e., complete features) and the\nfeatures of the video tail sequence (i.e., recent features). The HCR model has\ntwo primary mechanisms. The Guide-Feedback Loop (GFL) mechanism is proposed to\nmodel the relation between one recent feature and one complete feature. Based\non GFL, the MultiComplete-Recent Feature Aggregation (MCRFA) module is proposed\nto model the relation of one recent feature with multiscale complete features.\nBased on GFL and MCRFA, the HCR model can hierarchically explore the rich\ninterrelationships between multiscale complete features and multiscale recent\nfeatures. Through comparative and ablation experiments, we validate the\neffectiveness of our model on two well-known public datasets: EPIC-Kitchens and\nEGTEA Gaze+.\n","authors":["Zongnan Ma","Fuchun Zhang","Zhixiong Nan","Yao Ge"],"pdf_url":"https://arxiv.org/pdf/2403.12450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12448v1","updated":"2024-03-19T05:17:47Z","published":"2024-03-19T05:17:47Z","title":"Do Generated Data Always Help Contrastive Learning?","summary":"  Contrastive Learning (CL) has emerged as one of the most successful paradigms\nfor unsupervised visual representation learning, yet it often depends on\nintensive manual data augmentations. With the rise of generative models,\nespecially diffusion models, the ability to generate realistic images close to\nthe real data distribution has been well recognized. These generated\nhigh-equality images have been successfully applied to enhance contrastive\nrepresentation learning, a technique termed ``data inflation''. However, we\nfind that the generated data (even from a good diffusion model like DDPM) may\nsometimes even harm contrastive learning. We investigate the causes behind this\nfailure from the perspective of both data inflation and data augmentation. For\nthe first time, we reveal the complementary roles that stronger data inflation\nshould be accompanied by weaker augmentations, and vice versa. We also provide\nrigorous theoretical explanations for these phenomena via deriving its\ngeneralization bounds under data inflation. Drawing from these insights, we\npropose Adaptive Inflation (AdaInf), a purely data-centric strategy without\nintroducing any extra computation cost. On benchmark datasets, AdaInf can bring\nsignificant improvements for various contrastive learning methods. Notably,\nwithout using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10\nwith SimCLR, setting a new record that surpasses many sophisticated methods.\nCode is available at https://github.com/PKU-ML/adainf.\n","authors":["Yifei Wang","Jizhe Zhang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12448v1.pdf","comment":"19 pages. Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09981v2","updated":"2024-03-19T05:17:18Z","published":"2024-03-15T02:57:20Z","title":"Controllable Text-to-3D Generation via Surface-Aligned Gaussian\n  Splatting","summary":"  While text-to-3D and image-to-3D generation tasks have received considerable\nattention, one important but under-explored field between them is controllable\ntext-to-3D generation, which we mainly focus on in this work. To address this\ntask, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network\narchitecture designed to enhance existing pre-trained multi-view diffusion\nmodels by integrating additional input conditions, such as edge, depth, normal,\nand scribble maps. Our innovation lies in the introduction of a conditioning\nmodule that controls the base diffusion model using both local and global\nembeddings, which are computed from the input condition images and camera\nposes. Once trained, MVControl is able to offer 3D diffusion guidance for\noptimization-based 3D generation. And, 2) we propose an efficient multi-stage\n3D generation pipeline that leverages the benefits of recent large\nreconstruction models and score distillation algorithm. Building upon our\nMVControl architecture, we employ a unique hybrid diffusion guidance method to\ndirect the optimization process. In pursuit of efficiency, we adopt 3D\nGaussians as our representation instead of the commonly used implicit\nrepresentations. We also pioneer the use of SuGaR, a hybrid representation that\nbinds Gaussians to mesh triangle faces. This approach alleviates the issue of\npoor geometry in 3D Gaussians and enables the direct sculpting of fine-grained\ngeometry on the mesh. Extensive experiments demonstrate that our method\nachieves robust generalization and enables the controllable generation of\nhigh-quality 3D content.\n","authors":["Zhiqi Li","Yiming Chen","Lingzhe Zhao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09981v2.pdf","comment":"Project page: https://lizhiqi49.github.io/MVControl/"},{"id":"http://arxiv.org/abs/2403.12445v1","updated":"2024-03-19T05:10:10Z","published":"2024-03-19T05:10:10Z","title":"Boosting Transferability in Vision-Language Attacks via Diversification\n  along the Intersection Region of Adversarial Trajectory","summary":"  Vision-language pre-training (VLP) models exhibit remarkable capabilities in\ncomprehending both images and text, yet they remain susceptible to multimodal\nadversarial examples (AEs). Strengthening adversarial attacks and uncovering\nvulnerabilities, especially common issues in VLP models (e.g., high\ntransferable AEs), can stimulate further research on constructing reliable and\npractical VLP models. A recent work (i.e., Set-level guidance attack) indicates\nthat augmenting image-text pairs to increase AE diversity along the\noptimization path enhances the transferability of adversarial examples\nsignificantly. However, this approach predominantly emphasizes diversity around\nthe online adversarial examples (i.e., AEs in the optimization period), leading\nto the risk of overfitting the victim model and affecting the transferability.\nIn this study, we posit that the diversity of adversarial examples towards the\nclean input and online AEs are both pivotal for enhancing transferability\nacross VLP models. Consequently, we propose using diversification along the\nintersection region of adversarial trajectory to expand the diversity of AEs.\nTo fully leverage the interaction between modalities, we introduce text-guided\nadversarial example selection during optimization. Furthermore, to further\nmitigate the potential overfitting, we direct the adversarial text deviating\nfrom the last intersection region along the optimization path, rather than\nadversarial images as in existing methods. Extensive experiments affirm the\neffectiveness of our method in improving transferability across various VLP\nmodels and downstream vision-and-language tasks (e.g., Image-Text\nRetrieval(ITR), Visual Grounding(VG), Image Captioning(IC)).\n","authors":["Sensen Gao","Xiaojun Jia","Xuhong Ren","Ivor Tsang","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2403.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03774v3","updated":"2024-03-19T05:03:18Z","published":"2023-12-06T02:52:54Z","title":"OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using\n  Octree Queries","summary":"  Occupancy prediction has increasingly garnered attention in recent years for\nits fine-grained understanding of 3D scenes. Traditional approaches typically\nrely on dense, regular grid representations, which often leads to excessive\ncomputational demands and a loss of spatial details for small objects. This\npaper introduces OctreeOcc, an innovative 3D occupancy prediction framework\nthat leverages the octree representation to adaptively capture valuable\ninformation in 3D, offering variable granularity to accommodate object shapes\nand semantic regions of varying sizes and complexities. In particular, we\nincorporate image semantic information to improve the accuracy of initial\noctree structures and design an effective rectification mechanism to refine the\noctree structure iteratively. Our extensive evaluations show that OctreeOcc not\nonly surpasses state-of-the-art methods in occupancy prediction, but also\nachieves a 15%-24% reduction in computational overhead compared to\ndense-grid-based methods.\n","authors":["Yuhang Lu","Xinge Zhu","Tai Wang","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2312.03774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12440v1","updated":"2024-03-19T04:54:59Z","published":"2024-03-19T04:54:59Z","title":"Self-learning Canonical Space for Multi-view 3D Human Pose Estimation","summary":"  Multi-view 3D human pose estimation is naturally superior to single view one,\nbenefiting from more comprehensive information provided by images of multiple\nviews. The information includes camera poses, 2D/3D human poses, and 3D\ngeometry. However, the accurate annotation of these information is hard to\nobtain, making it challenging to predict accurate 3D human pose from multi-view\nimages. To deal with this issue, we propose a fully self-supervised framework,\nnamed cascaded multi-view aggregating network (CMANet), to construct a\ncanonical parameter space to holistically integrate and exploit multi-view\ninformation. In our framework, the multi-view information is grouped into two\ncategories: 1) intra-view information , 2) inter-view information. Accordingly,\nCMANet consists of two components: intra-view module (IRV) and inter-view\nmodule (IEV). IRV is used for extracting initial camera pose and 3D human pose\nof each view; IEV is to fuse complementary pose information and cross-view 3D\ngeometry for a final 3D human pose. To facilitate the aggregation of the intra-\nand inter-view, we define a canonical parameter space, depicted by per-view\ncamera pose and human pose and shape parameters ($\\theta$ and $\\beta$) of SMPL\nmodel, and propose a two-stage learning procedure. At first stage, IRV learns\nto estimate camera pose and view-dependent 3D human pose supervised by\nconfident output of an off-the-shelf 2D keypoint detector. At second stage, IRV\nis frozen and IEV further refines the camera pose and optimizes the 3D human\npose by implicitly encoding the cross-view complement and 3D geometry\nconstraint, achieved by jointly fitting predicted multi-view 2D keypoints. The\nproposed framework, modules, and learning strategy are demonstrated to be\neffective by comprehensive experiments and CMANet is superior to\nstate-of-the-art methods in extensive quantitative and qualitative analysis.\n","authors":["Xiaoben Li","Mancheng Meng","Ziyan Wu","Terrence Chen","Fan Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2403.12440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12438v1","updated":"2024-03-19T04:51:38Z","published":"2024-03-19T04:51:38Z","title":"Precise-Physics Driven Text-to-3D Generation","summary":"  Text-to-3D generation has shown great promise in generating novel 3D content\nbased on given text prompts. However, existing generative methods mostly focus\non geometric or visual plausibility while ignoring precise physics perception\nfor the generated 3D shapes. This greatly hinders the practicality of generated\n3D shapes in real-world applications. In this work, we propose Phy3DGen, a\nprecise-physics-driven text-to-3D generation method. By analyzing the solid\nmechanics of generated 3D shapes, we reveal that the 3D shapes generated by\nexisting text-to-3D generation methods are impractical for real-world\napplications as the generated 3D shapes do not conform to the laws of physics.\nTo this end, we leverage 3D diffusion models to provide 3D shape priors and\ndesign a data-driven differentiable physics layer to optimize 3D shape priors\nwith solid mechanics. This allows us to optimize geometry efficiently and learn\nprecise physics information about 3D shapes at the same time. Experimental\nresults demonstrate that our method can consider both geometric plausibility\nand precise physics perception, further bridging 3D virtual modeling and\nprecise physical worlds.\n","authors":["Qingshan Xu","Jiao Liu","Melvin Wong","Caishun Chen","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2403.12438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05804v2","updated":"2024-03-19T04:50:32Z","published":"2023-12-10T07:34:43Z","title":"Layered 3D Human Generation via Semantic-Aware Diffusion Model","summary":"  The generation of 3D clothed humans has attracted increasing attention in\nrecent years. However, existing work cannot generate layered high-quality 3D\nhumans with consistent body structures. As a result, these methods are unable\nto arbitrarily and separately change and edit the body and clothing of the\nhuman. In this paper, we propose a text-driven layered 3D human generation\nframework based on a novel physically-decoupled semantic-aware diffusion model.\nTo keep the generated clothing consistent with the target text, we propose a\nsemantic-confidence strategy for clothing that can eliminate the non-clothing\ncontent generated by the model. To match the clothing with different body\nshapes, we propose a SMPL-driven implicit field deformation network that\nenables the free transfer and reuse of clothing. Besides, we introduce uniform\nshape priors based on the SMPL model for body and clothing, respectively, which\ngenerates more diverse 3D content without being constrained by specific\ntemplates. The experimental results demonstrate that the proposed method not\nonly generates 3D humans with consistent body structures but also allows free\nediting in a layered manner. The source code will be made public.\n","authors":["Yi Wang","Jian Ma","Ruizhi Shao","Qiao Feng","Yu-Kun Lai","Yebin Liu","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2312.05804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05139v2","updated":"2024-03-19T04:49:40Z","published":"2024-03-08T08:12:18Z","title":"Improving Diffusion Models for Virtual Try-on","summary":"  This paper considers image-based virtual try-on, which renders an image of a\nperson wearing a curated garment, given a pair of images depicting the person\nand the garment, respectively. Previous works adapt existing exemplar-based\ninpainting diffusion models for virtual try-on to improve the naturalness of\nthe generated visuals compared to other methods (e.g., GAN-based), but they\nfail to preserve the identity of the garments. To overcome this limitation, we\npropose a novel diffusion model that improves garment fidelity and generates\nauthentic virtual try-on images. Our method, coined IDM-VTON, uses two\ndifferent modules to encode the semantics of garment image; given the base UNet\nof the diffusion model, 1) the high-level semantics extracted from a visual\nencoder are fused to the cross-attention layer, and then 2) the low-level\nfeatures extracted from parallel UNet are fused to the self-attention layer. In\naddition, we provide detailed textual prompts for both garment and person\nimages to enhance the authenticity of the generated visuals. Finally, we\npresent a customization method using a pair of person-garment images, which\nsignificantly improves fidelity and authenticity. Our experimental results show\nthat our method outperforms previous approaches (both diffusion-based and\nGAN-based) in preserving garment details and generating authentic virtual\ntry-on images, both qualitatively and quantitatively. Furthermore, the proposed\ncustomization method demonstrates its effectiveness in a real-world scenario.\nMore visualizations are available in our project page:\nhttps://idm-vton.github.io\n","authors":["Yisol Choi","Sangkyung Kwak","Kyungmin Lee","Hyungwon Choi","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2403.05139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12434v1","updated":"2024-03-19T04:47:56Z","published":"2024-03-19T04:47:56Z","title":"Human Mesh Recovery from Arbitrary Multi-view Images","summary":"  Human mesh recovery from arbitrary multi-view images involves two\ncharacteristics: the arbitrary camera poses and arbitrary number of camera\nviews. Because of the variability, designing a unified framework to tackle this\ntask is challenging. The challenges can be summarized as the dilemma of being\nable to simultaneously estimate arbitrary camera poses and recover human mesh\nfrom arbitrary multi-view images while maintaining flexibility. To solve this\ndilemma, we propose a divide and conquer framework for Unified Human Mesh\nRecovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR\nconsists of a decoupled structure and two main components: camera and body\ndecoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion\n(AVF). As camera poses and human body mesh are independent of each other, CBD\nsplits the estimation of them into two sub-tasks for two individual\nsub-networks (\\ie, CPE and AVF) to handle respectively, thus the two sub-tasks\nare disentangled. In CPE, since each camera pose is unrelated to the others, we\nadopt a shared MLP to process all views in a parallel way. In AVF, in order to\nfuse multi-view information and make the fusion operation independent of the\nnumber of views, we introduce a transformer decoder with a SMPL parameters\nquery token to extract cross-view features for mesh recovery. To demonstrate\nthe efficacy and flexibility of the proposed framework and effect of each\ncomponent, we conduct extensive experiments on three public datasets:\nHuman3.6M, MPI-INF-3DHP, and TotalCapture.\n","authors":["Xiaoben Li","Mancheng Meng","Ziyan Wu","Terrence Chen","Fan Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2403.12434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06793v2","updated":"2024-03-19T04:46:42Z","published":"2024-03-11T15:11:57Z","title":"Boosting Image Restoration via Priors from Pre-trained Models","summary":"  Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.\n","authors":["Xiaogang Xu","Shu Kong","Tao Hu","Zhe Liu","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2403.06793v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2401.04325v2","updated":"2024-03-19T04:45:47Z","published":"2024-01-09T02:40:03Z","title":"RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned\n  Metric Scale","summary":"  We present a novel approach for metric dense depth estimation based on the\nfusion of a single-view image and a sparse, noisy Radar point cloud. The direct\nfusion of heterogeneous Radar and image data, or their encodings, tends to\nyield dense depth maps with significant artifacts, blurred boundaries, and\nsuboptimal accuracy. To circumvent this issue, we learn to augment versatile\nand robust monocular depth prediction with the dense metric scale induced from\nsparse and noisy Radar data. We propose a Radar-Camera framework for highly\naccurate and fine-detailed dense depth estimation with four stages, including\nmonocular depth prediction, global scale alignment of monocular depth with\nsparse Radar points, quasi-dense scale estimation through learning the\nassociation between Radar points and image patches, and local scale refinement\nof dense depth using a scale map learner. Our proposed method significantly\noutperforms the state-of-the-art Radar-Camera depth estimation methods by\nreducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2%\non the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam\ndataset, respectively. Our code and dataset will be released at\n\\url{https://github.com/MMOCKING/RadarCam-Depth}.\n","authors":["Han Li","Yukai Ma","Yaqing Gu","Kewei Hu","Yong Liu","Xingxing Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.04325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03441v3","updated":"2024-03-19T04:45:07Z","published":"2023-12-06T11:50:14Z","title":"UFineBench: Towards Text-based Person Retrieval with Ultra-fine\n  Granularity","summary":"  Existing text-based person retrieval datasets often have relatively\ncoarse-grained text annotations. This hinders the model to comprehend the\nfine-grained semantics of query texts in real scenarios. To address this\nproblem, we contribute a new benchmark named \\textbf{UFineBench} for text-based\nperson retrieval with ultra-fine granularity.\n  Firstly, we construct a new \\textbf{dataset} named UFine6926. We collect a\nlarge number of person images and manually annotate each image with two\ndetailed textual descriptions, averaging 80.8 words each. The average word\ncount is three to four times that of the previous datasets. In addition of\nstandard in-domain evaluation, we also propose a special \\textbf{evaluation\nparadigm} more representative of real scenarios. It contains a new evaluation\nset with cross domains, cross textual granularity and cross textual styles,\nnamed UFine3C, and a new evaluation metric for accurately measuring retrieval\nability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a\nmore efficient \\textbf{algorithm} especially designed for text-based person\nretrieval with ultra fine-grained texts. It achieves fine granularity mining by\nadopting a shared cross-modal granularity decoder and hard negative match\nmechanism.\n  With standard in-domain evaluation, CFAM establishes competitive performance\nacross various datasets, especially on our ultra fine-grained UFine6926.\nFurthermore, by evaluating on UFine3C, we demonstrate that training on our\nUFine6926 significantly improves generalization to real scenarios compared with\nother coarse-grained datasets. The dataset and code will be made publicly\navailable at \\url{https://github.com/Zplusdragon/UFineBench}.\n","authors":["Jialong Zuo","Hanyu Zhou","Ying Nie","Feng Zhang","Tianyu Guo","Nong Sang","Yunhe Wang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2312.03441v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12432v1","updated":"2024-03-19T04:44:09Z","published":"2024-03-19T04:44:09Z","title":"Prototipo de video juego activo basado en una cámara 3D para motivar\n  la actividad física en niños y adultos mayores","summary":"  This document describes the development of a video game prototype designed to\nencourage physical activity among children and older adults. The prototype\nconsists of a laptop, a camera with 3D sensors, and optionally requires an LCD\nscreen or a projector. The programming component of this prototype was\ndeveloped in Scratch, a programming language geared towards children, which\ngreatly facilitates the creation of a game tailored to the users' preferences.\nThe idea to create such a prototype originated from the desire to offer an\noption that promotes physical activity among children and adults, given that a\nlack of physical exercise is a predominant factor in the development of chronic\ndegenerative diseases such as diabetes and hypertension, to name the most\ncommon. As a result of this initiative, an active video game prototype was\nsuccessfully developed, based on a ping-pong game, which allows both children\nand adults to interact in a fun way while encouraging the performance of\nphysical activities that can positively impact the users' health.\n","authors":["Benjamín Ojeda Magaña","José Guadalupe Robledo Hernández","Leopoldo Gómez Barba","Victor Manuel Rangel Cobián"],"pdf_url":"https://arxiv.org/pdf/2403.12432v1.pdf","comment":"13 pages, in Spanish language, 11 figures"},{"id":"http://arxiv.org/abs/2403.12431v1","updated":"2024-03-19T04:41:09Z","published":"2024-03-19T04:41:09Z","title":"Geometric Constraints in Deep Learning Frameworks: A Survey","summary":"  Stereophotogrammetry is an emerging technique of scene understanding. Its\norigins go back to at least the 1800s when people first started to investigate\nusing photographs to measure the physical properties of the world. Since then,\nthousands of approaches have been explored. The classic geometric techniques of\nShape from Stereo is built on using geometry to define constraints on scene and\ncamera geometry and then solving the non-linear systems of equations. More\nrecent work has taken an entirely different approach, using end-to-end deep\nlearning without any attempt to explicitly model the geometry. In this survey,\nwe explore the overlap for geometric-based and deep learning-based frameworks.\nWe compare and contrast geometry enforcing constraints integrated into a deep\nlearning framework for depth estimation or other closely related problems. We\npresent a new taxonomy for prevalent geometry enforcing constraints used in\nmodern deep learning frameworks. We also present insightful observations and\npotential future research directions.\n","authors":["Vibhas K Vats","David J Crandall"],"pdf_url":"https://arxiv.org/pdf/2403.12431v1.pdf","comment":"A preprint"},{"id":"http://arxiv.org/abs/2403.11127v2","updated":"2024-03-19T04:40:43Z","published":"2024-03-17T07:29:32Z","title":"GRA: Detecting Oriented Objects through Group-wise Rotating and\n  Attention","summary":"  Oriented object detection, an emerging task in recent years, aims to identify\nand locate objects across varied orientations. This requires the detector to\naccurately capture the orientation information, which varies significantly\nwithin and across images. Despite the existing substantial efforts,\nsimultaneously ensuring model effectiveness and parameter efficiency remains\nchallenging in this scenario. In this paper, we propose a lightweight yet\neffective Group-wise Rotating and Attention (GRA) module to replace the\nconvolution operations in backbone networks for oriented object detection. GRA\ncan adaptively capture fine-grained features of objects with diverse\norientations, comprising two key components: Group-wise Rotating and Group-wise\nAttention. Group-wise Rotating first divides the convolution kernel into\ngroups, where each group extracts different object features by rotating at a\nspecific angle according to the object orientation. Subsequently, Group-wise\nAttention is employed to adaptively enhance the object-related regions in the\nfeature. The collaborative effort of these components enables GRA to\neffectively capture the various orientation information while maintaining\nparameter efficiency. Extensive experimental results demonstrate the\nsuperiority of our method. For example, GRA achieves a new state-of-the-art\n(SOTA) on the DOTA-v2.0 benchmark, while saving the parameters by nearly 50%\ncompared to the previous SOTA method. Code will be released.\n","authors":["Jiangshan Wang","Yifan Pu","Yizeng Han","Jiayi Guo","Yiru Wang","Xiu Li","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11127v2.pdf","comment":"tech report"},{"id":"http://arxiv.org/abs/2403.11697v2","updated":"2024-03-19T04:37:01Z","published":"2024-03-18T11:54:35Z","title":"Urban Scene Diffusion through Semantic Occupancy Map","summary":"  Generating unbounded 3D scenes is crucial for large-scale scene understanding\nand simulation. Urban scenes, unlike natural landscapes, consist of various\ncomplex man-made objects and structures such as roads, traffic signs, vehicles,\nand buildings. To create a realistic and detailed urban scene, it is crucial to\naccurately represent the geometry and semantics of the underlying objects,\ngoing beyond their visual appearance. In this work, we propose UrbanDiffusion,\na 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and\ngenerates an urban scene with geometry and semantics in the form of semantic\noccupancy map. Our model introduces a novel paradigm that learns the data\ndistribution of scene-level structures within a latent space and further\nenables the expansion of the synthesized scene into an arbitrary scale. After\ntraining on real-world driving datasets, our model can generate a wide range of\ndiverse urban scenes given the BEV maps from the held-out set and also\ngeneralize to the synthesized maps from a driving simulator. We further\ndemonstrate its application to scene image synthesis with a pretrained image\ngenerator as a prior.\n","authors":["Junge Zhang","Qihang Zhang","Li Zhang","Ramana Rao Kompella","Gaowen Liu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.11697v2.pdf","comment":"The project website is https://metadriverse.github.io/urbandiff/"},{"id":"http://arxiv.org/abs/2403.12429v1","updated":"2024-03-19T04:36:41Z","published":"2024-03-19T04:36:41Z","title":"TransformMix: Learning Transformation and Mixing Strategies from Data","summary":"  Data augmentation improves the generalization power of deep learning models\nby synthesizing more training samples. Sample-mixing is a popular data\naugmentation approach that creates additional data by combining existing\nsamples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple\nmixing operations to blend multiple inputs. Although such a heuristic approach\nshows certain performance gains in some computer vision tasks, it mixes the\nimages blindly and does not adapt to different datasets automatically. A mixing\nstrategy that is effective for a particular dataset does not often generalize\nwell to other datasets. If not properly configured, the methods may create\nmisleading mixed images, which jeopardize the effectiveness of sample-mixing\naugmentations. In this work, we propose an automated approach, TransformMix, to\nlearn better transformation and mixing augmentation strategies from data. In\nparticular, TransformMix applies learned transformations and mixing masks to\ncreate compelling mixed images that contain correct and important information\nfor the target tasks. We demonstrate the effectiveness of TransformMix on\nmultiple datasets in transfer learning, classification, object detection, and\nknowledge distillation settings. Experimental results show that our method\nachieves better performance as well as efficiency when compared with strong\nsample-mixing baselines.\n","authors":["Tsz-Him Cheung","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2403.12429v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.12425v1","updated":"2024-03-19T04:25:54Z","published":"2024-03-19T04:25:54Z","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation","summary":"  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n","authors":["Jun Yu","Gongpeng Zhao","Yongqi Wan","Zhihong Wei","Yang Zheng","Zerui Zhang","Zhongpeng Cai","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12425v1.pdf","comment":"6 pages,1 figures"},{"id":"http://arxiv.org/abs/2310.16387v2","updated":"2024-03-19T04:15:28Z","published":"2023-10-25T05:59:25Z","title":"FTIC: Frequency-Aware Transformer for Learned Image Compression","summary":"  Learned image compression (LIC) has gained traction as an effective solution\nfor image storage and transmission in recent years. However, existing LIC\nmethods are redundant in latent representation due to limitations in capturing\nanisotropic frequency components and preserving directional details. To\novercome these challenges, we propose a novel frequency-aware transformer (FAT)\nblock that for the first time achieves multiscale directional ananlysis for\nLIC. The FAT block comprises frequency-decomposition window attention (FDWA)\nmodules to capture multiscale and directional frequency components of natural\nimages. Additionally, we introduce frequency-modulation feed-forward network\n(FMFFN) to adaptively modulate different frequency components, improving\nrate-distortion performance. Furthermore, we present a transformer-based\nchannel-wise autoregressive (T-CA) model that effectively exploits channel\ndependencies. Experiments show that our method achieves state-of-the-art\nrate-distortion performance compared to existing LIC methods, and evidently\noutperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in\nBD-rate on the Kodak, Tecnick, and CLIC datasets.\n","authors":["Han Li","Shaohui Li","Wenrui Dai","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16387v2.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2301.10048v2","updated":"2024-03-19T04:02:28Z","published":"2023-01-24T14:44:44Z","title":"Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting","summary":"  Transformers have been widely used for video processing owing to the\nmulti-head self attention (MHSA) mechanism. However, the MHSA mechanism\nencounters an intrinsic difficulty for video inpainting, since the features\nassociated with the corrupted regions are degraded and incur inaccurate self\nattention. This problem, termed query degradation, may be mitigated by first\ncompleting optical flows and then using the flows to guide the self attention,\nwhich was verified in our previous work - flow-guided transformer (FGT). We\nfurther exploit the flow guidance and propose FGT++ to pursue more effective\nand efficient video inpainting. First, we design a lightweight flow completion\nnetwork by using local aggregation and edge loss. Second, to address the query\ndegradation, we propose a flow guidance feature integration module, which uses\nthe motion discrepancy to enhance the features, together with a flow-guided\nfeature propagation module that warps the features according to the flows.\nThird, we decouple the transformer along the temporal and spatial dimensions,\nwhere flows are used to select the tokens through a temporally deformable MHSA\nmechanism, and global tokens are combined with the inner-window local tokens\nthrough a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to\nbe outperforming the existing video inpainting networks qualitatively and\nquantitatively.\n","authors":["Kaidong Zhang","Jialun Peng","Jingjing Fu","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2301.10048v2.pdf","comment":"Accepted to TPAMI. This manuscript is a journal extension of our ECCV\n  2022 paper (arXiv:2208.06768)"},{"id":"http://arxiv.org/abs/2403.12416v1","updated":"2024-03-19T03:59:14Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment Framework for Radiology","summary":"  In multi-modal frameworks, the alignment of cross-modal features presents a\nsignificant challenge. The predominant approach in multi-modal pre-training\nemphasizes either global or local alignment between modalities, utilizing\nextensive datasets. This bottom-up driven method often suffers from a lack of\ninterpretability, a critical concern in radiology. Previous studies have\nintegrated high-level labels in medical images or text, but these still rely on\nmanual annotation, a costly and labor-intensive process. Our work introduces a\nnovel approach by using eye-gaze data, collected synchronously by radiologists\nduring diagnostic evaluations. This data, indicating radiologists' focus areas,\nnaturally links chest X-rays to diagnostic texts. We propose the Eye-gaze\nGuided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for\nbetter alignment of image and text features, aiming to reduce reliance on\nmanual annotations and thus cut training costs. Our model demonstrates robust\nperformance, outperforming other state-of-the-art methods in zero-shot\nclassification and retrieval tasks. The incorporation of easily-obtained\neye-gaze data during routine radiological diagnoses signifies a step towards\nminimizing manual annotation dependency. Additionally, we explore the impact of\nvarying amounts of eye-gaze data on model performance, highlighting the\nfeasibility and utility of integrating this auxiliary data into multi-modal\npre-training.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Zihao Wu","Xiaowei Yu","Fang Zeng","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12415v1","updated":"2024-03-19T03:55:39Z","published":"2024-03-19T03:55:39Z","title":"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual\n  Navigation","summary":"  This paper explores the potential of Large Language Models(LLMs) in zero-shot\nanomaly detection for safe visual navigation. With the assistance of the\nstate-of-the-art real-time open-world object detection model Yolo-World and\nspecialized prompts, the proposed framework can identify anomalies within\ncamera-captured frames that include any possible obstacles, then generate\nconcise, audio-delivered descriptions emphasizing abnormalities, assist in safe\nvisual navigation in complex circumstances. Moreover, our proposed framework\nleverages the advantages of LLMs and the open-vocabulary object detection model\nto achieve the dynamic scenario switch, which allows users to transition\nsmoothly from scene to scene, which addresses the limitation of traditional\nvisual navigation. Furthermore, this paper explored the performance\ncontribution of different prompt components, provided the vision for future\nimprovement in visual accessibility, and paved the way for LLMs in video\nanomaly detection and vision-language understanding.\n","authors":["Hao Wang","Jiayou Qin","Ashish Bastola","Xiwen Chen","John Suchanek","Zihao Gong","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2403.12415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15562v3","updated":"2024-03-19T03:50:36Z","published":"2023-11-27T06:19:00Z","title":"Fully Authentic Visual Question Answering Dataset from Online\n  Communities","summary":"  Visual Question Answering (VQA) entails answering questions about images. We\nintroduce the first VQA dataset in which all contents originate from an\nauthentic use case. Sourced from online question answering community forums, we\ncall it VQAonline. We characterize this dataset and how it relates to eight\nmainstream VQA datasets. Observing that answers in our dataset tend to be much\nlonger (i.e., a mean of 173 words) and so incompatible with standard VQA\nevaluation metrics, we instead utilize popular metrics for longer text\nevaluation for evaluating six state-of-the-art VQA models on VQAonline and\nreport where they struggle most. Finally, we analyze which evaluation metrics\nalign best with human judgments. To facilitate future extensions, we\npublicly-share the dataset at: https://vqaonline.github.io/.\n","authors":["Chongyan Chen","Mengchen Liu","Noel Codella","Yunsheng Li","Lu Yuan","Danna Gurari"],"pdf_url":"https://arxiv.org/pdf/2311.15562v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08178v3","updated":"2024-03-19T03:47:39Z","published":"2024-01-16T07:51:00Z","title":"Key-point Guided Deformable Image Manipulation Using Diffusion Model","summary":"  In this paper, we introduce a Key-point-guided Diffusion probabilistic Model\n(KDM) that gains precise control over images by manipulating the object's\nkey-point. We propose a two-stage generative model incorporating an optical\nflow map as an intermediate output. By doing so, a dense pixel-wise\nunderstanding of the semantic relation between the image and sparse key point\nis configured, leading to more realistic image generation. Additionally, the\nintegration of optical flow helps regulate the inter-frame variance of\nsequential images, demonstrating an authentic sequential image generation. The\nKDM is evaluated with diverse key-point conditioned image synthesis tasks,\nincluding facial image generation, human pose synthesis, and echocardiography\nvideo prediction, demonstrating the KDM is proving consistency enhanced and\nphoto-realistic images compared with state-of-the-art models.\n","authors":["Seok-Hwan Oh","Guil Jung","Myeong-Gee Kim","Sang-Yun Kim","Young-Min Kim","Hyeon-Jik Lee","Hyuk-Sool Kwon","Hyeon-Min Bae"],"pdf_url":"https://arxiv.org/pdf/2401.08178v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2403.12409v1","updated":"2024-03-19T03:39:43Z","published":"2024-03-19T03:39:43Z","title":"ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware\n  Diffusion Guidance","summary":"  Generating high-quality 3D assets from a given image is highly desirable in\nvarious applications such as AR/VR. Recent advances in single-image 3D\ngeneration explore feed-forward models that learn to infer the 3D model of an\nobject without optimization. Though promising results have been achieved in\nsingle object generation, these methods often struggle to model complex 3D\nassets that inherently contain multiple objects. In this work, we present\nComboVerse, a 3D generation framework that produces high-quality 3D assets with\ncomplex compositions by learning to combine multiple models. 1) We first\nperform an in-depth analysis of this ``multi-object gap'' from both model and\ndata perspectives. 2) Next, with reconstructed 3D models of different objects,\nwe seek to adjust their sizes, rotation angles, and locations to create a 3D\nasset that matches the given image. 3) To automate this process, we apply\nspatially-aware score distillation sampling (SSDS) from pretrained diffusion\nmodels to guide the positioning of objects. Our proposed framework emphasizes\nspatial alignment of objects, compared with standard score distillation\nsampling, and thus achieves more accurate results. Extensive experiments\nvalidate ComboVerse achieves clear improvements over existing methods in\ngenerating compositional 3D assets.\n","authors":["Yongwei Chen","Tengfei Wang","Tong Wu","Xingang Pan","Kui Jia","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12409v1.pdf","comment":"https://cyw-3d.github.io/ComboVerse/"},{"id":"http://arxiv.org/abs/2403.11674v2","updated":"2024-03-19T03:33:22Z","published":"2024-03-18T11:21:52Z","title":"Towards Generalizing to Unseen Domains with Few Labels","summary":"  We approach the challenge of addressing semi-supervised domain generalization\n(SSDG). Specifically, our aim is to obtain a model that learns\ndomain-generalizable features by leveraging a limited subset of labelled data\nalongside a substantially larger pool of unlabeled data. Existing domain\ngeneralization (DG) methods which are unable to exploit unlabeled data perform\npoorly compared to semi-supervised learning (SSL) methods under SSDG setting.\nNevertheless, SSL methods have considerable room for performance improvement\nwhen compared to fully-supervised DG training. To tackle this underexplored,\nyet highly practical problem of SSDG, we make the following core contributions.\nFirst, we propose a feature-based conformity technique that matches the\nposterior distributions from the feature space with the pseudo-label from the\nmodel's output space. Second, we develop a semantics alignment loss to learn\nsemantically-compatible representations by regularizing the semantic structure\nin the feature space. Our method is plug-and-play and can be readily integrated\nwith different SSL-based SSDG baselines without introducing any additional\nparameters. Extensive experimental results across five challenging DG\nbenchmarks with four strong SSL baselines suggest that our method provides\nconsistent and notable gains in two different SSDG settings.\n","authors":["Chamuditha Jayanga Galappaththige","Sanoojan Baliah","Malitha Gunawardhana","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2403.11674v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12404v1","updated":"2024-03-19T03:27:01Z","published":"2024-03-19T03:27:01Z","title":"Understanding Training-free Diffusion Guidance: Mechanisms and\n  Limitations","summary":"  Adding additional control to pretrained diffusion models has become an\nincreasingly popular research area, with extensive applications in computer\nvision, reinforcement learning, and AI for science. Recently, several studies\nhave proposed training-free diffusion guidance by using off-the-shelf networks\npretrained on clean images. This approach enables zero-shot conditional\ngeneration for universal control formats, which appears to offer a free lunch\nin diffusion guidance. In this paper, we aim to develop a deeper understanding\nof the operational mechanisms and fundamental limitations of training-free\nguidance. We offer a theoretical analysis that supports training-free guidance\nfrom the perspective of optimization, distinguishing it from classifier-based\n(or classifier-free) guidance. To elucidate their drawbacks, we theoretically\ndemonstrate that training-free methods are more susceptible to adversarial\ngradients and exhibit slower convergence rates compared to classifier guidance.\nWe then introduce a collection of techniques designed to overcome the\nlimitations, accompanied by theoretical rationale and empirical evidence. Our\nexperiments in image and motion generation confirm the efficacy of these\ntechniques.\n","authors":["Yifei Shen","Xinyang Jiang","Yezhen Wang","Yifan Yang","Dongqi Han","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.12404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09297v2","updated":"2024-03-19T03:25:50Z","published":"2023-09-17T15:14:01Z","title":"Chasing Day and Night: Towards Robust and Efficient All-Day Object\n  Detection Guided by an Event Camera","summary":"  The ability to detect objects in all lighting (i.e., normal-, over-, and\nunder-exposed) conditions is crucial for real-world applications, such as\nself-driving.Traditional RGB-based detectors often fail under such varying\nlighting conditions.Therefore, recent works utilize novel event cameras to\nsupplement or guide the RGB modality; however, these methods typically adopt\nasymmetric network structures that rely predominantly on the RGB modality,\nresulting in limited robustness for all-day detection. In this paper, we\npropose EOLO, a novel object detection framework that achieves robust and\nefficient all-day detection by fusing both RGB and event modalities. Our EOLO\nframework is built based on a lightweight spiking neural network (SNN) to\nefficiently leverage the asynchronous property of events. Buttressed by it, we\nfirst introduce an Event Temporal Attention (ETA) module to learn the high\ntemporal information from events while preserving crucial edge information.\nSecondly, as different modalities exhibit varying levels of importance under\ndiverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion\n(SREF) module to effectively fuse RGB-Event features without relying on a\nspecific modality, thus ensuring a balanced and adaptive fusion for all-day\ndetection. In addition, to compensate for the lack of paired RGB-Event datasets\nfor all-day training and evaluation, we propose an event synthesis approach\nbased on the randomized optical flow that allows for directly generating the\nevent frame from a single exposure image. We further build two new datasets,\nE-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC.\nExtensive experiments demonstrate that our EOLO outperforms the\nstate-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in\nall lighting conditions.Our code and datasets will be available at\nhttps://vlislab22.github.io/EOLO/\n","authors":["Jiahang Cao","Xu Zheng","Yuanhuiyi Lyu","Jiaxu Wang","Renjing Xu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.09297v2.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.12401v1","updated":"2024-03-19T03:19:07Z","published":"2024-03-19T03:19:07Z","title":"VQ-NeRV: A Vector Quantized Neural Representation for Videos","summary":"  Implicit neural representations (INR) excel in encoding videos within neural\nnetworks, showcasing promise in computer vision tasks like video compression\nand denoising. INR-based approaches reconstruct video frames from\ncontent-agnostic embeddings, which hampers their efficacy in video frame\nregression and restricts their generalization ability for video interpolation.\nTo address these deficiencies, Hybrid Neural Representation for Videos (HNeRV)\nwas introduced with content-adaptive embeddings. Nevertheless, HNeRV's\ncompression ratios remain relatively low, attributable to an oversight in\nleveraging the network's shallow features and inter-frame residual information.\nIn this work, we introduce an advanced U-shaped architecture, Vector\nQuantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV\nBlock. This block incorporates a codebook mechanism to discretize the network's\nshallow residual features and inter-frame residual information effectively.\nThis approach proves particularly advantageous in video compression, as it\nresults in smaller size compared to quantized features. Furthermore, we\nintroduce an original codebook optimization technique, termed shallow codebook\noptimization, designed to refine the utility and efficiency of the codebook.\nThe experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video\nregression tasks, delivering superior reconstruction quality (with an increase\nof 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp)\nefficiency, and improved video inpainting outcomes.\n","authors":["Yunjie Xu","Xiang Feng","Feiwei Qin","Ruiquan Ge","Yong Peng","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12401v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.12396v1","updated":"2024-03-19T03:09:24Z","published":"2024-03-19T03:09:24Z","title":"OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation","summary":"  This paper studies a new open-set problem, the open-vocabulary category-level\nobject pose and size estimation. Given human text descriptions of arbitrary\nnovel object categories, the robot agent seeks to predict the position,\norientation, and size of the target object in the observed scene image. To\nenable such generalizability, we first introduce OO3D-9D, a large-scale\nphotorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the\nlargest and most diverse dataset in the field of category-level object pose and\nsize estimation. It includes additional annotations for the symmetry axis of\neach category, which help resolve symmetric ambiguity. Apart from the\nlarge-scale dataset, we find another key to enabling such generalizability is\nleveraging the strong prior knowledge in pre-trained visual-language foundation\nmodels. We then propose a framework built on pre-trained DinoV2 and\ntext-to-image stable diffusion models to infer the normalized object coordinate\nspace (NOCS) maps of the target instances. This framework fully leverages the\nvisual semantic prior from DinoV2 and the aligned visual and language knowledge\nwithin the text-to-image diffusion model, which enables generalization to\nvarious text descriptions of novel categories. Comprehensive quantitative and\nqualitative experiments demonstrate that the proposed open-vocabulary method,\ntrained on our large-scale synthesized data, significantly outperforms the\nbaseline and can effectively generalize to real-world images of unseen\ncategories. The project page is at https://ov9d.github.io.\n","authors":["Junhao Cai","Yisheng He","Weihao Yuan","Siyu Zhu","Zilong Dong","Liefeng Bo","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.12396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10147v2","updated":"2024-03-19T03:03:14Z","published":"2024-03-15T09:47:35Z","title":"GGRt: Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time","summary":"  This paper presents GGRt, a novel approach to generalizable novel view\nsynthesis that alleviates the need for real camera poses, complexity in\nprocessing high-resolution images, and lengthy optimization processes, thus\nfacilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in\nreal-world scenarios. Specifically, we design a novel joint learning framework\nthat consists of an Iterative Pose Optimization Network (IPO-Net) and a\nGeneralizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,\nthe proposed framework can inherently estimate robust relative pose information\nfrom the image observations and thus primarily alleviate the requirement of\nreal camera poses. Moreover, we implement a deferred back-propagation mechanism\nthat enables high-resolution training and inference, overcoming the resolution\nconstraints of previous methods. To enhance the speed and efficiency, we\nfurther introduce a progressive Gaussian cache module that dynamically adjusts\nduring training and inference. As the first pose-free generalizable 3D-GS\nframework, GGRt achieves inference at $\\ge$ 5 FPS and real-time rendering at\n$\\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our\nmethod outperforms existing NeRF-based pose-free techniques in terms of\ninference speed and effectiveness. It can also approach the real pose-based\n3D-GS methods. Our contributions provide a significant leap forward for the\nintegration of computer vision and computer graphics into practical\napplications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open\ndatasets and enabling real-time rendering for immersive experiences.\n","authors":["Hao Li","Yuanyuan Gao","Chenming Wu","Dingwen Zhang","Yalun Dai","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2403.10147v2.pdf","comment":"Project page:\n  \\href{https://3d-aigc.github.io/GGRt}{https://3d-aigc.github.io/GGRt}"},{"id":"http://arxiv.org/abs/2311.06455v2","updated":"2024-03-19T02:59:03Z","published":"2023-11-11T01:56:35Z","title":"Aria-NeRF: Multimodal Egocentric View Synthesis","summary":"  We seek to accelerate research in developing rich, multimodal scene models\ntrained from egocentric data, based on differentiable volumetric ray-tracing\ninspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like\nmodel from an egocentric image sequence plays a pivotal role in understanding\nhuman behavior and holds diverse applications within the realms of VR/AR. Such\negocentric NeRF-like models may be used as realistic simulations, contributing\nsignificantly to the advancement of intelligent agents capable of executing\ntasks in the real-world. The future of egocentric view synthesis may lead to\nnovel environment representations going beyond today's NeRFs by augmenting\nvisual data with multimodal sensors such as IMU for egomotion tracking, audio\nsensors to capture surface texture and human language context, and eye-gaze\ntrackers to infer human attention patterns in the scene. To support and\nfacilitate the development and evaluation of egocentric multimodal scene\nmodeling, we present a comprehensive multimodal egocentric video dataset. This\ndataset offers a comprehensive collection of sensory data, featuring RGB\nimages, eye-tracking camera footage, audio recordings from a microphone,\natmospheric pressure readings from a barometer, positional coordinates from\nGPS, connectivity details from Wi-Fi and Bluetooth, and information from\ndual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The\ndataset was collected with the Meta Aria Glasses wearable device platform. The\ndiverse data modalities and the real-world context captured within this dataset\nserve as a robust foundation for furthering our understanding of human behavior\nand enabling more immersive and intelligent experiences in the realms of VR,\nAR, and robotics.\n","authors":["Jiankai Sun","Jianing Qiu","Chuanyang Zheng","John Tucker","Javier Yu","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2311.06455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12385v1","updated":"2024-03-19T02:52:06Z","published":"2024-03-19T02:52:06Z","title":"VideoBadminton: A Video Dataset for Badminton Action Recognition","summary":"  In the dynamic and evolving field of computer vision, action recognition has\nbecome a key focus, especially with the advent of sophisticated methodologies\nlike Convolutional Neural Networks (CNNs), Convolutional 3D, Transformer, and\nspatial-temporal feature fusion. These technologies have shown promising\nresults on well-established benchmarks but face unique challenges in real-world\napplications, particularly in sports analysis, where the precise decomposition\nof activities and the distinction of subtly different actions are crucial.\nExisting datasets like UCF101, HMDB51, and Kinetics have offered a diverse\nrange of video data for various scenarios. However, there's an increasing need\nfor fine-grained video datasets that capture detailed categorizations and\nnuances within broader action categories. In this paper, we introduce the\nVideoBadminton dataset derived from high-quality badminton footage. Through an\nexhaustive evaluation of leading methodologies on this dataset, this study aims\nto advance the field of action recognition, particularly in badminton sports.\nThe introduction of VideoBadminton could not only serve for badminton action\nrecognition but also provide a dataset for recognizing fine-grained actions.\nThe insights gained from these evaluations are expected to catalyze further\nresearch in action comprehension, especially within sports contexts.\n","authors":["Qi Li","Tzu-Chen Chiu","Hsiang-Wei Huang","Min-Te Sun","Wei-Shinn Ku"],"pdf_url":"https://arxiv.org/pdf/2403.12385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12382v1","updated":"2024-03-19T02:47:33Z","published":"2024-03-19T02:47:33Z","title":"Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising","summary":"  Deep learning-based denoiser has been the focus of recent development on\nimage denoising. In the past few years, there has been increasing interest in\ndeveloping self-supervised denoising networks that only require noisy images,\nwithout the need for clean ground truth for training. However, a performance\ngap remains between current self-supervised methods and their supervised\ncounterparts. Additionally, these methods commonly depend on assumptions about\nnoise characteristics, thereby constraining their applicability in real-world\nscenarios. Inspired by the properties of the Frobenius norm expansion, we\ndiscover that incorporating a trace term reduces the optimization goal\ndisparity between self-supervised and supervised methods, thereby enhancing the\nperformance of self-supervised learning. To exploit this insight, we propose a\ntrace-constraint loss function and design the low-trace adaptation Noise2Noise\n(LoTA-N2N) model that bridges the gap between self-supervised and supervised\nlearning. Furthermore, we have discovered that several existing self-supervised\ndenoising frameworks naturally fall within the proposed trace-constraint loss\nas subcases. Extensive experiments conducted on natural and confocal image\ndatasets indicate that our method achieves state-of-the-art performance within\nthe realm of zero-shot self-supervised image denoising approaches, without\nrelying on any assumptions regarding the noise.\n","authors":["Jintong Hu","Bin Xia","Bingchen Li","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12382v1.pdf","comment":"11pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.10801v2","updated":"2024-03-19T02:45:48Z","published":"2024-03-16T04:23:46Z","title":"Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples","summary":"  With the evolution of self-supervised learning, the pre-training paradigm has\nemerged as a predominant solution within the deep learning landscape. Model\nproviders furnish pre-trained encoders designed to function as versatile\nfeature extractors, enabling downstream users to harness the benefits of\nexpansive models with minimal effort through fine-tuning. Nevertheless, recent\nworks have exposed a vulnerability in pre-trained encoders, highlighting their\nsusceptibility to downstream-agnostic adversarial examples (DAEs) meticulously\ncrafted by attackers. The lingering question pertains to the feasibility of\nfortifying the robustness of downstream models against DAEs, particularly in\nscenarios where the pre-trained encoders are publicly accessible to the\nattackers.\n  In this paper, we initially delve into existing defensive mechanisms against\nadversarial examples within the pre-training paradigm. Our findings reveal that\nthe failure of current defenses stems from the domain shift between\npre-training data and downstream tasks, as well as the sensitivity of encoder\nparameters. In response to these challenges, we propose Genetic\nEvolution-Nurtured Adversarial Fine-tuning (Gen-AF), a two-stage adversarial\nfine-tuning approach aimed at enhancing the robustness of downstream models.\nOur extensive experiments, conducted across ten self-supervised training\nmethods and six datasets, demonstrate that Gen-AF attains high testing accuracy\nand robust testing accuracy against state-of-the-art DAEs.\n","authors":["Ziqi Zhou","Minghui Li","Wei Liu","Shengshan Hu","Yechao Zhang","Wei Wan","Lulu Xue","Leo Yu Zhang","Dezhong Yao","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.10801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10523v2","updated":"2024-03-19T02:42:28Z","published":"2023-04-20T17:52:58Z","title":"GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape\n  Generative Models","summary":"  This paper introduces GenCorres, a novel unsupervised joint shape matching\n(JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized\ndeformable shape collection while constraining deformations between adjacent\nsynthetic shapes to preserve geometric structures such as local rigidity and\nlocal conformality. GenCorres presents three appealing advantages over existing\nJSM techniques. First, GenCorres performs JSM among a synthetic shape\ncollection whose size is much bigger than the input shapes and fully leverages\nthe datadriven power of JSM. Second, GenCorres unifies consistent shape\nmatching and pairwise matching (i.e., by enforcing deformation priors between\nadjacent synthetic shapes). Third, the generator provides a concise encoding of\nconsistent shape correspondences. However, learning a mesh generator from an\nunorganized shape collection is challenging, requiring a good initialization.\nGenCorres addresses this issue by learning an implicit generator from the input\nshapes, which provides intermediate shapes between two arbitrary shapes. We\nintroduce a novel approach for computing correspondences between adjacent\nimplicit surfaces, which we use to regularize the implicit generator. Synthetic\nshapes of the implicit generator then guide initial fittings (i.e., via\ntemplate-based deformation) for learning the mesh generator. Experimental\nresults show that GenCorres considerably outperforms state-of-the-art JSM\ntechniques. The synthetic shapes of GenCorres also achieve salient performance\ngains against state-of-the-art deformable shape generators.\n","authors":["Haitao Yang","Xiangru Huang","Bo Sun","Chandrajit Bajaj","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2304.10523v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12370v1","updated":"2024-03-19T02:29:34Z","published":"2024-03-19T02:29:34Z","title":"XPose: eXplainable Human Pose Estimation","summary":"  Current approaches in pose estimation primarily concentrate on enhancing\nmodel architectures, often overlooking the importance of comprehensively\nunderstanding the rationale behind model decisions. In this paper, we propose\nXPose, a novel framework that incorporates Explainable AI (XAI) principles into\npose estimation. This integration aims to elucidate the individual contribution\nof each keypoint to final prediction, thereby elevating the model's\ntransparency and interpretability. Conventional XAI techniques have\npredominantly addressed tasks with single-target tasks like classification.\nAdditionally, the application of Shapley value, a common measure in XAI, to\npose estimation has been hindered by prohibitive computational demands.\n  To address these challenges, this work introduces an innovative concept\ncalled Group Shapley Value (GSV). This approach strategically organizes\nkeypoints into clusters based on their interdependencies. Within these\nclusters, GSV meticulously calculates Shapley value for keypoints, while for\ninter-cluster keypoints, it opts for a more holistic group-level valuation.\nThis dual-level computation framework meticulously assesses keypoint\ncontributions to the final outcome, optimizing computational efficiency.\nBuilding on the insights into keypoint interactions, we devise a novel data\naugmentation technique known as Group-based Keypoint Removal (GKR). This method\ningeniously removes individual keypoints during training phases, deliberately\npreserving those with strong mutual connections, thereby refining the model's\npredictive prowess for non-visible keypoints. The empirical validation of GKR\nacross a spectrum of standard approaches attests to its efficacy. GKR's success\ndemonstrates how using Explainable AI (XAI) can directly enhance pose\nestimation models.\n","authors":["Luyu Qiu","Jianing Li","Lei Wen","Chi Su","Fei Hao","Chen Jason Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.12370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10911v2","updated":"2024-03-19T02:26:55Z","published":"2024-03-16T12:18:20Z","title":"Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation","summary":"  Test-time adaptation (TTA) addresses the unforeseen distribution shifts\noccurring during test time. In TTA, both performance and, memory and time\nconsumption serve as crucial considerations. A recent diffusion-based TTA\napproach for restoring corrupted images involves image-level updates. However,\nusing pixel space diffusion significantly increases resource requirements\ncompared to conventional model updating TTA approaches, revealing limitations\nas a TTA method. To address this, we propose a novel TTA method by leveraging a\nlatent diffusion model (LDM) based image editing model and fine-tuning it with\nour newly introduced corruption modeling scheme. This scheme enhances the\nrobustness of the diffusion model against distribution shifts by creating\n(clean, corrupted) image pairs and fine-tuning the model to edit corrupted\nimages into clean ones. Moreover, we introduce a distilled variant to\naccelerate the model for corruption editing using only 4 network function\nevaluations (NFEs). We extensively validated our method across various\narchitectures and datasets including image and video domains. Our model\nachieves the best performance with a 100 times faster runtime than that of a\ndiffusion-based baseline. Furthermore, it outpaces the speed of the model\nupdating TTA method based on data augmentation threefold, rendering an\nimage-level updating approach more practical.\n","authors":["Yeongtak Oh","Jonghyun Lee","Jooyoung Choi","Dahuin Jung","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.10911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12365v1","updated":"2024-03-19T02:22:21Z","published":"2024-03-19T02:22:21Z","title":"GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation","summary":"  Creating 4D fields of Gaussian Splatting from images or videos is a\nchallenging task due to its under-constrained nature. While the optimization\ncan draw photometric reference from the input videos or be regulated by\ngenerative models, directly supervising Gaussian motions remains underexplored.\nIn this paper, we introduce a novel concept, Gaussian flow, which connects the\ndynamics of 3D Gaussians and pixel velocities between consecutive frames. The\nGaussian flow can be efficiently obtained by splatting Gaussian dynamics into\nthe image space. This differentiable process enables direct dynamic supervision\nfrom optical flow. Our method significantly benefits 4D dynamic content\ngeneration and 4D novel view synthesis with Gaussian Splatting, especially for\ncontents with rich motions that are hard to be handled by existing methods. The\ncommon color drifting issue that happens in 4D generation is also resolved with\nimproved Guassian dynamics. Superior visual quality on extensive experiments\ndemonstrates our method's effectiveness. Quantitative and qualitative\nevaluations show that our method achieves state-of-the-art results on both\ntasks of 4D generation and 4D novel view synthesis. Project page:\nhttps://zerg-overmind.github.io/GaussianFlow.github.io/\n","authors":["Quankai Gao","Qiangeng Xu","Zhe Cao","Ben Mildenhall","Wenchao Ma","Le Chen","Danhang Tang","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/2403.12365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12364v1","updated":"2024-03-19T02:19:57Z","published":"2024-03-19T02:19:57Z","title":"Class and Region-Adaptive Constraints for Network Calibration","summary":"  In this work, we present a novel approach to calibrate segmentation networks\nthat considers the inherent challenges posed by different categories and object\nregions. In particular, we present a formulation that integrates class and\nregion-wise constraints into the learning objective, with multiple penalty\nweights to account for class and region differences. Finding the optimal\npenalty weights manually, however, might be unfeasible, and potentially hinder\nthe optimization process. To overcome this limitation, we propose an approach\nbased on Class and Region-Adaptive constraints (CRaC), which allows to learn\nthe class and region-wise penalty weights during training. CRaC is based on a\ngeneral Augmented Lagrangian method, a well-established technique in\nconstrained optimization. Experimental results on two popular segmentation\nbenchmarks, and two well-known segmentation networks, demonstrate the\nsuperiority of CRaC compared to existing approaches. The code is available at:\nhttps://github.com/Bala93/CRac/\n","authors":["Balamurali Murugesan","Julio Silva-Rodriguez","Ismail Ben Ayed","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2403.12364v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.02628v2","updated":"2024-03-19T02:19:52Z","published":"2024-03-05T03:37:28Z","title":"Interactive Continual Learning: Fast and Slow Thinking","summary":"  Advanced life forms, sustained by the synergistic interaction of neural\ncognitive mechanisms, continually acquire and transfer knowledge throughout\ntheir lifespan. In contrast, contemporary machine learning paradigms exhibit\nlimitations in emulating the facets of continual learning (CL). Nonetheless,\nthe emergence of large language models (LLMs) presents promising avenues for\nrealizing CL via interactions with these models. Drawing on Complementary\nLearning System theory, this paper presents a novel Interactive Continual\nLearning (ICL) framework, enabled by collaborative interactions among models of\nvarious sizes. Specifically, we assign the ViT model as System1 and multimodal\nLLM as System2. To enable the memory module to deduce tasks from class\ninformation and enhance Set2Set retrieval, we propose the Class-Knowledge-Task\nMulti-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in\nSystem1 through enhanced geometric representation, we introduce the CL-vMF\nmechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we\nintroduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI)\nstrategy to identify hard examples, thus enhancing collaboration between\nSystem1 and System2 for complex reasoning realization. Comprehensive evaluation\nof our proposed ICL demonstrates significant resistance to forgetting and\nsuperior performance relative to existing methods. Code is available at\ngithub.com/ICL.\n","authors":["Biqing Qi","Xingquan Chen","Junqi Gao","Dong Li","Jianxing Liu","Ligang Wu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.02628v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12362v1","updated":"2024-03-19T02:16:32Z","published":"2024-03-19T02:16:32Z","title":"DMAD: Dual Memory Bank for Real-World Anomaly Detection","summary":"  Training a unified model is considered to be more suitable for practical\nindustrial anomaly detection scenarios due to its generalization ability and\nstorage efficiency. However, this multi-class setting, which exclusively uses\nnormal data, overlooks the few but important accessible annotated anomalies in\nthe real world. To address the challenge of real-world anomaly detection, we\npropose a new framework named Dual Memory bank enhanced representation learning\nfor Anomaly Detection (DMAD). This framework handles both unsupervised and\nsemi-supervised scenarios in a unified (multi-class) setting. DMAD employs a\ndual memory bank to calculate feature distance and feature attention between\nnormal and abnormal patterns, thereby encapsulating knowledge about normal and\nabnormal instances. This knowledge is then used to construct an enhanced\nrepresentation for anomaly score learning. We evaluated DMAD on the MVTec-AD\nand VisA datasets. The results show that DMAD surpasses current\nstate-of-the-art methods, highlighting DMAD's capability in handling the\ncomplexities of real-world anomaly detection scenarios.\n","authors":["Jianlong Hu","Xu Chen","Zhenye Gan","Jinlong Peng","Shengchuan Zhang","Jiangning Zhang","Yabiao Wang","Chengjie Wang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.12362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11689v2","updated":"2024-03-19T02:13:46Z","published":"2024-03-18T11:38:47Z","title":"MoreStyle: Relax Low-frequency Constraint of Fourier-based Image\n  Reconstruction in Generalizable Medical Image Segmentation","summary":"  The task of single-source domain generalization (SDG) in medical image\nsegmentation is crucial due to frequent domain shifts in clinical image\ndatasets. To address the challenge of poor generalization across different\ndomains, we introduce a Plug-and-Play module for data augmentation called\nMoreStyle. MoreStyle diversifies image styles by relaxing low-frequency\nconstraints in Fourier space, guiding the image reconstruction network. With\nthe help of adversarial learning, MoreStyle further expands the style range and\npinpoints the most intricate style combinations within latent features. To\nhandle significant style variations, we introduce an uncertainty-weighted loss.\nThis loss emphasizes hard-to-classify pixels resulting only from style shifts\nwhile mitigating true hard-to-classify pixels in both MoreStyle-generated and\noriginal images. Extensive experiments on two widely used benchmarks\ndemonstrate that the proposed MoreStyle effectively helps to achieve good\ndomain generalization ability, and has the potential to further boost the\nperformance of some state-of-the-art SDG methods.\n","authors":["Haoyu Zhao","Wenhui Dong","Rui Yu","Zhou Zhao","Du Bo","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.11689v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11672v2","updated":"2024-03-19T02:07:11Z","published":"2024-03-18T11:20:11Z","title":"WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT\n  Denoising","summary":"  In clinical examinations and diagnoses, low-dose computed tomography (LDCT)\nis crucial for minimizing health risks compared with normal-dose computed\ntomography (NDCT). However, reducing the radiation dose compromises the\nsignal-to-noise ratio, leading to degraded quality of CT images. To address\nthis, we analyze LDCT denoising task based on experimental results from the\nfrequency perspective, and then introduce a novel self-supervised CT image\ndenoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND\ncomprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware\nMulti-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by\nmainly adding noise to the high-frequency components, which is the main\ndifference between LDCT and NDCT. Second, to better capture high-frequency\ncomponents and detailed information, Frequency-Aware Multi-scale Loss (FAM) is\nproposed by effectively utilizing multi-scale feature space. Extensive\nexperiments on two public LDCT denoising datasets demonstrate that our\nWIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art\nweakly-supervised and self-supervised methods.\n","authors":["Haoyu Zhao","Yuliang Gu","Zhou Zhao","Bo Du","Yongchao Xu","Rui Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11672v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.13206v1","updated":"2024-03-19T23:54:07Z","published":"2024-03-19T23:54:07Z","title":"Depth-guided NeRF Training via Earth Mover's Distance","summary":"  Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of\npredicted viewpoints. However, the photometric loss often does not provide\nenough information to disambiguate between different possible geometries\nyielding the same image. Previous work has thus incorporated depth supervision\nduring NeRF training, leveraging dense predictions from pre-trained depth\nnetworks as pseudo-ground truth. While these depth priors are assumed to be\nperfect once filtered for noise, in practice, their accuracy is more\nchallenging to capture. This work proposes a novel approach to uncertainty in\ndepth priors for NeRF supervision. Instead of using custom-trained depth or\nuncertainty priors, we use off-the-shelf pretrained diffusion models to predict\ndepth and capture uncertainty during the denoising process. Because we know\nthat depth priors are prone to errors, we propose to supervise the ray\ntermination distance distribution with Earth Mover's Distance instead of\nenforcing the rendered depth to replicate the depth prior exactly through\nL2-loss. Our depth-guided NeRF outperforms all baselines on standard depth\nmetrics by a large margin while maintaining performance on photometric\nmeasures.\n","authors":["Anita Rau","Josiah Aklilu","F. Christopher Holsinger","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.13206v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2403.13204v1","updated":"2024-03-19T23:50:11Z","published":"2024-03-19T23:50:11Z","title":"Diversity-Aware Agnostic Ensemble of Sharpness Minimizers","summary":"  There has long been plenty of theoretical and empirical evidence supporting\nthe success of ensemble learning. Deep ensembles in particular take advantage\nof training randomness and expressivity of individual neural networks to gain\nprediction diversity, ultimately leading to better generalization, robustness\nand uncertainty estimation. In respect of generalization, it is found that\npursuing wider local minima result in models being more robust to shifts\nbetween training and testing sets. A natural research question arises out of\nthese two approaches as to whether a boost in generalization ability can be\nachieved if ensemble learning and loss sharpness minimization are integrated.\nOur work investigates this connection and proposes DASH - a learning algorithm\nthat promotes diversity and flatness within deep ensembles. More concretely,\nDASH encourages base learners to move divergently towards low-loss regions of\nminimal sharpness. We provide a theoretical backbone for our method along with\nextensive empirical evidence demonstrating an improvement in ensemble\ngeneralizability.\n","authors":["Anh Bui","Vy Vo","Tung Pham","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2403.13204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13199v1","updated":"2024-03-19T23:23:35Z","published":"2024-03-19T23:23:35Z","title":"DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced\n  Images","summary":"  Neural radiance fields (NeRFs) show potential for transforming images\ncaptured worldwide into immersive 3D visual experiences. However, most of this\ncaptured visual data remains siloed in our camera rolls as these images contain\npersonal details. Even if made public, the problem of learning 3D\nrepresentations of billions of scenes captured daily in a centralized manner is\ncomputationally intractable. Our approach, DecentNeRF, is the first attempt at\ndecentralized, crowd-sourced NeRFs that require $\\sim 10^4\\times$ less server\ncomputing for a scene than a centralized approach. Instead of sending the raw\ndata, our approach requires users to send a 3D representation, distributing the\nhigh computation cost of training centralized NeRFs between the users. It\nlearns photorealistic scene representations by decomposing users' 3D views into\npersonal and global NeRFs and a novel optimally weighted aggregation of only\nthe latter. We validate the advantage of our approach to learn NeRFs with\nphotorealism and minimal server computation cost on structured synthetic and\nreal-world photo tourism datasets. We further analyze how secure aggregation of\nglobal NeRFs in DecentNeRF minimizes the undesired reconstruction of personal\ncontent by the server.\n","authors":["Zaid Tasneem","Akshat Dave","Abhishek Singh","Kushagra Tiwary","Praneeth Vepakomma","Ashok Veeraraghavan","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2403.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13196v1","updated":"2024-03-19T23:13:40Z","published":"2024-03-19T23:13:40Z","title":"ADAPT to Robustify Prompt Tuning Vision Transformers","summary":"  The performance of deep models, including Vision Transformers, is known to be\nvulnerable to adversarial attacks. Many existing defenses against these\nattacks, such as adversarial training, rely on full-model fine-tuning to induce\nrobustness in the models. These defenses require storing a copy of the entire\nmodel, that can have billions of parameters, for each task. At the same time,\nparameter-efficient prompt tuning is used to adapt large transformer-based\nmodels to downstream tasks without the need to save large copies. In this\npaper, we examine parameter-efficient prompt tuning of Vision Transformers for\ndownstream tasks under the lens of robustness. We show that previous\nadversarial defense methods, when applied to the prompt tuning paradigm, suffer\nfrom gradient obfuscation and are vulnerable to adaptive attacks. We introduce\nADAPT, a novel framework for performing adaptive adversarial training in the\nprompt tuning paradigm. Our method achieves competitive robust accuracy of ~40%\nw.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1%\nof the number of parameters.\n","authors":["Masih Eskandar","Tooba Imtiaz","Zifeng Wang","Jennifer Dy"],"pdf_url":"https://arxiv.org/pdf/2403.13196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13195v1","updated":"2024-03-19T23:06:10Z","published":"2024-03-19T23:06:10Z","title":"Hermite coordinate interpolation kernels: application to image zooming","summary":"  A number of basic image processing tasks, such as any geometric\ntransformation require interpolation at subpixel image values. In this work we\nutilize the multidimensional coordinate Hermite spline interpolation defined on\nnon-equal spaced, rectilinear grids and apply it to a very common image\nprocessing task, image zooming. Since Hermite interpolation utilizes function\nvalues, as well as partial derivative values, it is natural to apply it to\nimage processing tasks as a special case of equi-spaced grid, using numerical\napproximations of the image partial derivatives at each pixel. Furthermore, the\ntask of image interpolation requires the calculation of image values at\npositions with nono-zero fractional part. Thus, any spline interpolation can be\nwritten as convolution with an appropriate kernel. In this context we generate\nthe Hermite kernels according to the derived $n-$dimensional interpolant of\nTheorem 2 in [1]. We show that despite the increased complexity of the\ninterpolant, once the kernels are constructed, the Hermite spline interpolation\ncan be applied to images as efficiently as any other less complicated method.\nFinally, we perform illustrative numerical examples to showcase the\napplicability and high accuracy of the proposed Hermite kernels for image\nzooming, compared to other interpolation methods, both traditional\nconvolution-based, as well as employing deep learning, in terms of PSNR, as\nwell as SSIM error metrics. The proposed Hermite spline kernels outperform all\nother methods in the majority of the test images, in experiments using many\ncascaded repetitions of the zoom operation. Interesting conclusions can be\ndrawn considering all methods under comparison.\n","authors":["Konstantinos K. Delibasis","Iro Oikonomou","Aristides I. Kechriniotis","Georgios N. Tsigaridas"],"pdf_url":"https://arxiv.org/pdf/2403.13195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13016v4","updated":"2024-03-19T23:01:59Z","published":"2023-12-20T13:31:11Z","title":"DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View\n  Synthesis","summary":"  We present DiffPortrait3D, a conditional diffusion model that is capable of\nsynthesizing 3D-consistent photo-realistic novel views from as few as a single\nin-the-wild portrait. Specifically, given a single RGB input, we aim to\nsynthesize plausible but consistent facial details rendered from novel camera\nviews with retained both identity and facial expression. In lieu of\ntime-consuming optimization and fine-tuning, our zero-shot method generalizes\nwell to arbitrary face portraits with unposed camera views, extreme facial\nexpressions, and diverse artistic depictions. At its core, we leverage the\ngenerative prior of 2D diffusion models pre-trained on large-scale image\ndatasets as our rendering backbone, while the denoising is guided with\ndisentangled attentive control of appearance and camera pose. To achieve this,\nwe first inject the appearance context from the reference image into the\nself-attention layers of the frozen UNets. The rendering view is then\nmanipulated with a novel conditional control module that interprets the camera\npose by watching a condition image of a crossed subject from the same view.\nFurthermore, we insert a trainable cross-view attention module to enhance view\nconsistency, which is further strengthened with a novel 3D-aware noise\ngeneration process during inference. We demonstrate state-of-the-art results\nboth qualitatively and quantitatively on our challenging in-the-wild and\nmulti-view benchmarks.\n","authors":["Yuming Gu","You Xie","Hongyi Xu","Guoxian Song","Yichun Shi","Di Chang","Jing Yang","Linjie Luo"],"pdf_url":"https://arxiv.org/pdf/2312.13016v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13190v1","updated":"2024-03-19T23:01:14Z","published":"2024-03-19T23:01:14Z","title":"3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in\n  3D","summary":"  We study the task of 3D multi-object re-identification from embodied tours.\nSpecifically, an agent is given two tours of an environment (e.g. an apartment)\nunder two different layouts (e.g. arrangements of furniture). Its task is to\ndetect and re-identify objects in 3D - e.g. a \"sofa\" moved from location A to\nB, a new \"chair\" in the second layout at location C, or a \"lamp\" from location\nD in the first layout missing in the second. To support this task, we create an\nautomated infrastructure to generate paired egocentric tours of\ninitial/modified layouts in the Habitat simulator using Matterport3D scenes,\nYCB and Google-scanned objects. We present 3D Semantic MapNet (3D-SMNet) - a\ntwo-stage re-identification model consisting of (1) a 3D object detector that\noperates on RGB-D videos with known pose, and (2) a differentiable object\nmatching module that solves correspondence estimation between two sets of 3D\nbounding boxes. Overall, 3D-SMNet builds object-based maps of each layout and\nthen uses a differentiable matcher to re-identify objects across the tours.\nAfter training 3D-SMNet on our generated episodes, we demonstrate zero-shot\ntransfer to real-world rearrangement scenarios by instantiating our task in\nReplica, Active Vision, and RIO environments depicting rearrangements. On all\ndatasets, we find 3D-SMNet outperforms competitive baselines. Further, we show\njointly training on real and generated episodes can lead to significant\nimprovements over training on real data alone.\n","authors":["Vincent Cartillier","Neha Jain","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2403.13190v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2403.13188v1","updated":"2024-03-19T22:57:03Z","published":"2024-03-19T22:57:03Z","title":"Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation","summary":"  LiDAR semantic segmentation frameworks predominantly leverage geometry-based\nfeatures to differentiate objects within a scan. While these methods excel in\nscenarios with clear boundaries and distinct shapes, their performance declines\nin environments where boundaries are blurred, particularly in off-road\ncontexts. To address this, recent strides in 3D segmentation algorithms have\nfocused on harnessing raw LiDAR intensity measurements to improve prediction\naccuracy. Despite these efforts, current learning-based models struggle to\ncorrelate the intricate connections between raw intensity and factors such as\ndistance, incidence angle, material reflectivity, and atmospheric conditions.\nBuilding upon our prior work, this paper delves into the advantages of\nemploying calibrated intensity (also referred to as reflectivity) within\nlearning-based LiDAR semantic segmentation frameworks. We initially establish\nthat incorporating reflectivity as an input enhances the existing LiDAR\nsemantic segmentation model. Furthermore, we present findings that enable the\nmodel to learn to calibrate intensity can boost its performance. Through\nextensive experimentation on the off-road dataset Rellis-3D, we demonstrate\nnotable improvements. Specifically, converting intensity to reflectivity\nresults in a 4% increase in mean Intersection over Union (mIoU) when compared\nto using raw intensity in Off-road scenarios. Additionally, we also investigate\nthe possible benefits of using calibrated intensity in semantic segmentation in\nurban environments (SemanticKITTI) and cross-sensor domain adaptation.\n","authors":["Kasi Viswanath","Peng Jiang","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.13188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14565v4","updated":"2024-03-19T22:53:25Z","published":"2023-06-26T10:26:33Z","title":"Mitigating Hallucination in Large Multi-Modal Models via Robust\n  Instruction Tuning","summary":"  Despite the promising progress in multi-modal tasks, current large\nmulti-modal models (LMMs) are prone to hallucinating inconsistent descriptions\nwith respect to the associated image and human instructions. This paper\naddresses this issue by introducing the first large and diverse visual\ninstruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.\nOur dataset comprises 400k visual instructions generated by GPT4, covering 16\nvision-and-language tasks with open-ended instructions and answers. Unlike\nexisting studies that primarily focus on positive instruction samples, we\ndesign LRV-Instruction to include both positive and negative instructions for\nmore robust visual instruction tuning. Our negative instructions are designed\nat three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent\nObject Manipulation and (iii) Knowledge Manipulation. To efficiently measure\nthe hallucination generated by LMMs, we propose GPT4-Assisted Visual\nInstruction Evaluation (GAVIE), a stable approach to evaluate visual\ninstruction tuning like human experts. GAVIE does not require human-annotated\ngroundtruth answers and can adapt to diverse instruction formats. We conduct\ncomprehensive experiments to investigate the hallucination of LMMs. Our results\ndemonstrate existing LMMs exhibit significant hallucinations when presented\nwith our negative instructions, particularly Existent Object and Knowledge\nManipulation instructions. Moreover, we successfully mitigate hallucination by\nfinetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving\nperformance on several public datasets compared to state-of-the-art methods.\nAdditionally, we observed that a balanced ratio of positive and negative\ninstances in the training data leads to a more robust model. Code and data are\navailable at https://github.com/FuxiaoLiu/LRV-Instruction.\n","authors":["Fuxiao Liu","Kevin Lin","Linjie Li","Jianfeng Wang","Yaser Yacoob","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.14565v4.pdf","comment":"40 pages, 32 figures, ICLR 2024"},{"id":"http://arxiv.org/abs/2312.00210v2","updated":"2024-03-19T22:19:18Z","published":"2023-11-30T21:44:39Z","title":"DREAM: Diffusion Rectification and Estimation-Adaptive Models","summary":"  We present DREAM, a novel training framework representing Diffusion\nRectification and Estimation Adaptive Models, requiring minimal code changes\n(just three lines) yet significantly enhancing the alignment of training with\nsampling in diffusion models. DREAM features two components: diffusion\nrectification, which adjusts training to reflect the sampling process, and\nestimation adaptation, which balances perception against distortion. When\napplied to image super-resolution (SR), DREAM adeptly navigates the tradeoff\nbetween minimizing distortion and preserving high image quality. Experiments\ndemonstrate DREAM's superiority over standard diffusion-based SR methods,\nshowing a $2$ to $3\\times $ faster training convergence and a $10$ to\n$20\\times$ reduction in sampling steps to achieve comparable results. We hope\nDREAM will inspire a rethinking of diffusion model training paradigms.\n","authors":["Jinxin Zhou","Tianyu Ding","Tianyi Chen","Jiachen Jiang","Ilya Zharkov","Zhihui Zhu","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2312.00210v2.pdf","comment":"16 pages, 22 figures, 5 tables; the first two authors contributed to\n  this work equally"},{"id":"http://arxiv.org/abs/2403.13176v1","updated":"2024-03-19T22:05:32Z","published":"2024-03-19T22:05:32Z","title":"Castor: Competing shapelets for fast and accurate time series\n  classification","summary":"  Shapelets are discriminative subsequences, originally embedded in\nshapelet-based decision trees but have since been extended to shapelet-based\ntransformations. We propose Castor, a simple, efficient, and accurate time\nseries classification algorithm that utilizes shapelets to transform time\nseries. The transformation organizes shapelets into groups with varying\ndilation and allows the shapelets to compete over the time context to construct\na diverse feature representation. By organizing the shapelets into groups, we\nenable the transformation to transition between levels of competition,\nresulting in methods that more closely resemble distance-based transformations\nor dictionary-based transformations. We demonstrate, through an extensive\nempirical investigation, that Castor yields transformations that result in\nclassifiers that are significantly more accurate than several state-of-the-art\nclassifiers. In an extensive ablation study, we examine the effect of choosing\nhyperparameters and suggest accurate and efficient default values.\n","authors":["Isak Samsten","Zed Lee"],"pdf_url":"https://arxiv.org/pdf/2403.13176v1.pdf","comment":"Submitted to Data Mining and Knowledge Discovery Journal"},{"id":"http://arxiv.org/abs/2312.02350v2","updated":"2024-03-19T21:54:22Z","published":"2023-12-04T21:29:31Z","title":"Instant Uncertainty Calibration of NeRFs Using a Meta-calibrator","summary":"  Although Neural Radiance Fields (NeRFs) have markedly improved novel view\nsynthesis, accurate uncertainty quantification in their image predictions\nremains an open problem. The prevailing methods for estimating uncertainty,\nincluding the state-of-the-art Density-aware NeRF Ensembles (DANE) [29],\nquantify uncertainty without calibration. This frequently leads to over- or\nunder-confidence in image predictions, which can undermine their real-world\napplications. In this paper, we propose a method which, for the first time,\nachieves calibrated uncertainties for NeRFs. To accomplish this, we overcome a\nsignificant challenge in adapting existing calibration techniques to NeRFs: a\nneed to hold out ground truth images from the target scene, reducing the number\nof images left to train the NeRF. This issue is particularly problematic in\nsparse-view settings, where we can operate with as few as three images. To\naddress this, we introduce the concept of a meta-calibrator that performs\nuncertainty calibration for NeRFs with a single forward pass without the need\nfor holding out any images from the target scene. Our meta-calibrator is a\nneural network that takes as input the NeRF images and uncalibrated uncertainty\nmaps and outputs a scene-specific calibration curve that corrects the NeRF's\nuncalibrated uncertainties. We show that the meta-calibrator can generalize on\nunseen scenes and achieves well-calibrated and state-of-the-art uncertainty for\nNeRFs, significantly beating DANE and other approaches. This opens\nopportunities to improve applications that rely on accurate NeRF uncertainty\nestimates such as next-best view planning and potentially more trustworthy\nimage reconstruction for medical diagnosis.\n","authors":["Niki Amini-Naieni","Tomas Jakab","Andrea Vedaldi","Ronald Clark"],"pdf_url":"https://arxiv.org/pdf/2312.02350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13171v1","updated":"2024-03-19T21:52:19Z","published":"2024-03-19T21:52:19Z","title":"LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images","summary":"  Lithic Use-Wear Analysis (LUWA) using microscopic images is an underexplored\nvision-for-science research area. It seeks to distinguish the worked material,\nwhich is critical for understanding archaeological artifacts, material\ninteractions, tool functionalities, and dental records. However, this\nchallenging task goes beyond the well-studied image classification problem for\ncommon objects. It is affected by many confounders owing to the complex wear\nmechanism and microscopic imaging, which makes it difficult even for human\nexperts to identify the worked material successfully. In this paper, we\ninvestigate the following three questions on this unique vision task for the\nfirst time:(i) How well can state-of-the-art pre-trained models (like DINOv2)\ngeneralize to the rarely seen domain? (ii) How can few-shot learning be\nexploited for scarce microscopic images? (iii) How do the ambiguous\nmagnification and sensing modality influence the classification accuracy? To\nstudy these, we collaborated with archaeologists and built the first\nopen-source and the largest LUWA dataset containing 23,130 microscopic images\nwith different magnifications and sensing modalities. Extensive experiments\nshow that existing pre-trained models notably outperform human experts but\nstill leave a large gap for improvements. Most importantly, the LUWA dataset\nprovides an underexplored opportunity for vision and learning communities and\ncomplements existing image classification problems on common objects.\n","authors":["Jing Zhang","Irving Fang","Juexiao Zhang","Hao Wu","Akshat Kaushik","Alice Rodriguez","Hanwen Zhao Zhuo Zheng","Radu Iovita","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13171v1.pdf","comment":"CVPR"},{"id":"http://arxiv.org/abs/2309.04461v2","updated":"2024-03-19T21:48:59Z","published":"2023-09-08T17:49:44Z","title":"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language\n  Models","summary":"  Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2309.04461v2.pdf","comment":"NAACL 2024 Main Conference. The data is released at\n  https://github.com/Yangyi-Chen/CoTConsistency"}],"Graphics":[{"id":"http://arxiv.org/abs/2403.12961v1","updated":"2024-03-19T17:59:09Z","published":"2024-03-19T17:59:09Z","title":"TexTile: A Differentiable Metric for Texture Tileability","summary":"  We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.\n","authors":["Carlos Rodriguez-Pardo","Dan Casas","Elena Garces","Jorge Lopez-Moreno"],"pdf_url":"https://arxiv.org/pdf/2403.12961v1.pdf","comment":"CVPR 2024. Project page: https://mslab.es/projects/TexTile/"},{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.12032v2","updated":"2024-03-19T16:45:22Z","published":"2024-03-18T17:59:09Z","title":"Generic 3D Diffusion Adapter Using Controlled Multi-View Editing","summary":"  Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.\n","authors":["Hansheng Chen","Ruoxi Shi","Yulin Liu","Bokui Shen","Jiayuan Gu","Gordon Wetzstein","Hao Su","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2403.12032v2.pdf","comment":"V2 note: Fix missing acknowledgements. Project page:\n  https://lakonik.github.io/mvedit"},{"id":"http://arxiv.org/abs/2403.12820v1","updated":"2024-03-19T15:21:00Z","published":"2024-03-19T15:21:00Z","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","summary":"  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n","authors":["Zhiwei Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12820v1.pdf","comment":"Updates are kept with future progress. The code is available at:\n  https://github.com/Furkath/DL_Framework-for-PBS-Cloth-Simulation"},{"id":"http://arxiv.org/abs/2403.12387v1","updated":"2024-03-19T02:56:18Z","published":"2024-03-19T02:56:18Z","title":"ProgrammableGrass: A Shape-Changing Artificial Grass Display Adapted for\n  Dynamic and Interactive Display Features","summary":"  There are various proposals for employing grass materials as a green\nlandscape-friendly display. However, it is difficult for current techniques to\ndisplay smooth animations using 8-bit images and to adjust display resolution,\nsimilar to conventional displays. We present ProgrammableGrass, an artificial\ngrass display with scalable resolution, capable of swiftly controlling grass\ncolor at 8-bit levels. This grass display can control grass colors linearly at\nthe 8-bit level, similar to an LCD display, and can also display not only\n8-bit-based images but also videos. This display enables pixel-by-pixel color\ntransitions from yellow to green using fixed-length yellow and\nadjustable-length green grass. We designed a grass module that can be connected\nto other modules. Utilizing a proportional derivative control, the grass colors\nare manipulated to display animations at approximately 10 [fps]. Since the\nrelationship between grass lengths and colors is nonlinear, we developed a\ncalibration system for ProgrammableGrass. We revealed that this calibration\nsystem allows ProgrammableGrass to linearly control grass colors at 8-bit\nlevels through experiments under multiple conditions. Lastly, we demonstrate\nProgrammableGrass to show smooth animations with 8-bit grayscale images.\nMoreover, we show several application examples to illustrate the potential of\nProgrammableGrass. With the advancement of this technology, users will be able\nto treat grass as a green-based interactive display device.\n","authors":["Kojiro Tanaka","Akito Mizuno","Toranosuke Kato","Masahiko Mikawa","Makoto Fujisawa"],"pdf_url":"https://arxiv.org/pdf/2403.12387v1.pdf","comment":null}]},"2024-03-20T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.13801v1","updated":"2024-03-20T17:58:12Z","published":"2024-03-20T17:58:12Z","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied\n  Control with LLMs","summary":"  We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.\n","authors":["Yusuke Mikami","Andrew Melnik","Jun Miura","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2403.13801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.13783v1","updated":"2024-03-20T17:44:33Z","published":"2024-03-20T17:44:33Z","title":"A Convex Formulation of Frictional Contact for the Material Point Method\n  and Rigid Bodies","summary":"  In this paper, we introduce a novel convex formulation that seamlessly\nintegrates the Material Point Method (MPM) with articulated rigid body dynamics\nin frictional contact scenarios. We extend the linear corotational hyperelastic\nmodel into the realm of elastoplasticity and include an efficient return\nmapping algorithm. This approach is particularly effective for MPM simulations\ninvolving significant deformation and topology changes, while preserving the\nconvexity of the optimization problem. Our method ensures global convergence,\nenabling the use of large simulation time steps without compromising\nrobustness. We have validated our approach through rigorous testing and\nperformance evaluations, highlighting its superior capabilities in managing\ncomplex simulations relevant to robotics. Compared to previous MPM based\nrobotic simulators, our method significantly improves the stability of contact\nresolution -- a critical factor in robot manipulation tasks. We make our method\navailable in the open-source robotics toolkit, Drake.\n","authors":["Zeshun Zong","Chenfanfu Jiang","Xuchen Han"],"pdf_url":"https://arxiv.org/pdf/2403.13783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13778v1","updated":"2024-03-20T17:41:35Z","published":"2024-03-20T17:41:35Z","title":"Certified Human Trajectory Prediction","summary":"  Trajectory prediction plays an essential role in autonomous vehicles. While\nnumerous strategies have been developed to enhance the robustness of trajectory\nprediction models, these methods are predominantly heuristic and do not offer\nguaranteed robustness against adversarial attacks and noisy observations. In\nthis work, we propose a certification approach tailored for the task of\ntrajectory prediction. To this end, we address the inherent challenges\nassociated with trajectory prediction, including unbounded outputs, and\nmutli-modality, resulting in a model that provides guaranteed robustness.\nFurthermore, we integrate a denoiser into our method to further improve the\nperformance. Through comprehensive evaluations, we demonstrate the\neffectiveness of the proposed technique across various baselines and using\nstandard trajectory prediction datasets. The code will be made available\nonline: https://s-attack.github.io/\n","authors":["Mohammadhossein Bahari","Saeed Saadatnejad","Amirhossein Asgari Farsangi","Seyed-Mohsen Moosavi-Dezfooli","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.13778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13777v1","updated":"2024-03-20T17:41:21Z","published":"2024-03-20T17:41:21Z","title":"Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a\n  Compact Representation","summary":"  This paper presents the Embedding Pose Graph (EPG), an innovative method that\ncombines the strengths of foundation models with a simple 3D representation\nsuitable for robotics applications. Addressing the need for efficient spatial\nunderstanding in robotics, EPG provides a compact yet powerful approach by\nattaching foundation model features to the nodes of a pose graph. Unlike\ntraditional methods that rely on bulky data formats like voxel grids or point\nclouds, EPG is lightweight and scalable. It facilitates a range of robotic\ntasks, including open-vocabulary querying, disambiguation, image-based\nquerying, language-directed navigation, and re-localization in 3D environments.\nWe showcase the effectiveness of EPG in handling these tasks, demonstrating its\ncapacity to improve how robots interact with and navigate through complex\nspaces. Through both qualitative and quantitative assessments, we illustrate\nEPG's strong performance and its ability to outperform existing methods in\nre-localization. Our work introduces a crucial step forward in enabling robots\nto efficiently understand and operate within large-scale 3D spaces.\n","authors":["Hugues Thomas","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06192v5","updated":"2024-03-20T17:36:07Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v5.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.13730v1","updated":"2024-03-20T16:39:48Z","published":"2024-03-20T16:39:48Z","title":"Projection-free computation of robust controllable sets with constrained\n  zonotopes","summary":"  We study the problem of computing robust controllable sets for discrete-time\nlinear systems with additive uncertainty. We propose a tractable and scalable\napproach to inner- and outer-approximate robust controllable sets using\nconstrained zonotopes, when the additive uncertainty set is a symmetric,\nconvex, and compact set. Our least-squares-based approach uses novel\nclosed-form approximations of the Pontryagin difference between a constrained\nzonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike\nexisting approaches, our approach does not rely on convex optimization solvers,\nand is projection-free for ellipsoidal and zonotopic uncertainty sets. We also\npropose a least-squares-based approach to compute a convex, polyhedral\nouter-approximation to constrained zonotopes, and characterize sufficient\nconditions under which all these approximations are exact. We demonstrate the\ncomputational efficiency and scalability of our approach in several case\nstudies, including the design of abort-safe rendezvous trajectories for a\nspacecraft in near-rectilinear halo orbit under uncertainty. Our approach can\ninner-approximate a 20-step robust controllable set for a 100-dimensional\nlinear system in under 15 seconds on a standard computer.\n","authors":["Abraham P. Vinod","Avishai Weiss","Stefano Di Cairano"],"pdf_url":"https://arxiv.org/pdf/2403.13730v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13729v1","updated":"2024-03-20T16:39:17Z","published":"2024-03-20T16:39:17Z","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems:\n  a Replication and Extension Study","summary":"  In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.\n","authors":["Luca Giamattei","Matteo Biagiola","Roberto Pietrantuono","Stefano Russo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2403.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13714v1","updated":"2024-03-20T16:20:54Z","published":"2024-03-20T16:20:54Z","title":"DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with\n  Multiple Sensors for Large-Scale Localization and Mapping","summary":"  Visual simultaneous localization and mapping (VSLAM) has broad applications,\nwith state-of-the-art methods leveraging deep neural networks for better\nrobustness and applicability. However, there is a lack of research in fusing\nthese learning-based methods with multi-sensor information, which could be\nindispensable to push related applications to large-scale and complex\nscenarios. In this paper, we tightly integrate the trainable deep dense bundle\nadjustment (DBA) with multi-sensor information through a factor graph. In the\nframework, recurrent optical flow and DBA are performed among sequential\nimages. The Hessian information derived from DBA is fed into a generic factor\ngraph for multi-sensor fusion, which employs a sliding window and supports\nprobabilistic marginalization. A pipeline for visual-inertial integration is\nfirstly developed, which provides the minimum ability of metric-scale\nlocalization and mapping. Furthermore, other sensors (e.g., global navigation\nsatellite system) are integrated for driftless and geo-referencing\nfunctionality. Extensive tests are conducted on both public datasets and\nself-collected datasets. The results validate the superior localization\nperformance of our approach, which enables real-time dense mapping in\nlarge-scale environments. The code has been made open-source\n(https://github.com/GREAT-WHU/DBA-Fusion).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Xuanbin Wang","Shaoquan Feng","Yuxuan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.13714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05840v2","updated":"2024-03-20T16:18:26Z","published":"2024-02-08T17:17:06Z","title":"uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties","summary":"  The availability of a robust map-based localization system is essential for\nthe operation of many autonomously navigating vehicles. Since uncertainty is an\ninevitable part of perception, it is beneficial for the robustness of the robot\nto consider it in typical downstream tasks of navigation stacks. In particular\nlocalization and mapping methods, which in modern systems often employ\nconvolutional neural networks (CNNs) for perception tasks, require proper\nuncertainty estimates. In this work, we present uncertainty-aware Panoptic\nLocalization and Mapping (uPLAM), which employs pixel-wise uncertainty\nestimates for panoptic CNNs as a bridge to fuse modern perception with\nclassical probabilistic localization and mapping approaches. Beyond the\nperception, we introduce an uncertainty-based map aggregation technique to\ncreate accurate panoptic maps, containing surface semantics and landmark\ninstances. Moreover, we provide cell-wise map uncertainties, and present a\nparticle filter-based localization method that employs perception\nuncertainties. Extensive evaluations show that our proposed incorporation of\nuncertainties leads to more accurate maps with reliable uncertainty estimates\nand improved localization accuracy. Additionally, we present the Freiburg\nPanoptic Driving dataset for evaluating panoptic mapping and localization\nmethods. We make our code and dataset available at:\n\\url{http://uplam.cs.uni-freiburg.de}\n","authors":["Kshitij Sirohi","Daniel Büscher","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2402.05840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13701v1","updated":"2024-03-20T16:06:01Z","published":"2024-03-20T16:06:01Z","title":"What Matters for Active Texture Recognition With Vision-Based Tactile\n  Sensors","summary":"  This paper explores active sensing strategies that employ vision-based\ntactile sensors for robotic perception and classification of fabric textures.\nWe formalize the active sampling problem in the context of tactile fabric\nrecognition and provide an implementation of information-theoretic exploration\nstrategies based on minimizing predictive entropy and variance of probabilistic\nmodels. Through ablation studies and human experiments, we investigate which\ncomponents are crucial for quick and reliable texture recognition. Along with\nthe active sampling strategies, we evaluate neural network architectures,\nrepresentations of uncertainty, influence of data augmentation, and dataset\nvariability. By evaluating our method on a previously published Active Clothing\nPerception Dataset and on a real robotic system, we establish that the choice\nof the active exploration strategy has only a minor influence on the\nrecognition accuracy, whereas data augmentation and dropout rate play a\nsignificantly larger role. In a comparison study, while humans achieve 66.9%\nrecognition accuracy, our best approach reaches 90.0% in under 5 touches,\nhighlighting that vision-based tactile sensors are highly effective for fabric\ntexture recognition.\n","authors":["Alina Böhm","Tim Schneider","Boris Belousov","Alap Kshirsagar","Lisa Lin","Katja Doerschner","Knut Drewing","Constantin A. Rothkopf","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2403.13701v1.pdf","comment":"7 pages, 9 figures, accepted at 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2403.13695v1","updated":"2024-03-20T15:57:44Z","published":"2024-03-20T15:57:44Z","title":"Loss Regularizing Robotic Terrain Classification","summary":"  Locomotion mechanics of legged robots are suitable when pacing through\ndifficult terrains. Recognising terrains for such robots are important to fully\nyoke the versatility of their movements. Consequently, robotic terrain\nclassification becomes significant to classify terrains in real time with high\naccuracy. The conventional classifiers suffer from overfitting problem, low\naccuracy problem, high variance problem, and not suitable for live dataset. On\nthe other hand, classifying a growing dataset is difficult for convolution\nbased terrain classification. Supervised recurrent models are also not\npractical for this classification. Further, the existing recurrent\narchitectures are still evolving to improve accuracy of terrain classification\nbased on live variable-length sensory data collected from legged robots. This\npaper proposes a new semi-supervised method for terrain classification of\nlegged robots, avoiding preprocessing of long variable-length dataset. The\nproposed method has a stacked Long Short-Term Memory architecture, including a\nnew loss regularization. The proposed method solves the existing problems and\nimproves accuracy. Comparison with the existing architectures show the\nimprovements.\n","authors":["Shakti Deo Kumar","Sudhanshu Tripathi","Krishna Ujjwal","Sarvada Sakshi Jha","Suddhasil De"],"pdf_url":"https://arxiv.org/pdf/2403.13695v1.pdf","comment":"Preliminary draft of the work published in IEEE conference 2023"},{"id":"http://arxiv.org/abs/2403.13683v1","updated":"2024-03-20T15:41:32Z","published":"2024-03-20T15:41:32Z","title":"DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses","summary":"  Determining the relative pose of an object between two images is pivotal to\nthe success of generalizable object pose estimation. Existing approaches\ntypically approximate the continuous pose representation with a large number of\ndiscrete pose hypotheses, which incurs a computationally expensive process of\nscoring each hypothesis at test time. By contrast, we present a Deep Voxel\nMatching Network (DVMNet) that eliminates the need for pose hypotheses and\ncomputes the relative object pose in a single pass. To this end, we map the two\ninput RGB images, reference and query, to their respective voxelized 3D\nrepresentations. We then pass the resulting voxels through a pose estimation\nmodule, where the voxels are aligned and the pose is computed in an end-to-end\nfashion by solving a least-squares problem. To enhance robustness, we introduce\na weighted closest voxel algorithm capable of mitigating the impact of noisy\nvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse\ndatasets, demonstrating that our method delivers more accurate relative pose\nestimates for novel objects at a lower computational cost compared to\nstate-of-the-art methods. Our code is released at:\nhttps://github.com/sailor-z/DVMNet/.\n","authors":["Chen Zhao","Tong Zhang","Zheng Dang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.13683v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13674v1","updated":"2024-03-20T15:32:56Z","published":"2024-03-20T15:32:56Z","title":"Reward-Driven Automated Curriculum Learning for Interaction-Aware\n  Self-Driving at Unsignalized Intersections","summary":"  In this work, we present a reward-driven automated curriculum reinforcement\nlearning approach for interaction-aware self-driving at unsignalized\nintersections, taking into account the uncertainties associated with\nsurrounding vehicles (SVs). These uncertainties encompass the uncertainty of\nSVs' driving intention and also the quantity of SVs. To deal with this problem,\nthe curriculum set is specifically designed to accommodate a progressively\nincreasing number of SVs. By implementing an automated curriculum selection\nmechanism, the importance weights are rationally allocated across various\ncurricula, thereby facilitating improved sample efficiency and training\noutcomes. Furthermore, the reward function is meticulously designed to guide\nthe agent towards effective policy exploration. Thus the proposed framework\ncould proactively address the above uncertainties at unsignalized intersections\nby employing the automated curriculum learning technique that progressively\nincreases task difficulty, and this ensures safe self-driving through effective\ninteraction with SVs. Comparative experiments are conducted in $Highway\\_Env$,\nand the results indicate that our approach achieves the highest task success\nrate, attains strong robustness to initialization parameters of the curriculum\nselection module, and exhibits superior adaptability to diverse situational\nconfigurations at unsignalized intersections. Furthermore, the effectiveness of\nthe proposed method is validated using the high-fidelity CARLA simulator.\n","authors":["Zengqi Peng","Xiao Zhou","Lei Zheng","Yubin Wang","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13674v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13640v1","updated":"2024-03-20T14:43:51Z","published":"2024-03-20T14:43:51Z","title":"LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction\n  By Enhancing Laminar Characteristics in Human Flow","summary":"  Long-term human motion prediction (LHMP) is essential for safely operating\nautonomous robots and vehicles in populated environments. It is fundamental for\nvarious applications, including motion planning, tracking, human-robot\ninteraction and safety monitoring. However, accurate prediction of human\ntrajectories is challenging due to complex factors, including, for example,\nsocial norms and environmental conditions. The influence of such factors can be\ncaptured through Maps of Dynamics (MoDs), which encode spatial motion patterns\nlearned from (possibly scattered and partial) past observations of motion in\nthe environment and which can be used for data-efficient, interpretable motion\nprediction (MoD-LHMP). To address the limitations of prior work, especially\nregarding accuracy and sensitivity to anomalies in long-term prediction, we\npropose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach\nis inspired by data-driven airflow modelling, which estimates laminar and\nturbulent flow components and uses predominantly the laminar components to make\nflow predictions. Based on the hypothesis that human trajectory patterns also\nmanifest laminar flow (that represents predictable motion) and turbulent flow\ncomponents (that reflect more unpredictable and arbitrary motion), LaCE-LHMP\nextracts the laminar patterns in human dynamics and uses them for human motion\nprediction. We demonstrate the superior prediction performance of LaCE-LHMP\nthrough benchmark comparisons with state-of-the-art LHMP methods, offering an\nunconventional perspective and a more intuitive understanding of human movement\npatterns.\n","authors":["Yufei Zhu","Han Fan","Andrey Rudenko","Martin Magnusson","Erik Schaffernicht","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2403.13640v1.pdf","comment":"Accepted to the 2024 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2309.13139v3","updated":"2024-03-20T14:43:06Z","published":"2023-09-22T18:48:54Z","title":"Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of\n  Vision Algorithms","summary":"  Visual Odometry (VO) is one of the fundamental tasks in computer vision for\nrobotics. However, its performance is deeply affected by High Dynamic Range\n(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches\nto mitigate this have appeared, their comparison in a reproducible manner is\nproblematic. This stems from the fact that the behavior of AE depends on the\nenvironment, and it affects the image acquisition process. Consequently, AE has\ntraditionally only been benchmarked in an online manner, making the experiments\nnon-reproducible. To solve this, we propose a new methodology based on an\nemulator that can generate images at any exposure time. It leverages BorealHDR,\na unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories\nwith challenging illumination conditions. Moreover, it includes\nlidar-inertial-based global maps with pose estimation for each image frame as\nwell as Global Navigation Satellite System (GNSS) data, for comparison. We show\nthat using these images acquired at different exposure times, we can emulate\nrealistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared\nto ground truth images. To demonstrate the practicality of our approach for\noffline benchmarking, we compared three state-of-the-art AE algorithms on key\nelements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,\nagainst four baselines. Consequently, reproducible evaluation of AE is now\npossible, speeding up the development of future approaches. Our code and\ndataset are available online at this link:\nhttps://github.com/norlab-ulaval/BorealHDR\n","authors":["Olivier Gamache","Jean-Michel Fortin","Matěj Boxan","Maxime Vaidis","François Pomerleau","Philippe Giguère"],"pdf_url":"https://arxiv.org/pdf/2309.13139v3.pdf","comment":"8 pages, 6 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2302.05855v3","updated":"2024-03-20T14:37:54Z","published":"2023-02-12T04:41:59Z","title":"Investigation of Enhanced Inertial Navigation Algorithms by Functional\n  Iteration","summary":"  The defects of the traditional strapdown inertial navigation algorithms\nbecome well acknowledged and the corresponding enhanced algorithms have been\nquite recently proposed trying to mitigate both theoretical and algorithmic\ndefects. In this paper, the analytical accuracy evaluation of both the\ntraditional algorithms and the enhanced algorithms is investigated, against the\ntrue reference for the first time enabled by the functional iteration approach\nhaving provable convergence. The analyses by the help of MATLAB Symbolic\nToolbox show that the resultant error orders of all algorithms under\ninvestigation are consistent with those in the existing literatures, and the\nenhanced attitude algorithm notably reduces error orders of the traditional\ncounterpart, while the impact of the enhanced velocity algorithm on error order\nreduction is insignificant. Simulation results agree with analyses that the\nsuperiority of the enhanced algorithm over the traditional one in the\nbody-frame attitude computation scenario diminishes significantly in the entire\ninertial navigation computation scenario, while the functional iteration\napproach possesses significant accuracy superiority even under sustained lowly\ndynamic conditions.\n","authors":["Hongyan Jiang","Maoran Zhu","Yuanxin Wu"],"pdf_url":"https://arxiv.org/pdf/2302.05855v3.pdf","comment":"12 pages, 3 figs"},{"id":"http://arxiv.org/abs/2309.08854v2","updated":"2024-03-20T13:44:36Z","published":"2023-09-16T03:11:06Z","title":"Intention-Aware Planner for Robust and Safe Aerial Tracking","summary":"  Autonomous target tracking with quadrotors has wide applications in many\nscenarios, such as cinematographic follow-up shooting or suspect chasing.\nTarget motion prediction is necessary when designing the tracking planner.\nHowever, the widely used constant velocity or constant rotation assumption can\nnot fully capture the dynamics of the target. The tracker may fail when the\ntarget happens to move aggressively, such as sudden turn or deceleration. In\nthis paper, we propose an intention-aware planner by additionally considering\nthe intention of the target to enhance safety and robustness in aerial tracking\napplications. Firstly, a designated intention prediction method is proposed,\nwhich combines a user-defined potential assessment function and a state\nobservation function. A reachable region is generated to specifically evaluate\nthe turning intentions. Then we design an intention-driven hybrid A* method to\npredict the future possible positions for the target. Finally, an\nintention-aware optimization approach is designed to generate a\nspatial-temporal optimal trajectory, allowing the tracker to perceive\nunexpected situations from the target. Benchmark comparisons and real-world\nexperiments are conducted to validate the performance of our method.\n","authors":["Qiuyu Ren","Huan Yu","Jiajun Dai","Zhi Zheng","Jun Meng","Li Xu","Chao Xu","Fei Gao","Yanjun Cao"],"pdf_url":"https://arxiv.org/pdf/2309.08854v2.pdf","comment":"8 pages, 10 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11552v2","updated":"2024-03-20T13:15:39Z","published":"2024-03-18T08:03:47Z","title":"LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v2.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2307.07975v3","updated":"2024-03-20T12:30:01Z","published":"2023-07-16T07:55:35Z","title":"Pseudo-rigid body networks: learning interpretable deformable object\n  dynamics from partial observations","summary":"  Accurate prediction of deformable linear object (DLO) dynamics is challenging\nif the task at hand requires a human-interpretable yet computationally fast\nmodel. In this work, we draw inspiration from the pseudo-rigid body method\n(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is\nunrolled through time by a dynamics network. This dynamics network is trained\njointly with a physics-informed encoder which maps observed motion variables to\nthe DLO's hidden state. To encourage that the state acquires a physically\nmeaningful representation, we leverage the forward kinematics of the PRB model\nas decoder. We demonstrate in robot experiments that the proposed DLO dynamics\nmodel provides physically interpretable predictions from partial observations\nwhile being on par with black-box models regarding prediction accuracy. The\nproject code is available at: http://tinyurl.com/prb-networks\n","authors":["Shamil Mamedov","A. René Geist","Jan Swevers","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2307.07975v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13541v1","updated":"2024-03-20T12:26:02Z","published":"2024-03-20T12:26:02Z","title":"From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive\n  Processes","summary":"  In robotics, understanding human interaction with autonomous systems is\ncrucial for enhancing collaborative technologies. We focus on human-swarm\ninteraction (HSI), exploring how differently sized groups of active robots\naffect operators' cognitive and perceptual reactions over different durations.\nWe analyze the impact of different numbers of active robots within a 15-robot\nswarm on operators' time perception, emotional state, flow experience, and task\ndifficulty perception. Our findings indicate that managing multiple active\nrobots when compared to one active robot significantly alters time perception\nand flow experience, leading to a faster passage of time and increased flow.\nMore active robots and extended durations cause increased emotional arousal and\nperceived task difficulty, highlighting the interaction between robot the\nnumber of active robots and human cognitive processes. These insights inform\nthe creation of intuitive human-swarm interfaces and aid in developing swarm\nrobotic systems aligned with human cognitive structures, enhancing human-robot\ncollaboration.\n","authors":["Julian Kaduk","Müge Cavdan","Knut Drewing","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2403.13541v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13474v1","updated":"2024-03-20T10:22:22Z","published":"2024-03-20T10:22:22Z","title":"Iterative Active-Inactive Obstacle Classification for Time-Optimal\n  Collision Avoidance","summary":"  Time-optimal obstacle avoidance is a prevalent problem encountered in various\nfields, including robotics and autonomous vehicles, where the task involves\ndetermining a path for a moving vehicle to reach its goal while navigating\naround obstacles within its environment. This problem becomes increasingly\nchallenging as the number of obstacles in the environment rises. We propose an\niterative active-inactive obstacle approach, which involves identifying a\nsubset of the obstacles as \"active\", that considers solely the effect of the\n\"active\" obstacles on the path of the moving vehicle. The remaining obstacles\nare considered \"inactive\" and are not considered in the path planning process.\nThe obstacles are classified as 'active' on the basis of previous findings\nderived from prior iterations. This approach allows for a more efficient\ncalculation of the optimal path by reducing the number of obstacles that need\nto be considered. The effectiveness of the proposed method is demonstrated with\ntwo different dynamic models using the various number of obstacles. The results\nshow that the proposed method is able to find the optimal path in a timely\nmanner, while also being able to handle a large number of obstacles in the\nenvironment and the constraints on the motion of the object.\n","authors":["Mehmetcan Kaymaz","Nazim Kemal Ure"],"pdf_url":"https://arxiv.org/pdf/2403.13474v1.pdf","comment":"This paper is under review in IROS24"},{"id":"http://arxiv.org/abs/2403.13467v1","updated":"2024-03-20T10:17:39Z","published":"2024-03-20T10:17:39Z","title":"CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language\n  Models","summary":"  This paper introduces CLIPSwarm, a new algorithm designed to automate the\nmodeling of swarm drone formations based on natural language. The algorithm\nbegins by enriching a provided word, to compose a text prompt that serves as\ninput to an iterative approach to find the formation that best matches the\nprovided word. The algorithm iteratively refines formations of robots to align\nwith the textual description, employing different steps for \"exploration\" and\n\"exploitation\". Our framework is currently evaluated on simple formation\ntargets, limited to contour shapes. A formation is visually represented through\nalpha-shape contours and the most representative color is automatically found\nfor the input word. To measure the similarity between the description and the\nvisual representation of the formation, we use CLIP [1], encoding text and\nimages into vectors and assessing their similarity. Subsequently, the algorithm\nrearranges the formation to visually represent the word more effectively,\nwithin the given constraints of available drones. Control actions are then\nassigned to the drones, ensuring robotic behavior and collision-free movement.\nExperimental results demonstrate the system's efficacy in accurately modeling\nrobot formations from natural language descriptions. The algorithm's\nversatility is showcased through the execution of drone shows in photorealistic\nsimulation with varying shapes. We refer the reader to the supplementary video\nfor a visual reference of the results.\n","authors":["Pablo Pueyo","Eduardo Montijano","Ana C. Murillo","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2403.13467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13455v1","updated":"2024-03-20T10:01:52Z","published":"2024-03-20T10:01:52Z","title":"FACT: Fast and Active Coordinate Initialization for Vision-based Drone\n  Swarms","summary":"  Swarm robots have sparked remarkable developments across a range of fields.\nWhile it is necessary for various applications in swarm robots, a fast and\nrobust coordinate initialization in vision-based drone swarms remains elusive.\nTo this end, our paper proposes a complete system to recover a swarm's initial\nrelative pose on platforms with size, weight, and power (SWaP) constraints. To\novercome limited coverage of field-of-view (FoV), the drones rotate in place to\nobtain observations. To tackle the anonymous measurements, we formulate a\nnon-convex rotation estimation problem and transform it into a semi-definite\nprogramming (SDP) problem, which can steadily obtain global optimal values.\nThen we utilize the Hungarian algorithm to recover relative translation and\ncorrespondences between observations and drone identities. To safely acquire\ncomplete observations, we actively search for positions and generate feasible\ntrajectories to avoid collisions. To validate the practicability of our system,\nwe conduct experiments on a vision-based drone swarm with only stereo cameras\nand inertial measurement units (IMUs) as sensors. The results demonstrate that\nthe system can robustly get accurate relative poses in real time with limited\nonboard computation resources. The source code is released.\n","authors":["Yuan Li","Anke Zhao","Yingjian Wang","Ziyi Xu","Xin Zhou","Jinni Zhou","Chao Xu","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.13455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13452v1","updated":"2024-03-20T09:53:31Z","published":"2024-03-20T09:53:31Z","title":"Mobile Robot Localization: a Modular, Odometry-Improving Approach","summary":"  Despite the number of works published in recent years, vehicle localization\nremains an open, challenging problem. While map-based localization and SLAM\nalgorithms are getting better and better, they remain a single point of failure\nin typical localization pipelines. This paper proposes a modular localization\narchitecture that fuses sensor measurements with the outputs of off-the-shelf\nlocalization algorithms. The fusion filter estimates model uncertainties to\nimprove odometry in case absolute pose measurements are lost entirely. The\narchitecture is validated experimentally on a real robot navigating\nautonomously proving a reduction of the position error of more than 90% with\nrespect to the odometrical estimate without uncertainty estimation in a\ntwo-minute navigation period without position measurements.\n","authors":["Luca Mozzarelli","Luca Cattaneo","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2403.13452v1.pdf","comment":"Accepted at IEEE European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.13443v1","updated":"2024-03-20T09:39:39Z","published":"2024-03-20T09:39:39Z","title":"Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking","summary":"  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion\nstates of surrounding obstacles, essential for robotic perception. However,\ncurrent 3D trackers face issues with accuracy and latency consistency. In this\npaper, we propose Fast-Poly, a fast and effective filter-based method for 3D\nMOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object\nrotational anisotropy in 3D space, enhances local computation densification,\nand leverages parallelization technique, improving inference speed and\nprecision. Fast-Poly is extensively tested on two large-scale tracking\nbenchmarks with Python implementation. On the nuScenes dataset, Fast-Poly\nachieves new state-of-the-art performance with 75.8% AMOTA among all methods\nand can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly\nexhibits competitive accuracy with 63.6% MOTA and impressive inference speed\n(35.5 FPS). The source code is publicly available at\nhttps://github.com/lixiaoyu2000/FastPoly.\n","authors":["Xiaoyu Li","Dedong Liu","Lijun Zhao","Yitao Wu","Xian Wu","Jinghan Gao"],"pdf_url":"https://arxiv.org/pdf/2403.13443v1.pdf","comment":"1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS"},{"id":"http://arxiv.org/abs/2403.13431v1","updated":"2024-03-20T09:18:19Z","published":"2024-03-20T09:18:19Z","title":"Automatic Navigation Map Generation for Mobile Robots in Urban\n  Environments","summary":"  A fundamental prerequisite for safe and efficient navigation of mobile robots\nis the availability of reliable navigation maps upon which trajectories can be\nplanned. With the increasing industrial interest in mobile robotics, especially\nin urban environments, the process of generating navigation maps has become of\nparticular interest, being a labor intensive step of the deployment process.\nAutomating this step is challenging and becomes even more arduous when the\nperception capabilities are limited by cost considerations. This paper proposes\nan algorithm to automatically generate navigation maps using a typical\nnavigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The\nproposed method is designed and validated with the urban environment as the\nmain use case: it is shown to be able to produce accurate maps featuring\ndifferent terrain types, positive obstacles of different heights as well as\nnegative obstacles. The algorithm is applied to data collected in a typical\nurban environment with a wheeled inverted pendulum robot, showing its\nrobustness against localization, perception and dynamic uncertainties. The\ngenerated map is validated against a human-made map.\n","authors":["Luca Mozzarelli","Simone Specchia","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2403.13431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04530v2","updated":"2024-03-20T09:12:21Z","published":"2023-12-07T18:50:01Z","title":"Camera Height Doesn't Change: Unsupervised Training for Metric Monocular\n  Road-Scene Depth Estimation","summary":"  In this paper, we introduce a novel training method for making any monocular\ndepth network learn absolute scale and estimate metric road-scene depth just\nfrom regular training data, i.e., driving videos. We refer to this training\nframework as StableCamH. The key idea is to leverage cars found on the road as\nsources of scale supervision but to incorporate them in the training robustly.\nStableCamH detects and estimates the sizes of cars in the frame and aggregates\nscale information extracted from them into a camera height estimate whose\nconsistency across the entire video sequence is enforced as scale supervision.\nThis realizes robust unsupervised training of any, otherwise scale-oblivious,\nmonocular depth network to become not only scale-aware but also metric-accurate\nwithout the need for auxiliary sensors and extra supervision. Extensive\nexperiments on the KITTI and Cityscapes datasets show the effectiveness of\nStableCamH and its state-of-the-art accuracy compared with related methods. We\nalso show that StableCamH enables training on mixed datasets of different\ncamera heights, which leads to larger-scale training and thus higher\ngeneralization. Metric depth reconstruction is essential in any road-scene\nvisual modeling, and StableCamH democratizes its deployment by establishing the\nmeans to train any model as a metric depth estimator.\n","authors":["Genki Kinoshita","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13421v1","updated":"2024-03-20T09:07:23Z","published":"2024-03-20T09:07:23Z","title":"Caching-Augmented Lifelong Multi-Agent Path Finding","summary":"  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths\nfor multiple robots, is crucial in various applications. Lifelong MAPF, where\ntargets are reassigned to agents as soon as they complete their initial\nobjectives, offers a more accurate approximation of real-world warehouse\nplanning. In this paper, we present a novel mechanism named Caching-Augmented\nLifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.\nWe have developed a new map grid type called cache for temporary item storage\nand replacement and designed a lock mechanism for it to improve the stability\nof the planning solution. This cache mechanism was evaluated using various\ncache replacement policies and a spectrum of input task distributions. We\nidentified three main factors significantly impacting CAL-MAPF performance\nthrough experimentation: suitable input task distribution, high cache hit rate,\nand smooth traffic. Overall, CAL-MAPF has demonstrated potential for\nperformance improvements in certain task distributions, maps and agent\nconfigurations.\n","authors":["Yimin Tang","Zhenghong Yu","Yi Zheng","T. K. Satish Kumar","Jiaoyang Li","Sven Koenig"],"pdf_url":"https://arxiv.org/pdf/2403.13421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08221v2","updated":"2024-03-20T08:52:04Z","published":"2023-07-17T03:45:47Z","title":"NDT-Map-Code: A 3D global descriptor for real-time loop closure\n  detection in lidar SLAM","summary":"  Loop-closure detection, also known as place recognition, aiming to identify\npreviously visited locations, is an essential component of a SLAM system.\nExisting research on lidar-based loop closure heavily relies on dense point\ncloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal\nDistribution Transform) based global descriptor, NDT-Map-Code, designed for\nboth on-road driving and underground valet parking scenarios. NDT-Map-Code can\nbe directly extracted from the NDT map without the need for a dense point\ncloud, resulting in excellent scalability and low maintenance cost. The NDT\nrepresentation is leveraged to identify representative patterns, which are\nfurther encoded according to their spatial location (bearing, range, and\nheight). Experimental results on the NIO underground parking lot dataset and\nthe KITTI dataset demonstrate that our method achieves significantly better\nperformance compared to the state-of-the-art.\n","authors":["Lizhou Liao","Wenlei Yan","Li Sun","Xinhui Bai","Zhenxing You","Hongyuan Yuan","Chunyun Fu"],"pdf_url":"https://arxiv.org/pdf/2307.08221v2.pdf","comment":"8 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2206.06112v3","updated":"2024-03-20T08:41:49Z","published":"2022-06-13T12:53:54Z","title":"Vision-State Fusion: Improving Deep Neural Networks for Autonomous\n  Robotics","summary":"  Vision-based deep learning perception fulfills a paramount role in robotics,\nfacilitating solutions to many challenging scenarios, such as acrobatic\nmaneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted\nhigh-precision surgery. Control-oriented end-to-end perception approaches,\nwhich directly output control variables for the robot, commonly take advantage\nof the robot's state estimation as an auxiliary input. When intermediate\noutputs are estimated and fed to a lower-level controller, i.e. mediated\napproaches, the robot's state is commonly used as an input only for egocentric\ntasks, which estimate physical properties of the robot itself. In this work, we\npropose to apply a similar approach for the first time -- to the best of our\nknowledge -- to non-egocentric mediated tasks, where the estimated outputs\nrefer to an external subject. We prove how our general methodology improves the\nregression performance of deep convolutional neural networks (CNNs) on a broad\nclass of non-egocentric 3D pose estimation problems, with minimal computational\ncost. By analyzing three highly-different use cases, spanning from grasping\nwith a robotic arm to following a human subject with a pocket-sized UAV, our\nresults consistently improve the R\\textsuperscript{2} regression metric, up to\n+0.51, compared to their stateless baselines. Finally, we validate the in-field\nperformance of a closed-loop autonomous cm-scale UAV on the human pose\nestimation task. Our results show a significant reduction, i.e., 24\\% on\naverage, on the mean absolute error of our stateful CNN, compared to a\nState-of-the-Art stateless counterpart.\n","authors":["Elia Cereda","Stefano Bonato","Mirko Nava","Alessandro Giusti","Daniele Palossi"],"pdf_url":"https://arxiv.org/pdf/2206.06112v3.pdf","comment":"This paper has been accepted for publication in the Journal of\n  Intelligent & Robotic Systems. \\copyright 2024 Springer"},{"id":"http://arxiv.org/abs/2403.13395v1","updated":"2024-03-20T08:35:57Z","published":"2024-03-20T08:35:57Z","title":"Unifying Local and Global Multimodal Features for Place Recognition in\n  Aliased and Low-Texture Environments","summary":"  Perceptual aliasing and weak textures pose significant challenges to the task\nof place recognition, hindering the performance of Simultaneous Localization\nand Mapping (SLAM) systems. This paper presents a novel model, called UMF\n(standing for Unifying Local and Global Multimodal Features) that 1) leverages\nmulti-modality by cross-attention blocks between vision and LiDAR features, and\n2) includes a re-ranking stage that re-orders based on local feature matching\nthe top-k candidates retrieved using a global representation. Our experiments,\nparticularly on sequences captured on a planetary-analogous environment, show\nthat UMF outperforms significantly previous baselines in those challenging\naliased environments. Since our work aims to enhance the reliability of SLAM in\nall situations, we also explore its performance on the widely used RobotCar\ndataset, for broader applicability. Code and models are available at\nhttps://github.com/DLR-RM/UMF\n","authors":["Alberto García-Hernández","Riccardo Giubilato","Klaus H. Strobl","Javier Civera","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2403.13395v1.pdf","comment":"Accepted submission to International Conference on Robotics and\n  Automation (ICRA), 2024"},{"id":"http://arxiv.org/abs/2401.01657v3","updated":"2024-03-20T08:15:37Z","published":"2024-01-03T10:31:12Z","title":"Distributed Pose-graph Optimization with Multi-level Partitioning for\n  Collaborative SLAM","summary":"  The back-end module of Distributed Collaborative Simultaneous Localization\nand Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)\nunder a distributed setting, also known as SE(d)-synchronization. Most existing\ndistributed graph optimization algorithms employ a simple sequential\npartitioning scheme, which may result in unbalanced subgraph dimensions due to\nthe different geographic locations of each robot, and hence imposes extra\ncommunication load. Moreover, the performance of current Riemannian\noptimization algorithms can be further accelerated. In this letter, we propose\na novel distributed pose graph optimization algorithm combining multi-level\npartitioning with an accelerated Riemannian optimization method. Firstly, we\nemploy the multi-level graph partitioning algorithm to preprocess the naive\npose graph to formulate a balanced optimization problem. In addition, inspired\nby the accelerated coordinate descent method, we devise an Improved Riemannian\nBlock Coordinate Descent (IRBCD) algorithm and the critical point obtained is\nglobally optimal. Finally, we evaluate the effects of four common graph\npartitioning approaches on the correlation of the inter-subgraphs, and discover\nthat the Highest scheme has the best partitioning performance. Also, we\nimplement simulations to quantitatively demonstrate that our proposed algorithm\noutperforms the state-of-the-art distributed pose graph optimization protocols.\n","authors":["Cunhao Li","Peng Yi","Guanghui Guo","Yiguang Hong"],"pdf_url":"https://arxiv.org/pdf/2401.01657v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01233v2","updated":"2024-03-20T07:52:35Z","published":"2024-03-02T15:32:15Z","title":"Results and Lessons Learned from Autonomous Driving Transportation\n  Services in Airfield, Crowded Indoor, and Urban Environments","summary":"  Autonomous vehicles have been actively investigated over the past few\ndecades. Several recent works show the potential of autonomous vehicles in\nurban environments with impressive experimental results. However, these works\nnote that autonomous vehicles are still occasionally inferior to expert drivers\nin complex scenarios. Furthermore, they do not focus on the possibilities of\nautonomous driving transportation services in other areas beyond urban\nenvironments. This paper presents the research results and lessons learned from\nautonomous driving transportation services in airfield, crowded indoor, and\nurban environments. We discuss how we address several unique challenges in\nthese diverse environments. We also offer an overview of remaining challenges\nthat have not received much attention but must be addressed. This paper aims to\nshare our unique experience to support researchers who are interested in\nexploring autonomous driving transportation services in various real-world\nenvironments.\n","authors":["Doosan Baek","Sanghyun Kim","Seung-Woo Seo","Sang-Hyun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01233v2.pdf","comment":"8 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.13366v1","updated":"2024-03-20T07:51:53Z","published":"2024-03-20T07:51:53Z","title":"Centroidal State Estimation based on the Koopman Embedding for Dynamic\n  Legged Locomotion","summary":"  In this paper, we introduce a novel approach to centroidal state estimation,\nwhich plays a crucial role in predictive model-based control strategies for\ndynamic legged locomotion. Our approach uses the Koopman operator theory to\ntransform the robot's complex nonlinear dynamics into a linear system, by\nemploying dynamic mode decomposition and deep learning for model construction.\nWe evaluate both models on their linearization accuracy and capability to\ncapture both fast and slow dynamic system responses. We then select the most\nsuitable model for estimation purposes, and integrate it within a moving\nhorizon estimator. This estimator is formulated as a convex quadratic program,\nto facilitate robust, real-time centroidal state estimation. Through extensive\nsimulation experiments on a quadruped robot executing various dynamic gaits,\nour data-driven framework outperforms conventional filtering techniques based\non nonlinear dynamics. Our estimator addresses challenges posed by force/torque\nmeasurement noise in highly dynamic motions and accurately recovers the\ncentroidal states, demonstrating the adaptability and effectiveness of the\nKoopman-based linear representation for complex locomotive behaviors.\nImportantly, our model based on dynamic mode decomposition, trained with two\nlocomotion patterns (trot and jump), successfully estimates the centroidal\nstates for a different motion (bound) without retraining.\n","authors":["Shahram Khorshidi","Murad Dawood","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.13366v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13365v1","updated":"2024-03-20T07:48:32Z","published":"2024-03-20T07:48:32Z","title":"ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation\n  in Robotics","summary":"  Robotic manipulation in everyday scenarios, especially in unstructured\nenvironments, requires skills in pose-aware object manipulation (POM), which\nadapts robots' grasping and handling according to an object's 6D pose.\nRecognizing an object's position and orientation is crucial for effective\nmanipulation. For example, if a mug is lying on its side, it's more effective\nto grasp it by the rim rather than the handle. Despite its importance, research\nin POM skills remains limited, because learning manipulation skills requires\npose-varying simulation environments and datasets. This paper introduces\nManiPose, a pioneering benchmark designed to advance the study of pose-varying\nmanipulation tasks. ManiPose encompasses: 1) Simulation environments for POM\nfeature tasks ranging from 6D pose-specific pick-and-place of single objects to\ncluttered scenes, further including interactions with articulated objects. 2) A\ncomprehensive dataset featuring geometrically consistent and\nmanipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects\nand 100 articulated objects across 59 categories. 3) A baseline for POM,\nleveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the\nrelationship between 6D pose and task-specific requirements, offers enhanced\npose-aware grasp prediction and motion planning capabilities. Our benchmark\ndemonstrates notable advancements in pose estimation, pose-aware manipulation,\nand real-robot skill transfer, setting new standards for POM research. We will\nopen-source the ManiPose benchmark with the final version paper, inviting the\ncommunity to engage with our resources, available at our\nwebsite:https://sites.google.com/view/manipose.\n","authors":["Qiaojun Yu","Ce Hao","Junbo Wang","Wenhai Liu","Liu Liu","Yao Mu","Yang You","Hengxu Yan","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13365v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.13358v1","updated":"2024-03-20T07:36:43Z","published":"2024-03-20T07:36:43Z","title":"GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped\n  Robot","summary":"  Multi-task robot learning holds significant importance in tackling diverse\nand complex scenarios. However, current approaches are hindered by performance\nissues and difficulties in collecting training datasets. In this paper, we\npropose GeRM (Generalist Robotic Model). We utilize offline reinforcement\nlearning to optimize data utilization strategies to learn from both\ndemonstrations and sub-optimal data, thus surpassing the limitations of human\ndemonstrations. Thereafter, we employ a transformer-based VLA network to\nprocess multi-modal inputs and output actions. By introducing the\nMixture-of-Experts structure, GeRM allows faster inference speed with higher\nwhole model capacity, and thus resolves the issue of limited RL parameters,\nenhancing model performance in multi-task learning while controlling\ncomputational costs. Through a series of experiments, we demonstrate that GeRM\noutperforms other methods across all tasks, while also validating its\nefficiency in both training and inference processes. Additionally, we uncover\nits potential to acquire emergent skills. Additionally, we contribute the\nQUARD-Auto dataset, collected automatically to support our training approach\nand foster advancements in multi-task quadruped robot learning. This work\npresents a new paradigm for reducing the cost of collecting robot data and\ndriving progress in the multi-task learning community.\n","authors":["Wenxuan Song","Han Zhao","Pengxiang Ding","Can Cui","Shangke Lyu","Yaning Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13348v1","updated":"2024-03-20T07:19:53Z","published":"2024-03-20T07:19:53Z","title":"MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with\n  Wireless Coordination","summary":"  This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework\nthat leverages wireless signal-based coordination between robots and Neural\nRadiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D\nreconstruction, including inter-robot pose estimation, localization uncertainty\nquantification, and active best-next-view selection. We introduce a method for\nusing wireless Angle-of-Arrival (AoA) and ranging measurements to estimate\nrelative poses between robots, as well as quantifying and incorporating the\nuncertainty embedded in the wireless localization of these pose estimates into\nthe NeRF training loss to mitigate the impact of inaccurate camera poses.\nFurthermore, we propose an active view selection approach that accounts for\nrobot pose uncertainty when determining the next-best views to improve the 3D\nreconstruction, enabling faster convergence through intelligent view selection.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our framework in theory and in practice. Leveraging wireless\ncoordination and localization uncertainty-aware training, MULAN-WC can achieve\nhigh-quality 3d reconstruction which is close to applying the ground truth\ncamera poses. Furthermore, the quantification of the information gain from a\nnovel view enables consistent rendering quality improvement with incrementally\ncaptured images by commending the robot the novel view position. Our hardware\nexperiments showcase the practicality of deploying MULAN-WC to real robotic\nsystems.\n","authors":["Weiying Wang","Victor Cai","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2403.13348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13336v1","updated":"2024-03-20T06:42:12Z","published":"2024-03-20T06:42:12Z","title":"Discretizing SO(2)-Equivariant Features for Robotic Kitting","summary":"  Robotic kitting has attracted considerable attention in logistics and\nindustrial settings. However, existing kitting methods encounter challenges\nsuch as low precision and poor efficiency, limiting their widespread\napplications. To address these issues, we present a novel kitting framework\nthat improves both the precision and computational efficiency of complex\nkitting tasks. Firstly, our approach introduces a fine-grained orientation\nestimation technique in the picking module, significantly enhancing orientation\nprecision while effectively decoupling computational load from orientation\ngranularity. This approach combines an SO(2)-equivariant network with a group\ndiscretization operation to preciously predict discrete orientation\ndistributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to\nevaluate the performance of different solutions in handling\norientation-sensitive kitting tasks. This dataset comprises a diverse\ncollection of hand tools and synthetically created kits, which reflects the\ncomplexities encountered in real-world kitting scenarios. Finally, a series of\nexperiments are conducted to evaluate the performance of the proposed method.\nThe results demonstrate that our approach offers remarkable precision and\nenhanced computational efficiency in robotic kitting tasks.\n","authors":["Jiadong Zhou","Yadan Zeng","Huixu Dong","I-Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13336v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13331v1","updated":"2024-03-20T06:22:37Z","published":"2024-03-20T06:22:37Z","title":"AMP: Autoregressive Motion Prediction Revisited with Next Token\n  Prediction for Autonomous Driving","summary":"  As an essential task in autonomous driving (AD), motion prediction aims to\npredict the future states of surround objects for navigation. One natural\nsolution is to estimate the position of other agents in a step-by-step manner\nwhere each predicted time-step is conditioned on both observed time-steps and\npreviously predicted time-steps, i.e., autoregressive prediction. Pioneering\nworks like SocialLSTM and MFP design their decoders based on this intuition.\nHowever, almost all state-of-the-art works assume that all predicted time-steps\nare independent conditioned on observed time-steps, where they use a single\nlinear layer to generate positions of all time-steps simultaneously. They\ndominate most motion prediction leaderboards due to the simplicity of training\nMLPs compared to autoregressive networks.\n  In this paper, we introduce the GPT style next token prediction into motion\nforecasting. In this way, the input and output could be represented in a\nunified space and thus the autoregressive prediction becomes more feasible.\nHowever, different from language data which is composed of homogeneous units\n-words, the elements in the driving scene could have complex spatial-temporal\nand semantic relations. To this end, we propose to adopt three factorized\nattention modules with different neighbors for information aggregation and\ndifferent position encoding styles to capture their relations, e.g., encoding\nthe transformation between coordinate systems for spatial relativity while\nadopting RoPE for temporal relativity. Empirically, by equipping with the\naforementioned tailored designs, the proposed method achieves state-of-the-art\nperformance in the Waymo Open Motion and Waymo Interaction datasets. Notably,\nAMP outperforms other recent autoregressive motion prediction methods: MotionLM\nand StateTransformer, which demonstrates the effectiveness of the proposed\ndesigns.\n","authors":["Xiaosong Jia","Shaoshuai Shi","Zijun Chen","Li Jiang","Wenlong Liao","Tao He","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.13331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13321v1","updated":"2024-03-20T05:57:20Z","published":"2024-03-20T05:57:20Z","title":"Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow\n  around a Quadrotor","summary":"  The widespread adoption of quadrotors for diverse applications, from\nagriculture to public safety, necessitates an understanding of the aerodynamic\ndisturbances they create. This paper introduces a computationally lightweight\nmodel for estimating the time-averaged magnitude of the induced flow below\nquadrotors in hover. Unlike related approaches that rely on expensive\ncomputational fluid dynamics (CFD) simulations or time-consuming empirical\nmeasurements, our method leverages classical theory from turbulent flows. By\nanalyzing over 9 hours of flight data from drones of varying sizes within a\nlarge motion capture system, we show that the combined flow from all propellers\nof the drone is well-approximated by a turbulent jet. Through the use of a\nnovel normalization and scaling, we have developed and experimentally validated\na unified model that describes the mean velocity field of the induced flow for\ndifferent drone sizes. The model accurately describes the far-field airflow in\na very large volume below the drone which is difficult to simulate in CFD. Our\nmodel, which requires only the drone's mass, propeller size, and drone size for\ncalculations, offers a practical tool for dynamic planning in multi-agent\nscenarios, ensuring safer operations near humans and optimizing sensor\nplacements.\n","authors":["Leonard Bauersfeld","Koen Muller","Dominic Ziegler","Filippo Coletti","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.13321v1.pdf","comment":"7+1 pages"},{"id":"http://arxiv.org/abs/2403.13318v1","updated":"2024-03-20T05:46:56Z","published":"2024-03-20T05:46:56Z","title":"Workload Estimation for Unknown Tasks: A Survey of Machine Learning\n  Under Distribution Shift","summary":"  Human-robot teams involve humans and robots collaborating to achieve tasks\nunder various environmental conditions. Successful teaming will require robots\nto adapt autonomously to a human teammate's internal state. An important\nelement of such adaptation is the ability to estimate the human teammates'\nworkload in unknown situations. Existing workload models use machine learning\nto model the relationships between physiological metrics and workload; however,\nthese methods are susceptible to individual differences and are heavily\ninfluenced by other factors. These methods cannot generalize to unknown tasks,\nas they rely on standard machine learning approaches that assume data consists\nof independent and identically distributed (IID) samples. This assumption does\nnot necessarily hold for estimating workload for new tasks. A survey of non-IID\nmachine learning techniques is presented, where commonly used techniques are\nevaluated using three criteria: portability, model complexity, and\nadaptability. These criteria are used to argue which techniques are most\napplicable for estimating workload for unknown tasks in dynamic, real-time\nenvironments.\n","authors":["Josh Bhagat Smith","Julie A. Adams"],"pdf_url":"https://arxiv.org/pdf/2403.13318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13311v1","updated":"2024-03-20T05:23:24Z","published":"2024-03-20T05:23:24Z","title":"Multi-Robot Connected Fermat Spiral Coverage","summary":"  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel\nalgorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts\nConnected Fermat Spiral (CFS) from the computer graphics community to\nmulti-robot coordination for the first time. MCFS uniquely enables the\norchestration of multiple robots to generate coverage paths that contour around\narbitrarily shaped obstacles, a feature that is notably lacking in traditional\nmethods. Our framework not only enhances area coverage and optimizes task\nperformance, particularly in terms of makespan, for workspaces rich in\nirregular obstacles but also addresses the challenges of path continuity and\ncurvature critical for non-holonomic robots by generating smooth paths without\ndecomposing the workspace. MCFS solves MCPP by constructing a graph of isolines\nand transforming MCPP into a combinatorial optimization problem, aiming to\nminimize the makespan while covering all vertices. Our contributions include\ndeveloping a unified CFS version for scalable and adaptable MCPP, extending it\nto MCPP with novel optimization techniques for cost reduction and path\ncontinuity and smoothness, and demonstrating through extensive experiments that\nMCFS outperforms existing MCPP methods in makespan, path curvature, coverage\nratio, and overlapping ratio. Our research marks a significant step in MCPP,\nshowcasing the fusion of computer graphics and automated planning principles to\nadvance the capabilities of multi-robot systems in complex environments. Our\ncode is available at https://github.com/reso1/MCFS.\n","authors":["Jingtao Tang","Hang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13311v1.pdf","comment":"accepted to ICAPS24"},{"id":"http://arxiv.org/abs/2403.13297v1","updated":"2024-03-20T04:39:15Z","published":"2024-03-20T04:39:15Z","title":"POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable\n  Satisfaction of Hard Constraints","summary":"  In this paper, we seek to learn a robot policy guaranteed to satisfy state\nconstraints. To encourage constraint satisfaction, existing RL algorithms\ntypically rely on Constrained Markov Decision Processes and discourage\nconstraint violations through reward shaping. However, such soft constraints\ncannot offer verifiable safety guarantees. To address this gap, we propose\nPOLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard\nconstraints in closed-loop with a black-box environment. Our key insight is to\nforce the learned policy to be affine around the unsafe set and use this affine\nregion as a repulsive buffer to prevent trajectories from violating the\nconstraint. We prove that such policies exist and guarantee constraint\nsatisfaction. Our proposed framework is applicable to both systems with\ncontinuous and discrete state and action spaces and is agnostic to the choice\nof the RL training algorithm. Our results demonstrate the capacity of POLICEd\nRL to enforce hard constraints in robotic tasks while significantly\noutperforming existing methods.\n","authors":["Jean-Baptiste Bouvier","Kartik Nagpal","Negar Mehr"],"pdf_url":"https://arxiv.org/pdf/2403.13297v1.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.13294v1","updated":"2024-03-20T04:25:09Z","published":"2024-03-20T04:25:09Z","title":"Map-Aware Human Pose Prediction for Robot Follow-Ahead","summary":"  In the robot follow-ahead task, a mobile robot is tasked to maintain its\nrelative position in front of a moving human actor while keeping the actor in\nsight. To accomplish this task, it is important that the robot understand the\nfull 3D pose of the human (since the head orientation can be different than the\ntorso) and predict future human poses so as to plan accordingly. This\nprediction task is especially tricky in a complex environment with junctions\nand multiple corridors. In this work, we address the problem of forecasting the\nfull 3D trajectory of a human in such environments. Our main insight is to show\nthat one can first predict the 2D trajectory and then estimate the full 3D\ntrajectory by conditioning the estimator on the predicted 2D trajectory. With\nthis approach, we achieve results comparable or better than the\nstate-of-the-art methods three times faster. As part of our contribution, we\npresent a new dataset where, in contrast to existing datasets, the human motion\nis in a much larger area than a single room. We also present a complete robot\nsystem that integrates our human pose forecasting network on the mobile robot\nto enable real-time robot follow-ahead and present results from real-world\nexperiments in multiple buildings on campus. Our project page, including\nsupplementary material and videos, can be found at:\nhttps://qingyuan-jiang.github.io/iros2024_poseForecasting/\n","authors":["Qingyuan Jiang","Burak Susam","Jun-Jee Chao","Volkan Isler"],"pdf_url":"https://arxiv.org/pdf/2403.13294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13284v1","updated":"2024-03-20T03:53:20Z","published":"2024-03-20T03:53:20Z","title":"Look Before You Leap: Socially Acceptable High-Speed Ground Robot\n  Navigation in Crowded Hallways","summary":"  To operate safely and efficiently, autonomous warehouse/delivery robots must\nbe able to accomplish tasks while navigating in dynamic environments and\nhandling the large uncertainties associated with the motions/behaviors of other\nrobots and/or humans. A key scenario in such environments is the hallway\nproblem, where robots must operate in the same narrow corridor as human traffic\ngoing in one or both directions. Traditionally, robot planners have tended to\nfocus on socially acceptable behavior in the hallway scenario at the expense of\nperformance. This paper proposes a planner that aims to address the consequent\n\"robot freezing problem\" in hallways by allowing for \"peek-and-pass\" maneuvers.\nWe then go on to demonstrate in simulation how this planner improves robot time\nto goal without violating social norms. Finally, we show initial hardware\ndemonstrations of this planner in the real world.\n","authors":["Lakshay Sharma","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2403.13284v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13281v1","updated":"2024-03-20T03:42:03Z","published":"2024-03-20T03:42:03Z","title":"Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks","summary":"  Robot arms should be able to learn new tasks. One framework here is\nreinforcement learning, where the robot is given a reward function that encodes\nthe task, and the robot autonomously learns actions to maximize its reward.\nExisting approaches to reinforcement learning often frame this problem as a\nMarkov decision process, and learn a policy (or a hierarchy of policies) to\ncomplete the task. These policies reason over hundreds of fine-grained actions\nthat the robot arm needs to take: e.g., moving slightly to the right or\nrotating the end-effector a few degrees. But the manipulation tasks that we\nwant robots to perform can often be broken down into a small number of\nhigh-level motions: e.g., reaching an object or turning a handle. In this paper\nwe therefore propose a waypoint-based approach for model-free reinforcement\nlearning. Instead of learning a low-level policy, the robot now learns a\ntrajectory of waypoints, and then interpolates between those waypoints using\nexisting controllers. Our key novelty is framing this waypoint-based setting as\na sequence of multi-armed bandits: each bandit problem corresponds to one\nwaypoint along the robot's motion. We theoretically show that an ideal solution\nto this reformulation has lower regret bounds than standard frameworks. We also\nintroduce an approximate posterior sampling solution that builds the robot's\nmotion one waypoint at a time. Results across benchmark simulations and two\nreal-world experiments suggest that this proposed approach learns new tasks\nmore quickly than state-of-the-art baselines. See videos here:\nhttps://youtu.be/MMEd-lYfq4Y\n","authors":["Shaunak A. Mehta","Soheil Habibian","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2403.13281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13274v1","updated":"2024-03-20T03:17:07Z","published":"2024-03-20T03:17:07Z","title":"UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric\n  Estimation and Model Predictive Control","summary":"  Nonprehensile manipulation through precise pushing is an essential skill that\nhas been commonly challenged by perception and physical uncertainties, such as\nthose associated with contacts, object geometries, and physical properties. For\nthis, we propose a unified framework that jointly addresses system modeling,\naction generation, and control. While most existing approaches either heavily\nrely on a priori system information for analytic modeling, or leverage a large\ndataset to learn dynamic models, our framework approximates a system transition\nfunction via non-parametric learning only using a small number of exploratory\nactions (ca. 10). The approximated function is then integrated with model\npredictive control to provide precise pushing manipulation. Furthermore, we\nshow that the approximated system transition functions can be robustly\ntransferred across novel objects while being online updated to continuously\nimprove the manipulation accuracy. Through extensive experiments on a real\nrobot platform with a set of novel objects and comparing against a\nstate-of-the-art baseline, we show that the proposed unified framework is a\nlight-weight and highly effective approach to enable precise pushing\nmanipulation all by itself. Our evaluation results illustrate that the system\ncan robustly ensure millimeter-level precision and can straightforwardly work\non any novel object.\n","authors":["Gaotian Wang","Kejia Ren","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13266v1","updated":"2024-03-20T03:03:22Z","published":"2024-03-20T03:03:22Z","title":"Enhancing Security in Multi-Robot Systems through Co-Observation\n  Planning, Reachability Analysis, and Network Flow","summary":"  This paper addresses security challenges in multi-robot systems (MRS) where\nadversaries may compromise robot control, risking unauthorized access to\nforbidden areas. We propose a novel multi-robot optimal planning algorithm that\nintegrates mutual observations and introduces reachability constraints for\nenhanced security. This ensures that, even with adversarial movements,\ncompromised robots cannot breach forbidden regions without missing scheduled\nco-observations. The reachability constraint uses ellipsoidal\nover-approximation for efficient intersection checking and gradient\ncomputation. To enhance system resilience and tackle feasibility challenges, we\nalso introduce sub-teams. These cohesive units replace individual robot\nassignments along each route, enabling redundant robots to deviate for\nco-observations across different trajectories, securing multiple sub-teams\nwithout requiring modifications. We formulate the cross-trajectory\nco-observation plan by solving a network flow coverage problem on the\ncheckpoint graph generated from the original unsecured MRS trajectories,\nproviding the same security guarantees against plan-deviation attacks. We\ndemonstrate the effectiveness and robustness of our proposed algorithm, which\nsignificantly strengthens the security of multi-robot systems in the face of\nadversarial threats.\n","authors":["Ziqi Yang","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.13266v1.pdf","comment":"12 pages, 6 figures, submitted to IEEE Transactions on Control of\n  Network Systems"},{"id":"http://arxiv.org/abs/2403.13251v1","updated":"2024-03-20T02:31:23Z","published":"2024-03-20T02:31:23Z","title":"A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on\n  Responsibility-Sensitive Safety","summary":"  Lane merging is one of the critical tasks for self-driving cars, and how to\nperform lane-merge maneuvers effectively and safely has become one of the\nimportant standards in measuring the capability of autonomous driving systems.\nHowever, due to the ambiguity in driving intentions and right-of-way issues,\nthe lane merging process in autonomous driving remains deficient in terms of\nmaintaining or ceding the right-of-way and attributing liability, which could\nresult in protracted durations for merging and problems such as trajectory\noscillation. Hence, we present a rule-compliance path planner (RCPP) for\nlane-merge scenarios, which initially employs the extended\nresponsibility-sensitive safety (RSS) to elucidate the right-of-way, followed\nby the potential field-based sigmoid planner for path generation. In the\nsimulation, we have validated the efficacy of the proposed algorithm. The\nalgorithm demonstrated superior performance over previous approaches in aspects\nsuch as merging time (Saved 72.3%), path length (reduced 53.4%), and\neliminating the trajectory oscillation.\n","authors":["Pengfei Lin","Ehsan Javanmardi","Yuze Jiang","Manabu Tsukada"],"pdf_url":"https://arxiv.org/pdf/2403.13251v1.pdf","comment":"Submitted to IEEE IROS 2024"},{"id":"http://arxiv.org/abs/2403.13245v1","updated":"2024-03-20T02:16:54Z","published":"2024-03-20T02:16:54Z","title":"Federated reinforcement learning for robot motion planning with\n  zero-shot generalization","summary":"  This paper considers the problem of learning a control policy for robot\nmotion planning with zero-shot generalization, i.e., no data collection and\npolicy adaptation is needed when the learned policy is deployed in new\nenvironments. We develop a federated reinforcement learning framework that\nenables collaborative learning of multiple learners and a central server, i.e.,\nthe Cloud, without sharing their raw data. In each iteration, each learner\nuploads its local control policy and the corresponding estimated normalized\narrival time to the Cloud, which then computes the global optimum among the\nlearners and broadcasts the optimal policy to the learners. Each learner then\nselects between its local control policy and that from the Cloud for next\niteration. The proposed framework leverages on the derived zero-shot\ngeneralization guarantees on arrival time and safety. Theoretical guarantees on\nalmost-sure convergence, almost consensus, Pareto improvement and optimality\ngap are also provided. Monte Carlo simulation is conducted to evaluate the\nproposed framework.\n","authors":["Zhenyuan Yuan","Siyuan Xu","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13235v1","updated":"2024-03-20T01:57:24Z","published":"2024-03-20T01:57:24Z","title":"AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for\n  Quadruped Robot Navigation in Outdoor Environments","summary":"  We present AMCO, a novel navigation method for quadruped robots that\nadaptively combines vision-based and proprioception-based perception\ncapabilities. Our approach uses three cost maps: general knowledge map;\ntraversability history map; and current proprioception map; which are derived\nfrom a robot's vision and proprioception data, and couples them to obtain a\ncoupled traversability cost map for navigation. The general knowledge map\nencodes terrains semantically segmented from visual sensing, and represents a\nterrain's typically expected traversability. The traversability history map\nencodes the robot's recent proprioceptive measurements on a terrain and its\nsemantic segmentation as a cost map. Further, the robot's present\nproprioceptive measurement is encoded as a cost map in the current\nproprioception map. As the general knowledge map and traversability history map\nrely on semantic segmentation, we evaluate the reliability of the visual\nsensory data by estimating the brightness and motion blur of input RGB images\nand accordingly combine the three cost maps to obtain the coupled\ntraversability cost map used for navigation. Leveraging this adaptive coupling,\nthe robot can depend on the most reliable input modality available. Finally, we\npresent a novel planner that selects appropriate gaits and velocities for\ntraversing challenging outdoor environments using the coupled traversability\ncost map. We demonstrate AMCO's navigation performance in different real-world\noutdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability\nmetrics, and up to 50% improvement in terms of success rate compared to current\nnavigation methods.\n","authors":["Mohamed Elnoor","Kasun Weerakoon","Adarsh Jagan Sathyamoorthy","Tianrui Guan","Vignesh Rajagopal","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.13235v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.12245v2","updated":"2024-03-20T01:48:46Z","published":"2024-03-18T20:51:08Z","title":"Improving Out-of-Distribution Generalization of Learned Dynamics by\n  Learning Pseudometrics and Constraint Manifolds","summary":"  We propose a method for improving the prediction accuracy of learned robot\ndynamics models on out-of-distribution (OOD) states. We achieve this by\nleveraging two key sources of structure often present in robot dynamics: 1)\nsparsity, i.e., some components of the state may not affect the dynamics, and\n2) physical limits on the set of possible motions, in the form of nonholonomic\nconstraints. Crucially, we do not assume this structure is known a priori, and\ninstead learn it from data. We use contrastive learning to obtain a distance\npseudometric that uncovers the sparsity pattern in the dynamics, and use it to\nreduce the input space when learning the dynamics. We then learn the unknown\nconstraint manifold by approximating the normal space of possible motions from\nthe data, which we use to train a Gaussian process (GP) representation of the\nconstraint manifold. We evaluate our approach on a physical differential-drive\nrobot and a simulated quadrotor, showing improved prediction accuracy on OOD\ndata relative to baselines.\n","authors":["Yating Lin","Glen Chou","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2403.12245v2.pdf","comment":"Accept to ICRA 2024, 6 pages + references"},{"id":"http://arxiv.org/abs/2403.13221v1","updated":"2024-03-20T00:56:27Z","published":"2024-03-20T00:56:27Z","title":"A Contact Model based on Denoising Diffusion to Learn Variable Impedance\n  Control for Contact-rich Manipulation","summary":"  In this paper, a novel approach is proposed for learning robot control in\ncontact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM).\nPrevious methods of learning such tasks relied on impedance control with\ntime-varying stiffness tuning by performing Bayesian optimization by\ntrial-and-error with robots. The proposed approach aims to reduce the cost of\nrobot operation by predicting the robot contact trajectories from the variable\nstiffness inputs and using neural models. However, contact dynamics are\ninherently highly nonlinear, and their simulation requires iterative\ncomputations such as convex optimization. Moreover, approximating such\ncomputations by using finite-layer neural models is difficult. To overcome\nthese limitations, the proposed DCM used the denoising diffusion models that\ncould simulate the complex dynamics via iterative computations of multi-step\ndenoising, thus improving the prediction accuracy. Stiffness tuning experiments\nconducted in simulated and real environments showed that the DCM achieved\ncomparable performance to a conventional robot-based optimization method while\nreducing the number of robot trials.\n","authors":["Masashi Okada","Mayumi Komatsu","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2403.13221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14115v2","updated":"2024-03-20T00:23:39Z","published":"2023-12-21T18:40:34Z","title":"LingoQA: Video Question Answering for Autonomous Driving","summary":"  Autonomous driving has long faced a challenge with public acceptance due to\nthe lack of explainability in the decision-making process. Video\nquestion-answering (QA) in natural language provides the opportunity for\nbridging this gap. Nonetheless, evaluating the performance of Video QA models\nhas proved particularly tough due to the absence of comprehensive benchmarks.\nTo fill this gap, we introduce LingoQA, a benchmark specifically for autonomous\ndriving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman\ncorrelation coefficient with human evaluations. We introduce a Video QA dataset\nof central London consisting of 419k samples that we release with the paper. We\nestablish a baseline vision-language model and run extensive ablation studies\nto understand its performance.\n","authors":["Ana-Maria Marcu","Long Chen","Jan Hünermann","Alice Karnsund","Benoit Hanotte","Prajwal Chidananda","Saurabh Nair","Vijay Badrinarayanan","Alex Kendall","Jamie Shotton","Elahe Arani","Oleg Sinavski"],"pdf_url":"https://arxiv.org/pdf/2312.14115v2.pdf","comment":"Benchmark and dataset are available at\n  https://github.com/wayveai/LingoQA/"},{"id":"http://arxiv.org/abs/2403.14041v1","updated":"2024-03-20T23:36:30Z","published":"2024-03-20T23:36:30Z","title":"\"It's Not a Replacement:\" Enabling Parent-Robot Collaboration to Support\n  In-Home Learning Experiences of Young Children","summary":"  Learning companion robots for young children are increasingly adopted in\ninformal learning environments. Although parents play a pivotal role in their\nchildren's learning, very little is known about how parents prefer to\nincorporate robots into their children's learning activities. We developed\nprototype capabilities for a learning companion robot to deliver educational\nprompts and responses to parent-child pairs during reading sessions and\nconducted in-home user studies involving 10 families with children aged 3-5.\nOur data indicates that parents want to work with robots as collaborators to\naugment parental activities to foster children's learning, introducing the\nnotion of parent-robot collaboration. Our findings offer an empirical\nunderstanding of the needs and challenges of parent-child interaction in\ninformal learning scenarios and design opportunities for integrating a\ncompanion robot into these interactions. We offer insights into how robots\nmight be designed to facilitate parent-robot collaboration, including parenting\npolicies, collaboration patterns, and interaction paradigms.\n","authors":["Hui-Ru Ho","Edward Hubbard","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2403.14041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14029v1","updated":"2024-03-20T22:57:34Z","published":"2024-03-20T22:57:34Z","title":"Quadcopter Team Configurable Motion Guided by a Quadruped","summary":"  The paper focuses on modeling and experimental evaluation of a quadcopter\nteam configurable coordination guided by a single quadruped robot. We consider\nthe quadcopter team as particles of a two-dimensional deformable body and\npropose a two-dimensional affine transformation model for safe and\ncollision-free configurable coordination of this heterogeneous robotic system.\nThe proposed affine transformation is decomposed into translation, that is\nspecified by the quadruped global position, and configurable motion of the\nquadcopters, which is determined by a nonsingular Jacobian matrix so that the\nquadcopter team can safely navigate a constrained environment while avoiding\ncollision. We propose two methods to experimentally evaluate the proposed\nheterogeneous robot coordination model. The first method measures real\npositions of quadcopters, quadruped, and environmental objects all with respect\nto the global coordinate system. On the other hand, the second method measures\nposition with respect to the local coordinate system fixed on the dog robot\nwhich in turn enables safe planning the Jacobian matrix of the quadcopter team\nwhile the world is virtually approached the robotic system.\n","authors":["Mohammad Ghufran","Sourish Tetakayala","Jack Hughes","Aron Wilson","Hossein Rastgoftar"],"pdf_url":"https://arxiv.org/pdf/2403.14029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14025v1","updated":"2024-03-20T22:51:29Z","published":"2024-03-20T22:51:29Z","title":"HRI Curriculum for a Liberal Arts Education","summary":"  In this paper, we discuss the opportunities and challenges of teaching a\nhuman-robot interaction course at an undergraduate liberal arts college. We\nprovide a sample syllabus adapted from a previous version of a course.\n","authors":["Jason R. Wilson","Emily Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.14025v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.14014v1","updated":"2024-03-20T22:24:52Z","published":"2024-03-20T22:24:52Z","title":"Crowdsourcing Task Traces for Service Robotics","summary":"  Demonstration is an effective end-user development paradigm for teaching\nrobots how to perform new tasks. In this paper, we posit that demonstration is\nuseful not only as a teaching tool, but also as a way to understand and assist\nend-user developers in thinking about a task at hand. As a first step toward\ngaining this understanding, we constructed a lightweight web interface to\ncrowdsource step-by-step instructions of common household tasks, leveraging the\nimaginations and past experiences of potential end-user developers. As evidence\nof the utility of our interface, we deployed the interface on Amazon Mechanical\nTurk and collected 207 task traces that span 18 different task categories. We\ndescribe our vision for how these task traces can be operationalized as task\nmodels within end-user development tools and provide a roadmap for future work.\n","authors":["David Porfirio","Allison Sauppé","Maya Cakmak","Aws Albarghouthi","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2403.14014v1.pdf","comment":"Published in the companion proceedings of the 2023 ACM/IEEE\n  International Conference on Human-Robot Interaction"},{"id":"http://arxiv.org/abs/2403.02316v2","updated":"2024-03-20T22:05:06Z","published":"2024-03-04T18:52:15Z","title":"Designing Library of Skill-Agents for Hardware-Level Reusability","summary":"  To use new robot hardware in a new environment, it is necessary to develop a\ncontrol program tailored to that specific robot in that environment.\nConsidering the reusability of software among robots is crucial to minimize the\neffort involved in this process and maximize software reuse across different\nrobots in different environments. This paper proposes a method to remedy this\nprocess by considering hardware-level reusability, using\nLearning-from-observation (LfO) paradigm with a pre-designed skill-agent\nlibrary. The LfO framework represents the required actions in\nhardware-independent representations, referred to as task models, from\nobserving human demonstrations, capturing the necessary parameters for the\ninteraction between the environment and the robot. When executing the desired\nactions from the task models, a set of skill agents is employed to convert the\nrepresentations into robot commands. This paper focuses on the latter part of\nthe LfO framework, utilizing the set to generate robot actions from the task\nmodels, and explores a hardware-independent design approach for these skill\nagents. These skill agents are described in a hardware-independent manner,\nconsidering the relative relationship between the robot's hand position and the\nenvironment. As a result, it is possible to execute these actions on robots\nwith different hardware configurations by simply swapping the inverse\nkinematics solver. This paper, first, defines a necessary and sufficient\nskill-agent set corresponding to cover all possible actions, and considers the\ndesign principles for these skill agents in the library. We provide concrete\nexamples of such skill agents and demonstrate the practicality of using these\nskill agents by showing that the same representations can be executed on two\ndifferent robots, Nextage and Fetch, using the proposed skill-agents set.\n","authors":["Jun Takamatsu","Daichi Saito","Katsushi Ikeuchi","Atsushi Kanehira","Kazuhiro Sasabuchi","Naoki Wake"],"pdf_url":"https://arxiv.org/pdf/2403.02316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14000v1","updated":"2024-03-20T21:57:14Z","published":"2024-03-20T21:57:14Z","title":"Visual Imitation Learning of Task-Oriented Object Grasping and\n  Rearrangement","summary":"  Task-oriented object grasping and rearrangement are critical skills for\nrobots to accomplish different real-world manipulation tasks. However, they\nremain challenging due to partial observations of the objects and shape\nvariations in categorical objects. In this paper, we propose the Multi-feature\nImplicit Model (MIMO), a novel object representation that encodes multiple\nspatial features between a point and an object in an implicit neural field.\nTraining such a model on multiple features ensures that it embeds the object\nshapes consistently in different aspects, thus improving its performance in\nobject shape reconstruction from partial observation, shape similarity measure,\nand modeling spatial relations between objects. Based on MIMO, we propose a\nframework to learn task-oriented object grasping and rearrangement from single\nor multiple human demonstration videos. The evaluations in simulation show that\nour approach outperforms the state-of-the-art methods for multi- and\nsingle-view observations. Real-world experiments demonstrate the efficacy of\nour approach in one- and few-shot imitation learning of manipulation tasks.\n","authors":["Yichen Cai","Jianfeng Gao","Christoph Pohl","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.14000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13988v1","updated":"2024-03-20T21:41:44Z","published":"2024-03-20T21:41:44Z","title":"Goal-Oriented End-User Programming of Robots","summary":"  End-user programming (EUP) tools must balance user control with the robot's\nability to plan and act autonomously. Many existing task-oriented EUP tools\nenforce a specific level of control, e.g., by requiring that users hand-craft\ndetailed sequences of actions, rather than offering users the flexibility to\nchoose the level of task detail they wish to express. We thereby created a\nnovel EUP system, Polaris, that in contrast to most existing EUP tools, uses\ngoal predicates as the fundamental building block of programs. Users can\nthereby express high-level robot objectives or lower-level checkpoints at their\nchoosing, while an off-the-shelf task planner fills in any remaining program\ndetail. To ensure that goal-specified programs adhere to user expectations of\nrobot behavior, Polaris is equipped with a Plan Visualizer that exposes the\nplanner's output to the user before runtime. In what follows, we describe our\ndesign of Polaris and its evaluation with 32 human participants. Our results\nsupport the Plan Visualizer's ability to help users craft higher-quality\nprograms. Furthermore, there are strong associations between user perception of\nthe robot and Plan Visualizer usage, and evidence that robot familiarity has a\nkey role in shaping user experience.\n","authors":["David Porfirio","Mark Roberts","Laura M. Hiatt"],"pdf_url":"https://arxiv.org/pdf/2403.13988v1.pdf","comment":"Published in the proceedings of the 2024 ACM/IEEE International\n  Conference on Human-Robot Interaction"},{"id":"http://arxiv.org/abs/2403.13960v1","updated":"2024-03-20T20:13:39Z","published":"2024-03-20T20:13:39Z","title":"Open Access NAO (OAN): a ROS2-based software framework for HRI\n  applications with the NAO robot","summary":"  This paper presents a new software framework for HRI experimentation with the\nsixth version of the common NAO robot produced by the United Robotics Group.\nEmbracing the common demand of researchers for better performance and new\nfeatures for NAO, the authors took advantage of the ability to run ROS2 onboard\non the NAO to develop a framework independent of the APIs provided by the\nmanufacturer. Such a system provides NAO with not only the basic skills of a\nhumanoid robot such as walking and reproducing movements of interest but also\nfeatures often used in HRI such as: speech recognition/synthesis, face and\nobject detention, and the use of Generative Pre-trained Transformer (GPT)\nmodels for conversation. The developed code is therefore configured as a\nready-to-use but also highly expandable and improvable tool thanks to the\npossibilities provided by the ROS community.\n","authors":["Antonio Bono","Kenji Brameld","Luigi D'Alfonso","Giuseppe Fedele"],"pdf_url":"https://arxiv.org/pdf/2403.13960v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.02352v2","updated":"2024-03-20T19:57:24Z","published":"2023-12-04T21:32:00Z","title":"Working Backwards: Learning to Place by Picking","summary":"  We present placing via picking (PvP), a method to autonomously collect\nreal-world demonstrations for a family of placing tasks in which objects must\nbe manipulated to specific contact-constrained locations. With PvP, we approach\nthe collection of robotic object placement demonstrations by reversing the\ngrasping process and exploiting the inherent symmetry of the pick and place\nproblems. Specifically, we obtain placing demonstrations from a set of grasp\nsequences of objects initially located at their target placement locations. Our\nsystem can collect hundreds of demonstrations in contact-constrained\nenvironments without human intervention by combining two modules: tactile\nregrasping and compliant control for grasps. We train a policy directly from\nvisual observations through behavioral cloning, using the\nautonomously-collected demonstrations. By doing so, the policy can generalize\nto object placement scenarios outside of the training environment without\nprivileged information (e.g., placing a plate picked up from a table). We\nvalidate our approach in home robotic scenarios that include dishwasher loading\nand table setting. Our approach yields robotic placing policies that outperform\npolicies trained with kinesthetic teaching, both in terms of performance and\ndata efficiency, while requiring no human supervision.\n","authors":["Oliver Limoyo","Abhisek Konar","Trevor Ablett","Jonathan Kelly","Francois R. Hogan","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2312.02352v2.pdf","comment":"Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024"},{"id":"http://arxiv.org/abs/2401.11061v2","updated":"2024-03-20T19:44:07Z","published":"2024-01-19T23:34:48Z","title":"PhotoBot: Reference-Guided Interactive Photography via Natural Language","summary":"  We introduce PhotoBot, a framework for fully automated photo acquisition\nbased on an interplay between high-level human language guidance and a robot\nphotographer. We propose to communicate photography suggestions to the user via\nreference images that are selected from a curated gallery. We leverage a visual\nlanguage model (VLM) and an object detector to characterize the reference\nimages via textual descriptions and then use a large language model (LLM) to\nretrieve relevant reference images based on a user's language query through\ntext-based reasoning. To correspond the reference image and the observed scene,\nwe exploit pre-trained features from a vision transformer capable of capturing\nsemantic similarity across marked appearance variations. Using these features,\nwe compute pose adjustments for an RGB-D camera by solving a\nperspective-n-point (PnP) problem. We demonstrate our approach using a\nmanipulator equipped with a wrist camera. Our user studies show that photos\ntaken by PhotoBot are often more aesthetically pleasing than those taken by\nusers themselves, as measured by human feedback. We also show that PhotoBot can\ngeneralize to other reference sources such as paintings.\n","authors":["Oliver Limoyo","Jimmy Li","Dmitriy Rivkin","Jonathan Kelly","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2401.11061v2.pdf","comment":"Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024"},{"id":"http://arxiv.org/abs/2403.04169v2","updated":"2024-03-20T19:28:56Z","published":"2024-03-07T02:58:15Z","title":"Social Robots for Sleep Health: A Scoping Review","summary":"  Poor sleep health is an increasingly concerning public healthcare crisis,\nespecially when coupled with a dwindling number of health professionals\nqualified to combat it. However, there is a growing body of scientific\nliterature on the use of digital technologies in supporting and sustaining\nindividuals' healthy sleep habits. Social robots are a relatively recent\ntechnology that has been used to facilitate health care interventions and may\nhave potential in improving sleep health outcomes, as well. Social robots'\nunique characteristics -- such as anthropomorphic physical embodiment or\neffective communication methods -- help to engage users and motivate them to\ncomply with specific interventions, thus improving the interventions' outcomes.\nThis scoping review aims to evaluate current scientific evidence for employing\nsocial robots in sleep health interventions, identify critical research gaps,\nand suggest future directions for developing and using social robots to improve\npeople's sleep health. Our analysis of the reviewed studies found them limited\ndue to a singular focus on the older adult population, use of small sample\nsizes, limited intervention durations, and other compounding factors.\nNevertheless, the reviewed studies reported several positive outcomes,\nhighlighting the potential social robots hold in this field. Although our\nreview found limited clinical evidence for the efficacy of social robots as\npurveyors of sleep health interventions, it did elucidate the potential for a\nsuccessful future in this domain if current limitations are addressed and more\nresearch is conducted.\n","authors":["Victor Nikhil Antony","Mengchi Li","Shu-Han Lin","Junxin Li","Chien-Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.04169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13941v1","updated":"2024-03-20T19:26:27Z","published":"2024-03-20T19:26:27Z","title":"Sensory Glove-Based Surgical Robot User Interface","summary":"  Robotic surgery has reached a high level of maturity and has become an\nintegral part of standard surgical care. However, existing surgeon consoles are\nbulky and take up valuable space in the operating room, present challenges for\nsurgical team coordination, and their proprietary nature makes it difficult to\ntake advantage of recent technological advances, especially in virtual and\naugmented reality. One potential area for further improvement is the\nintegration of modern sensory gloves into robotic platforms, allowing surgeons\nto control robotic arms directly with their hand movements intuitively. We\npropose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3\nXR sensory glove, and God Vision wireless smart glasses. The system controls\none arm of a da Vinci surgical robot. In addition to moving the arm, the\nsurgeon can use fingers to control the end-effector of the surgical instrument.\nHand gestures are used to implement clutching and similar functions. In\nparticular, we introduce clutching of the instrument orientation, a\nfunctionality not available in the da Vinci system. The vibrotactile elements\nof the glove are used to provide feedback to the user when gesture commands are\ninvoked. A preliminary evaluation of the system shows that it has excellent\ntracking accuracy and allows surgeons to efficiently perform common surgical\ntraining tasks with minimal practice with the new interface; this suggests that\nthe interface is highly intuitive. The proposed system is inexpensive, allows\nrapid prototyping, and opens opportunities for further innovations in the\ndesign of surgical robot interfaces.\n","authors":["Leonardo Borgioli","Ki-Hwan Oh","Alberto Mangano","Alvaro Ducas","Luciano Ambrosini","Federico Pinto","Paula A Lopez","Jessica Cassiani","Milos Zefran","Liaohai Chen","Pier Cristoforo Giulianotti"],"pdf_url":"https://arxiv.org/pdf/2403.13941v1.pdf","comment":"6 pages, 5 figures, 7 tables, submitted to International Conference\n  on Intelligent Robots and Systems (IROS)2024"},{"id":"http://arxiv.org/abs/2403.13929v1","updated":"2024-03-20T19:03:26Z","published":"2024-03-20T19:03:26Z","title":"Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic\n  Environments","summary":"  Autonomous collision avoidance requires accurate environmental perception;\nhowever, flight systems often possess limited sensing capabilities with\nfield-of-view (FOV) restrictions. To navigate this challenge, we present a\nsafety-aware approach for online determination of the optimal sensor-pointing\ndirection $\\psi_\\text{d}$ which utilizes control barrier functions (CBFs).\nFirst, we generate a spatial density function $\\Phi$ which leverages CBF\nconstraints to map the collision risk of all local coordinates. Then, we\nconvolve $\\Phi$ with an attitude-dependent sensor FOV quality function to\nproduce the objective function $\\Gamma$ which quantifies the total observed\nrisk for a given pointing direction. Finally, by finding the global optimizer\nfor $\\Gamma$, we identify the value of $\\psi_\\text{d}$ which maximizes the\nperception of risk within the FOV. We incorporate $\\psi_\\text{d}$ into a\nsafety-critical flight architecture and conduct a numerical analysis using\nmultiple simulated mission profiles. Our algorithm achieves a success rate of\n$88-96\\%$, constituting a $16-29\\%$ improvement compared to the best heuristic\nmethods. We demonstrate the functionality of our approach via a flight\ndemonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori\nobstacle knowledge, the quadrotor follows a dynamic flight path while\nsimultaneously calculating and tracking $\\psi_\\text{d}$ to perceive and avoid\ntwo static obstacles with an average computation time of 371 $\\mu$s.\n","authors":["Ryan M. Bena","Chongbo Zhao","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.13929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15821v2","updated":"2024-03-20T18:45:54Z","published":"2023-09-27T17:45:49Z","title":"LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic\n  Object Rearrangement","summary":"  We introduce a novel approach to the executable semantic object rearrangement\nproblem. In this challenge, a robot seeks to create an actionable plan that\nrearranges objects within a scene according to a pattern dictated by a natural\nlanguage description. Unlike existing methods such as StructFormer and\nStructDiffusion, which tackle the issue in two steps by first generating poses\nand then leveraging a task planner for action plan formulation, our method\nconcurrently addresses pose generation and action planning. We achieve this\nintegration using a Language-Guided Monte-Carlo Tree Search (LGMCTS).\nQuantitative evaluations are provided on two simulation datasets, and\ncomplemented by qualitative tests with a real robot.\n","authors":["Haonan Chang","Kai Gao","Kowndinya Boyalakuntla","Alex Lee","Baichuan Huang","Harish Udhaya Kumar","Jinjin Yu","Abdeslam Boularias"],"pdf_url":"https://arxiv.org/pdf/2309.15821v2.pdf","comment":"Our code and supplementary materials are accessible at\n  https://github.com/changhaonan/LG-MCTS"},{"id":"http://arxiv.org/abs/2403.13910v1","updated":"2024-03-20T18:30:12Z","published":"2024-03-20T18:30:12Z","title":"Augmented Reality Demonstrations for Scalable Robot Imitation Learning","summary":"  Robot Imitation Learning (IL) is a widely used method for training robots to\nperform manipulation tasks that involve mimicking human demonstrations to\nacquire skills. However, its practicality has been limited due to its\nrequirement that users be trained in operating real robot arms to provide\ndemonstrations. This paper presents an innovative solution: an Augmented\nReality (AR)-assisted framework for demonstration collection, empowering\nnon-roboticist users to produce demonstrations for robot IL using devices like\nthe HoloLens 2. Our framework facilitates scalable and diverse demonstration\ncollection for real-world tasks. We validate our approach with experiments on\nthree classical robotics tasks: reach, push, and pick-and-place. The real robot\nperforms each task successfully while replaying demonstrations collected via\nAR.\n","authors":["Yue Yang","Bryce Ikeda","Gedas Bertasius","Daniel Szafir"],"pdf_url":"https://arxiv.org/pdf/2403.13910v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.13808v1","updated":"2024-03-20T17:59:58Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13807v1","updated":"2024-03-20T17:59:57Z","published":"2024-03-20T17:59:57Z","title":"Editing Massive Concepts in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.\n","authors":["Tianwei Xiong","Yue Wu","Enze Xie","Yue Wu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13807v1.pdf","comment":"Project page: https://silentview.github.io/EMCID/ . Code:\n  https://github.com/SilentView/EMCID"},{"id":"http://arxiv.org/abs/2403.13805v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","summary":"  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n","authors":["Ziyu Liu","Zeyi Sun","Yuhang Zang","Wei Li","Pan Zhang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13805v1.pdf","comment":"Project: https://github.com/Liuziyu77/RAR"},{"id":"http://arxiv.org/abs/2403.13806v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS","summary":"  Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.\n","authors":["Michael Niemeyer","Fabian Manhardt","Marie-Julie Rakotosaona","Michael Oechsle","Daniel Duckworth","Rama Gosula","Keisuke Tateno","John Bates","Dominik Kaeser","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2403.13806v1.pdf","comment":"Project page at https://m-niemeyer.github.io/radsplat/"},{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13803v1","updated":"2024-03-20T17:59:16Z","published":"2024-03-20T17:59:16Z","title":"Bounding Box Stability against Feature Dropout Reflects Detector\n  Generalization across Environments","summary":"  Bounding boxes uniquely characterize object detection, where a good detector\ngives accurate bounding boxes of categories of interest. However, in the\nreal-world where test ground truths are not provided, it is non-trivial to find\nout whether bounding boxes are accurate, thus preventing us from assessing the\ndetector generalization ability. In this work, we find under feature map\ndropout, good detectors tend to output bounding boxes whose locations do not\nchange much, while bounding boxes of poor detectors will undergo noticeable\nposition changes. We compute the box stability score (BoS score) to reflect\nthis stability. Specifically, given an image, we compute a normal set of\nbounding boxes and a second set after feature map dropout. To obtain BoS score,\nwe use bipartite matching to find the corresponding boxes between the two sets\nand compute the average Intersection over Union (IoU) across the entire test\nset. We contribute to finding that BoS score has a strong, positive correlation\nwith detection accuracy measured by mean average precision (mAP) under various\ntest environments. This relationship allows us to predict the accuracy of\ndetectors on various real-world test sets without accessing test ground truths,\nverified on canonical detection tasks such as vehicle detection and pedestrian\ndetection. Code and data are available at https://github.com/YangYangGirl/BoS.\n","authors":["Yang Yang","Wenhai Wang","Zhe Chen","Jifeng Dai","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.13803v1.pdf","comment":"ICLR 2024 spotlight"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2312.06644v2","updated":"2024-03-20T17:58:05Z","published":"2023-12-11T18:56:37Z","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","summary":"  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n","authors":["Rao Fu","Zehao Wen","Zichen Liu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.06644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13800v1","updated":"2024-03-20T17:57:02Z","published":"2024-03-20T17:57:02Z","title":"TimeRewind: Rewinding Time with Image-and-Events Video Diffusion","summary":"  This paper addresses the novel challenge of ``rewinding'' time from a single\ncaptured image to recover the fleeting moments missed just before the shutter\nbutton is pressed. This problem poses a significant challenge in computer\nvision and computational photography, as it requires predicting plausible\npre-capture motion from a single static frame, an inherently ill-posed task due\nto the high degree of freedom in potential pixel movements. We overcome this\nchallenge by leveraging the emerging technology of neuromorphic event cameras,\nwhich capture motion information with high temporal resolution, and integrating\nthis data with advanced image-to-video diffusion models. Our proposed framework\nintroduces an event motion adaptor conditioned on event camera data, guiding\nthe diffusion model to generate videos that are visually coherent and\nphysically grounded in the captured events. Through extensive experimentation,\nwe demonstrate the capability of our approach to synthesize high-quality videos\nthat effectively ``rewind'' time, showcasing the potential of combining event\ncamera technology with generative models. Our work opens new avenues for\nresearch at the intersection of computer vision, computational photography, and\ngenerative modeling, offering a forward-thinking solution to capturing missed\nmoments and enhancing future consumer cameras and smartphones. Please see the\nproject page at https://timerewind.github.io/ for video results and code\nrelease.\n","authors":["Jingxi Chen","Brandon Y. Feng","Haoming Cai","Mingyang Xie","Christopher Metzler","Cornelia Fermuller","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2403.13800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13798v1","updated":"2024-03-20T17:55:21Z","published":"2024-03-20T17:55:21Z","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","summary":"  Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.\n","authors":["Lauren Okamoto","Paritosh Parmar"],"pdf_url":"https://arxiv.org/pdf/2403.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13797v1","updated":"2024-03-20T17:54:58Z","published":"2024-03-20T17:54:58Z","title":"Bridge the Modality and Capacity Gaps in Vision-Language Model Selection","summary":"  Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.\n","authors":["Chao Yi","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v1","updated":"2024-03-20T17:51:53Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Monocular depth estimation is crucial for numerous downstream vision tasks\nand applications. Current discriminative approaches to this problem are limited\ndue to blurry artifacts, while state-of-the-art generative methods suffer from\nslow sampling due to their SDE nature. Rather than starting from noise, we seek\na direct mapping from input image to depth map. We observe that this can be\neffectively framed using flow matching, since its straight trajectories through\nsolution space offer efficiency and high quality. Our study demonstrates that a\npre-trained image diffusion model can serve as an adequate prior for a flow\nmatching depth model, allowing efficient training on only synthetic data to\ngeneralize to real images. We find that an auxiliary surface normals loss\nfurther improves the depth estimates. Due to the generative nature of our\napproach, our model reliably predicts the confidence of its depth estimates. On\nstandard benchmarks of complex natural scenes, our lightweight approach\nexhibits state-of-the-art performance at favorable low computational cost\ndespite only being trained on little synthetic data.\n","authors":["Ming Gui","Johannes S. Fischer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13778v1","updated":"2024-03-20T17:41:35Z","published":"2024-03-20T17:41:35Z","title":"Certified Human Trajectory Prediction","summary":"  Trajectory prediction plays an essential role in autonomous vehicles. While\nnumerous strategies have been developed to enhance the robustness of trajectory\nprediction models, these methods are predominantly heuristic and do not offer\nguaranteed robustness against adversarial attacks and noisy observations. In\nthis work, we propose a certification approach tailored for the task of\ntrajectory prediction. To this end, we address the inherent challenges\nassociated with trajectory prediction, including unbounded outputs, and\nmutli-modality, resulting in a model that provides guaranteed robustness.\nFurthermore, we integrate a denoiser into our method to further improve the\nperformance. Through comprehensive evaluations, we demonstrate the\neffectiveness of the proposed technique across various baselines and using\nstandard trajectory prediction datasets. The code will be made available\nonline: https://s-attack.github.io/\n","authors":["Mohammadhossein Bahari","Saeed Saadatnejad","Amirhossein Asgari Farsangi","Seyed-Mohsen Moosavi-Dezfooli","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.13778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09368v2","updated":"2024-03-20T17:36:35Z","published":"2024-02-14T18:13:51Z","title":"Magic-Me: Identity-Specific Video Customized Diffusion","summary":"  Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.\n","authors":["Ze Ma","Daquan Zhou","Chun-Hsiao Yeh","Xue-She Wang","Xiuyu Li","Huanrui Yang","Zhen Dong","Kurt Keutzer","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2402.09368v2.pdf","comment":"Project Page at https://magic-me-webpage.github.io"},{"id":"http://arxiv.org/abs/2403.11085v2","updated":"2024-03-20T17:35:15Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13771v1","updated":"2024-03-20T17:33:02Z","published":"2024-03-20T17:33:02Z","title":"Describe-and-Dissect: Interpreting Neurons in Vision Networks with\n  Language Models","summary":"  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2 times as likely to be selected as the best\nexplanation for a neuron than the best baseline.\n","authors":["Nicholas Bai","Rahul A. Iyer","Tuomas Oikarinen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2403.13771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13765v1","updated":"2024-03-20T17:28:17Z","published":"2024-03-20T17:28:17Z","title":"Towards Principled Representation Learning from Videos for Reinforcement\n  Learning","summary":"  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n","authors":["Dipendra Misra","Akanksha Saran","Tengyang Xie","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2403.13765v1.pdf","comment":"ICLR 2024 Spotlight Conference Paper"},{"id":"http://arxiv.org/abs/2312.00651v2","updated":"2024-03-20T17:28:02Z","published":"2023-12-01T15:24:38Z","title":"TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion\n  Models","summary":"  Despite remarkable achievements in video synthesis, achieving granular\ncontrol over complex dynamics, such as nuanced movement among multiple\ninteracting objects, still presents a significant hurdle for dynamic world\nmodeling, compounded by the necessity to manage appearance and disappearance,\ndrastic scale changes, and ensure consistency for instances across frames.\nThese challenges hinder the development of video generation that can faithfully\nmimic real-world complexity, limiting utility for applications requiring\nhigh-level realism and controllability, including advanced scene simulation and\ntraining of perception systems. To address that, we propose TrackDiffusion, a\nnovel video generation framework affording fine-grained trajectory-conditioned\nmotion control via diffusion models, which facilitates the precise manipulation\nof the object trajectories and interactions, overcoming the prevalent\nlimitation of scale and continuity disruptions. A pivotal component of\nTrackDiffusion is the instance enhancer, which explicitly ensures inter-frame\nconsistency of multiple objects, a critical factor overlooked in the current\nliterature. Moreover, we demonstrate that generated video sequences by our\nTrackDiffusion can be used as training data for visual perception models. To\nthe best of our knowledge, this is the first work to apply video diffusion\nmodels with tracklet conditions and demonstrate that generated frames can be\nbeneficial for improving the performance of object trackers.\n","authors":["Pengxiang Li","Kai Chen","Zhili Liu","Ruiyuan Gao","Lanqing Hong","Guo Zhou","Hua Yao","Dit-Yan Yeung","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2312.00651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13763v1","updated":"2024-03-20T17:26:22Z","published":"2024-03-20T17:26:22Z","title":"Practical End-to-End Optical Music Recognition for Pianoform Music","summary":"  The majority of recent progress in Optical Music Recognition (OMR) has been\nachieved with Deep Learning methods, especially models following the end-to-end\nparadigm, reading input images and producing a linear sequence of tokens.\nUnfortunately, many music scores, especially piano music, cannot be easily\nconverted to a linear sequence. This has led OMR researchers to use custom\nlinearized encodings, instead of broadly accepted structured formats for music\nnotation. Their diversity makes it difficult to compare the performance of OMR\nsystems directly. To bring recent OMR model progress closer to useful results:\n(a) We define a sequential format called Linearized MusicXML, allowing to train\nan end-to-end model directly and maintaining close cohesion and compatibility\nwith the industry-standard MusicXML format. (b) We create a dev and test set\nfor benchmarking typeset OMR with MusicXML ground truth based on the OpenScore\nLieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an\nimage from IMSLP. (c) We train and fine-tune an end-to-end model to serve as a\nbaseline on the dataset and employ the TEDn metric to evaluate the model. We\nalso test our model against the recently published synthetic pianoform dataset\nGrandStaff and surpass the state-of-the-art results.\n","authors":["Jiří Mayer","Milan Straka","Jan Hajič jr.","Pavel Pecina"],"pdf_url":"https://arxiv.org/pdf/2403.13763v1.pdf","comment":"15+4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13761v1","updated":"2024-03-20T17:20:48Z","published":"2024-03-20T17:20:48Z","title":"HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text\n  Recognition","summary":"  Text recognition, especially for complex scripts like Chinese, faces unique\nchallenges due to its intricate character structures and vast vocabulary.\nTraditional one-hot encoding methods struggle with the representation of\nhierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and\non-device deployment due to their computational intensity. To address these\nchallenges, we propose HierCode, a novel and lightweight codebook that exploits\nthe innate hierarchical nature of Chinese characters. HierCode employs a\nmulti-hot encoding strategy, leveraging hierarchical binary tree encoding and\nprototype learning to create distinctive, informative representations for each\ncharacter. This approach not only facilitates zero-shot recognition of OOV\ncharacters by utilizing shared radicals and structures but also excels in\nline-level recognition tasks by computing similarity with visual features, a\nnotable advantage over existing methods. Extensive experiments across diverse\nbenchmarks, including handwritten, scene, document, web, and ancient text, have\nshowcased HierCode's superiority for both conventional and zero-shot Chinese\ncharacter or text recognition, exhibiting state-of-the-art performance with\nsignificantly fewer parameters and fast inference speed.\n","authors":["Yuyi Zhang","Yuanzhi Zhu","Dezhi Peng","Peirong Zhang","Zhenhua Yang","Zhibo Yang","Cong Yao","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2403.13761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13762v1","updated":"2024-03-20T17:20:48Z","published":"2024-03-20T17:20:48Z","title":"When Cars meet Drones: Hyperbolic Federated Learning for Source-Free\n  Domain Adaptation in Adverse Weather","summary":"  In Federated Learning (FL), multiple clients collaboratively train a global\nmodel without sharing private data. In semantic segmentation, the Federated\nsource Free Domain Adaptation (FFreeDA) setting is of particular interest,\nwhere clients undergo unsupervised training after supervised pretraining at the\nserver side. While few recent works address FL for autonomous vehicles,\nintrinsic real-world challenges such as the presence of adverse weather\nconditions and the existence of different autonomous agents are still\nunexplored. To bridge this gap, we address both problems and introduce a new\nfederated semantic segmentation setting where both car and drone clients\nco-exist and collaborate. Specifically, we propose a novel approach for this\nsetting which exploits a batch-norm weather-aware strategy to dynamically adapt\nthe model to the different weather conditions, while hyperbolic space\nprototypes are used to align the heterogeneous client representations. Finally,\nwe introduce FLYAWARE, the first semantic segmentation dataset with adverse\nweather data for aerial vehicles.\n","authors":["Giulia Rizzoli","Matteo Caligiuri","Donald Shenaj","Francesco Barbato","Pietro Zanuttigh"],"pdf_url":"https://arxiv.org/pdf/2403.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16355v3","updated":"2024-03-20T17:13:53Z","published":"2024-01-29T17:59:19Z","title":"PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding\n  and Reasoning in Pathology","summary":"  The emergence of large multimodal models has unlocked remarkable potential in\nAI, particularly in pathology. However, the lack of specialized, high-quality\nbenchmark impeded their development and precise evaluation. To address this, we\nintroduce PathMMU, the largest and highest-quality expert-validated pathology\nbenchmark for Large Multimodal Models (LMMs). It comprises 33,428 multimodal\nmulti-choice questions and 24,067 images from various sources, each accompanied\nby an explanation for the correct answer. The construction of PathMMU harnesses\nGPT-4V's advanced capabilities, utilizing over 30,000 image-caption pairs to\nenrich captions and generate corresponding Q&As in a cascading process.\nSignificantly, to maximize PathMMU's authority, we invite seven pathologists to\nscrutinize each question under strict standards in PathMMU's validation and\ntest sets, while simultaneously setting an expert-level performance benchmark\nfor PathMMU. We conduct extensive evaluations, including zero-shot assessments\nof 14 open-sourced and 4 closed-sourced LMMs and their robustness to image\ncorruption. We also fine-tune representative LMMs to assess their adaptability\nto PathMMU. The empirical findings indicate that advanced LMMs struggle with\nthe challenging PathMMU benchmark, with the top-performing LMM, GPT-4V,\nachieving only a 49.8% zero-shot performance, significantly lower than the\n71.8% demonstrated by human pathologists. After fine-tuning, significantly\nsmaller open-sourced LMMs can outperform GPT-4V but still fall short of the\nexpertise shown by pathologists. We hope that the PathMMU will offer valuable\ninsights and foster the development of more specialized, next-generation LMMs\nfor pathology.\n","authors":["Yuxuan Sun","Hao Wu","Chenglu Zhu","Sunyi Zheng","Qizi Chen","Kai Zhang","Yunlong Zhang","Dan Wan","Xiaoxiao Lan","Mengyue Zheng","Jingxiong Li","Xinheng Lyu","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16355v3.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.13756v1","updated":"2024-03-20T17:03:38Z","published":"2024-03-20T17:03:38Z","title":"Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge\n  Augmentation in Vision Language Model","summary":"  We present a knowledge augmentation strategy for assessing the diagnostic\ngroups and gait impairment from monocular gait videos. Based on a large-scale\npre-trained Vision Language Model (VLM), our model learns and improves visual,\ntextual, and numerical representations of patient gait videos, through a\ncollective learning across three distinct modalities: gait videos,\nclass-specific descriptions, and numerical gait parameters. Our specific\ncontributions are two-fold: First, we adopt a knowledge-aware prompt tuning\nstrategy to utilize the class-specific medical description in guiding the text\nprompt learning. Second, we integrate the paired gait parameters in the form of\nnumerical texts to enhance the numeracy of the textual representation. Results\ndemonstrate that our model not only significantly outperforms state-of-the-art\n(SOTA) in video-based classification tasks but also adeptly decodes the learned\nclass-specific text features into natural language descriptions using the\nvocabulary of quantitative gait parameters. The code and the model will be made\navailable at our project page.\n","authors":["Diwei Wang","Kun Yuan","Candice Muller","Frédéric Blanc","Nicolas Padoy","Hyewon Seo"],"pdf_url":"https://arxiv.org/pdf/2403.13756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13747v1","updated":"2024-03-20T16:54:55Z","published":"2024-03-20T16:54:55Z","title":"Leveraging High-Resolution Features for Improved Deep Hashing-based\n  Image Retrieval","summary":"  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n","authors":["Aymene Berriche","Mehdi Adjal Zakaria","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.13747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13745v1","updated":"2024-03-20T16:53:45Z","published":"2024-03-20T16:53:45Z","title":"Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific\n  Adaptation","summary":"  Video outpainting is a challenging task, aiming at generating video content\noutside the viewport of the input video while maintaining inter-frame and\nintra-frame consistency. Existing methods fall short in either generation\nquality or flexibility. We introduce MOTIA Mastering Video Outpainting Through\nInput-Specific Adaptation, a diffusion-based pipeline that leverages both the\nintrinsic data-specific patterns of the source video and the image/video\ngenerative prior for effective outpainting. MOTIA comprises two main phases:\ninput-specific adaptation and pattern-aware outpainting. The input-specific\nadaptation phase involves conducting efficient and effective pseudo outpainting\nlearning on the single-shot source video. This process encourages the model to\nidentify and learn patterns within the source video, as well as bridging the\ngap between standard generative processes and outpainting. The subsequent\nphase, pattern-aware outpainting, is dedicated to the generalization of these\nlearned patterns to generate outpainting outcomes. Additional strategies\nincluding spatial-aware insertion and noise travel are proposed to better\nleverage the diffusion model's generative prior and the acquired video patterns\nfrom source videos. Extensive evaluations underscore MOTIA's superiority,\noutperforming existing state-of-the-art methods in widely recognized\nbenchmarks. Notably, these advancements are achieved without necessitating\nextensive, task-specific tuning.\n","authors":["Fu-Yun Wang","Xiaoshi Wu","Zhaoyang Huang","Xiaoyu Shi","Dazhong Shen","Guanglu Song","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.13745v1.pdf","comment":"Code will be available at https://github.com/G-U-N/Be-Your-Outpainter"},{"id":"http://arxiv.org/abs/2302.05666v5","updated":"2024-03-20T16:50:25Z","published":"2023-02-11T11:56:06Z","title":"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels","summary":"  Intersection over Union (IoU) losses are surrogates that directly optimize\nthe Jaccard index. Leveraging IoU losses as part of the loss function have\ndemonstrated superior performance in semantic segmentation tasks compared to\noptimizing pixel-wise losses such as the cross-entropy loss alone. However, we\nidentify a lack of flexibility in these losses to support vital training\ntechniques like label smoothing, knowledge distillation, and semi-supervised\nlearning, mainly due to their inability to process soft labels. To address\nthis, we introduce Jaccard Metric Losses (JMLs), which are identical to the\nsoft Jaccard loss in standard settings with hard labels but are fully\ncompatible with soft labels. We apply JMLs to three prominent use cases of soft\nlabels: label smoothing, knowledge distillation and semi-supervised learning,\nand demonstrate their potential to enhance model accuracy and calibration. Our\nexperiments show consistent improvements over the cross-entropy loss across 4\nsemantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)\nand 13 architectures, including classic CNNs and recent vision transformers.\nRemarkably, our straightforward approach significantly outperforms\nstate-of-the-art knowledge distillation and semi-supervised learning methods.\nThe code is available at\n\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.\n","authors":["Zifu Wang","Xuefei Ning","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2302.05666v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.18561v2","updated":"2024-03-20T16:27:53Z","published":"2023-11-30T13:53:50Z","title":"Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and\n  Real-time Rendering","summary":"  Modeling dynamic, large-scale urban scenes is challenging due to their highly\nintricate geometric structures and unconstrained dynamics in both space and\ntime. Prior methods often employ high-level architectural priors, separating\nstatic and dynamic elements, resulting in suboptimal capture of their\nsynergistic interactions. To address this challenge, we present a unified\nrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds upon\nthe efficient 3D Gaussian splatting technique, originally designed for static\nscene representation, by introducing periodic vibration-based temporal\ndynamics. This innovation enables PVG to elegantly and uniformly represent the\ncharacteristics of various objects and elements in dynamic urban scenes. To\nenhance temporally coherent and large scene representation learning with sparse\ntraining data, we introduce a novel temporal smoothing mechanism and a\nposition-aware adaptive control strategy respectively. Extensive experiments on\nWaymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses\nstate-of-the-art alternatives in both reconstruction and novel view synthesis\nfor both dynamic and static scenes. Notably, PVG achieves this without relying\non manually labeled object bounding boxes or expensive optical flow estimation.\nMoreover, PVG exhibits 900-fold acceleration in rendering over the best\nalternative.\n","authors":["Yurui Chen","Chun Gu","Junzhe Jiang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.18561v2.pdf","comment":"Project page: https://fudan-zvg.github.io/PVG/"},{"id":"http://arxiv.org/abs/2310.13805v2","updated":"2024-03-20T16:23:20Z","published":"2023-10-20T20:32:43Z","title":"Normalizing flow-based deep variational Bayesian network for seismic\n  multi-hazards and impacts estimation from InSAR imagery","summary":"  Onsite disasters like earthquakes can trigger cascading hazards and impacts,\nsuch as landslides and infrastructure damage, leading to catastrophic losses;\nthus, rapid and accurate estimates are crucial for timely and effective\npost-disaster responses. Interferometric Synthetic aperture radar (InSAR) data\nis important in providing high-resolution onsite information for rapid hazard\nestimation. Most recent methods using InSAR imagery signals predict a single\ntype of hazard and thus often suffer low accuracy due to noisy and complex\nsignals induced by co-located hazards, impacts, and irrelevant environmental\nchanges (e.g., vegetation changes, human activities). We introduce a novel\nstochastic variational inference with normalizing flows derived to jointly\napproximate posteriors of multiple unobserved hazards and impacts from noisy\nInSAR imagery.\n","authors":["Xuechun Li","Paula M. Burgi","Wei Ma","Hae Young Noh","David J. Wald","Susu Xu"],"pdf_url":"https://arxiv.org/pdf/2310.13805v2.pdf","comment":"This paper needs to be reviewed by the USGS"},{"id":"http://arxiv.org/abs/2303.17783v5","updated":"2024-03-20T16:21:33Z","published":"2023-03-31T03:14:44Z","title":"Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with\n  Wavelet Augmentation Transformer","summary":"  Unsupervised Domain Adaptation (UDA) can effectively address domain gap\nissues in real-world image Super-Resolution (SR) by accessing both the source\nand target data. Considering privacy policies or transmission restrictions of\nsource data in practical scenarios, we propose a SOurce-free Domain Adaptation\nframework for image SR (SODA-SR) to address this issue, i.e., adapt a\nsource-trained model to a target domain with only unlabeled target data.\nSODA-SR leverages the source-trained model to generate refined pseudo-labels\nfor teacher-student learning. To better utilize pseudo-labels, we propose a\nnovel wavelet-based augmentation method, named Wavelet Augmentation Transformer\n(WAT), which can be flexibly incorporated with existing networks, to implicitly\nproduce useful augmented data. WAT learns low-frequency information of varying\nlevels across diverse samples, which is aggregated efficiently via deformable\nattention. Furthermore, an uncertainty-aware self-training mechanism is\nproposed to improve the accuracy of pseudo-labels, with inaccurate predictions\nbeing rectified by uncertainty estimation. To acquire better SR results and\navoid overfitting pseudo-labels, several regularization losses are proposed to\nconstrain target LR and SR images in the frequency domain. Experiments show\nthat without accessing source data, SODA-SR outperforms state-of-the-art UDA\nmethods in both synthetic$\\rightarrow$real and real$\\rightarrow$real adaptation\nsettings, and is not constrained by specific network architectures.\n","authors":["Yuang Ai","Xiaoqiang Zhou","Huaibo Huang","Lei Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17783v5.pdf","comment":"11 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.13714v1","updated":"2024-03-20T16:20:54Z","published":"2024-03-20T16:20:54Z","title":"DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with\n  Multiple Sensors for Large-Scale Localization and Mapping","summary":"  Visual simultaneous localization and mapping (VSLAM) has broad applications,\nwith state-of-the-art methods leveraging deep neural networks for better\nrobustness and applicability. However, there is a lack of research in fusing\nthese learning-based methods with multi-sensor information, which could be\nindispensable to push related applications to large-scale and complex\nscenarios. In this paper, we tightly integrate the trainable deep dense bundle\nadjustment (DBA) with multi-sensor information through a factor graph. In the\nframework, recurrent optical flow and DBA are performed among sequential\nimages. The Hessian information derived from DBA is fed into a generic factor\ngraph for multi-sensor fusion, which employs a sliding window and supports\nprobabilistic marginalization. A pipeline for visual-inertial integration is\nfirstly developed, which provides the minimum ability of metric-scale\nlocalization and mapping. Furthermore, other sensors (e.g., global navigation\nsatellite system) are integrated for driftless and geo-referencing\nfunctionality. Extensive tests are conducted on both public datasets and\nself-collected datasets. The results validate the superior localization\nperformance of our approach, which enables real-time dense mapping in\nlarge-scale environments. The code has been made open-source\n(https://github.com/GREAT-WHU/DBA-Fusion).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Xuanbin Wang","Shaoquan Feng","Yuxuan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.13714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09988v2","updated":"2024-03-20T16:19:49Z","published":"2023-12-15T18:01:47Z","title":"Towards Architecture-Agnostic Untrained Network Priors for Image\n  Reconstruction with Frequency Regularization","summary":"  Untrained networks inspired by deep image prior have shown promising\ncapabilities in recovering a high-quality image from noisy or partial\nmeasurements, without requiring training data. Their success has been widely\nattributed to the spectral bias acting as an implicit regularization induced by\nsuitable network architectures. However, applications of such network-based\npriors often entail superfluous architectural decisions, overfitting risks, and\nslow optimization, all of which hinder their practicality. In this work, we\npropose efficient, architecture-agnostic methods for a more direct frequency\ncontrol over the network priors: 1) constraining the bandwidth of the\nwhite-noise input, 2) controlling the bandwidth of the interpolation-based\nupsamplers, and 3) regularizing the Lipschitz constants of the layers. We show\nthat even with just one extra line of code, the overfitting issues in\nunderperforming architectures can be alleviated such that their performance\ngaps with the high-performing counterparts can be largely closed despite their\ndistinct configurations, mitigating the need for architecture tuning. This then\nmakes it possible to employ a more compact model to achieve similar or superior\nperformance to larger models with greater efficiency. Our regularized network\npriors compare favorably with current supervised and self-supervised methods on\nMRI reconstruction and image inpainting tasks, serving as a stronger zero-shot\nbaseline reconstructor. Our code will be made publicly available.\n","authors":["Yilin Liu","Yunkui Pang","Jiang Li","Yong Chen","Pew-Thian Yap"],"pdf_url":"https://arxiv.org/pdf/2312.09988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18649v2","updated":"2024-03-20T16:18:09Z","published":"2023-11-30T15:57:34Z","title":"Simple Semantic-Aided Few-Shot Learning","summary":"  Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nfew-shot learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on six benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks. Code is available at\nhttps://github.com/zhangdoudou123/SemFew.\n","authors":["Hai Zhang","Junzhe Xu","Shanlin Jiang","Zhenan He"],"pdf_url":"https://arxiv.org/pdf/2311.18649v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2309.07915v3","updated":"2024-03-20T16:17:02Z","published":"2023-09-14T17:59:17Z","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning","summary":"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Xiaojian Ma","Kaikai An","Liang Chen","Zixuan Liu","Sheng Wang","Wenjuan Han","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2309.07915v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2312.02918v2","updated":"2024-03-20T16:12:57Z","published":"2023-12-05T17:47:11Z","title":"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and\n  Fidelity for All-in-One Image Restoration","summary":"  Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks underscore the superiority of MPerceiver in terms of\nadaptiveness, generalizability and fidelity.\n","authors":["Yuang Ai","Huaibo Huang","Xiaoqiang Zhou","Jiexiang Wang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2312.02918v2.pdf","comment":"13 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2312.04539v2","updated":"2024-03-20T16:11:22Z","published":"2023-12-07T18:55:52Z","title":"Auto-Vocabulary Semantic Segmentation","summary":"  Open-ended image understanding tasks gained significant attention from the\nresearch community, particularly with the emergence of Vision-Language Models.\nOpen-Vocabulary Segmentation (OVS) methods are capable of performing semantic\nsegmentation without relying on a fixed vocabulary, and in some cases, they\noperate without the need for training or fine-tuning. However, OVS methods\ntypically require users to specify the vocabulary based on the task or dataset\nat hand. In this paper, we introduce \\textit{Auto-Vocabulary Semantic\nSegmentation (AVS)}, advancing open-ended image understanding by eliminating\nthe necessity to predefine object categories for segmentation. Our approach,\n\\ours, presents a framework that autonomously identifies relevant class names\nusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.\nGiven that open-ended object category predictions cannot be directly compared\nwith a fixed ground truth, we develop a Large Language Model-based\nAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automatically\ngenerated class names and their corresponding segments. Our method sets new\nbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapes\nfor AVS and showcases competitive performance to OVS methods that require\nspecified class names.\n","authors":["Osman Ülger","Maksymilian Kulicki","Yuki Asano","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.04539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03320v3","updated":"2024-03-20T16:10:27Z","published":"2023-09-06T19:01:58Z","title":"CoNeS: Conditional neural fields with shift modulation for\n  multi-sequence MRI translation","summary":"  Multi-sequence magnetic resonance imaging (MRI) has found wide applications\nin both modern clinical studies and deep learning research. However, in\nclinical practice, it frequently occurs that one or more of the MRI sequences\nare missing due to different image acquisition protocols or contrast agent\ncontraindications of patients, limiting the utilization of deep learning models\ntrained on multi-sequence data. One promising approach is to leverage\ngenerative models to synthesize the missing sequences, which can serve as a\nsurrogate acquisition. State-of-the-art methods tackling this problem are based\non convolutional neural networks (CNN) which usually suffer from spectral\nbiases, resulting in poor reconstruction of high-frequency fine details. In\nthis paper, we propose Conditional Neural fields with Shift modulation (CoNeS),\na model that takes voxel coordinates as input and learns a representation of\nthe target images for multi-sequence MRI translation. The proposed model uses a\nmulti-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel\nmapping. Hence, each target image is represented as a neural field that is\nconditioned on the source image via shift modulation with a learned latent\ncode. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular\nschwannoma patients showed that the proposed method outperformed\nstate-of-the-art methods for multi-sequence MRI translation both visually and\nquantitatively. Moreover, we conducted spectral analysis, showing that CoNeS\nwas able to overcome the spectral bias issue common in conventional CNN models.\nTo further evaluate the usage of synthesized images in clinical downstream\ntasks, we tested a segmentation network using the synthesized images at\ninference.\n","authors":["Yunjie Chen","Marius Staring","Olaf M. Neve","Stephan R. Romeijn","Erik F. Hensen","Berit M. Verbist","Jelmer M. Wolterink","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2309.03320v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:004"},{"id":"http://arxiv.org/abs/2403.13703v1","updated":"2024-03-20T16:07:04Z","published":"2024-03-20T16:07:04Z","title":"Fostc3net:A Lightweight YOLOv5 Based On the Network Structure\n  Optimization","summary":"  Transmission line detection technology is crucial for automatic monitoring\nand ensuring the safety of electrical facilities. The YOLOv5 series is\ncurrently one of the most advanced and widely used methods for object\ndetection. However, it faces inherent challenges, such as high computational\nload on devices and insufficient detection accuracy. To address these concerns,\nthis paper presents an enhanced lightweight YOLOv5 technique customized for\nmobile devices, specifically intended for identifying objects associated with\ntransmission lines. The C3Ghost module is integrated into the convolutional\nnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in the\nfeature channel fusion process and improve feature expression performance. In\naddition, a FasterNet module is introduced to replace the c3 module in the\nYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only\na portion of the input channels, improving feature extraction efficiency and\nreducing computational overhead. To address the imbalance between simple and\nchallenging samples in the dataset and the diversity of aspect ratios of\nbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate\nthe performance of the proposed approach, Experiments are conducted on a custom\ndataset of transmission line poles. The results show that the proposed model\nachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a\n26% decrease in model parameters compared to the existing YOLOv5.In the\nablation experiment, it was also discovered that while the Fastnet module and\nthe CSghost module improved the precision of the original YOLOv5 baseline\nmodel, they caused a decrease in the mAP@.5-.95 metric. However, the\nimprovement of the wIoUv3 loss function significantly mitigated the decline of\nthe mAP@.5-.95 metric.\n","authors":["Danqing Ma","Shaojie Li","Bo Dang","Hengyi Zang","Xinqi Dong"],"pdf_url":"https://arxiv.org/pdf/2403.13703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13698v1","updated":"2024-03-20T16:03:01Z","published":"2024-03-20T16:03:01Z","title":"Insight Into the Collocation of Multi-Source Satellite Imagery for\n  Multi-Scale Vessel Detection","summary":"  Ship detection from satellite imagery using Deep Learning (DL) is an\nindispensable solution for maritime surveillance. However, applying DL models\ntrained on one dataset to others having differences in spatial resolution and\nradiometric features requires many adjustments. To overcome this issue, this\npaper focused on the DL models trained on datasets that consist of different\noptical images and a combination of radar and optical data. When dealing with a\nlimited number of training images, the performance of DL models via this\napproach was satisfactory. They could improve 5-20% of average precision,\ndepending on the optical images tested. Likewise, DL models trained on the\ncombined optical and radar dataset could be applied to both optical and radar\nimages. Our experiments showed that the models trained on an optical dataset\ncould be used for radar images, while those trained on a radar dataset offered\nvery poor scores when applied to optical images.\n","authors":["Tran-Vu La","Minh-Tan Pham","Marco Chini"],"pdf_url":"https://arxiv.org/pdf/2403.13698v1.pdf","comment":"5 pages, accepted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.13690v1","updated":"2024-03-20T15:53:07Z","published":"2024-03-20T15:53:07Z","title":"MotorEase: Automated Detection of Motor Impairment Accessibility Issues\n  in Mobile App UIs","summary":"  Recent research has begun to examine the potential of automatically finding\nand fixing accessibility issues that manifest in software. However, while\nrecent work makes important progress, it has generally been skewed toward\nidentifying issues that affect users with certain disabilities, such as those\nwith visual or hearing impairments. However, there are other groups of users\nwith different types of disabilities that also need software tooling support to\nimprove their experience. As such, this paper aims to automatically identify\naccessibility issues that affect users with motor-impairments.\n  To move toward this goal, this paper introduces a novel approach, called\nMotorEase, capable of identifying accessibility issues in mobile app UIs that\nimpact motor-impaired users. Motor-impaired users often have limited ability to\ninteract with touch-based devices, and instead may make use of a switch or\nother assistive mechanism -- hence UIs must be designed to support both limited\ntouch gestures and the use of assistive devices. MotorEase adapts computer\nvision and text processing techniques to enable a semantic understanding of app\nUI screens, enabling the detection of violations related to four popular,\npreviously unexplored UI design guidelines that support motor-impaired users,\nincluding: (i) visual touch target size, (ii) expanding sections, (iii)\npersisting elements, and (iv) adjacent icon visual distance. We evaluate\nMotorEase on a newly derived benchmark, called MotorCheck, that contains 555\nmanually annotated examples of violations to the above accessibility\nguidelines, across 1599 screens collected from 70 applications via a mobile app\ntesting tool. Our experiments illustrate that MotorEase is able to identify\nviolations with an average accuracy of ~90%, and a false positive rate of less\nthan 9%, outperforming baseline techniques.\n","authors":["Arun Krishnavajjala","SM Hasan Mansur","Justin Jose","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2403.13690v1.pdf","comment":"Accepted to ICSE 2024 Research Track, 13 pages"},{"id":"http://arxiv.org/abs/2303.16296v4","updated":"2024-03-20T15:52:49Z","published":"2023-03-28T20:35:38Z","title":"Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels","summary":"  The soft Dice loss (SDL) has taken a pivotal role in numerous automated\nsegmentation pipelines in the medical imaging community. Over the last years,\nsome reasons behind its superior functioning have been uncovered and further\noptimizations have been explored. However, there is currently no implementation\nthat supports its direct utilization in scenarios involving soft labels. Hence,\na synergy between the use of SDL and research leveraging the use of soft\nlabels, also in the context of model calibration, is still missing. In this\nwork, we introduce Dice semimetric losses (DMLs), which (i) are by design\nidentical to SDL in a standard setting with hard labels, but (ii) can be\nemployed in settings with soft labels. Our experiments on the public QUBIQ,\nLiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels\n(e.g. averaging, label smoothing, and knowledge distillation) over hard labels\n(e.g. majority voting and random selection). As a result, we obtain superior\nDice scores and model calibration, which supports the wider adoption of DMLs in\npractice. The code is available at https://github.com/zifuwanggg/JDTLosses\n","authors":["Zifu Wang","Teodora Popordanoska","Jeroen Bertels","Robin Lemmens","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2303.16296v4.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2403.13684v1","updated":"2024-03-20T15:41:39Z","published":"2024-03-20T15:41:39Z","title":"SPTNet: An Efficient Alternative Framework for Generalized Category\n  Discovery with Spatial Prompt Tuning","summary":"  Generalized Category Discovery (GCD) aims to classify unlabelled images from\nboth `seen' and `unseen' classes by transferring knowledge from a set of\nlabelled `seen' class images. A key theme in existing GCD approaches is\nadapting large-scale pre-trained models for the GCD task. An alternate\nperspective, however, is to adapt the data representation itself for better\nalignment with the pre-trained model. As such, in this paper, we introduce a\ntwo-stage adaptation approach termed SPTNet, which iteratively optimizes model\nparameters (i.e., model-finetuning) and data parameters (i.e., prompt\nlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)\nwhich considers the spatial property of image data, enabling the method to\nbetter focus on object parts, which can transfer between seen and unseen\nclasses. We thoroughly evaluate our SPTNet on standard benchmarks and\ndemonstrate that our method outperforms existing GCD methods. Notably, we find\nour method achieves an average accuracy of 61.4% on the SSB, surpassing prior\nstate-of-the-art methods by approximately 10%. The improvement is particularly\nremarkable as our method yields extra parameters amounting to only 0.117% of\nthose in the backbone architecture. Project page:\nhttps://visual-ai.github.io/sptnet.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2403.13684v1.pdf","comment":"Accepted as a conference paper at ICLR 2024; Project page:\n  https://visual-ai.github.io/sptnet"},{"id":"http://arxiv.org/abs/2403.13683v1","updated":"2024-03-20T15:41:32Z","published":"2024-03-20T15:41:32Z","title":"DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses","summary":"  Determining the relative pose of an object between two images is pivotal to\nthe success of generalizable object pose estimation. Existing approaches\ntypically approximate the continuous pose representation with a large number of\ndiscrete pose hypotheses, which incurs a computationally expensive process of\nscoring each hypothesis at test time. By contrast, we present a Deep Voxel\nMatching Network (DVMNet) that eliminates the need for pose hypotheses and\ncomputes the relative object pose in a single pass. To this end, we map the two\ninput RGB images, reference and query, to their respective voxelized 3D\nrepresentations. We then pass the resulting voxels through a pose estimation\nmodule, where the voxels are aligned and the pose is computed in an end-to-end\nfashion by solving a least-squares problem. To enhance robustness, we introduce\na weighted closest voxel algorithm capable of mitigating the impact of noisy\nvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse\ndatasets, demonstrating that our method delivers more accurate relative pose\nestimates for novel objects at a lower computational cost compared to\nstate-of-the-art methods. Our code is released at:\nhttps://github.com/sailor-z/DVMNet/.\n","authors":["Chen Zhao","Tong Zhang","Zheng Dang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.13683v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13680v1","updated":"2024-03-20T15:38:53Z","published":"2024-03-20T15:38:53Z","title":"Step-Calibrated Diffusion for Biomedical Optical Image Restoration","summary":"  High-quality, high-resolution medical imaging is essential for clinical care.\nRaman-based biomedical optical imaging uses non-ionizing infrared radiation to\nevaluate human tissues in real time and is used for early cancer detection,\nbrain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,\noptical imaging is vulnerable to image degradation due to laser scattering and\nabsorption, which can result in diagnostic errors and misguided treatment.\nRestoration of optical images is a challenging computer vision task because the\nsources of image degradation are multi-factorial, stochastic, and\ntissue-dependent, preventing a straightforward method to obtain paired\nlow-quality/high-quality data. Here, we present Restorative Step-Calibrated\nDiffusion (RSCD), an unpaired image restoration method that views the image\nrestoration problem as completing the finishing steps of a diffusion-based\nimage generation task. RSCD uses a step calibrator model to dynamically\ndetermine the severity of image degradation and the number of steps required to\ncomplete the reverse diffusion process for image restoration. RSCD outperforms\nother widely used unpaired image restoration methods on both image quality and\nperceptual evaluation metrics for restoring optical images. Medical imaging\nexperts consistently prefer images restored using RSCD in blinded comparison\nexperiments and report minimal to no hallucinations. Finally, we show that RSCD\nimproves performance on downstream clinical imaging tasks, including automated\nbrain tumor diagnosis and deep tissue imaging. Our code is available at\nhttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.\n","authors":["Yiwei Lyu","Sung Jik Cha","Cheng Jiang","Asadur Chowdury","Xinhai Hou","Edward Harake","Akhil Kondepudi","Christian Freudiger","Honglak Lee","Todd C. Hollon"],"pdf_url":"https://arxiv.org/pdf/2403.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13678v1","updated":"2024-03-20T15:37:19Z","published":"2024-03-20T15:37:19Z","title":"AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and\n  GPT-2 in Wild Audiovisual Contexts","summary":"  Leveraging the synergy of both audio data and visual data is essential for\nunderstanding human emotions and behaviors, especially in in-the-wild setting.\nTraditional methods for integrating such multimodal information often stumble,\nleading to less-than-ideal outcomes in the task of facial action unit\ndetection. To overcome these shortcomings, we propose a novel approach\nutilizing audio-visual multimodal data. This method enhances audio feature\nextraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel\nspectrogram features alongside a pre-trained VGGish network. Moreover, this\npaper adaptively captures fusion features across modalities by modeling the\ntemporal relationships, and ultilizes a pre-trained GPT-2 model for\nsophisticated context-aware fusion of multimodal information. Our method\nnotably improves the accuracy of AU detection by understanding the temporal and\ncontextual nuances of the data, showcasing significant advancements in the\ncomprehension of intricate scenarios. These findings underscore the potential\nof integrating temporal dynamics and contextual interpretation, paving the way\nfor future research endeavors.\n","authors":["Jun Yu","Zerui Zhang","Zhihong Wei","Gongpeng Zhao","Zhongpeng Cai","Yongqi Wang","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13677v1","updated":"2024-03-20T15:35:36Z","published":"2024-03-20T15:35:36Z","title":"Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into\n  Vision Transformers","summary":"  Humans see low and high spatial frequency components at the same time, and\ncombine the information from both to form a visual scene. Drawing on this\nneuroscientific inspiration, we propose an altered Vision Transformer\narchitecture where patches from scaled down versions of the input image are\nadded to the input of the first Transformer Encoder layer. We name this model\nRetina Vision Transformer (RetinaViT) due to its inspiration from the human\nvisual system. Our experiments show that when trained on the ImageNet-1K\ndataset with a moderate configuration, RetinaViT achieves a 3.3% performance\nimprovement over the original ViT. We hypothesize that this improvement can be\nattributed to the inclusion of low spatial frequency components in the input,\nwhich improves the ability to capture structural features, and to select and\nforward important features to deeper layers. RetinaViT thereby opens doors to\nfurther investigations into vertical pathways and attention patterns.\n","authors":["Yuyang Shu","Michael E. Bain"],"pdf_url":"https://arxiv.org/pdf/2403.13677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06258v2","updated":"2024-03-20T15:31:28Z","published":"2024-03-10T16:56:44Z","title":"Poly Kernel Inception Network for Remote Sensing Detection","summary":"  Object detection in remote sensing images (RSIs) often suffers from several\nincreasing challenges, including the large variation in object scales and the\ndiverse-ranging context. Prior methods tried to address these challenges by\nexpanding the spatial receptive field of the backbone, either through\nlarge-kernel convolution or dilated convolution. However, the former typically\nintroduces considerable background noise, while the latter risks generating\noverly sparse feature representations. In this paper, we introduce the Poly\nKernel Inception Network (PKINet) to handle the above challenges. PKINet\nemploys multi-scale convolution kernels without dilation to extract object\nfeatures of varying scales and capture local context. In addition, a Context\nAnchor Attention (CAA) module is introduced in parallel to capture long-range\ncontextual information. These two components work jointly to advance the\nperformance of PKINet on four challenging remote sensing detection benchmarks,\nnamely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.\n","authors":["Xinhao Cai","Qiuxia Lai","Yuwei Wang","Wenguan Wang","Zeren Sun","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2403.06258v2.pdf","comment":"accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition, 2024"},{"id":"http://arxiv.org/abs/2308.03001v2","updated":"2024-03-20T15:29:56Z","published":"2023-08-06T03:28:08Z","title":"Weakly supervised segmentation of intracranial aneurysms using a novel\n  3D focal modulation UNet","summary":"  Accurate identification and quantification of unruptured intracranial\naneurysms (UIAs) is crucial for the risk assessment and treatment of this\ncerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonance\nangiography (MRA) is suboptimal and time-consuming. In addition, one major\nissue in medical image segmentation is the need for large well-annotated data,\nwhich can be expensive to obtain. Techniques that mitigate this requirement,\nsuch as weakly supervised learning with coarse labels are highly desirable. In\nthe paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detect\nan aneurysm and offer an initial, coarse segmentation of it from time-of-flight\nMRA image patches, which is further refined with a dense conditional random\nfield (CRF) post-processing layer to produce a final segmentation map. We\ntrained and evaluated our model on a public dataset, and in terms of UIA\ndetection, our model showed a low false-positive rate of 0.21 and a high\nsensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dice\nscore of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating its\nstrong performance. We evaluated our algorithms against the state-of-the-art 3D\nResidual-UNet and Swin-UNETR, and illustrated the superior performance of our\nproposed FocalSegNet, highlighting the advantages of employing focal modulation\nfor this task.\n","authors":["Amirhossein Rasoulian","Arash Harirpoush","Soorena Salari","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2308.03001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13667v1","updated":"2024-03-20T15:24:57Z","published":"2024-03-20T15:24:57Z","title":"DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance","summary":"  Choreographers determine what the dances look like, while cameramen determine\nthe final presentation of dances. Recently, various methods and datasets have\nshowcased the feasibility of dance synthesis. However, camera movement\nsynthesis with music and dance remains an unsolved challenging problem due to\nthe scarcity of paired data. Thus, we present DCM, a new multi-modal 3D\ndataset, which for the first time combines camera movement with dance motion\nand music audio. This dataset encompasses 108 dance sequences (3.2 hours) of\npaired dance-camera-music data from the anime community, covering 4 music\ngenres. With this dataset, we uncover that dance camera movement is\nmultifaceted and human-centric, and possesses multiple influencing factors,\nmaking dance camera synthesis a more challenging task compared to camera or\ndance synthesis alone. To overcome these difficulties, we propose\nDanceCamera3D, a transformer-based diffusion model that incorporates a novel\nbody attention loss and a condition separation strategy. For evaluation, we\ndevise new metrics measuring camera movement quality, diversity, and dancer\nfidelity. Utilizing these metrics, we conduct extensive experiments on our DCM\ndataset, providing both quantitative and qualitative evidence showcasing the\neffectiveness of our DanceCamera3D model. Code and video demos are available at\nhttps://github.com/Carmenw1203/DanceCamera3D-Official.\n","authors":["Zixuan Wang","Jia Jia","Shikun Sun","Haozhe Wu","Rong Han","Zhenyu Li","Di Tang","Jiaqing Zhou","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13667v1.pdf","comment":"Accept to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11868v2","updated":"2024-03-20T15:22:12Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13663v1","updated":"2024-03-20T15:14:22Z","published":"2024-03-20T15:14:22Z","title":"T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh\n  Generation from a Single Image","summary":"  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a\nsingle color image through coarse-to-fine mesh deformation. Although P2M is\ncapable of generating plausible global shapes, its Graph Convolution Network\n(GCN) often produces overly smooth results, causing the loss of fine-grained\ngeometry details. Moreover, P2M generates non-credible features for occluded\nregions and struggles with the domain gap from synthetic data to real-world\nimages, which is a common challenge for single-view 3D reconstruction methods.\nTo address these challenges, we propose a novel Transformer-boosted\narchitecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of\nP2M. Specifically, we use a global Transformer to control the holistic shape\nand a local Transformer to progressively refine the local geometry details with\ngraph-based point upsampling. To enhance real-world reconstruction, we present\nthe simple yet effective Linear Scale Search (LSS), which serves as prompt\ntuning during the input preprocessing. Our experiments on ShapeNet demonstrate\nstate-of-the-art performance, while results on real-world data show the\ngeneralization capability.\n","authors":["Shijie Zhang","Boyan Jiang","Keke He","Junwei Zhu","Ying Tai","Chengjie Wang","Yinda Zhang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.13663v1.pdf","comment":"Received by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13660v1","updated":"2024-03-20T15:08:57Z","published":"2024-03-20T15:08:57Z","title":"ProMamba: Prompt-Mamba for polyp segmentation","summary":"  Detecting polyps through colonoscopy is an important task in medical image\nsegmentation, which provides significant assistance and reference value for\nclinical surgery. However, accurate segmentation of polyps is a challenging\ntask due to two main reasons. Firstly, polyps exhibit various shapes and\ncolors. Secondly, the boundaries between polyps and their normal surroundings\nare often unclear. Additionally, significant differences between different\ndatasets lead to limited generalization capabilities of existing methods. To\naddress these issues, we propose a segmentation model based on Prompt-Mamba,\nwhich incorporates the latest Vision-Mamba and prompt technologies. Compared to\nprevious models trained on the same dataset, our model not only maintains high\nsegmentation accuracy on the validation part of the same dataset but also\ndemonstrates superior accuracy on unseen datasets, exhibiting excellent\ngeneralization capabilities. Notably, we are the first to apply the\nVision-Mamba architecture to polyp segmentation and the first to utilize prompt\ntechnology in a polyp segmentation model. Our model efficiently accomplishes\nsegmentation tasks, surpassing previous state-of-the-art methods by an average\nof 5% across six datasets. Furthermore, we have developed multiple versions of\nour model with scaled parameter counts, achieving better performance than\nprevious models even with fewer parameters. Our code and trained weights will\nbe released soon.\n","authors":["Jianhao Xie","Ruofan Liao","Ziang Zhang","Sida Yi","Yuesheng Zhu","Guibo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13660v1.pdf","comment":"10 pages, 2 figures,3 tabels"},{"id":"http://arxiv.org/abs/2403.13659v1","updated":"2024-03-20T15:08:43Z","published":"2024-03-20T15:08:43Z","title":"Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional\n  Emotion Recognition","summary":"  Multi-modal emotion recognition has recently gained a lot of attention since\nit can leverage diverse and complementary relationships over multiple\nmodalities, such as audio, visual, and text. Most state-of-the-art methods for\nmultimodal fusion rely on recurrent networks or conventional attention\nmechanisms that do not effectively leverage the complementary nature of the\nmodalities. In this paper, we focus on dimensional emotion recognition based on\nthe fusion of facial, vocal, and text modalities extracted from videos.\nSpecifically, we propose a recursive cross-modal attention (RCMA) to\neffectively capture the complementary relationships across the modalities in a\nrecursive fashion. The proposed model is able to effectively capture the\ninter-modal relationships by computing the cross-attention weights across the\nindividual modalities and the joint representation of the other two modalities.\nTo further improve the inter-modal relationships, the obtained attended\nfeatures of the individual modalities are again fed as input to the cross-modal\nattention to refine the feature representations of the individual modalities.\nIn addition to that, we have used Temporal convolution networks (TCNs) to\ncapture the temporal modeling (intra-modal relationships) of the individual\nmodalities. By deploying the TCNs as well cross-modal attention in a recursive\nfashion, we are able to effectively capture both intra- and inter-modal\nrelationships across the audio, visual, and text modalities. Experimental\nresults on validation-set videos from the AffWild2 dataset indicate that our\nproposed fusion model is able to achieve significant improvement over the\nbaseline for the sixth challenge of Affective Behavior Analysis in-the-Wild\n2024 (ABAW6) competition.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.13659v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.09068;\n  text overlap with arXiv:2203.14779 by other authors"},{"id":"http://arxiv.org/abs/2403.13658v1","updated":"2024-03-20T15:06:49Z","published":"2024-03-20T15:06:49Z","title":"Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics\n  Instability Detection","summary":"  Recent advancements in non-invasive detection of cardiac hemodynamic\ninstability (CHDI) primarily focus on applying machine learning techniques to a\nsingle data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite\ntheir potential, these approaches often fall short especially when the size of\nlabeled patient data is limited, a common challenge in the medical domain.\nFurthermore, only a few studies have explored multimodal methods to study CHDI,\nwhich mostly rely on costly modalities such as cardiac MRI and echocardiogram.\nIn response to these limitations, we propose a novel multimodal variational\nautoencoder ($\\text{CardioVAE}_\\text{X,G}$) to integrate low-cost chest X-ray\n(CXR) and electrocardiogram (ECG) modalities with pre-training on a large\nunlabeled dataset. Specifically, $\\text{CardioVAE}_\\text{X,G}$ introduces a\nnovel tri-stream pre-training strategy to learn both shared and\nmodality-specific features, thus enabling fine-tuning with both unimodal and\nmultimodal datasets. We pre-train $\\text{CardioVAE}_\\text{X,G}$ on a large,\nunlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then\nfine-tune the pre-trained model on a labeled dataset of $795$ subjects from the\nASPIRE registry. Comprehensive evaluations against existing methods show that\n$\\text{CardioVAE}_\\text{X,G}$ offers promising performance (AUROC $=0.79$ and\nAccuracy $=0.77$), representing a significant step forward in non-invasive\nprediction of CHDI. Our model also excels in producing fine interpretations of\npredictions directly associated with clinical features, thereby supporting\nclinical decision-making.\n","authors":["Mohammod N. I. Suvon","Prasun C. Tripathi","Wenrui Fan","Shuo Zhou","Xianyuan Liu","Samer Alabed","Venet Osmani","Andrew J. Swift","Chen Chen","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13653v1","updated":"2024-03-20T14:58:40Z","published":"2024-03-20T14:58:40Z","title":"Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction","summary":"  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n","authors":["Florian Strohm","Mihai Bâce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13652v1","updated":"2024-03-20T14:58:09Z","published":"2024-03-20T14:58:09Z","title":"ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer","summary":"  Deep learning models achieve high accuracy in segmentation tasks among\nothers, yet domain shift often degrades the models' performance, which can be\ncritical in real-world scenarios where no target images are available. This\npaper proposes a zero-shot domain adaptation method based on diffusion models,\ncalled ZoDi, which is two-fold by the design: zero-shot image transfer and\nmodel adaptation. First, we utilize an off-the-shelf diffusion model to\nsynthesize target-like images by transferring the domain of source images to\nthe target domain. In this we specifically try to maintain the layout and\ncontent by utilising layout-to-image diffusion models with stochastic\ninversion. Secondly, we train the model using both source images and\nsynthesized images with the original segmentation maps while maximizing the\nfeature similarity of images from the two domains to learn domain-robust\nrepresentations. Through experiments we show benefits of ZoDi in the task of\nimage segmentation over state-of-the-art methods. It is also more applicable\nthan existing CLIP-based methods because it assumes no specific backbone or\nmodels, and it enables to estimate the model's performance without target\nimages by inspecting generated images. Our implementation will be publicly\navailable.\n","authors":["Hiroki Azuma","Yusuke Matsui","Atsuto Maki"],"pdf_url":"https://arxiv.org/pdf/2403.13652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13647v1","updated":"2024-03-20T14:54:33Z","published":"2024-03-20T14:54:33Z","title":"Meta-Point Learning and Refining for Category-Agnostic Pose Estimation","summary":"  Category-agnostic pose estimation (CAPE) aims to predict keypoints for\narbitrary classes given a few support images annotated with keypoints. Existing\nmethods only rely on the features extracted at support keypoints to predict or\nrefine the keypoints on query image, but a few support feature vectors are\nlocal and inadequate for CAPE. Considering that human can quickly perceive\npotential keypoints of arbitrary objects, we propose a novel framework for CAPE\nbased on such potential keypoints (named as meta-points). Specifically, we\nmaintain learnable embeddings to capture inherent information of various\nkeypoints, which interact with image feature maps to produce meta-points\nwithout any support. The produced meta-points could serve as meaningful\npotential keypoints for CAPE. Due to the inevitable gap between inherency and\nannotation, we finally utilize the identities and details offered by support\nkeypoints to assign and refine meta-points to desired keypoints in query image.\nIn addition, we propose a progressive deformable point decoder and a slacked\nregression loss for better prediction and supervision. Our novel framework not\nonly reveals the inherency of keypoints but also outperforms existing methods\nof CAPE. Comprehensive experiments and in-depth studies on large-scale MP-100\ndataset demonstrate the effectiveness of our framework.\n","authors":["Junjie Chen","Jiebin Yan","Yuming Fang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2403.13647v1.pdf","comment":"Published in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.02075v2","updated":"2024-03-20T14:52:27Z","published":"2024-03-04T14:21:51Z","title":"DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with\n  Non-linear Prediction","summary":"  In Multiple Object Tracking, objects often exhibit non-linear motion of\nacceleration and deceleration, with irregular direction changes.\nTacking-by-detection (TBD) trackers with Kalman Filter motion prediction work\nwell in pedestrian-dominant scenarios but fall short in complex situations when\nmultiple objects perform non-linear and diverse motion simultaneously. To\ntackle the complex non-linear motion, we propose a real-time diffusion-based\nMOT approach named DiffMOT. Specifically, for the motion predictor component,\nwe propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It\nmodels the entire distribution of various motion presented by the data as a\nwhole. It also predicts an individual object's motion conditioning on an\nindividual's historical motion information. Furthermore, it optimizes the\ndiffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT\nis real-time at 22.7FPS, and also outperforms the state-of-the-art on\nDanceTrack and SportsMOT datasets with $62.3\\%$ and $76.2\\%$ in HOTA metrics,\nrespectively. To the best of our knowledge, DiffMOT is the first to introduce a\ndiffusion probabilistic model into the MOT to tackle non-linear motion\nprediction.\n","authors":["Weiyi Lv","Yuhang Huang","Ning Zhang","Ruei-Sung Lin","Mei Han","Dan Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.02075v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.13642v1","updated":"2024-03-20T14:49:52Z","published":"2024-03-20T14:49:52Z","title":"H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation","summary":"  In the field of medical image segmentation, variant models based on\nConvolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base\nmodules have been very widely developed and applied. However, CNNs are often\nlimited in their ability to deal with long sequences of information, while the\nlow sensitivity of ViTs to local feature information and the problem of\nsecondary computational complexity limit their development. Recently, the\nemergence of state-space models (SSMs), especially 2D-selective-scan (SS2D),\nhas had an impact on the longtime dominance of traditional CNNs and ViTs as the\nfoundational modules of visual neural networks. In this paper, we extend the\nadaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for\nmedical image segmentation. Among them, the proposed High-order\n2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant\ninformation during SS2D operations through higher-order interactions. In\naddition, the proposed Local-SS2D module improves the learning ability of local\nfeatures of SS2D at each order of interaction. We conducted comparison and\nablation experiments on three publicly available medical image datasets\n(ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the\nstrong competitiveness of H-vmunet in medical image segmentation tasks. The\ncode is available from https://github.com/wurenkai/H-vmunet .\n","authors":["Renkai Wu","Yinghao Liu","Pengchen Liang","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2403.13642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17128v3","updated":"2024-03-20T14:49:16Z","published":"2024-02-27T01:48:19Z","title":"OSCaR: Object State Captioning and State Change Representation","summary":"  The capability of intelligent models to extrapolate and comprehend changes in\nobject states is a crucial yet demanding aspect of AI research, particularly\nthrough the lens of human interaction in real-world settings. This task\ninvolves describing complex visual environments, identifying active objects,\nand interpreting their changes as conveyed through language. Traditional\nmethods, which isolate object captioning and state change detection, offer a\nlimited view of dynamic environments. Moreover, relying on a small set of\nsymbolic words to represent changes has restricted the expressiveness of the\nlanguage. To address these challenges, in this paper, we introduce the Object\nState Captioning and State Change Representation (OSCaR) dataset and benchmark.\nOSCaR consists of 14,084 annotated video segments with nearly 1,000 unique\nobjects from various egocentric video collections. It sets a new testbed for\nevaluating multimodal large language models (MLLMs). Our experiments\ndemonstrate that while MLLMs show some skill, they lack a full understanding of\nobject state changes. The benchmark includes a fine-tuned model that, despite\ninitial capabilities, requires significant improvements in accuracy and\ngeneralization ability for effective understanding of these changes. Our code\nand dataset are available at https://github.com/nguyennm1024/OSCaR.\n","authors":["Nguyen Nguyen","Jing Bi","Ali Vosoughi","Yapeng Tian","Pooyan Fazli","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17128v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2208.08270v3","updated":"2024-03-20T14:13:44Z","published":"2022-08-17T13:02:17Z","title":"On the Privacy Effect of Data Enhancement via the Lens of Memorization","summary":"  Machine learning poses severe privacy concerns as it has been shown that the\nlearned models can reveal sensitive information about their training data. Many\nworks have investigated the effect of widely adopted data augmentation and\nadversarial training techniques, termed data enhancement in the paper, on the\nprivacy leakage of machine learning models. Such privacy effects are often\nmeasured by membership inference attacks (MIAs), which aim to identify whether\na particular example belongs to the training set or not. We propose to\ninvestigate privacy from a new perspective called memorization. Through the\nlens of memorization, we find that previously deployed MIAs produce misleading\nresults as they are less likely to identify samples with higher privacy risks\nas members compared to samples with low privacy risks. To solve this problem,\nwe deploy a recent attack that can capture individual samples' memorization\ndegrees for evaluation. Through extensive experiments, we unveil several\nfindings about the connections between three essential properties of machine\nlearning models, including privacy, generalization gap, and adversarial\nrobustness. We demonstrate that the generalization gap and privacy leakage are\nless correlated than those of the previous results. Moreover, there is not\nnecessarily a trade-off between adversarial robustness and privacy as stronger\nadversarial robustness does not make the model more susceptible to privacy\nattacks.\n","authors":["Xiao Li","Qiongxiu Li","Zhanhao Hu","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2208.08270v3.pdf","comment":"Accepted by IEEE TIFS, 17 pages"},{"id":"http://arxiv.org/abs/2403.12425v2","updated":"2024-03-20T13:56:56Z","published":"2024-03-19T04:25:54Z","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation","summary":"  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n","authors":["Jun Yu","Gongpeng Zhao","Yongqi Wang","Zhihong Wei","Yang Zheng","Zerui Zhang","Zhongpeng Cai","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12425v2.pdf","comment":"8 pages,3 figures"},{"id":"http://arxiv.org/abs/2403.13600v1","updated":"2024-03-20T13:48:50Z","published":"2024-03-20T13:48:50Z","title":"VL-Mamba: Exploring State Space Models for Multimodal Learning","summary":"  Multimodal large language models (MLLMs) have attracted widespread interest\nand have rich applications. However, the inherent attention mechanism in its\nTransformer structure requires quadratic complexity and results in expensive\ncomputational overhead. Therefore, in this work, we propose VL-Mamba, a\nmultimodal large language model based on state space models, which have been\nshown to have great potential for long-sequence modeling with fast inference\nand linear scaling in sequence length. Specifically, we first replace the\ntransformer-based backbone language model such as LLama or Vicuna with the\npre-trained Mamba language model. Then, we empirically explore how to\neffectively apply the 2D vision selective scan mechanism for multimodal\nlearning and the combinations of different vision encoders and variants of\npretrained Mamba language models. The extensive experiments on diverse\nmultimodal benchmarks with competitive performance show the effectiveness of\nour proposed VL-Mamba and demonstrate the great potential of applying state\nspace models for multimodal learning tasks.\n","authors":["Yanyuan Qiao","Zheng Yu","Longteng Guo","Sihan Chen","Zijia Zhao","Mingzhen Sun","Qi Wu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13589v1","updated":"2024-03-20T13:37:29Z","published":"2024-03-20T13:37:29Z","title":"ReGround: Improving Textual and Spatial Grounding at No Cost","summary":"  When an image generation process is guided by both a text prompt and spatial\ncues, such as a set of bounding boxes, do these elements work in harmony, or\ndoes one dominate the other? Our analysis of a pretrained image diffusion model\nthat integrates gated self-attention into the U-Net reveals that spatial\ngrounding often outweighs textual grounding due to the sequential flow from\ngated self-attention to cross-attention. We demonstrate that such bias can be\nsignificantly mitigated without sacrificing accuracy in either grounding by\nsimply rewiring the network architecture, changing from sequential to parallel\nfor gated self-attention and cross-attention. This surprisingly simple yet\neffective solution does not require any fine-tuning of the network but\nsignificantly reduces the trade-off between the two groundings. Our experiments\ndemonstrate significant improvements from the original GLIGEN to the rewired\nversion in the trade-off between textual grounding and spatial grounding.\n","authors":["Yuseung Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.13589v1.pdf","comment":"Project page: https://re-ground.github.io/"},{"id":"http://arxiv.org/abs/2403.13575v1","updated":"2024-03-20T13:20:05Z","published":"2024-03-20T13:20:05Z","title":"Leveraging feature communication in federated learning for remote\n  sensing image classification","summary":"  In the realm of Federated Learning (FL) applied to remote sensing image\nclassification, this study introduces and assesses several innovative\ncommunication strategies. Our exploration includes feature-centric\ncommunication, pseudo-weight amalgamation, and a combined method utilizing both\nweights and features. Experiments conducted on two public scene classification\ndatasets unveil the effectiveness of these strategies, showcasing accelerated\nconvergence, heightened privacy, and reduced network information exchange. This\nresearch provides valuable insights into the implications of feature-centric\ncommunication in FL, offering potential applications tailored for remote\nsensing scenarios.\n","authors":["Anh-Kiet Duong","Hoàng-Ân Lê","Minh-Tan Pham"],"pdf_url":"https://arxiv.org/pdf/2403.13575v1.pdf","comment":"5 pages, to appear in IGARSS 2024"},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10261v2","updated":"2024-03-20T13:15:28Z","published":"2024-03-15T12:48:44Z","title":"Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face\n  Deepfake Detection","summary":"  The deepfake threats to society and cybersecurity have provoked significant\npublic apprehension, driving intensified efforts within the realm of deepfake\nvideo detection. Current video-level methods are mostly based on {3D CNNs}\nresulting in high computational demands, although have achieved good\nperformance. This paper introduces an elegantly simple yet effective strategy\nnamed Thumbnail Layout (TALL), which transforms a video clip into a pre-defined\nlayout to realize the preservation of spatial and temporal dependencies. This\ntransformation process involves sequentially masking frames at the same\npositions within each frame. These frames are then resized into sub-frames and\nreorganized into the predetermined layout, forming thumbnails. TALL is\nmodel-agnostic and has remarkable simplicity, necessitating only minimal code\nmodifications. Furthermore, we introduce a graph reasoning block (GRB) and\nsemantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB\nenhances interactions between different semantic regions to capture\nsemantic-level inconsistency clues. The semantic consistency loss imposes\nconsistency constraints on semantic features to improve model generalization\nability. Extensive experiments on intra-dataset, cross-dataset,\ndiffusion-generated image detection, and deepfake generation method recognition\nshow that TALL++ achieves results surpassing or comparable to the\nstate-of-the-art methods, demonstrating the effectiveness of our approaches for\nvarious deepfake detection problems. The code is available at\nhttps://github.com/rainy-xu/TALL4Deepfake.\n","authors":["Yuting Xu","Jian Liang","Lijun Sheng","Xiao-Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10261v2.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2302.09389v2","updated":"2024-03-20T13:11:19Z","published":"2023-02-18T17:45:11Z","title":"Vulnerability analysis of captcha using Deep learning","summary":"  Several websites improve their security and avoid dangerous Internet attacks\nby implementing CAPTCHAs (Completely Automated Public Turing test to tell\nComputers and Humans Apart), a type of verification to identify whether the\nend-user is human or a robot. The most prevalent type of CAPTCHA is text-based,\ndesigned to be easily recognized by humans while being unsolvable towards\nmachines or robots. However, as deep learning technology progresses,\ndevelopment of convolutional neural network (CNN) models that predict\ntext-based CAPTCHAs becomes easier. The purpose of this research is to\ninvestigate the flaws and vulnerabilities in the CAPTCHA generating systems in\norder to design more resilient CAPTCHAs. To achieve this, we created CapNet, a\nConvolutional Neural Network. The proposed platform can evaluate both numerical\nand alphanumerical CAPTCHAs\n","authors":["Jaskaran Singh Walia","Aryan Odugoudar"],"pdf_url":"https://arxiv.org/pdf/2302.09389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13570v1","updated":"2024-03-20T13:09:54Z","published":"2024-03-20T13:09:54Z","title":"Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer","summary":"  In this paper, we propose a novel learning approach for feed-forward one-shot\n4D head avatar synthesis. Different from existing methods that often learn from\nreconstructing monocular videos guided by 3DMM, we employ pseudo multi-view\nvideos to learn a 4D head synthesizer in a data-driven manner, avoiding\nreliance on inaccurate 3DMM reconstruction that could be detrimental to the\nsynthesis performance. The key idea is to first learn a 3D head synthesizer\nusing synthetic multi-view images to convert monocular real videos into\nmulti-view ones, and then utilize the pseudo multi-view videos to learn a 4D\nhead synthesizer via cross-view self-reenactment. By leveraging a simple vision\ntransformer backbone with motion-aware cross-attentions, our method exhibits\nsuperior performance compared to previous methods in terms of reconstruction\nfidelity, geometry consistency, and motion control accuracy. We hope our method\noffers novel insights into integrating 3D priors with 2D supervisions for\nimproved 4D head avatar creation.\n","authors":["Yu Deng","Duomin Wang","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13570v1.pdf","comment":"Project page: https://yudeng.github.io/Portrait4D-v2/"},{"id":"http://arxiv.org/abs/2312.02696v2","updated":"2024-03-20T12:58:14Z","published":"2023-12-05T11:55:47Z","title":"Analyzing and Improving the Training Dynamics of Diffusion Models","summary":"  Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.\n","authors":["Tero Karras","Miika Aittala","Jaakko Lehtinen","Janne Hellsten","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2312.02696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13556v1","updated":"2024-03-20T12:51:30Z","published":"2024-03-20T12:51:30Z","title":"Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments","summary":"  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available in the supplementary material.\n","authors":["Djamahl Etchegaray","Zi Huang","Tatsuya Harada","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13551v1","updated":"2024-03-20T12:40:32Z","published":"2024-03-20T12:40:32Z","title":"Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute\n  Editing","summary":"  Despite recent advancements in text-to-image diffusion models facilitating\nvarious image editing techniques, complex text prompts often lead to an\noversight of some requests due to a bottleneck in processing text information.\nTo tackle this challenge, we present Ground-A-Score, a simple yet powerful\nmodel-agnostic image editing method by incorporating grounding during score\ndistillation. This approach ensures a precise reflection of intricate prompt\nrequirements in the editing outcomes, taking into account the prior knowledge\nof the object locations within the image. Moreover, the selective application\nwith a new penalty coefficient and contrastive loss helps to precisely target\nediting areas while preserving the integrity of the objects in the source\nimage. Both qualitative assessments and quantitative analyses confirm that\nGround-A-Score successfully adheres to the intricate details of extended and\nmultifaceted prompts, ensuring high-quality outcomes that respect the original\nimage attributes.\n","authors":["Hangeol Chang","Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09008v2","updated":"2024-03-20T12:39:52Z","published":"2023-12-11T09:53:12Z","title":"Style Injection in Diffusion: A Training-free Approach for Adapting\n  Large-scale Diffusion Models for Style Transfer","summary":"  Despite the impressive generative capabilities of diffusion models, existing\ndiffusion model-based style transfer methods require inference-stage\noptimization (e.g. fine-tuning or textual inversion of style) which is\ntime-consuming, or fails to leverage the generative ability of large-scale\ndiffusion models. To address these issues, we introduce a novel artistic style\ntransfer method based on a pre-trained large-scale diffusion model without any\noptimization. Specifically, we manipulate the features of self-attention layers\nas the way the cross-attention mechanism works; in the generation process,\nsubstituting the key and value of content with those of style image. This\napproach provides several desirable characteristics for style transfer\nincluding 1) preservation of content by transferring similar styles into\nsimilar image patches and 2) transfer of style based on similarity of local\ntexture (e.g. edge) between content and style images. Furthermore, we introduce\nquery preservation and attention temperature scaling to mitigate the issue of\ndisruption of original content, and initial latent Adaptive Instance\nNormalization (AdaIN) to deal with the disharmonious color (failure to transfer\nthe colors of style). Our experimental results demonstrate that our proposed\nmethod surpasses state-of-the-art methods in both conventional and\ndiffusion-based style transfer baselines.\n","authors":["Jiwoo Chung","Sangeek Hyun","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2312.09008v2.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://jiwoogit.github.io/StyleID_site"},{"id":"http://arxiv.org/abs/2303.15263v4","updated":"2024-03-20T12:39:28Z","published":"2023-03-27T14:52:08Z","title":"Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning","summary":"  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To simplify the age prediction,\nwe create age groups for the age estimation. We make extensive evaluations and\ncomparisons of both convolution-based and transformer-based deep learning\narchitectures on a publicly available 11k hands dataset. Our experimental\nanalysis shows that it is possible to efficiently estimate not only identity\nbut also other attributes such as gender and age of suspects jointly from hand\nimages for criminal investigations, which is crucial in assisting international\npolice forces in the court to identify and convict abusers.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2303.15263v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.04821"},{"id":"http://arxiv.org/abs/2403.13548v1","updated":"2024-03-20T12:36:41Z","published":"2024-03-20T12:36:41Z","title":"Diversity-aware Channel Pruning for StyleGAN Compression","summary":"  StyleGAN has shown remarkable performance in unconditional image generation.\nHowever, its high computational cost poses a significant challenge for\npractical applications. Although recent efforts have been made to compress\nStyleGAN while preserving its performance, existing compressed models still lag\nbehind the original model, particularly in terms of sample diversity. To\novercome this, we propose a novel channel pruning method that leverages varying\nsensitivities of channels to latent vectors, which is a key factor in sample\ndiversity. Specifically, by assessing channel importance based on their\nsensitivities to latent vector perturbations, our method enhances the diversity\nof samples in the compressed model. Since our method solely focuses on the\nchannel pruning stage, it has complementary benefits with prior training\nschemes without additional training cost. Extensive experiments demonstrate\nthat our method significantly enhances sample diversity across various\ndatasets. Moreover, in terms of FID scores, our method not only surpasses\nstate-of-the-art by a large margin but also achieves comparable scores with\nonly half training iterations.\n","authors":["Jiwoo Chung","Sangeek Hyun","Sang-Heon Shim","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2403.13548v1.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://jiwoogit.github.io/DCP-GAN_site"},{"id":"http://arxiv.org/abs/2403.13545v1","updated":"2024-03-20T12:31:13Z","published":"2024-03-20T12:31:13Z","title":"Next day fire prediction via semantic segmentation","summary":"  In this paper we present a deep learning pipeline for next day fire\nprediction. The next day fire prediction task consists in learning models that\nreceive as input the available information for an area up until a certain day,\nin order to predict the occurrence of fire for the next day. Starting from our\nprevious problem formulation as a binary classification task on instances\n(daily snapshots of each area) represented by tabular feature vectors, we\nreformulate the problem as a semantic segmentation task on images; there, each\npixel corresponds to a daily snapshot of an area, while its channels represent\nthe formerly tabular training features. We demonstrate that this problem\nformulation, built within a thorough pipeline achieves state of the art\nresults.\n","authors":["Konstantinos Alexis","Stella Girtsou","Alexis Apostolakis","Giorgos Giannopoulos","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2403.13545v1.pdf","comment":"Accepted in MACLEAN@ECML/PKDD 2023"},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13535v1","updated":"2024-03-20T12:13:04Z","published":"2024-03-20T12:13:04Z","title":"IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models","summary":"  Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images.\n","authors":["Siying Cui","Jiankang Deng","Jia Guo","Xiang An","Yongle Zhao","Xinyu Wei","Ziyong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13535v1.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.09031v2","updated":"2024-03-20T12:00:59Z","published":"2023-12-14T15:31:33Z","title":"iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via\n  Comparing and Matching","summary":"  We present a method named iComMa to address the 6D camera pose estimation\nproblem in computer vision. Conventional pose estimation methods typically rely\non the target's CAD model or necessitate specific network training tailored to\nparticular object classes. Some existing methods have achieved promising\nresults in mesh-free object and scene pose estimation by inverting the Neural\nRadiance Fields (NeRF). However, they still struggle with adverse\ninitializations such as large rotations and translations. To address this\nissue, we propose an efficient method for accurate camera pose estimation by\ninverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based\ndifferentiable framework optimizes camera pose by minimizing the residual\nbetween the query image and the rendered image, requiring no training. An\nend-to-end matching module is designed to enhance the model's robustness\nagainst adverse initializations, while minimizing pixel-level comparing loss\naids in precise pose estimation. Experimental results on synthetic and complex\nreal-world data demonstrate the effectiveness of the proposed approach in\nchallenging conditions and the accuracy of camera pose estimation.\n","authors":["Yuan Sun","Xuan Wang","Yunfan Zhang","Jie Zhang","Caigui Jiang","Yu Guo","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11959v2","updated":"2024-03-20T11:58:23Z","published":"2024-03-18T16:56:47Z","title":"IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting","summary":"  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n","authors":["Hang Wang","Zhi-Qi Cheng","Youtian Du","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11959v2.pdf","comment":"Source code: https://github.com/hwang-cs-ime/IVAC-P2L"},{"id":"http://arxiv.org/abs/2403.13524v1","updated":"2024-03-20T11:51:04Z","published":"2024-03-20T11:51:04Z","title":"Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image","summary":"  3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.\n","authors":["Bowen Zhang","Tianyu Yang","Yu Li","Lei Zhang","Xi Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13522v1","updated":"2024-03-20T11:48:10Z","published":"2024-03-20T11:48:10Z","title":"REAL: Representation Enhanced Analytic Learning for Exemplar-free\n  Class-incremental Learning","summary":"  Exemplar-free class-incremental learning (EFCIL) aims to mitigate\ncatastrophic forgetting in class-incremental learning without available\nhistorical data. Compared with its counterpart (replay-based CIL) that stores\nhistorical samples, the EFCIL suffers more from forgetting issues under the\nexemplar-free constraint. In this paper, inspired by the recently developed\nanalytic learning (AL) based CIL, we propose a representation enhanced analytic\nlearning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining\n(DS-BPT) and a representation enhancing distillation (RED) process to enhance\nthe representation of the extractor. The DS-BPT pretrains model in streams of\nboth supervised learning and self-supervised contrastive learning (SSCL) for\nbase knowledge extraction. The RED process distills the supervised knowledge to\nthe SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that\nconverts the CIL to a recursive least-square problem. Our method addresses the\nissue of insufficient discriminability in representations of unseen data caused\nby a frozen backbone in the existing AL-based CIL. Empirical results on various\ndatasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that\nour REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or\neven more superior performance compared with the replay-based methods.\n","authors":["Run He","Huiping Zhuang","Di Fang","Yizhu Chen","Kai Tong","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13513v1","updated":"2024-03-20T11:27:20Z","published":"2024-03-20T11:27:20Z","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects\n  in Large Multimodal Models","summary":"  This paper presents a way of enhancing the reliability of Large Multimodal\nModels (LMMs) in addressing hallucination effects, where models generate\nincorrect or unrelated responses. Without additional instruction tuning\nparadigm, we introduce Counterfactual Inception, a novel method that implants\ncounterfactual thoughts into LMMs using carefully chosen, misaligned\ncounterfactual keywords. This method is grounded in the concept of\ncounterfactual thinking, a cognitive process where humans consider alternative\nrealities and outcomes. By applying this human-like reasoning mechanism to\nLMMs, we aim to reduce hallucination effects and improve the models'\ntrustworthiness. We also propose Dual-modality Verification Process (DVP), a\nrigorous framework for selecting optimal counterfactual keywords to trigger\ncounterfactual thinking into LMMs, concurrently considering visual and\nlinguistic context. Our extensive experiments across various LMMs, including\nboth open-source and proprietary models, corroborate that our method\nsignificantly mitigates hallucination phenomena across different datasets.\n","authors":["Junho Kim","Yeon Ju Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.13513v1.pdf","comment":"under review, code available:\n  https://github.com/IVY-LVLM/Counterfactual-Inception"},{"id":"http://arxiv.org/abs/2403.13512v1","updated":"2024-03-20T11:21:22Z","published":"2024-03-20T11:21:22Z","title":"Scale Decoupled Distillation","summary":"  Logit knowledge distillation attracts increasing attention due to its\npracticality in recent studies. However, it often suffers inferior performance\ncompared to the feature knowledge distillation. In this paper, we argue that\nexisting logit-based methods may be sub-optimal since they only leverage the\nglobal logit output that couples multiple semantic knowledge. This may transfer\nambiguous knowledge to the student and mislead its learning. To this end, we\npropose a simple but effective method, i.e., Scale Decoupled Distillation\n(SDD), for logit knowledge distillation. SDD decouples the global logit output\ninto multiple local logit outputs and establishes distillation pipelines for\nthem. This helps the student to mine and inherit fine-grained and unambiguous\nlogit knowledge. Moreover, the decoupled knowledge can be further divided into\nconsistent and complementary logit knowledge that transfers the semantic\ninformation and sample ambiguity, respectively. By increasing the weight of\ncomplementary parts, SDD can guide the student to focus more on ambiguous\nsamples, improving its discrimination ability. Extensive experiments on several\nbenchmark datasets demonstrate the effectiveness of SDD for wide\nteacher-student pairs, especially in the fine-grained classification task. Code\nis available at: https://github.com/shicaiwei123/SDD-CVPR2024\n","authors":["Shicai Wei Chunbo Luo Yang Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13512v1.pdf","comment":"Accepted to CVPR2024 10 pages 6figure"},{"id":"http://arxiv.org/abs/2403.13509v1","updated":"2024-03-20T11:12:57Z","published":"2024-03-20T11:12:57Z","title":"High-confidence pseudo-labels for domain adaptation in COVID-19\n  detection","summary":"  This paper outlines our submission for the 4th COV19D competition as part of\nthe `Domain adaptation, Explainability, Fairness in AI for Medical Image\nAnalysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition\nConference (CVPR). The competition consists of two challenges. The first is to\ntrain a classifier to detect the presence of COVID-19 from over one thousand CT\nscans from the COV19-CT-DB database. The second challenge is to perform domain\nadaptation by taking the dataset from Challenge 1 and adding a small number of\nscans (some annotated and other not) for a different distribution. We\npreprocessed the CT scans to segment the lungs, and output volumes with the\nlungs individually and together. We then trained 3D ResNet and Swin Transformer\nmodels on these inputs. We annotated the unlabeled CT scans using an ensemble\nof these models and chose the high-confidence predictions as pseudo-labels for\nfine-tuning. This resulted in a best cross-validation mean F1 score of 93.39\\%\nfor Challenge 1 and a mean F1 score of 92.15 for Challenge 2.\n","authors":["Robert Turnbull","Simon Mutch"],"pdf_url":"https://arxiv.org/pdf/2403.13509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13507v1","updated":"2024-03-20T11:05:07Z","published":"2024-03-20T11:05:07Z","title":"FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based\n  LLMs","summary":"  Despite the remarkable performance of video-based large language models\n(LLMs), their adversarial threat remains unexplored. To fill this gap, we\npropose the first adversarial attack tailored for video-based LLMs by crafting\nflow-based multi-modal adversarial perturbations on a small fraction of frames\nwithin a video, dubbed FMM-Attack. Extensive experiments show that our attack\ncan effectively induce video-based LLMs to generate incorrect answers when\nvideos are added with imperceptible adversarial perturbations. Intriguingly,\nour FMM-Attack can also induce garbling in the model output, prompting\nvideo-based LLMs to hallucinate. Overall, our observations inspire a further\nunderstanding of multi-modal robustness and safety-related feature alignment\nacross different modalities, which is of great importance for various large\nmulti-modal models. Our code is available at\nhttps://github.com/THU-Kingmin/FMM-Attack.\n","authors":["Jinmin Li","Kuofeng Gao","Yang Bai","Jingyun Zhang","Shu-tao Xia","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13501v1","updated":"2024-03-20T10:58:58Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v1.pdf","comment":"Project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2403.13499v1","updated":"2024-03-20T10:57:17Z","published":"2024-03-20T10:57:17Z","title":"Improved Baselines for Data-efficient Perceptual Augmentation of LLMs","summary":"  The abilities of large language models (LLMs) have recently progressed to\nunprecedented levels, paving the way to novel applications in a wide variety of\nareas. In computer vision, LLMs can be used to prime vision-language tasks such\nimage captioning and visual question answering when coupled with pre-trained\nvision backbones. While different approaches have been explored to interface\nLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,\nthey are often explored for different tasks, different datasets, and using\ndifferent perceptual backbones and language models, hindering direct comparison\nof the interfacing mechanisms. To remedy this lack of comparability between\nmethods, we present an extensive experimental evaluation of different\ninterfacing mechanisms, across multiple tasks (including image, video, and\naudio captioning as well as visual question answering), datasets and backbones,\npaying special attention to low-data settings. We find improved performance\nusing existing mechanisms over state-of-the-art results, and identify a new\ninterfacing mechanism that yields (near) optimal results across different\ntasks, while obtaining a 4x reduction in training time.\n","authors":["Théophane Vallaeys","Mustafa Shukor","Matthieu Cord","Jakob Verbeek"],"pdf_url":"https://arxiv.org/pdf/2403.13499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13480v1","updated":"2024-03-20T10:34:40Z","published":"2024-03-20T10:34:40Z","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels","summary":"  Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.\n","authors":["Haochen Han","Minnan Luo","Huan Liu","Fang Nan"],"pdf_url":"https://arxiv.org/pdf/2403.13480v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13479v1","updated":"2024-03-20T10:33:10Z","published":"2024-03-20T10:33:10Z","title":"Deepfake Detection without Deepfakes: Generalization via Synthetic\n  Frequency Patterns Injection","summary":"  Deepfake detectors are typically trained on large sets of pristine and\ngenerated images, resulting in limited generalization capacity; they excel at\nidentifying deepfakes created through methods encountered during training but\nstruggle with those generated by unknown techniques. This paper introduces a\nlearning approach aimed at significantly enhancing the generalization\ncapabilities of deepfake detectors. Our method takes inspiration from the\nunique \"fingerprints\" that image generation processes consistently introduce\ninto the frequency domain. These fingerprints manifest as structured and\ndistinctly recognizable frequency patterns. We propose to train detectors using\nonly pristine images injecting in part of them crafted frequency patterns,\nsimulating the effects of various deepfake generation techniques without being\nspecific to any. These synthetic patterns are based on generic shapes, grids,\nor auras. We evaluated our approach using diverse architectures across 25\ndifferent generation methods. The models trained with our approach were able to\nperform state-of-the-art deepfake detection, demonstrating also superior\ngeneralization capabilities in comparison with previous methods. Indeed, they\nare untied to any specific generation technique and can effectively identify\ndeepfakes regardless of how they were made.\n","authors":["Davide Alessandro Coccomini","Roberto Caldelli","Claudio Gennaro","Giuseppe Fiameni","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2403.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11427v2","updated":"2024-03-20T10:23:49Z","published":"2023-02-22T15:07:29Z","title":"Enhanced Face Authentication With Separate Loss Functions","summary":"  The overall objective of the main project is to propose and develop a system\nof facial authentication in unlocking phones or applications in phones using\nfacial recognition. The system will include four separate architectures: face\ndetection, face recognition, face spoofing, and classification of closed eyes.\nIn which, we consider the problem of face recognition to be the most important,\ndetermining the true identity of the person standing in front of the screen\nwith absolute accuracy is what facial recognition systems need to achieve.\nAlong with the development of the face recognition problem, the problem of the\nanti-fake face is also gradually becoming popular and equally important. Our\ngoal is to propose and develop two loss functions: LMCot and Double Loss. Then\napply them to the face authentication process.\n","authors":["Anh-Kiet Duong","Hoang-Lan Nguyen","Toan-Thinh Truong"],"pdf_url":"https://arxiv.org/pdf/2302.11427v2.pdf","comment":"in Vietnamese language"},{"id":"http://arxiv.org/abs/2403.13470v1","updated":"2024-03-20T10:19:05Z","published":"2024-03-20T10:19:05Z","title":"Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion","summary":"  Computer vision techniques play a central role in the perception stack of\nautonomous vehicles. Such methods are employed to perceive the vehicle\nsurroundings given sensor data. 3D LiDAR sensors are commonly used to collect\nsparse 3D point clouds from the scene. However, compared to human perception,\nsuch systems struggle to deduce the unseen parts of the scene given those\nsparse point clouds. In this matter, the scene completion task aims at\npredicting the gaps in the LiDAR measurements to achieve a more complete scene\nrepresentation. Given the promising results of recent diffusion models as\ngenerative models for images, we propose extending them to achieve scene\ncompletion from a single 3D LiDAR scan. Previous works used diffusion models\nover range images extracted from LiDAR data, directly applying image-based\ndiffusion methods. Distinctly, we propose to directly operate on the points,\nreformulating the noising and denoising diffusion process such that it can\nefficiently work at scene scale. Together with our approach, we propose a\nregularization loss to stabilize the noise predicted during the denoising\nprocess. Our experimental evaluation shows that our method can complete the\nscene given a single LiDAR scan as input, producing a scene with more details\ncompared to state-of-the-art scene completion methods. We believe that our\nproposed diffusion process formulation can support further research in\ndiffusion models applied to scene-scale point cloud data.\n","authors":["Lucas Nunes","Rodrigo Marcuzzi","Benedikt Mersch","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2403.13470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13469v1","updated":"2024-03-20T10:18:20Z","published":"2024-03-20T10:18:20Z","title":"Progressive trajectory matching for medical dataset distillation","summary":"  It is essential but challenging to share medical image datasets due to\nprivacy issues, which prohibit building foundation models and knowledge\ntransfer. In this paper, we propose a novel dataset distillation method to\ncondense the original medical image datasets into a synthetic one that\npreserves useful information for building an analysis model without accessing\nthe original datasets. Existing methods tackle only natural images by randomly\nmatching parts of the training trajectories of the model parameters trained by\nthe whole real datasets. However, through extensive experiments on medical\nimage datasets, the training process is extremely unstable and achieves\ninferior distillation results. To solve these barriers, we propose to design a\nnovel progressive trajectory matching strategy to improve the training\nstability for medical image dataset distillation. Additionally, it is observed\nthat improved stability prevents the synthetic dataset diversity and final\nperformance improvements. Therefore, we propose a dynamic overlap mitigation\nmodule that improves the synthetic dataset diversity by dynamically eliminating\nthe overlap across different images and retraining parts of the synthetic\nimages for better convergence. Finally, we propose a new medical image dataset\ndistillation benchmark of various modalities and configurations to promote fair\nevaluations. It is validated that our proposed method achieves 8.33%\nimprovement over previous state-of-the-art methods on average, and 11.7%\nimprovement when ipc=2 (i.e., image per class is 2). Codes and benchmarks will\nbe released.\n","authors":["Zhen Yu","Yang Liu","Qingchao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13467v1","updated":"2024-03-20T10:17:39Z","published":"2024-03-20T10:17:39Z","title":"CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language\n  Models","summary":"  This paper introduces CLIPSwarm, a new algorithm designed to automate the\nmodeling of swarm drone formations based on natural language. The algorithm\nbegins by enriching a provided word, to compose a text prompt that serves as\ninput to an iterative approach to find the formation that best matches the\nprovided word. The algorithm iteratively refines formations of robots to align\nwith the textual description, employing different steps for \"exploration\" and\n\"exploitation\". Our framework is currently evaluated on simple formation\ntargets, limited to contour shapes. A formation is visually represented through\nalpha-shape contours and the most representative color is automatically found\nfor the input word. To measure the similarity between the description and the\nvisual representation of the formation, we use CLIP [1], encoding text and\nimages into vectors and assessing their similarity. Subsequently, the algorithm\nrearranges the formation to visually represent the word more effectively,\nwithin the given constraints of available drones. Control actions are then\nassigned to the drones, ensuring robotic behavior and collision-free movement.\nExperimental results demonstrate the system's efficacy in accurately modeling\nrobot formations from natural language descriptions. The algorithm's\nversatility is showcased through the execution of drone shows in photorealistic\nsimulation with varying shapes. We refer the reader to the supplementary video\nfor a visual reference of the results.\n","authors":["Pablo Pueyo","Eduardo Montijano","Ana C. Murillo","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2403.13467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13466v1","updated":"2024-03-20T10:16:40Z","published":"2024-03-20T10:16:40Z","title":"An AI-Assisted Skincare Routine Recommendation System in XR","summary":"  In recent years, there has been an increasing interest in the use of\nartificial intelligence (AI) and extended reality (XR) in the beauty industry.\nIn this paper, we present an AI-assisted skin care recommendation system\nintegrated into an XR platform. The system uses a convolutional neural network\n(CNN) to analyse an individual's skin type and recommend personalised skin care\nproducts in an immersive and interactive manner. Our methodology involves\ncollecting data from individuals through a questionnaire and conducting skin\nanalysis using a provided facial image in an immersive environment. This data\nis then used to train the CNN model, which recognises the skin type and\nexisting issues and allows the recommendation engine to suggest personalised\nskin care products. We evaluate our system in terms of the accuracy of the CNN\nmodel, which achieves an average score of 93% in correctly classifying existing\nskin issues. Being integrated into an XR system, this approach has the\npotential to significantly enhance the beauty industry by providing immersive\nand engaging experiences to users, leading to more efficient and consistent\nskincare routines.\n","authors":["Gowravi Malalur Rajegowda","Yannis Spyridis","Barbara Villarini","Vasileios Argyriou"],"pdf_url":"https://arxiv.org/pdf/2403.13466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09380v2","updated":"2024-03-20T10:09:42Z","published":"2024-03-14T13:31:56Z","title":"Impact of Synthetic Images on Morphing Attack Detection Using a Siamese\n  Network","summary":"  This paper evaluated the impact of synthetic images on Morphing Attack\nDetection (MAD) using a Siamese network with a semi-hard-loss function. Intra\nand cross-dataset evaluations were performed to measure synthetic image\ngeneralisation capabilities using a cross-dataset for evaluation. Three\ndifferent pre-trained networks were used as feature extractors from traditional\nMobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained\non EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in\ncomparison with SOTA. Conversely, worse performances were reached when the\nsystem was trained only with synthetic images. A mixed approach (synthetic +\ndigital) database may help to improve MAD and reduce the error rate. This fact\nshows that we still need to keep going with our efforts to include synthetic\nimages in the training process.\n","authors":["Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2403.09380v2.pdf","comment":"Arxiv version of CIARP2023 - fixed typo errors"},{"id":"http://arxiv.org/abs/2311.13261v2","updated":"2024-03-20T10:06:09Z","published":"2023-11-22T09:25:08Z","title":"Immunohistochemistry guided segmentation of benign epithelial cells, in\n  situ lesions, and invasive epithelial cells in breast cancer slides","summary":"  Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation\n","authors":["Maren Høibø","André Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"pdf_url":"https://arxiv.org/pdf/2311.13261v2.pdf","comment":"19 pages, 6 figures. Submitted to a scientific journal"},{"id":"http://arxiv.org/abs/2403.06225v2","updated":"2024-03-20T10:05:02Z","published":"2024-03-10T14:11:25Z","title":"MoST: Motion Style Transformer between Diverse Action Contents","summary":"  While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with `part-attentive style modulator across body\nparts' and `Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.\n","authors":["Boeun Kim","Jungho Kim","Hyung Jin Chang","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06225v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2305.11488v2","updated":"2024-03-20T09:44:50Z","published":"2023-05-19T07:39:17Z","title":"AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning","summary":"  Continual learning aims to enable a model to incrementally learn knowledge\nfrom sequentially arrived data. Previous works adopt the conventional\nclassification architecture, which consists of a feature extractor and a\nclassifier. The feature extractor is shared across sequentially arrived tasks\nor classes, but one specific group of weights of the classifier corresponding\nto one new class should be incrementally expanded. Consequently, the parameters\nof a continual learner gradually increase. Moreover, as the classifier contains\nall historical arrived classes, a certain size of the memory is usually\nrequired to store rehearsal data to mitigate classifier bias and catastrophic\nforgetting. In this paper, we propose a non-incremental learner, named\nAttriCLIP, to incrementally extract knowledge of new classes or tasks.\nSpecifically, AttriCLIP is built upon the pre-trained visual-language model\nCLIP. Its image encoder and text encoder are fixed to extract features from\nboth images and text. Text consists of a category name and a fixed number of\nlearnable parameters which are selected from our designed attribute word bank\nand serve as attributes. As we compute the visual and textual similarity for\nclassification, AttriCLIP is a non-incremental learner. The attribute prompts,\nwhich encode the common knowledge useful for classification, can effectively\nmitigate the catastrophic forgetting and avoid constructing a replay memory. We\nevaluate our AttriCLIP and compare it with CLIP-based and previous\nstate-of-the-art continual learning methods in realistic settings with\ndomain-shift and long-sequence learning. The results show that our method\nperforms favorably against previous state-of-the-arts. The implementation code\ncan be available at https://github.com/bhrqw/AttriCLIP.\n","authors":["Runqi Wang","Xiaoyue Duan","Guoliang Kang","Jianzhuang Liu","Shaohui Lin","Songcen Xu","Jinhu Lv","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13447v1","updated":"2024-03-20T09:42:43Z","published":"2024-03-20T09:42:43Z","title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models","summary":"  Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.\n","authors":["Wenqiao Zhang","Tianwei Lin","Jiang Liu","Fangxun Shu","Haoyuan Li","Lei Zhang","He Wanggui","Hao Zhou","Zheqi Lv","Hao Jiang","Juncheng Li","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.13447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13444v1","updated":"2024-03-20T09:40:11Z","published":"2024-03-20T09:40:11Z","title":"MedCycle: Unpaired Medical Report Generation via Cycle-Consistency","summary":"  Generating medical reports for X-ray images presents a significant challenge,\nparticularly in unpaired scenarios where access to paired image-report data for\ntraining is unavailable. Previous works have typically learned a joint\nembedding space for images and reports, necessitating a specific labeling\nschema for both. We introduce an innovative approach that eliminates the need\nfor consistent labeling schemas, thereby enhancing data accessibility and\nenabling the use of incompatible datasets. This approach is based on\ncycle-consistent mapping functions that transform image embeddings into report\nembeddings, coupled with report auto-encoding for medical report generation.\nOur model and objectives consider intricate local details and the overarching\nsemantic context within images and reports. This approach facilitates the\nlearning of effective mapping functions, resulting in the generation of\ncoherent reports. It outperforms state-of-the-art results in unpaired chest\nX-ray report generation, demonstrating improvements in both language and\nclinical metrics.\n","authors":["Elad Hirsch","Gefen Dawidowicz","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2403.13444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13443v1","updated":"2024-03-20T09:39:39Z","published":"2024-03-20T09:39:39Z","title":"Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking","summary":"  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion\nstates of surrounding obstacles, essential for robotic perception. However,\ncurrent 3D trackers face issues with accuracy and latency consistency. In this\npaper, we propose Fast-Poly, a fast and effective filter-based method for 3D\nMOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object\nrotational anisotropy in 3D space, enhances local computation densification,\nand leverages parallelization technique, improving inference speed and\nprecision. Fast-Poly is extensively tested on two large-scale tracking\nbenchmarks with Python implementation. On the nuScenes dataset, Fast-Poly\nachieves new state-of-the-art performance with 75.8% AMOTA among all methods\nand can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly\nexhibits competitive accuracy with 63.6% MOTA and impressive inference speed\n(35.5 FPS). The source code is publicly available at\nhttps://github.com/lixiaoyu2000/FastPoly.\n","authors":["Xiaoyu Li","Dedong Liu","Lijun Zhao","Yitao Wu","Xian Wu","Jinghan Gao"],"pdf_url":"https://arxiv.org/pdf/2403.13443v1.pdf","comment":"1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS"},{"id":"http://arxiv.org/abs/2403.13439v1","updated":"2024-03-20T09:27:49Z","published":"2024-03-20T09:27:49Z","title":"Stochastic Geometry Models for Texture Synthesis of Machined Metallic\n  Surfaces: Sandblasting and Milling","summary":"  Training defect detection algorithms for visual surface inspection systems\nrequires a large and representative set of training data. Often there is not\nenough real data available which additionally cannot cover the variety of\npossible defects. Synthetic data generated by a synthetic visual surface\ninspection environment can overcome this problem. Therefore, a digital twin of\nthe object is needed, whose micro-scale surface topography is modeled by\ntexture synthesis models. We develop stochastic texture models for sandblasted\nand milled surfaces based on topography measurements of such surfaces. As the\nsurface patterns differ significantly, we use separate modeling approaches for\nthe two cases. Sandblasted surfaces are modeled by a combination of data-based\ntexture synthesis methods that rely entirely on the measurements. In contrast,\nthe model for milled surfaces is procedural and includes all process-related\nparameters known from the machine settings.\n","authors":["Natascha Jeziorski","Claudia Redenbach"],"pdf_url":"https://arxiv.org/pdf/2403.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13434v1","updated":"2024-03-20T09:22:22Z","published":"2024-03-20T09:22:22Z","title":"Advancing 6D Pose Estimation in Augmented Reality -- Overcoming\n  Projection Ambiguity with Uncontrolled Imagery","summary":"  This study addresses the challenge of accurate 6D pose estimation in\nAugmented Reality (AR), a critical component for seamlessly integrating virtual\nobjects into real-world environments. Our research primarily addresses the\ndifficulty of estimating 6D poses from uncontrolled RGB images, a common\nscenario in AR applications, which lacks metadata such as focal length. We\npropose a novel approach that strategically decomposes the estimation of z-axis\ntranslation and focal length, leveraging the neural-render and compare strategy\ninherent in the FocalPose architecture. This methodology not only streamlines\nthe 6D pose estimation process but also significantly enhances the accuracy of\n3D object overlaying in AR settings. Our experimental results demonstrate a\nmarked improvement in 6D pose estimation accuracy, with promising applications\nin manufacturing and robotics. Here, the precise overlay of AR visualizations\nand the advancement of robotic vision systems stand to benefit substantially\nfrom our findings.\n","authors":["Mayura Manawadu","Sieun Park","Soon-Yong Park"],"pdf_url":"https://arxiv.org/pdf/2403.13434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13430v1","updated":"2024-03-20T09:17:22Z","published":"2024-03-20T09:17:22Z","title":"MTP: Advancing Remote Sensing Foundation Model via Multi-Task\n  Pretraining","summary":"  Foundation models have reshaped the landscape of Remote Sensing (RS) by\nenhancing various image interpretation tasks. Pretraining is an active research\ntopic, encompassing supervised and self-supervised learning methods to\ninitialize model weights effectively. However, transferring the pretrained\nmodels to downstream tasks may encounter task discrepancy due to their\nformulation of pretraining as image classification or object discrimination\ntasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for\nRS foundation models to address this issue. Using a shared encoder and\ntask-specific decoder architecture, we conduct multi-task supervised\npretraining on the SAMRS dataset, encompassing semantic segmentation, instance\nsegmentation, and rotated object detection. MTP supports both convolutional\nneural networks and vision transformer foundation models with over 300 million\nparameters. The pretrained models are finetuned on various RS downstream tasks,\nsuch as scene classification, horizontal and rotated object detection, semantic\nsegmentation, and change detection. Extensive experiments across 14 datasets\ndemonstrate the superiority of our models over existing ones of similar size\nand their competitive performance compared to larger state-of-the-art models,\nthus validating the effectiveness of MTP.\n","authors":["Di Wang","Jing Zhang","Minqiang Xu","Lin Liu","Dongsheng Wang","Erzhong Gao","Chengxi Han","Haonan Guo","Bo Du","Dacheng Tao","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13430v1.pdf","comment":"The codes and pretrained models will be released at\n  https://github.com/ViTAE-Transformer/MTP"},{"id":"http://arxiv.org/abs/2312.04530v2","updated":"2024-03-20T09:12:21Z","published":"2023-12-07T18:50:01Z","title":"Camera Height Doesn't Change: Unsupervised Training for Metric Monocular\n  Road-Scene Depth Estimation","summary":"  In this paper, we introduce a novel training method for making any monocular\ndepth network learn absolute scale and estimate metric road-scene depth just\nfrom regular training data, i.e., driving videos. We refer to this training\nframework as StableCamH. The key idea is to leverage cars found on the road as\nsources of scale supervision but to incorporate them in the training robustly.\nStableCamH detects and estimates the sizes of cars in the frame and aggregates\nscale information extracted from them into a camera height estimate whose\nconsistency across the entire video sequence is enforced as scale supervision.\nThis realizes robust unsupervised training of any, otherwise scale-oblivious,\nmonocular depth network to become not only scale-aware but also metric-accurate\nwithout the need for auxiliary sensors and extra supervision. Extensive\nexperiments on the KITTI and Cityscapes datasets show the effectiveness of\nStableCamH and its state-of-the-art accuracy compared with related methods. We\nalso show that StableCamH enables training on mixed datasets of different\ncamera heights, which leads to larger-scale training and thus higher\ngeneralization. Metric depth reconstruction is essential in any road-scene\nvisual modeling, and StableCamH democratizes its deployment by establishing the\nmeans to train any model as a metric depth estimator.\n","authors":["Genki Kinoshita","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13417v1","updated":"2024-03-20T09:00:19Z","published":"2024-03-20T09:00:19Z","title":"Diversified and Personalized Multi-rater Medical Image Segmentation","summary":"  Annotation ambiguity due to inherent data uncertainties such as blurred\nboundaries in medical scans and different observer expertise and preferences\nhas become a major obstacle for training deep-learning based medical image\nsegmentation models. To address it, the common practice is to gather multiple\nannotations from different experts, leading to the setting of multi-rater\nmedical image segmentation. Existing works aim to either merge different\nannotations into the \"groundtruth\" that is often unattainable in numerous\nmedical contexts, or generate diverse results, or produce personalized results\ncorresponding to individual expert raters. Here, we bring up a more ambitious\ngoal for multi-rater medical image segmentation, i.e., obtaining both\ndiversified and personalized results. Specifically, we propose a two-stage\nframework named D-Persona (first Diversification and then Personalization). In\nStage I, we exploit multiple given annotations to train a Probabilistic U-Net\nmodel, with a bound-constrained loss to improve the prediction diversity. In\nthis way, a common latent space is constructed in Stage I, where different\nlatent codes denote diversified expert opinions. Then, in Stage II, we design\nmultiple attention-based projection heads to adaptively query the corresponding\nexpert prompts from the shared latent space, and then perform the personalized\nmedical image segmentation. We evaluated the proposed model on our in-house\nNasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e.,\nLIDC-IDRI). Extensive experiments demonstrated our D-Persona can provide\ndiversified and personalized results at the same time, achieving new SOTA\nperformance for multi-rater medical image segmentation. Our code will be\nreleased at https://github.com/ycwu1997/D-Persona.\n","authors":["Yicheng Wu","Xiangde Luo","Zhe Xu","Xiaoqing Guo","Lie Ju","Zongyuan Ge","Wenjun Liao","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2403.13417v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13412v1","updated":"2024-03-20T08:53:56Z","published":"2024-03-20T08:53:56Z","title":"Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment\n  and Pairwise Detection","summary":"  3D cell tracking in a living organism has a crucial role in live cell image\nanalysis. Cell tracking in C. elegans has two difficulties. First, cell\nmigration in a consecutive frame is large since they move their head during\nscanning. Second, cell detection is often inconsistent in consecutive frames\ndue to touching cells and low-contrast images, and these inconsistent\ndetections affect the tracking performance worse. In this paper, we propose a\ncell tracking method to address these issues, which has two main contributions.\nFirst, we introduce cell position heatmap-based non-rigid alignment with\ntest-time fine-tuning, which can warp the detected points to near the positions\nat the next frame. Second, we propose a pairwise detection method, which uses\nthe information of detection results at the previous frame for detecting cells\nat the current frame. The experimental results demonstrate the effectiveness of\neach module, and the proposed method achieved the best performance in\ncomparison.\n","authors":["Kaito Shiku","Hiromitsu Shirai","Takeshi Ishihara","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2403.13412v1.pdf","comment":"4 pages, 5 figures, Accepted in EMBC 2023"},{"id":"http://arxiv.org/abs/2403.13408v1","updated":"2024-03-20T08:50:15Z","published":"2024-03-20T08:50:15Z","title":"S2DM: Sector-Shaped Diffusion Models for Video Generation","summary":"  Diffusion models have achieved great success in image generation. However,\nwhen leveraging this idea for video generation, we face significant challenges\nin maintaining the consistency and continuity across video frames. This is\nmainly caused by the lack of an effective framework to align frames of videos\nwith desired temporal features while preserving consistent semantic and\nstochastic features. In this work, we propose a novel Sector-Shaped Diffusion\nModel (S2DM) whose sector-shaped diffusion region is formed by a set of\nray-shaped reverse diffusion processes starting at the same noise point. S2DM\ncan generate a group of intrinsically related data sharing the same semantic\nand stochastic features while varying on temporal features with appropriate\nguided conditions. We apply S2DM to video generation tasks, and explore the use\nof optical flow as temporal conditions. Our experimental results show that S2DM\noutperforms many existing methods in the task of video generation without any\ntemporal-feature modelling modules. For text-to-video generation tasks where\ntemporal conditions are not explicitly given, we propose a two-stage generation\nstrategy which can decouple the generation of temporal features from\nsemantic-content features. We show that, without additional training, our model\nintegrated with another temporal conditions generative model can still achieve\ncomparable performance with existing works. Our results can be viewd at\nhttps://s2dm.github.io/S2DM/.\n","authors":["Haoran Lang","Yuxuan Ge","Zheng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.13408v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13405v1","updated":"2024-03-20T08:47:51Z","published":"2024-03-20T08:47:51Z","title":"DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation","summary":"  Depth-based 3D hand pose estimation is an important but challenging research\ntask in human-machine interaction community. Recently, dense regression methods\nhave attracted increasing attention in 3D hand pose estimation task, which\nprovide a low computational burden and high accuracy regression way by densely\nregressing hand joint offset maps. However, large-scale regression offset\nvalues are often affected by noise and outliers, leading to a significant drop\nin accuracy. To tackle this, we re-formulate 3D hand pose estimation as a dense\nordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose\nNetwork (DOR3D-Net). Specifically, we first decompose offset value regression\ninto sub-tasks of binary classifications with ordinal constraints. Then, each\nbinary classifier can predict the probability of a binary spatial relationship\nrelative to joint, which is easier to train and yield much lower level of\nnoise. The estimated hand joint positions are inferred by aggregating the\nordinal regression results at local positions with a weighted sum. Furthermore,\nboth joint regression loss and ordinal regression loss are used to train our\nDOR3D-Net in an end-to-end manner. Extensive experiments on public datasets\n(ICVL, MSRA, NYU and HANDS2017) show that our design provides significant\nimprovements over SOTA methods.\n","authors":["Yamin Mao","Zhihua Liu","Weiming Li","SoonYong Cho","Qiang Wang","Xiaoshuai Hao"],"pdf_url":"https://arxiv.org/pdf/2403.13405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13395v1","updated":"2024-03-20T08:35:57Z","published":"2024-03-20T08:35:57Z","title":"Unifying Local and Global Multimodal Features for Place Recognition in\n  Aliased and Low-Texture Environments","summary":"  Perceptual aliasing and weak textures pose significant challenges to the task\nof place recognition, hindering the performance of Simultaneous Localization\nand Mapping (SLAM) systems. This paper presents a novel model, called UMF\n(standing for Unifying Local and Global Multimodal Features) that 1) leverages\nmulti-modality by cross-attention blocks between vision and LiDAR features, and\n2) includes a re-ranking stage that re-orders based on local feature matching\nthe top-k candidates retrieved using a global representation. Our experiments,\nparticularly on sequences captured on a planetary-analogous environment, show\nthat UMF outperforms significantly previous baselines in those challenging\naliased environments. Since our work aims to enhance the reliability of SLAM in\nall situations, we also explore its performance on the widely used RobotCar\ndataset, for broader applicability. Code and models are available at\nhttps://github.com/DLR-RM/UMF\n","authors":["Alberto García-Hernández","Riccardo Giubilato","Klaus H. Strobl","Javier Civera","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2403.13395v1.pdf","comment":"Accepted submission to International Conference on Robotics and\n  Automation (ICRA), 2024"},{"id":"http://arxiv.org/abs/2309.09947v2","updated":"2024-03-20T08:35:08Z","published":"2023-09-18T17:12:43Z","title":"End-to-end Learned Visual Odometry with Events and Frames","summary":"  Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. To improve robustness,\nrecent model-based VO systems have begun combining standard and event-based\ncameras. Event cameras excel in low-light and high-speed motion, while standard\ncameras provide dense and easier-to-track features, even in low-textured areas.\nHowever, the field of image- and event-based VO still predominantly relies on\nmodel-based methods and is yet to fully integrate recent image-only\nadvancements leveraging end-to-end learning-based architectures. Seamlessly\nintegrating the two modalities remains challenging due to their different\nnature, one asynchronous, the other not, limiting the potential for a more\neffective image- and event-based VO. We introduce RAMP-VO, the first end-to-end\nlearned image- and event-based VO system. It leverages novel Recurrent,\nAsynchronous, and Massively Parallel (RAMP) encoders capable of fusing\nasynchronous events with image data, providing 8x faster inference and 33% more\naccurate predictions than existing solutions. Despite being trained only in\nsimulation, RAMP-VO outperforms image- and event-based methods by 46% and 60%,\nrespectively, on traditional, real-world benchmarks as well as newly introduced\nApollo and Malapert landing sequences, paving the way for robust and\nasynchronous VO in space.\n","authors":["Roberto Pellerito","Marco Cannici","Daniel Gehrig","Joris Belhadj","Olivier Dubois-Matra","Massimo Casasco","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2309.09947v2.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.13392v1","updated":"2024-03-20T08:33:40Z","published":"2024-03-20T08:33:40Z","title":"Robust image segmentation model based on binary level set","summary":"  In order to improve the robustness of traditional image segmentation models\nto noise, this paper models the illumination term in intensity inhomogeneity\nimages. Additionally, to enhance the model's robustness to noisy images, we\nincorporate the binary level set model into the proposed model. Compared to the\ntraditional level set, the binary level set eliminates the need for continuous\nreinitialization. Moreover, by introducing the variational operator GL, our\nmodel demonstrates better capability in segmenting noisy images. Finally, we\nemploy the three-step splitting operator method for solving, and the\neffectiveness of the proposed model is demonstrated on various images.\n","authors":["Wenqi Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13392v1.pdf","comment":"SCI"},{"id":"http://arxiv.org/abs/2403.13378v1","updated":"2024-03-20T08:21:00Z","published":"2024-03-20T08:21:00Z","title":"IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis","summary":"  Semantic image synthesis aims to generate high-quality images given semantic\nconditions, i.e. segmentation masks and style reference images. Existing\nmethods widely adopt generative adversarial networks (GANs). GANs take all\nconditional inputs and directly synthesize images in a single forward step. In\nthis paper, semantic image synthesis is treated as an image denoising task and\nis handled with a novel image-to-image diffusion model (IIDM). Specifically,\nthe style reference is first contaminated with random noise and then\nprogressively denoised by IIDM, guided by segmentation masks. Moreover, three\ntechniques, refinement, color-transfer and model ensembles, are proposed to\nfurther boost the generation quality. They are plug-in inference modules and do\nnot require additional training. Extensive experiments show that our IIDM\noutperforms existing state-of-the-art methods by clear margins. Further\nanalysis is provided via detailed demonstrations. We have implemented IIDM\nbased on the Jittor framework; code is available at\nhttps://github.com/ader47/jittor-jieke-semantic_images_synthesis.\n","authors":["Feng Liu"," Xiaobin-Chang"],"pdf_url":"https://arxiv.org/pdf/2403.13378v1.pdf","comment":"6 pages, 7 figures, accetped by CVMJ 2024"},{"id":"http://arxiv.org/abs/2403.06866v3","updated":"2024-03-20T08:19:52Z","published":"2024-03-11T16:21:50Z","title":"QUASAR: QUality and Aesthetics Scoring with Advanced Representations","summary":"  This paper introduces a new data-driven, non-parametric method for image\nquality and aesthetics assessment, surpassing existing approaches and requiring\nno prompt engineering or fine-tuning. We eliminate the need for expressive\ntextual embeddings by proposing efficient image anchors in the data. Through\nextensive evaluations of 7 state-of-the-art self-supervised models, our method\ndemonstrates superior performance and robustness across various datasets and\nbenchmarks. Notably, it achieves high agreement with human assessments even\nwith limited data and shows high robustness to the nature of data and their\npre-processing pipeline. Our contributions offer a streamlined solution for\nassessment of images while providing insights into the perception of visual\ninformation.\n","authors":["Sergey Kastryulin","Denis Prokopenko","Artem Babenko","Dmitry V. Dylov"],"pdf_url":"https://arxiv.org/pdf/2403.06866v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13376v1","updated":"2024-03-20T08:15:34Z","published":"2024-03-20T08:15:34Z","title":"Correlation Clustering of Organoid Images","summary":"  In biological and medical research, scientists now routinely acquire\nmicroscopy images of hundreds of morphologically heterogeneous organoids and\nare then faced with the task of finding patterns in the image collection, i.e.,\nsubsets of organoids that appear similar and potentially represent the same\nmorphological class. We adopt models and algorithms for correlating organoid\nimages, i.e., for quantifying the similarity in appearance and geometry of the\norganoids they depict, and for clustering organoid images by consolidating\nconflicting correlations. For correlating organoid images, we adopt and compare\ntwo alternatives, a partial quadratic assignment problem and a twin network.\nFor clustering organoid images, we employ the correlation clustering problem.\nEmpirically, we learn the parameters of these models, infer a clustering of\norganoid images, and quantify the accuracy of the inferred clusters, with\nrespect to a training set and a test set we contribute of state-of-the-art\nlight microscopy images of organoids clustered manually by biologists.\n","authors":["Jannik Presberger","Rashmiparvathi Keshara","David Stein","Yung Hae Kim","Anne Grapin-Botton","Bjoern Andres"],"pdf_url":"https://arxiv.org/pdf/2403.13376v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2403.13375v1","updated":"2024-03-20T08:15:18Z","published":"2024-03-20T08:15:18Z","title":"Few-shot Oriented Object Detection with Memorable Contrastive Learning\n  in Remote Sensing Images","summary":"  Few-shot object detection (FSOD) has garnered significant research attention\nin the field of remote sensing due to its ability to reduce the dependency on\nlarge amounts of annotated data. However, two challenges persist in this area:\n(1) axis-aligned proposals, which can result in misalignment for arbitrarily\noriented objects, and (2) the scarcity of annotated data still limits the\nperformance for unseen object categories. To address these issues, we propose a\nnovel FSOD method for remote sensing images called Few-shot Oriented object\ndetection with Memorable Contrastive learning (FOMC). Specifically, we employ\noriented bounding boxes instead of traditional horizontal bounding boxes to\nlearn a better feature representation for arbitrary-oriented aerial objects,\nleading to enhanced detection performance. To the best of our knowledge, we are\nthe first to address oriented object detection in the few-shot setting for\nremote sensing images. To address the challenging issue of object\nmisclassification, we introduce a supervised contrastive learning module with a\ndynamically updated memory bank. This module enables the use of large batches\nof negative samples and enhances the model's capability to learn discriminative\nfeatures for unseen classes. We conduct comprehensive experiments on the DOTA\nand HRSC2016 datasets, and our model achieves state-of-the-art performance on\nthe few-shot oriented object detection task. Code and pretrained models will be\nreleased.\n","authors":["Jiawei Zhou","Wuzhou Li","Yi Cao","Hongtao Cai","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.13375v1.pdf","comment":"13 pages, 8 tables, 10 figures"},{"id":"http://arxiv.org/abs/2403.13370v1","updated":"2024-03-20T08:04:00Z","published":"2024-03-20T08:04:00Z","title":"Counting Network for Learning from Majority Label","summary":"  The paper proposes a novel problem in multi-class Multiple-Instance Learning\n(MIL) called Learning from the Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag's label. LML aims to classify\ninstances using bag-level majority classes. This problem is valuable in various\napplications. Existing MIL methods are unsuitable for LML due to aggregating\nconfidences, which may lead to inconsistency between the bag-level label and\nthe label obtained by counting the number of instances for each class. This may\nlead to incorrect instance-level classification. We propose a counting network\ntrained to produce the bag-level majority labels estimated by counting the\nnumber of instances for each class. This led to the consistency of the majority\nclass between the network outputs and one obtained by counting the number of\ninstances. Experimental results show that our counting network outperforms\nconventional MIL methods on four datasets The code is publicly available at\nhttps://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label.\n","authors":["Kaito Shiku","Shinnosuke Matsuo","Daiki Suehiro","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2403.13370v1.pdf","comment":"5 pages, 4 figures, Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.12483v2","updated":"2024-03-20T07:56:29Z","published":"2024-03-19T06:40:06Z","title":"A Hybrid Transformer-Sequencer approach for Age and Gender\n  classification from in-wild facial images","summary":"  The advancements in computer vision and image processing techniques have led\nto emergence of new application in the domain of visual surveillance, targeted\nadvertisement, content-based searching, and human-computer interaction etc. Out\nof the various techniques in computer vision, face analysis, in particular, has\ngained much attention. Several previous studies have tried to explore different\napplications of facial feature processing for a variety of tasks, including age\nand gender classification. However, despite several previous studies having\nexplored the problem, the age and gender classification of in-wild human faces\nis still far from the achieving the desired levels of accuracy required for\nreal-world applications. This paper, therefore, attempts to bridge this gap by\nproposing a hybrid model that combines self-attention and BiLSTM approaches for\nage and gender classification problems. The proposed models performance is\ncompared with several state-of-the-art model proposed so far. An improvement of\napproximately 10percent and 6percent over the state-of-the-art implementations\nfor age and gender classification, respectively, are noted for the proposed\nmodel. The proposed model is thus found to achieve superior performance and is\nfound to provide a more generalized learning. The model can, therefore, be\napplied as a core classification component in various image processing and\ncomputer vision problems.\n","authors":["Aakash Singh","Vivek Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2403.12483v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.13365v1","updated":"2024-03-20T07:48:32Z","published":"2024-03-20T07:48:32Z","title":"ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation\n  in Robotics","summary":"  Robotic manipulation in everyday scenarios, especially in unstructured\nenvironments, requires skills in pose-aware object manipulation (POM), which\nadapts robots' grasping and handling according to an object's 6D pose.\nRecognizing an object's position and orientation is crucial for effective\nmanipulation. For example, if a mug is lying on its side, it's more effective\nto grasp it by the rim rather than the handle. Despite its importance, research\nin POM skills remains limited, because learning manipulation skills requires\npose-varying simulation environments and datasets. This paper introduces\nManiPose, a pioneering benchmark designed to advance the study of pose-varying\nmanipulation tasks. ManiPose encompasses: 1) Simulation environments for POM\nfeature tasks ranging from 6D pose-specific pick-and-place of single objects to\ncluttered scenes, further including interactions with articulated objects. 2) A\ncomprehensive dataset featuring geometrically consistent and\nmanipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects\nand 100 articulated objects across 59 categories. 3) A baseline for POM,\nleveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the\nrelationship between 6D pose and task-specific requirements, offers enhanced\npose-aware grasp prediction and motion planning capabilities. Our benchmark\ndemonstrates notable advancements in pose estimation, pose-aware manipulation,\nand real-robot skill transfer, setting new standards for POM research. We will\nopen-source the ManiPose benchmark with the final version paper, inviting the\ncommunity to engage with our resources, available at our\nwebsite:https://sites.google.com/view/manipose.\n","authors":["Qiaojun Yu","Ce Hao","Junbo Wang","Wenhai Liu","Liu Liu","Yao Mu","Yang You","Hengxu Yan","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13365v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2312.07920v3","updated":"2024-03-20T07:36:27Z","published":"2023-12-13T06:30:51Z","title":"DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic\n  Autonomous Driving Scenes","summary":"  We present DrivingGaussian, an efficient and effective framework for\nsurrounding dynamic autonomous driving scenes. For complex scenes with moving\nobjects, we first sequentially and progressively model the static background of\nthe entire scene with incremental static 3D Gaussians. We then leverage a\ncomposite dynamic Gaussian graph to handle multiple moving objects,\nindividually reconstructing each object and restoring their accurate positions\nand occlusion relationships within the scene. We further use a LiDAR prior for\nGaussian Splatting to reconstruct scenes with greater details and maintain\npanoramic consistency. DrivingGaussian outperforms existing methods in dynamic\ndriving scene reconstruction and enables photorealistic surround-view synthesis\nwith high-fidelity and multi-camera consistency. Our project page is at:\nhttps://github.com/VDIGPKU/DrivingGaussian.\n","authors":["Xiaoyu Zhou","Zhiwei Lin","Xiaojun Shan","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13352v1","updated":"2024-03-20T07:31:07Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Haoran Feng","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13351v1","updated":"2024-03-20T07:25:24Z","published":"2024-03-20T07:25:24Z","title":"OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and\n  Pruning","summary":"  Redundancy is a persistent challenge in Capsule Networks (CapsNet),leading to\nhigh computational costs and parameter counts. Although previous works have\nintroduced pruning after the initial capsule layer, dynamic routing's fully\nconnected nature and non-orthogonal weight matrices reintroduce redundancy in\ndeeper layers. Besides, dynamic routing requires iterating to converge, further\nincreasing computational demands. In this paper, we propose an Orthogonal\nCapsule Network (OrthCaps) to reduce redundancy, improve routing performance\nand decrease parameter counts. Firstly, an efficient pruned capsule layer is\nintroduced to discard redundant capsules. Secondly, dynamic routing is replaced\nwith orthogonal sparse attention routing, eliminating the need for iterations\nand fully connected structures. Lastly, weight matrices during routing are\northogonalized to sustain low capsule similarity, which is the first approach\nto introduce orthogonality into CapsNet as far as we know. Our experiments on\nbaseline datasets affirm the efficiency and robustness of OrthCaps in\nclassification tasks, in which ablation studies validate the criticality of\neach component. Remarkably, OrthCaps-Shallow outperforms other Capsule Network\nbenchmarks on four datasets, utilizing only 110k parameters, which is a mere\n1.25% of a standard Capsule Network's total. To the best of our knowledge, it\nachieves the smallest parameter count among existing Capsule Networks.\nSimilarly, OrthCaps-Deep demonstrates competitive performance across four\ndatasets, utilizing only 1.2% of the parameters required by its counterparts.\n","authors":["Xinyu Geng","Jiaming Wang","Jiawei Gong","Yuerong Xue","Jun Xu","Fanglin Chen","Xiaolin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.13351v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.13349v1","updated":"2024-03-20T07:21:37Z","published":"2024-03-20T07:21:37Z","title":"Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified\n  Anomaly Detection","summary":"  Unified anomaly detection (AD) is one of the most challenges for anomaly\ndetection, where one unified model is trained with normal samples from multiple\nclasses with the objective to detect anomalies in these classes. For such a\nchallenging task, popular normalizing flow (NF) based AD methods may fall into\na \"homogeneous mapping\" issue,where the NF-based AD models are biased to\ngenerate similar latent representations for both normal and abnormal features,\nand thereby lead to a high missing rate of anomalies. In this paper, we propose\na novel Hierarchical Gaussian mixture normalizing flow modeling method for\naccomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists\nof two key components: inter-class Gaussian mixture modeling and intra-class\nmixed class centers learning. Compared to the previous NF-based AD methods, the\nhierarchical Gaussian mixture modeling approach can bring stronger\nrepresentation capability to the latent space of normalizing flows, so that\neven complex multi-class distribution can be well represented and learned in\nthe latent space. In this way, we can avoid mapping different class\ndistributions into the same single Gaussian prior, thus effectively avoiding or\nmitigating the \"homogeneous mapping\" issue. We further indicate that the more\ndistinguishable different class centers, the more conducive to avoiding the\nbias issue. Thus, we further propose a mutual information maximization loss for\nbetter structuring the latent feature space. We evaluate our method on four\nreal-world AD benchmarks, where we can significantly improve the previous\nNF-based AD methods and also outperform the SOTA unified AD methods.\n","authors":["Xincheng Yao","Ruoqi Li","Zefeng Qian","Lu Wang","Chongyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13349v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2307.10711v3","updated":"2024-03-20T07:17:19Z","published":"2023-07-20T09:06:21Z","title":"AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of\n  Diffusion Probabilistic Models","summary":"  Existing customization methods require access to multiple reference examples\nto align pre-trained diffusion probabilistic models (DPMs) with user-provided\nconcepts. This paper aims to address the challenge of DPM customization when\nthe only available supervision is a differentiable metric defined on the\ngenerated contents. Since the sampling procedure of DPMs involves recursive\ncalls to the denoising UNet, na\\\"ive gradient backpropagation requires storing\nthe intermediate states of all iterations, resulting in extremely high memory\nconsumption. To overcome this issue, we propose a novel method AdjointDPM,\nwhich first generates new samples from diffusion models by solving the\ncorresponding probability-flow ODEs. It then uses the adjoint sensitivity\nmethod to backpropagate the gradients of the loss to the models' parameters\n(including conditioning signals, network weights, and initial noises) by\nsolving another augmented ODE. To reduce numerical errors in both the forward\ngeneration and gradient backpropagation processes, we further reparameterize\nthe probability-flow ODE and augmented ODE as simple non-stiff ODEs using\nexponential integration. Finally, we demonstrate the effectiveness of\nAdjointDPM on three interesting tasks: converting visual effects into\nidentification text embeddings, finetuning DPMs for specific types of\nstylization, and optimizing initial noise to generate adversarial samples for\nsecurity auditing.\n","authors":["Jiachun Pan","Jun Hao Liew","Vincent Y. F. Tan","Jiashi Feng","Hanshu Yan"],"pdf_url":"https://arxiv.org/pdf/2307.10711v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13347v1","updated":"2024-03-20T07:15:22Z","published":"2024-03-20T07:15:22Z","title":"vid-TLDR: Training Free Token merging for Light-weight Video Transformer","summary":"  Video Transformers have become the prevalent solution for various video\ndownstream tasks with superior expressive power and flexibility. However, these\nvideo transformers suffer from heavy computational costs induced by the massive\nnumber of tokens across the entire video frames, which has been the major\nbarrier to training the model. Further, the patches irrelevant to the main\ncontents, e.g., backgrounds, degrade the generalization performance of models.\nTo tackle these issues, we propose training free token merging for lightweight\nvideo Transformer (vid-TLDR) that aims to enhance the efficiency of video\nTransformers by merging the background tokens without additional training. For\nvid-TLDR, we introduce a novel approach to capture the salient regions in\nvideos only with the attention map. Further, we introduce the saliency-aware\ntoken merging strategy by dropping the background tokens and sharpening the\nobject scores. Our experiments show that vid-TLDR significantly mitigates the\ncomputational complexity of video Transformers while achieving competitive\nperformance compared to the base model without vid-TLDR. Code is available at\nhttps://github.com/mlvlab/vid-TLDR.\n","authors":["Joonmyung Choi","Sanghyeok Lee","Jaewon Chu","Minhyuk Choi","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.13347v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.10099v2","updated":"2024-03-20T07:12:12Z","published":"2024-03-15T08:44:56Z","title":"KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and\n  Deformation","summary":"  In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and\nDeformation framework that takes object scans as input and jointly retrieves\nand deforms the most geometrically similar CAD models from a pre-processed\ndatabase to tightly match the target. Unlike existing dense matching based\nmethods that typically struggle with noisy partial scans, we propose to\nleverage category-consistent sparse keypoints to naturally handle both full and\npartial object scans. Specifically, we first employ a lightweight retrieval\nmodule to establish a keypoint-based embedding space, measuring the similarity\namong objects by dynamically aggregating deformation-aware local-global\nfeatures around extracted keypoints. Objects that are close in the embedding\nspace are considered similar in geometry. Then we introduce the neural\ncage-based deformation module that estimates the influence vector of each\nkeypoint upon cage vertices inside its local support region to control the\ndeformation of the retrieved shape. Extensive experiments on the synthetic\ndataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED\nsurpasses existing state-of-the-art approaches by a large margin. Codes and\ntrained models will be released in https://github.com/lolrudy/KP-RED.\n","authors":["Ruida Zhang","Chenyangguang Zhang","Yan Di","Fabian Manhardt","Xingyu Liu","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.10099v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.08359v2","updated":"2024-03-20T07:05:55Z","published":"2024-02-13T10:40:10Z","title":"Learning to Produce Semi-dense Correspondences for Visual Localization","summary":"  This study addresses the challenge of performing visual localization in\ndemanding conditions such as night-time scenarios, adverse weather, and\nseasonal changes. While many prior studies have focused on improving\nimage-matching performance to facilitate reliable dense keypoint matching\nbetween images, existing methods often heavily rely on predefined feature\npoints on a reconstructed 3D model. Consequently, they tend to overlook\nunobserved keypoints during the matching process. Therefore, dense keypoint\nmatches are not fully exploited, leading to a notable reduction in accuracy,\nparticularly in noisy scenes. To tackle this issue, we propose a novel\nlocalization method that extracts reliable semi-dense 2D-3D matching points\nbased on dense keypoint matches. This approach involves regressing semi-dense\n2D keypoints into 3D scene coordinates using a point inference network. The\nnetwork utilizes both geometric and visual cues to effectively infer 3D\ncoordinates for unobserved keypoints from the observed ones. The abundance of\nmatching information significantly enhances the accuracy of camera pose\nestimation, even in scenarios involving noisy or sparse 3D models.\nComprehensive evaluations demonstrate that the proposed method outperforms\nother methods in challenging scenes and achieves competitive results in\nlarge-scale visual localization benchmarks. The code will be available.\n","authors":["Khang Truong Giang","Soohwan Song","Sungho Jo"],"pdf_url":"https://arxiv.org/pdf/2402.08359v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.06731v3","updated":"2024-03-20T07:00:39Z","published":"2023-12-11T09:44:41Z","title":"Genixer: Empowering Multimodal Large Language Models as a Powerful Data\n  Generator","summary":"  Instruction tuning data is essential for training the Multimodal Large\nLanguage Models (MLLMs). However, the creation of high-quality instruction\ntuning data presents significant challenges. Prior methods that depended on\nGPT-4 for data generation were not only costly but also lacked satisfactory\nperformance in complex tasks (i.e., grounding-based reasoning tasks). To\naddress these issues, we developed an innovative data generation pipeline,\nGenixer, to generate various high-quality instruction tuning data, including\nnine representative tasks, e.g., Common VQA, REC, REG, and PointQ.\nSpecifically, Genixer provides a unified solution with four key steps for\nalleviating the difficulty of data generation: (i) instruction data collection,\n(ii) instruction template design, (iii) empowering MLLM, and (iv) data\ngeneration and filtering. Subsequently, the superior qualitative results of our\nGenixer demonstrate that current MLLMs have a strong potential to evolve into\npowerful data generators. Additionally, to validate the efficacy of generated\ndata quantitatively, we add the instruction tuning data produced by Genixer\ninto the training of two representative MLLMs and observe the consistent\nimprovements on various VQA tasks and multimodal benchmarks.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.06731v3.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.13343v1","updated":"2024-03-20T07:00:03Z","published":"2024-03-20T07:00:03Z","title":"TiBiX: Leveraging Temporal Information for Bidirectional X-ray and\n  Report Generation","summary":"  With the emergence of vision language models in the medical imaging domain,\nnumerous studies have focused on two dominant research activities: (1) report\ngeneration from Chest X-rays (CXR), and (2) synthetic scan generation from text\nor reports. Despite some research incorporating multi-view CXRs into the\ngenerative process, prior patient scans and reports have been generally\ndisregarded. This can inadvertently lead to the leaving out of important\nmedical information, thus affecting generation quality. To address this, we\npropose TiBiX: Leveraging Temporal information for Bidirectional X-ray and\nReport Generation. Considering previous scans, our approach facilitates\nbidirectional generation, primarily addressing two challenging problems: (1)\ngenerating the current image from the previous image and current report and (2)\ngenerating the current report based on both the previous and current images.\nMoreover, we extract and release a curated temporal benchmark dataset derived\nfrom the MIMIC-CXR dataset, which focuses on temporal data. Our comprehensive\nexperiments and ablation studies explore the merits of incorporating prior CXRs\nand achieve state-of-the-art (SOTA) results on the report generation task.\nFurthermore, we attain on-par performance with SOTA image generation efforts,\nthus serving as a new baseline in longitudinal bidirectional CXR-to-report\ngeneration. The code is available at https://github.com/BioMedIA-MBZUAI/TiBiX.\n","authors":["Santosh Sanjeev","Fadillah Adamsyah Maani","Arsen Abzhanov","Vijay Ram Papineni","Ibrahim Almakky","Bartłomiej W. Papież","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.13343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16244v3","updated":"2024-03-20T06:50:19Z","published":"2023-12-25T11:39:00Z","title":"Modality-missing RGBT Tracking: Invertible Prompt Learning and\n  High-quality Benchmarks","summary":"  Current RGBT tracking research relies on the complete multi-modal input, but\nmodal information might miss due to some factors such as thermal sensor\nself-calibration and data transmission error, called modality-missing challenge\nin this work. To address this challenge, we propose a novel invertible prompt\nlearning approach, which integrates the content-preserving prompts into a\nwell-trained tracking model to adapt to various modality-missing scenarios, for\nrobust RGBT tracking. Given one modality-missing scenario, we propose to\nutilize the available modality to generate the prompt of the missing modality\nto adapt to RGBT tracking model. However, the cross-modality gap between\navailable and missing modalities usually causes semantic distortion and\ninformation loss in prompt generation. To handle this issue, we design the\ninvertible prompter by incorporating the full reconstruction of the input\navailable modality from the generated prompt. To provide a comprehensive\nevaluation platform, we construct several high-quality benchmark datasets, in\nwhich various modality-missing scenarios are considered to simulate real-world\nchallenges. Extensive experiments on three modality-missing benchmark datasets\nshow that our method achieves significant performance improvements compared\nwith state-of-the-art methods. We have released the code and simulation\ndatasets at:\n\\href{https://github.com/Alexadlu/Modality-missing-RGBT-Tracking.git}{https://github.com/Alexadlu/Modality-missing-RGBT-Tracking.git}.\n","authors":["Andong Lu","Jiacong Zhao","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2312.16244v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13341v1","updated":"2024-03-20T06:48:48Z","published":"2024-03-20T06:48:48Z","title":"FissionFusion: Fast Geometric Generation and Hierarchical Souping for\n  Medical Image Analysis","summary":"  The scarcity of well-annotated medical datasets requires leveraging transfer\nlearning from broader datasets like ImageNet or pre-trained models like CLIP.\nModel soups averages multiple fine-tuned models aiming to improve performance\non In-Domain (ID) tasks and enhance robustness against Out-of-Distribution\n(OOD) datasets. However, applying these methods to the medical imaging domain\nfaces challenges and results in suboptimal performance. This is primarily due\nto differences in error surface characteristics that stem from data\ncomplexities such as heterogeneity, domain shift, class imbalance, and\ndistributional shifts between training and testing phases. To address this\nissue, we propose a hierarchical merging approach that involves local and\nglobal aggregation of models at various levels based on models' hyperparameter\nconfigurations. Furthermore, to alleviate the need for training a large number\nof models in the hyperparameter search, we introduce a computationally\nefficient method using a cyclical learning rate scheduler to produce multiple\nmodels for aggregation in the weight space. Our method demonstrates significant\nimprovements over the model souping approach across multiple datasets (around\n6% gain in HAM10000 and CheXpert datasets) while maintaining low computational\ncosts for model generation and selection. Moreover, we achieve better results\non OOD datasets than model soups. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/FissionFusion.\n","authors":["Santosh Sanjeev","Nuren Zhaksylyk","Ibrahim Almakky","Anees Ur Rehman Hashmi","Mohammad Areeb Qazi","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.13341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13338v1","updated":"2024-03-20T06:46:01Z","published":"2024-03-20T06:46:01Z","title":"Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion\n  Prediction with T1-MRI-based Brain Network","summary":"  Prediction the conversion to early-stage dementia is critical for mitigating\nits progression but remains challenging due to subtle cognitive impairments and\nstructural brain changes. Traditional T1-weighted magnetic resonance imaging\n(T1-MRI) research focus on identifying brain atrophy regions but often fails to\naddress the intricate connectivity between them. This limitation underscores\nthe necessity of focuing on inter-regional connectivity for a comprehensive\nunderstand of the brain's complex network. Moreover, there is a pressing demand\nfor methods that adaptively preserve and extract critical information,\nparticularly specialized subgraph mining techniques for brain networks. These\nare essential for developing high-quality feature representations that reveal\ncritical spatial impacts of structural brain changes and its topology. In this\npaper, we propose Brain-SubGNN, a novel graph representation network to mine\nand enhance critical subgraphs based on T1-MRI. This network provides a\nsubgraph-level interpretation, enhancing interpretability and insights for\ngraph analysis. The process begins by extracting node features and a\ncorrelation matrix between nodes to construct a task-oriented brain network.\nBrain-SubGNN then adaptively identifies and enhances critical subgraphs,\ncapturing both loop and neighbor subgraphs. This method reflects the loop\ntopology and local changes, indicative of long-range connections, and maintains\nlocal and global brain attributes. Extensive experiments validate the\neffectiveness and advantages of Brain-SubGNN, demonstrating its potential as a\npowerful tool for understanding and diagnosing early-stage dementia. Source\ncode is available at https://github.com/Leng-10/Brain-SubGNN.\n","authors":["Yilin Leng","Wenju Cui","Bai Chen","Xi Jiang","Shuangqing Chen","Jian Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.13338v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.13337v1","updated":"2024-03-20T06:44:26Z","published":"2024-03-20T06:44:26Z","title":"Learning Novel View Synthesis from Heterogeneous Low-light Captures","summary":"  Neural radiance field has achieved fundamental success in novel view\nsynthesis from input views with the same brightness level captured under fixed\nnormal lighting. Unfortunately, synthesizing novel views remains to be a\nchallenge for input views with heterogeneous brightness level captured under\nlow-light condition. The condition is pretty common in the real world. It\ncauses low-contrast images where details are concealed in the darkness and\ncamera sensor noise significantly degrades the image quality. To tackle this\nproblem, we propose to learn to decompose illumination, reflectance, and noise\nfrom input views according to that reflectance remains invariant across\nheterogeneous views. To cope with heterogeneous brightness and noise levels\nacross multi-views, we learn an illumination embedding and optimize a noise map\nindividually for each view. To allow intuitive editing of the illumination, we\ndesign an illumination adjustment module to enable either brightening or\ndarkening of the illumination component. Comprehensive experiments demonstrate\nthat this approach enables effective intrinsic decomposition for low-light\nmulti-view noisy images and achieves superior visual quality and numerical\nperformance for synthesizing novel views compared to state-of-the-art methods.\n","authors":["Quan Zheng","Hao Sun","Huiyao Xu","Fanjiang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.13337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01482v2","updated":"2024-03-20T06:38:15Z","published":"2024-03-03T11:24:16Z","title":"EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised\n  Semantic Segmentation","summary":"  Semantic segmentation has innately relied on extensive pixel-level annotated\ndata, leading to the emergence of unsupervised methodologies. Among them,\nleveraging self-supervised Vision Transformers for unsupervised semantic\nsegmentation (USS) has been making steady progress with expressive deep\nfeatures. Yet, for semantically segmenting images with complex objects, a\npredominant challenge remains: the lack of explicit object-level semantic\nencoding in patch-level features. This technical limitation often leads to\ninadequate segmentation of complex objects with diverse structures. To address\nthis gap, we present a novel approach, EAGLE, which emphasizes object-centric\nrepresentation learning for unsupervised semantic segmentation. Specifically,\nwe introduce EiCue, a spectral technique providing semantic and structural cues\nthrough an eigenbasis derived from the semantic similarity matrix of deep image\nfeatures and color affinity from an image. Further, by incorporating our\nobject-centric contrastive loss with EiCue, we guide our model to learn\nobject-level representations with intra- and inter-image object-feature\nconsistency, thereby enhancing semantic accuracy. Extensive experiments on\nCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art\nUSS results of EAGLE with accurate and consistent semantic segmentation across\ncomplex scenes.\n","authors":["Chanyoung Kim","Woojung Han","Dayun Ju","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.01482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03379v3","updated":"2024-03-20T06:33:20Z","published":"2024-01-07T03:35:04Z","title":"Towards Effective Multiple-in-One Image Restoration: A Sequential and\n  Prompt Learning Strategy","summary":"  While single task image restoration (IR) has achieved significant successes,\nit remains a challenging issue to train a single model which can tackle\nmultiple IR tasks. In this work, we investigate in-depth the multiple-in-one\n(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO\nIR faces two pivotal challenges: the optimization of diverse objectives and the\nadaptation to multiple tasks. To tackle these challenges, we present two simple\nyet effective strategies. The first strategy, referred to as sequential\nlearning, attempts to address how to optimize the diverse objectives, which\nguides the network to incrementally learn individual IR tasks in a sequential\nmanner rather than mixing them together. The second strategy, i.e., prompt\nlearning, attempts to address how to adapt to the different IR tasks, which\nassists the network to understand the specific task and improves the\ngeneralization ability. By evaluating on 19 test sets, we demonstrate that the\nsequential and prompt learning strategies can significantly enhance the MiO\nperformance of commonly used CNN and Transformer backbones. Our experiments\nalso reveal that the two strategies can supplement each other to learn better\ndegradation representations and enhance the model robustness. It is expected\nthat our proposed MiO IR formulation and strategies could facilitate the\nresearch on how to train IR models with higher generalization capabilities.\n","authors":["Xiangtao Kong","Chao Dong","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03379v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13331v1","updated":"2024-03-20T06:22:37Z","published":"2024-03-20T06:22:37Z","title":"AMP: Autoregressive Motion Prediction Revisited with Next Token\n  Prediction for Autonomous Driving","summary":"  As an essential task in autonomous driving (AD), motion prediction aims to\npredict the future states of surround objects for navigation. One natural\nsolution is to estimate the position of other agents in a step-by-step manner\nwhere each predicted time-step is conditioned on both observed time-steps and\npreviously predicted time-steps, i.e., autoregressive prediction. Pioneering\nworks like SocialLSTM and MFP design their decoders based on this intuition.\nHowever, almost all state-of-the-art works assume that all predicted time-steps\nare independent conditioned on observed time-steps, where they use a single\nlinear layer to generate positions of all time-steps simultaneously. They\ndominate most motion prediction leaderboards due to the simplicity of training\nMLPs compared to autoregressive networks.\n  In this paper, we introduce the GPT style next token prediction into motion\nforecasting. In this way, the input and output could be represented in a\nunified space and thus the autoregressive prediction becomes more feasible.\nHowever, different from language data which is composed of homogeneous units\n-words, the elements in the driving scene could have complex spatial-temporal\nand semantic relations. To this end, we propose to adopt three factorized\nattention modules with different neighbors for information aggregation and\ndifferent position encoding styles to capture their relations, e.g., encoding\nthe transformation between coordinate systems for spatial relativity while\nadopting RoPE for temporal relativity. Empirically, by equipping with the\naforementioned tailored designs, the proposed method achieves state-of-the-art\nperformance in the Waymo Open Motion and Waymo Interaction datasets. Notably,\nAMP outperforms other recent autoregressive motion prediction methods: MotionLM\nand StateTransformer, which demonstrates the effectiveness of the proposed\ndesigns.\n","authors":["Xiaosong Jia","Shaoshuai Shi","Zijun Chen","Li Jiang","Wenlong Liao","Tao He","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.13331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13330v1","updated":"2024-03-20T06:20:54Z","published":"2024-03-20T06:20:54Z","title":"Efficient scene text image super-resolution with semantic guidance","summary":"  Scene text image super-resolution has significantly improved the accuracy of\nscene text recognition. However, many existing methods emphasize performance\nover efficiency and ignore the practical need for lightweight solutions in\ndeployment scenarios. Faced with the issues, our work proposes an efficient\nframework called SGENet to facilitate deployment on resource-limited platforms.\nSGENet contains two branches: super-resolution branch and semantic guidance\nbranch. We apply a lightweight pre-trained recognizer as a semantic extractor\nto enhance the understanding of text information. Meanwhile, we design the\nvisual-semantic alignment module to achieve bidirectional alignment between\nimage features and semantics, resulting in the generation of highquality prior\nguidance. We conduct extensive experiments on benchmark dataset, and the\nproposed SGENet achieves excellent performance with fewer computational costs.\nCode is available at https://github.com/SijieLiu518/SGENet\n","authors":["LeoWu TomyEnrique","Xiangcheng Du","Kangliang Liu","Han Yuan","Zhao Zhou","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2403.13330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13327v1","updated":"2024-03-20T06:19:41Z","published":"2024-03-20T06:19:41Z","title":"Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation\n  for Natural Camera Motion","summary":"  High-quality scene reconstruction and novel view synthesis based on Gaussian\nSplatting (3DGS) typically require steady, high-quality photographs, often\nimpractical to capture with handheld cameras. We present a method that adapts\nto camera motion and allows high-quality scene reconstruction with handheld\nvideo data suffering from motion blur and rolling shutter distortion. Our\napproach is based on detailed modelling of the physical image formation process\nand utilizes velocities estimated using visual-inertial odometry (VIO). Camera\nposes are considered non-static during the exposure time of a single image\nframe and camera poses are further optimized in the reconstruction process. We\nformulate a differentiable rendering pipeline that leverages screen space\napproximation to efficiently incorporate rolling-shutter and motion blur\neffects into the 3DGS framework. Our results with both synthetic and real data\ndemonstrate superior performance in mitigating camera motion over existing\nmethods, thereby advancing 3DGS in naturalistic settings.\n","authors":["Otto Seiskari","Jerry Ylilammi","Valtteri Kaatrasalo","Pekka Rantalankila","Matias Turkulainen","Juho Kannala","Esa Rahtu","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2403.13327v1.pdf","comment":"Source code available at https://github.com/SpectacularAI/3dgs-deblur"},{"id":"http://arxiv.org/abs/2403.13324v1","updated":"2024-03-20T06:04:05Z","published":"2024-03-20T06:04:05Z","title":"Out-of-Distribution Detection Using Peer-Class Generated by Large\n  Language Model","summary":"  Out-of-distribution (OOD) detection is a critical task to ensure the\nreliability and security of machine learning models deployed in real-world\napplications. Conventional methods for OOD detection that rely on single-modal\ninformation, often struggle to capture the rich variety of OOD instances. The\nprimary difficulty in OOD detection arises when an input image has numerous\nsimilarities to a particular class in the in-distribution (ID) dataset, e.g.,\nwolf to dog, causing the model to misclassify it. Nevertheless, it may be easy\nto distinguish these classes in the semantic domain. To this end, in this\npaper, a novel method called ODPC is proposed, in which specific prompts to\ngenerate OOD peer classes of ID semantics are designed by a large language\nmodel as an auxiliary modality to facilitate detection. Moreover, a contrastive\nloss based on OOD peer classes is devised to learn compact representations of\nID classes and improve the clarity of boundaries between different classes. The\nextensive experiments on five benchmark datasets show that the method we\npropose can yield state-of-the-art results.\n","authors":["K Huang","G Song","Hanwen Su","Jiyan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13322v1","updated":"2024-03-20T06:00:53Z","published":"2024-03-20T06:00:53Z","title":"DD-RobustBench: An Adversarial Robustness Benchmark for Dataset\n  Distillation","summary":"  Dataset distillation is an advanced technique aimed at compressing datasets\ninto significantly smaller counterparts, while preserving formidable training\nperformance. Significant efforts have been devoted to promote evaluation\naccuracy under limited compression ratio while overlooked the robustness of\ndistilled dataset. In this work, we introduce a comprehensive benchmark that,\nto the best of our knowledge, is the most extensive to date for evaluating the\nadversarial robustness of distilled datasets in a unified way. Our benchmark\nsignificantly expands upon prior efforts by incorporating a wider range of\ndataset distillation methods, including the latest advancements such as TESLA\nand SRe2L, a diverse array of adversarial attack methods, and evaluations\nacross a broader and more extensive collection of datasets such as ImageNet-1K.\nMoreover, we assessed the robustness of these distilled datasets against\nrepresentative adversarial attack algorithms like PGD and AutoAttack, while\nexploring their resilience from a frequency perspective. We also discovered\nthat incorporating distilled data into the training batches of the original\ndataset can yield to improvement of robustness.\n","authors":["Yifan Wu","Jiawei Du","Ping Liu","Yuewei Lin","Wenqing Cheng","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.13322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13831v2","updated":"2024-03-20T05:55:56Z","published":"2023-11-23T07:25:31Z","title":"Posterior Distillation Sampling","summary":"  We introduce Posterior Distillation Sampling (PDS), a novel optimization\nmethod for parametric image editing based on diffusion models. Existing\noptimization-based methods, which leverage the powerful 2D prior of diffusion\nmodels to handle various parametric images, have mainly focused on generation.\nUnlike generation, editing requires a balance between conforming to the target\nattribute and preserving the identity of the source content. Recent 2D image\nediting methods have achieved this balance by leveraging the stochastic latent\nencoded in the generative process of diffusion models. To extend the editing\ncapabilities of diffusion models shown in pixel space to parameter space, we\nreformulate the 2D image editing method into an optimization form named PDS.\nPDS matches the stochastic latents of the source and the target, enabling the\nsampling of targets in diverse parameter spaces that align with a desired\nattribute while maintaining the source's identity. We demonstrate that this\noptimization resembles running a generative process with the target attribute,\nbut aligning this process with the trajectory of the source's generative\nprocess. Extensive editing results in Neural Radiance Fields and Scalable\nVector Graphics representations demonstrate that PDS is capable of sampling\ntargets to fulfill the aforementioned balance across various parameter spaces.\n","authors":["Juil Koo","Chanho Park","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2311.13831v2.pdf","comment":"Project page: https://posterior-distillation-sampling.github.io/"},{"id":"http://arxiv.org/abs/2403.13319v1","updated":"2024-03-20T05:50:04Z","published":"2024-03-20T05:50:04Z","title":"HyperFusion: A Hypernetwork Approach to Multimodal Integration of\n  Tabular and Medical Imaging Data for Predictive Modeling","summary":"  The integration of diverse clinical modalities such as medical imaging and\nthe tabular data obtained by the patients' Electronic Health Records (EHRs) is\na crucial aspect of modern healthcare. The integrative analysis of multiple\nsources can provide a comprehensive understanding of a patient's condition and\ncan enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs)\nconsistently showcase outstanding performance in a wide range of multimodal\ntasks in the medical domain. However, the complex endeavor of effectively\nmerging medical imaging with clinical, demographic and genetic information\nrepresented as numerical tabular data remains a highly active and ongoing\nresearch pursuit.\n  We present a novel framework based on hypernetworks to fuse clinical imaging\nand tabular data by conditioning the image processing on the EHR's values and\nmeasurements. This approach aims to leverage the complementary information\npresent in these modalities to enhance the accuracy of various medical\napplications. We demonstrate the strength and the generality of our method on\ntwo different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely,\nbrain age prediction conditioned by subject's sex, and multiclass Alzheimer's\nDisease (AD) classification conditioned by tabular data. We show that our\nframework outperforms both single-modality models and state-of-the-art\nMRI-tabular data fusion methods. The code, enclosed to this manuscript will be\nmade publicly available.\n","authors":["Daniel Duenias","Brennan Nichyporuk","Tal Arbel","Tammy Riklin Raviv"],"pdf_url":"https://arxiv.org/pdf/2403.13319v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.13315v1","updated":"2024-03-20T05:37:24Z","published":"2024-03-20T05:37:24Z","title":"PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models\n  with Abstract Visual Patterns","summary":"  Large multimodal models extend the impressive capabilities of large language\nmodels by integrating multimodal understanding abilities. However, it is not\nclear how they can emulate the general intelligence and reasoning ability of\nhumans. As recognizing patterns and abstracting concepts are key to general\nintelligence, we introduce PuzzleVQA, a collection of puzzles based on abstract\npatterns. With this dataset, we evaluate large multimodal models with abstract\npatterns based on fundamental concepts, including colors, numbers, sizes, and\nshapes. Through our experiments on state-of-the-art large multimodal models, we\nfind that they are not able to generalize well to simple abstract patterns.\nNotably, even GPT-4V cannot solve more than half of the puzzles. To diagnose\nthe reasoning challenges in large multimodal models, we progressively guide the\nmodels with our ground truth reasoning explanations for visual perception,\ninductive reasoning, and deductive reasoning. Our systematic analysis finds\nthat the main bottlenecks of GPT-4V are weaker visual perception and inductive\nreasoning abilities. Through this work, we hope to shed light on the\nlimitations of large multimodal models and how they can better emulate human\ncognitive processes in the future (Our data and code will be released publicly\nat https://github.com/declare-lab/LLM-PuzzleTest).\n","authors":["Yew Ken Chia","Vernon Toh Yan Han","Deepanway Ghosal","Lidong Bing","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2403.13315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13307v1","updated":"2024-03-20T05:11:10Z","published":"2024-03-20T05:11:10Z","title":"LaserHuman: Language-guided Scene-aware Human Motion Generation in Free\n  Environment","summary":"  Language-guided scene-aware human motion generation has great significance\nfor entertainment and robotics. In response to the limitations of existing\ndatasets, we introduce LaserHuman, a pioneering dataset engineered to\nrevolutionize Scene-Text-to-Motion research. LaserHuman stands out with its\ninclusion of genuine human motions within 3D environments, unbounded free-form\nnatural language descriptions, a blend of indoor and outdoor scenarios, and\ndynamic, ever-changing scenes. Diverse modalities of capture data and rich\nannotations present great opportunities for the research of conditional motion\ngeneration, and can also facilitate the development of real-life applications.\nMoreover, to generate semantically consistent and physically plausible human\nmotions, we propose a multi-conditional diffusion model, which is simple but\neffective, achieving state-of-the-art performance on existing datasets.\n","authors":["Peishan Cong","Ziyi WangZhiyang Dou","Yiming Ren","Wei Yin","Kai Cheng","Yujing Sun","Xiaoxiao Long","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16759v4","updated":"2024-03-20T05:00:06Z","published":"2023-05-26T09:21:56Z","title":"StyleHumanCLIP: Text-guided Garment Manipulation for StyleGAN-Human","summary":"  This paper tackles text-guided control of StyleGAN for editing garments in\nfull-body human images. Existing StyleGAN-based methods suffer from handling\nthe rich diversity of garments and body shapes and poses. We propose a\nframework for text-guided full-body human image synthesis via an\nattention-based latent code mapper, which enables more disentangled control of\nStyleGAN than existing mappers. Our latent code mapper adopts an attention\nmechanism that adaptively manipulates individual latent codes on different\nStyleGAN layers under text guidance. In addition, we introduce feature-space\nmasking at inference time to avoid unwanted changes caused by text inputs. Our\nquantitative and qualitative evaluations reveal that our method can control\ngenerated images more faithfully to given texts than existing methods.\n","authors":["Takato Yoshikawa","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2305.16759v4.pdf","comment":"VISIAPP 2024, project page:\n  https://www.cgg.cs.tsukuba.ac.jp/~yoshikawa/pub/style_human_clip/"},{"id":"http://arxiv.org/abs/2403.13304v1","updated":"2024-03-20T04:58:03Z","published":"2024-03-20T04:58:03Z","title":"DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced\n  Data Generation and Perception","summary":"  Current perceptive models heavily depend on resource-intensive datasets,\nprompting the need for innovative solutions. Leveraging recent advances in\ndiffusion models, synthetic data, by constructing image inputs from various\nannotations, proves beneficial for downstream tasks. While prior methods have\nseparately addressed generative and perceptive models, DetDiffusion, for the\nfirst time, harmonizes both, tackling the challenges in generating effective\ndata for perceptive models. To enhance image generation with perceptive models,\nwe introduce perception-aware loss (P.A. loss) through segmentation, improving\nboth quality and controllability. To boost the performance of specific\nperceptive models, our method customizes data augmentation by extracting and\nutilizing perception-aware attribute (P.A. Attr) during generation.\nExperimental results from the object detection task highlight DetDiffusion's\nsuperior performance, establishing a new state-of-the-art in layout-guided\ngeneration. Furthermore, image syntheses from DetDiffusion can effectively\naugment training data, significantly enhancing downstream detection\nperformance.\n","authors":["Yibo Wang","Ruiyuan Gao","Kai Chen","Kaiqiang Zhou","Yingjie Cai","Lanqing Hong","Zhenguo Li","Lihui Jiang","Dit-Yan Yeung","Qiang Xu","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13304v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12894v2","updated":"2024-03-20T04:56:03Z","published":"2024-03-19T16:46:29Z","title":"MEDBind: Unifying Language and Multimodal Medical Data Embeddings","summary":"  Medical vision-language pretraining models (VLPM) have achieved remarkable\nprogress in fusing chest X-rays (CXR) with clinical texts, introducing\nimage-text data binding approaches that enable zero-shot learning and\ndownstream clinical tasks. However, the current landscape lacks the holistic\nintegration of additional medical modalities, such as electrocardiograms (ECG).\nWe present MEDBind (Medical Electronic patient recorD), which learns joint\nembeddings across CXR, ECG, and medical text. Using text data as the central\nanchor, MEDBind features tri-modality binding, delivering competitive\nperformance in top-K retrieval, zero-shot, and few-shot benchmarks against\nestablished VLPM, and the ability for CXR-to-ECG zero-shot classification and\nretrieval. This seamless integration is achieved through combination of\ncontrastive loss on modality-text pairs with our proposed contrastive loss\nfunction, Edge-Modality Contrastive Loss, fostering a cohesive embedding space\nfor CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve\ndownstream tasks by directly integrating CXR and ECG embeddings into a\nlarge-language model for multimodal prompt tuning.\n","authors":["Yuan Gao","Sangwook Kim","David E Austin","Chris McIntosh"],"pdf_url":"https://arxiv.org/pdf/2403.12894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12236v2","updated":"2024-03-20T04:47:38Z","published":"2023-03-21T23:43:58Z","title":"SALAD: Part-Level Latent Diffusion for 3D Shape Generation and\n  Manipulation","summary":"  We present a cascaded diffusion model based on a part-level implicit 3D\nrepresentation. Our model achieves state-of-the-art generation quality and also\nenables part-level shape editing and manipulation without any additional\ntraining in conditional setup. Diffusion models have demonstrated impressive\ncapabilities in data generation as well as zero-shot completion and editing via\na guided reverse process. Recent research on 3D diffusion models has focused on\nimproving their generation capabilities with various data representations,\nwhile the absence of structural information has limited their capability in\ncompletion and editing tasks. We thus propose our novel diffusion model using a\npart-level implicit representation. To effectively learn diffusion with\nhigh-dimensional embedding vectors of parts, we propose a cascaded framework,\nlearning diffusion first on a low-dimensional subspace encoding extrinsic\nparameters of parts and then on the other high-dimensional subspace encoding\nintrinsic attributes. In the experiments, we demonstrate the outperformance of\nour method compared with the previous ones both in generation and part-level\ncompletion and manipulation tasks.\n","authors":["Juil Koo","Seungwoo Yoo","Minh Hieu Nguyen","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2303.12236v2.pdf","comment":"Project page: https://salad3d.github.io"},{"id":"http://arxiv.org/abs/2403.13298v1","updated":"2024-03-20T04:47:13Z","published":"2024-03-20T04:47:13Z","title":"Rotary Position Embedding for Vision Transformer","summary":"  Rotary Position Embedding (RoPE) performs remarkably on language models,\nespecially for length extrapolation of Transformers. However, the impacts of\nRoPE on computer vision domains have been underexplored, even though RoPE\nappears capable of enhancing Vision Transformer (ViT) performance in a way\nsimilar to the language domain. This study provides a comprehensive analysis of\nRoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D\nvision data. The analysis reveals that RoPE demonstrates impressive\nextrapolation performance, i.e., maintaining precision while increasing image\nresolution at inference. It eventually leads to performance improvement for\nImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study\nprovides thorough guidelines to apply RoPE into ViT, promising improved\nbackbone performance with minimal extra computational overhead. Our code and\npre-trained models are available at https://github.com/naver-ai/rope-vit\n","authors":["Byeongho Heo","Song Park","Dongyoon Han","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.13298v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.02313v2","updated":"2024-03-20T04:45:17Z","published":"2023-11-04T03:55:38Z","title":"LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields\n  for Large-Scale 3D Scenes","summary":"  Large-scale semantic mapping is crucial for outdoor autonomous agents to\nfulfill high-level tasks such as planning and navigation. This paper proposes a\nnovel method for large-scale 3D semantic reconstruction through implicit\nrepresentations from posed LiDAR measurements alone. We first leverage an\noctree-based and hierarchical structure to store implicit features, then these\nimplicit features are decoded to semantic information and signed distance value\nthrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf\nalgorithms to predict the semantic labels and instance IDs of point clouds. We\nthen jointly optimize the feature embeddings and MLPs parameters with a\nself-supervision paradigm for point cloud geometry and a pseudo-supervision\nparadigm for semantic and panoptic labels. Subsequently, categories and\ngeometric structures for novel points are regressed, and marching cubes are\nexploited to subdivide and visualize the scenes in the inferring stage. For\nscenarios with memory constraints, a map stitching strategy is also developed\nto merge sub-maps into a complete map. Experiments on two real-world datasets,\nSemanticKITTI and SemanticPOSS, demonstrate the superior segmentation\nefficiency and mapping effectiveness of our framework compared to current\nstate-of-the-art 3D LiDAR mapping methods.\n","authors":["Jianyuan Zhang","Zhiliu Yang","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.02313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03177v6","updated":"2024-03-20T04:43:27Z","published":"2023-07-06T17:57:02Z","title":"PanoDiffusion: 360-degree Panorama Outpainting via Diffusion","summary":"  Generating complete 360-degree panoramas from narrow field of view images is\nongoing research as omnidirectional RGB data is not readily available. Existing\nGAN-based approaches face some barriers to achieving higher quality output, and\nhave poor generalization performance over different mask types. In this paper,\nwe present our 360-degree indoor RGB-D panorama outpainting model using latent\ndiffusion models (LDM), called PanoDiffusion. We introduce a new bi-modal\nlatent diffusion structure that utilizes both RGB and depth panoramic data\nduring training, which works surprisingly well to outpaint depth-free RGB\nimages during inference. We further propose a novel technique of introducing\nprogressive camera rotations during each diffusion denoising step, which leads\nto substantial improvement in achieving panorama wraparound consistency.\nResults show that our PanoDiffusion not only significantly outperforms\nstate-of-the-art methods on RGB-D panorama outpainting by producing diverse\nwell-structured results for different types of masks, but can also synthesize\nhigh-quality depth panoramas to provide realistic 3D indoor models.\n","authors":["Tianhao Wu","Chuanxia Zheng","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2307.03177v6.pdf","comment":"Project Page: https://sm0kywu.github.io/panodiffusion/"},{"id":"http://arxiv.org/abs/2403.13293v1","updated":"2024-03-20T04:18:38Z","published":"2024-03-20T04:18:38Z","title":"Building Optimal Neural Architectures using Interpretable Knowledge","summary":"  Neural Architecture Search is a costly practice. The fact that a search space\ncan span a vast number of design choices with each architecture evaluation\ntaking nontrivial overhead makes it hard for an algorithm to sufficiently\nexplore candidate networks. In this paper, we propose AutoBuild, a scheme which\nlearns to align the latent embeddings of operations and architecture modules\nwith the ground-truth performance of the architectures they appear in. By doing\nso, AutoBuild is capable of assigning interpretable importance scores to\narchitecture modules, such as individual operation features and larger macro\noperation sequences such that high-performance neural networks can be\nconstructed without any need for search. Through experiments performed on\nstate-of-the-art image classification, segmentation, and Stable Diffusion\nmodels, we show that by mining a relatively small set of evaluated\narchitectures, AutoBuild can learn to build high-quality architectures directly\nor help to reduce search space to focus on relevant areas, finding better\narchitectures that outperform both the original labeled ones and ones found by\nsearch baselines. Code available at\nhttps://github.com/Ascend-Research/AutoBuild\n","authors":["Keith G. Mills","Fred X. Han","Mohammad Salameh","Shengyao Lu","Chunhua Zhou","Jiao He","Fengyu Sun","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2403.13293v1.pdf","comment":"CVPR'24; 18 Pages, 18 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2312.09570v2","updated":"2024-03-20T04:05:37Z","published":"2023-12-15T07:04:27Z","title":"CAGE: Controllable Articulation GEneration","summary":"  We address the challenge of generating 3D articulated objects in a\ncontrollable fashion. Currently, modeling articulated 3D objects is either\nachieved through laborious manual authoring, or using methods from prior work\nthat are hard to scale and control directly. We leverage the interplay between\npart shape, connectivity, and motion using a denoising diffusion-based method\nwith attention modules designed to extract correlations between part\nattributes. Our method takes an object category label and a part connectivity\ngraph as input and generates an object's geometry and motion parameters. The\ngenerated objects conform to user-specified constraints on the object category,\npart shape, and part articulation. Our experiments show that our method\noutperforms the state-of-the-art in articulated object generation, producing\nmore realistic objects while conforming better to user constraints.\n  Video Summary at: http://youtu.be/cH_rbKbyTpE\n","authors":["Jiayi Liu","Hou In Ivan Tam","Ali Mahdavi-Amiri","Manolis Savva"],"pdf_url":"https://arxiv.org/pdf/2312.09570v2.pdf","comment":"CVPR 2024. Project page: https://3dlg-hcvc.github.io/cage/"},{"id":"http://arxiv.org/abs/2403.13289v1","updated":"2024-03-20T04:03:44Z","published":"2024-03-20T04:03:44Z","title":"Text-to-3D Shape Generation","summary":"  Recent years have seen an explosion of work and interest in text-to-3D shape\ngeneration. Much of the progress is driven by advances in 3D representations,\nlarge-scale pretraining and representation learning for text and image data\nenabling generative AI models, and differentiable rendering. Computational\nsystems that can perform text-to-3D shape generation have captivated the\npopular imagination as they enable non-expert users to easily create 3D content\ndirectly from text. However, there are still many limitations and challenges\nremaining in this problem space. In this state-of-the-art report, we provide a\nsurvey of the underlying technology and methods enabling text-to-3D shape\ngeneration to summarize the background literature. We then derive a systematic\ncategorization of recent work on text-to-3D shape generation based on the type\nof supervision data required. Finally, we discuss limitations of the existing\ncategories of methods, and delineate promising directions for future work.\n","authors":["Han-Hung Lee","Manolis Savva","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2403.13289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02626v2","updated":"2024-03-20T03:56:57Z","published":"2024-03-05T03:34:11Z","title":"Modeling Collaborator: Enabling Subjective Vision Classification With\n  Minimal Human Effort via LLM Tool-Use","summary":"  From content moderation to wildlife conservation, the number of applications\nthat require models to recognize nuanced or subjective visual concepts is\ngrowing. Traditionally, developing classifiers for such concepts requires\nsubstantial manual effort measured in hours, days, or even months to identify\nand annotate data needed for training. Even with recently proposed Agile\nModeling techniques, which enable rapid bootstrapping of image classifiers,\nusers are still required to spend 30 minutes or more of monotonous, repetitive\ndata labeling just to train a single classifier. Drawing on Fiske's Cognitive\nMiser theory, we propose a new framework that alleviates manual effort by\nreplacing human labeling with natural language interactions, reducing the total\neffort required to define a concept by an order of magnitude: from labeling\n2,000 images to only 100 plus some natural language interactions. Our framework\nleverages recent advances in foundation models, both large language models and\nvision-language models, to carve out the concept space through conversation and\nby automatically labeling training data points. Most importantly, our framework\neliminates the need for crowd-sourced annotations. Moreover, our framework\nultimately produces lightweight classification models that are deployable in\ncost-sensitive scenarios. Across 15 subjective concepts and across 2 public\nimage classification datasets, our trained models outperform traditional Agile\nModeling as well as state-of-the-art zero-shot classification models like\nALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.\n","authors":["Imad Eddine Toubal","Aditya Avinash","Neil Gordon Alldrin","Jan Dlabal","Wenlei Zhou","Enming Luo","Otilia Stretcu","Hao Xiong","Chun-Ta Lu","Howard Zhou","Ranjay Krishna","Ariel Fuxman","Tom Duerig"],"pdf_url":"https://arxiv.org/pdf/2403.02626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13282v1","updated":"2024-03-20T03:47:53Z","published":"2024-03-20T03:47:53Z","title":"AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models\n  Adapting","summary":"  Recently, prompt-based methods have emerged as a new alternative\n`parameter-efficient fine-tuning' paradigm, which only fine-tunes a small\nnumber of additional parameters while keeping the original model frozen.\nHowever, despite achieving notable results, existing prompt methods mainly\nfocus on `what to add', while overlooking the equally important aspect of\n`where to add', typically relying on the manually crafted placement. To this\nend, we propose a region-based Adaptive Visual Prompt, named AdaViPro, which\nintegrates the `where to add' optimization of the prompt into the learning\nprocess. Specifically, we reconceptualize the `where to add' optimization as a\nproblem of regional decision-making. During inference, AdaViPro generates a\nregionalized mask map for the whole image, which is composed of 0 and 1, to\ndesignate whether to apply or discard the prompt in each specific area.\nTherefore, we employ Gumbel-Softmax sampling to enable AdaViPro's end-to-end\nlearning through standard back-propagation. Extensive experiments demonstrate\nthat our AdaViPro yields new efficiency and accuracy trade-offs for adapting\npre-trained models.\n","authors":["Mengyu Yang","Ye Tian","Lanshan Zhang","Xiao Liang","Xuming Ran","Wendong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13282v1.pdf","comment":"Submitted to ICIP 2024"},{"id":"http://arxiv.org/abs/2403.12052v2","updated":"2024-03-20T03:45:13Z","published":"2024-01-04T11:14:01Z","title":"A Dataset and Benchmark for Copyright Protection from Text-to-Image\n  Diffusion Models","summary":"  Copyright is a legal right that grants creators the exclusive authority to\nreproduce, distribute, and profit from their creative works. However, the\nrecent advancements in text-to-image generation techniques have posed\nsignificant challenges to copyright protection, as these methods have\nfacilitated the learning of unauthorized content, artistic creations, and\nportraits, which are subsequently utilized to generate and disseminate\nuncontrolled content. Especially, the use of stable diffusion, an emerging\nmodel for text-to-image generation, poses an increased risk of unauthorized\ncopyright infringement and distribution. Currently, there is a lack of\nsystematic studies evaluating the potential correlation between content\ngenerated by stable diffusion and those under copyright protection. Conducting\nsuch studies faces several challenges, including i) the intrinsic ambiguity\nrelated to copyright infringement in text-to-image models, ii) the absence of a\ncomprehensive large-scale dataset, and iii) the lack of standardized metrics\nfor defining copyright infringement. This work provides the first large-scale\nstandardized dataset and benchmark on copyright protection. Specifically, we\npropose a pipeline to coordinate CLIP, ChatGPT, and diffusion models to\ngenerate a dataset that contains anchor images, corresponding prompts, and\nimages generated by text-to-image models, reflecting the potential abuses of\ncopyright. Furthermore, we explore a suite of evaluation metrics to judge the\neffectiveness of copyright protection methods. The proposed dataset, benchmark\nlibrary, and evaluation metrics will be open-sourced to facilitate future\nresearch and application. The website and dataset can be accessed website\ndataset.\n","authors":["Rui Ma","Qiang Zhou","Bangjun Xiao","Yizhu Jin","Daquan Zhou","Xiuyu Li","Aishani Singh","Yi Qu","Kurt Keutzer","Xiaodong Xie","Jingtong Hu","Zhen Dong","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12052v2.pdf","comment":"Improve experimental content"},{"id":"http://arxiv.org/abs/2403.08381v2","updated":"2024-03-20T03:41:07Z","published":"2024-03-13T09:47:04Z","title":"Tackling the Singularities at the Endpoints of Time Intervals in\n  Diffusion Models","summary":"  Most diffusion models assume that the reverse process adheres to a Gaussian\ndistribution. However, this approximation has not been rigorously validated,\nespecially at singularities, where t=0 and t=1. Improperly dealing with such\nsingularities leads to an average brightness issue in applications, and limits\nthe generation of images with extreme brightness or darkness. We primarily\nfocus on tackling singularities from both theoretical and practical\nperspectives. Initially, we establish the error bounds for the reverse process\napproximation, and showcase its Gaussian characteristics at singularity time\nsteps. Based on this theoretical insight, we confirm the singularity at t=1 is\nconditionally removable while it at t=0 is an inherent property. Upon these\nsignificant conclusions, we propose a novel plug-and-play method SingDiffusion\nto address the initial singular time step sampling, which not only effectively\nresolves the average brightness issue for a wide range of diffusion models\nwithout extra training efforts, but also enhances their generation capability\nin achieving notable lower FID scores.\n","authors":["Pengze Zhang","Hubery Yin","Chen Li","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.08381v2.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2205.00415v3","updated":"2024-03-20T03:23:11Z","published":"2022-05-01T07:51:22Z","title":"Don't Blame the Annotator: Bias Already Starts in the Annotation\n  Instructions","summary":"  In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write many similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n","authors":["Mihir Parmar","Swaroop Mishra","Mor Geva","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2205.00415v3.pdf","comment":"EACL 2023 (Outstanding Paper Award)"},{"id":"http://arxiv.org/abs/2312.01027v3","updated":"2024-03-20T03:19:41Z","published":"2023-12-02T04:31:51Z","title":"LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models","summary":"  Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB\nimage is a significant challenge for modern digital cameras. Prior approaches\nhave difficulties in recovering fine-grained details and true colors of the\nscene under extremely low-light environments due to near-to-zero SNR.\nMeanwhile, diffusion models have shown significant progress towards general\ndomain image generation. In this paper, we propose to leverage the pre-trained\nlatent diffusion model to perform the neural ISP for enhancing extremely\nlow-light images. Specifically, to tailor the pre-trained latent diffusion\nmodel to operate on the RAW domain, we train a set of lightweight taming\nmodules to inject the RAW information into the diffusion denoising process via\nmodulating the intermediate features of UNet. We further observe different\nroles of UNet denoising and decoder reconstruction in the latent diffusion\nmodel, which inspires us to decompose the low-light image enhancement task into\nlatent-space low-frequency content generation and decoding-phase high-frequency\ndetail maintenance. Through extensive experiments on representative datasets,\nwe demonstrate our simple design not only achieves state-of-the-art performance\nin quantitative evaluations but also shows significant superiority in visual\ncomparisons over strong baselines, which highlight the effectiveness of\npowerful generative priors for neural ISP under extremely low-light\nenvironments. The project page is available at\nhttps://csqiangwen.github.io/projects/ldm-isp/\n","authors":["Qiang Wen","Yazhou Xing","Zhefan Rao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.01027v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04834v2","updated":"2024-03-20T03:07:26Z","published":"2023-08-09T09:46:26Z","title":"View while Moving: Efficient Video Recognition in Long-untrimmed Videos","summary":"  Recent adaptive methods for efficient video recognition mostly follow the\ntwo-stage paradigm of \"preview-then-recognition\" and have achieved great\nsuccess on multiple video benchmarks. However, this two-stage paradigm involves\ntwo visits of raw frames from coarse-grained to fine-grained during inference\n(cannot be parallelized), and the captured spatiotemporal features cannot be\nreused in the second stage (due to varying granularity), being not friendly to\nefficiency and computation optimization. To this end, inspired by human\ncognition, we propose a novel recognition paradigm of \"View while Moving\" for\nefficient long-untrimmed video recognition. In contrast to the two-stage\nparadigm, our paradigm only needs to access the raw frame once. The two phases\nof coarse-grained sampling and fine-grained recognition are combined into\nunified spatiotemporal modeling, showing great performance. Moreover, we\ninvestigate the properties of semantic units in video and propose a\nhierarchical mechanism to efficiently capture and reason about the unit-level\nand video-level temporal semantics in long-untrimmed videos respectively.\nExtensive experiments on both long-untrimmed and short-trimmed videos\ndemonstrate that our approach outperforms state-of-the-art methods in terms of\naccuracy as well as efficiency, yielding new efficiency and accuracy trade-offs\nfor video spatiotemporal modeling.\n","authors":["Ye Tian","Mengyu Yang","Lanshan Zhang","Zhizhen Zhang","Yang Liu","Xiaohui Xie","Xirong Que","Wendong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.04834v2.pdf","comment":"Published on ACM MM 2023"},{"id":"http://arxiv.org/abs/2403.13263v1","updated":"2024-03-20T03:00:21Z","published":"2024-03-20T03:00:21Z","title":"SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large\n  Vision Language Models","summary":"  Recent trends in Large Vision Language Models (LVLMs) research have been\nincreasingly focusing on advancing beyond general image understanding towards\nmore nuanced, object-level referential comprehension. In this paper, we present\nand delve into the self-consistency capability of LVLMs, a crucial aspect that\nreflects the models' ability to both generate informative captions for specific\nobjects and subsequently utilize these captions to accurately re-identify the\nobjects in a closed-loop process. This capability significantly mirrors the\nprecision and reliability of fine-grained visual-language understanding. Our\nfindings reveal that the self-consistency level of existing LVLMs falls short\nof expectations, posing limitations on their practical applicability and\npotential. To address this gap, we introduce a novel fine-tuning paradigm named\nSelf-Consistency Tuning (SC-Tune). It features the synergistic learning of a\ncyclic describer-locator system. This paradigm is not only data-efficient but\nalso exhibits generalizability across multiple LVLMs. Through extensive\nexperiments, we demonstrate that SC-Tune significantly elevates performance\nacross a spectrum of object-level vision-language benchmarks and maintains\ncompetitive or improved performance on image-level vision-language benchmarks.\nBoth our model and code will be publicly available at\nhttps://github.com/ivattyue/SC-Tune.\n","authors":["Tongtian Yue","Jie Cheng","Longteng Guo","Xingyuan Dai","Zijia Zhao","Xingjian He","Gang Xiong","Yisheng Lv","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13263v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.13261v1","updated":"2024-03-20T02:58:45Z","published":"2024-03-20T02:58:45Z","title":"Self-Supervised Class-Agnostic Motion Prediction with Spatial and\n  Temporal Consistency Regularizations","summary":"  The perception of motion behavior in a dynamic environment holds significant\nimportance for autonomous driving systems, wherein class-agnostic motion\nprediction methods directly predict the motion of the entire point cloud. While\nmost existing methods rely on fully-supervised learning, the manual labeling of\npoint cloud data is laborious and time-consuming. Therefore, several\nannotation-efficient methods have been proposed to address this challenge.\nAlthough effective, these methods rely on weak annotations or additional\nmulti-modal data like images, and the potential benefits inherent in the point\ncloud sequence are still underexplored. To this end, we explore the feasibility\nof self-supervised motion prediction with only unlabeled LiDAR point clouds.\nInitially, we employ an optimal transport solver to establish coarse\ncorrespondences between current and future point clouds as the coarse pseudo\nmotion labels. Training models directly using such coarse labels leads to\nnoticeable spatial and temporal prediction inconsistencies. To mitigate these\nissues, we introduce three simple spatial and temporal regularization losses,\nwhich facilitate the self-supervised training process effectively. Experimental\nresults demonstrate the significant superiority of our approach over the\nstate-of-the-art self-supervised methods.\n","authors":["Kewei Wang","Yizheng Wu","Jun Cen","Zhiyu Pan","Xingyi Li","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13261v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.13258v1","updated":"2024-03-20T02:39:15Z","published":"2024-03-20T02:39:15Z","title":"SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts","summary":"  Segment anything model (SAM), a foundation model with superior versatility\nand generalization across diverse segmentation tasks, has attracted widespread\nattention in medical imaging. However, it has been proved that SAM would\nencounter severe performance degradation due to the lack of medical knowledge\nin training and local feature encoding. Though several SAM-based models have\nbeen proposed for tuning SAM in medical imaging, they still suffer from\ninsufficient feature extraction and highly rely on high-quality prompts. In\nthis paper, we construct a large CT dataset consisting of 1.1M CT images and 5M\nmasks from public datasets and propose a powerful foundation model SAMCT\nallowing labor-free prompts. Specifically, based on SAM, SAMCT is further\nequipped with a U-shaped CNN image encoder, a cross-branch interaction module,\nand a task-indicator prompt encoder. The U-shaped CNN image encoder works in\nparallel with the ViT image encoder in SAM to supplement local features.\nCross-branch interaction enhances the feature expression capability of the CNN\nimage encoder and the ViT image encoder by exchanging global perception and\nlocal features from one to the other. The task-indicator prompt encoder is a\nplug-and-play component to effortlessly encode task-related indicators into\nprompt embeddings. In this way, SAMCT can work in an automatic manner in\naddition to the semi-automatic interactive strategy in SAM. Extensive\nexperiments demonstrate the superiority of SAMCT against the state-of-the-art\ntask-specific and SAM-based medical foundation models on various tasks. The\ncode, data, and models are released at https://github.com/xianlin7/SAMCT.\n","authors":["Xian Lin","Yangyang Xiang","Zhehao Wang","Kwang-Ting Cheng","Zengqiang Yan","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2403.13258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09233v2","updated":"2024-03-20T02:38:44Z","published":"2024-03-14T09:57:15Z","title":"D-YOLO a robust framework for object detection in adverse weather\n  conditions","summary":"  Adverse weather conditions including haze, snow and rain lead to decline in\nimage qualities, which often causes a decline in performance for deep-learning\nbased detection networks. Most existing approaches attempts to rectify hazy\nimages before performing object detection, which increases the complexity of\nthe network and may result in the loss in latent information. To better\nintegrate image restoration and object detection tasks, we designed a\ndouble-route network with an attention feature fusion module, taking both hazy\nand dehazed features into consideration. We also proposed a subnetwork to\nprovide haze-free features to the detection network. Specifically, our D-YOLO\nimproves the performance of the detection network by minimizing the distance\nbetween the clear feature extraction subnetwork and detection network.\nExperiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates\nbetter performance compared to the state-of-the-art methods. It is a robust\ndetection framework for bridging the gap between low-level dehazing and\nhigh-level detection.\n","authors":["Zihan Chu"],"pdf_url":"https://arxiv.org/pdf/2403.09233v2.pdf","comment":"Object detection in adverse weather conditions. arXiv admin note:\n  text overlap with arXiv:2209.01373 by other authors"},{"id":"http://arxiv.org/abs/2403.08505v2","updated":"2024-03-20T02:35:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10297v2","updated":"2024-03-20T02:34:27Z","published":"2024-03-15T13:40:37Z","title":"Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints\n  Scene Coordinate Regression","summary":"  Classical structural-based visual localization methods offer high accuracy\nbut face trade-offs in terms of storage, speed, and privacy. A recent\ninnovation, keypoint scene coordinate regression (KSCR) named D2S addresses\nthese issues by leveraging graph attention networks to enhance keypoint\nrelationships and predict their 3D coordinates using a simple multilayer\nperceptron (MLP). Camera pose is then determined via PnP+RANSAC, using\nestablished 2D-3D correspondences. While KSCR achieves competitive results,\nrivaling state-of-the-art image-retrieval methods like HLoc across multiple\nbenchmarks, its performance is hindered when data samples are limited due to\nthe deep learning model's reliance on extensive data. This paper proposes a\nsolution to this challenge by introducing a pipeline for keypoint descriptor\nsynthesis using Neural Radiance Field (NeRF). By generating novel poses and\nfeeding them into a trained NeRF model to create new views, our approach\nenhances the KSCR's generalization capabilities in data-scarce environments.\nThe proposed system could significantly improve localization accuracy by up to\n50% and cost only a fraction of time for data synthesis. Furthermore, its\nmodular design allows for the integration of multiple NeRFs, offering a\nversatile and efficient solution for visual localization. The implementation is\npublicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.\n","authors":["Huy-Hoang Bui","Bach-Thuan Bui","Dinh-Tuan Tran","Joo-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20210v2","updated":"2024-03-20T02:34:21Z","published":"2023-10-31T06:19:09Z","title":"UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale\n  Transformer","summary":"  Underwater images often exhibit poor quality, distorted color balance and low\ncontrast due to the complex and intricate interplay of light, water, and\nobjects. Despite the significant contributions of previous underwater\nenhancement techniques, there exist several problems that demand further\nimprovement: (i) The current deep learning methods rely on Convolutional Neural\nNetworks (CNNs) that lack the multi-scale enhancement, and global perception\nfield is also limited. (ii) The scarcity of paired real-world underwater\ndatasets poses a significant challenge, and the utilization of synthetic image\npairs could lead to overfitting. To address the aforementioned problems, this\npaper introduces a Multi-scale Transformer-based Network called UWFormer for\nenhancing images at multiple frequencies via semi-supervised learning, in which\nwe propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale\nFusion Feed-forward Network for low-frequency enhancement. Besides, we\nintroduce a special underwater semi-supervised training strategy, where we\npropose a Subaqueous Perceptual Loss function to generate reliable pseudo\nlabels. Experiments using full-reference and non-reference underwater\nbenchmarks demonstrate that our method outperforms state-of-the-art methods in\nterms of both quantity and visual quality.\n","authors":["Yingtie Lei","Weiwen Chen","Shenghong Luo","Ziyang Zhou","Mingxian Li","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2310.20210v2.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2309.06670v3","updated":"2024-03-20T02:31:16Z","published":"2023-09-13T02:15:29Z","title":"ShaDocFormer: A Shadow-Attentive Threshold Detector With Cascaded Fusion\n  Refiner for Document Shadow Removal","summary":"  Document shadow is a common issue that arises when capturing documents using\nmobile devices, which significantly impacts readability. Current methods\nencounter various challenges, including inaccurate detection of shadow masks\nand estimation of illumination. In this paper, we propose ShaDocFormer, a\nTransformer-based architecture that integrates traditional methodologies and\ndeep learning techniques to tackle the problem of document shadow removal. The\nShaDocFormer architecture comprises two components: the Shadow-attentive\nThreshold Detector (STD) and the Cascaded Fusion Refiner (CFR). The STD module\nemploys a traditional thresholding technique and leverages the attention\nmechanism of the Transformer to gather global information, thereby enabling\nprecise detection of shadow masks. The cascaded and aggregative structure of\nthe CFR module facilitates a coarse-to-fine restoration process for the entire\nimage. As a result, ShaDocFormer excels in accurately detecting and capturing\nvariations in both shadow and illumination, thereby enabling effective removal\nof shadows. Extensive experiments demonstrate that ShaDocFormer outperforms\ncurrent state-of-the-art methods in both qualitative and quantitative\nmeasurements.\n","authors":["Weiwen Chen","Yingtie Lei","Shenghong Luo","Xuhang Chen","Ziyang Zhou","Mingxian Li","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2309.06670v3.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.13249v1","updated":"2024-03-20T02:21:44Z","published":"2024-03-20T02:21:44Z","title":"A Unified and General Framework for Continual Learning","summary":"  Continual Learning (CL) focuses on learning from dynamic and changing data\ndistributions while retaining previously acquired knowledge. Various methods\nhave been developed to address the challenge of catastrophic forgetting,\nincluding regularization-based, Bayesian-based, and memory-replay-based\ntechniques. However, these methods lack a unified framework and common\nterminology for describing their approaches. This research aims to bridge this\ngap by introducing a comprehensive and overarching framework that encompasses\nand reconciles these existing methodologies. Notably, this new framework is\ncapable of encompassing established CL approaches as special instances within a\nunified and general optimization objective. An intriguing finding is that\ndespite their diverse origins, these methods share common mathematical\nstructures. This observation highlights the compatibility of these seemingly\ndistinct techniques, revealing their interconnectedness through a shared\nunderlying optimization objective. Moreover, the proposed general framework\nintroduces an innovative concept called refresh learning, specifically designed\nto enhance the CL performance. This novel approach draws inspiration from\nneuroscience, where the human brain often sheds outdated information to improve\nthe retention of crucial knowledge and facilitate the acquisition of new\ninformation. In essence, refresh learning operates by initially unlearning\ncurrent data and subsequently relearning it. It serves as a versatile plug-in\nthat seamlessly integrates with existing CL methods, offering an adaptable and\neffective enhancement to the learning process. Extensive experiments on CL\nbenchmarks and theoretical analysis demonstrate the effectiveness of the\nproposed refresh learning. Code is available at\n\\url{https://github.com/joey-wang123/CL-refresh-learning}.\n","authors":["Zhenyi Wang","Yan Li","Li Shen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2403.13249v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.13248v1","updated":"2024-03-20T02:19:21Z","published":"2024-03-20T02:19:21Z","title":"Mora: Enabling Generalist Video Generation via A Multi-Agent Framework","summary":"  Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents.\n","authors":["Zhengqing Yuan","Ruoxi Chen","Zhaoxu Li","Haolong Jia","Lifang He","Chi Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12434v2","updated":"2024-03-20T02:04:21Z","published":"2024-03-19T04:47:56Z","title":"Human Mesh Recovery from Arbitrary Multi-view Images","summary":"  Human mesh recovery from arbitrary multi-view images involves two\ncharacteristics: the arbitrary camera poses and arbitrary number of camera\nviews. Because of the variability, designing a unified framework to tackle this\ntask is challenging. The challenges can be summarized as the dilemma of being\nable to simultaneously estimate arbitrary camera poses and recover human mesh\nfrom arbitrary multi-view images while maintaining flexibility. To solve this\ndilemma, we propose a divide and conquer framework for Unified Human Mesh\nRecovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR\nconsists of a decoupled structure and two main components: camera and body\ndecoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion\n(AVF). As camera poses and human body mesh are independent of each other, CBD\nsplits the estimation of them into two sub-tasks for two individual\nsub-networks (\\ie, CPE and AVF) to handle respectively, thus the two sub-tasks\nare disentangled. In CPE, since each camera pose is unrelated to the others, we\nadopt a shared MLP to process all views in a parallel way. In AVF, in order to\nfuse multi-view information and make the fusion operation independent of the\nnumber of views, we introduce a transformer decoder with a SMPL parameters\nquery token to extract cross-view features for mesh recovery. To demonstrate\nthe efficacy and flexibility of the proposed framework and effect of each\ncomponent, we conduct extensive experiments on three public datasets:\nHuman3.6M, MPI-INF-3DHP, and TotalCapture.\n","authors":["Xiaoben Li","Mancheng Meng","Ziyan Wu","Terrence Chen","Fan Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2403.12434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02317v4","updated":"2024-03-20T02:03:52Z","published":"2024-01-04T15:34:44Z","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","summary":"  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously. Code is available at\nhttps://github.com/zongzi13545329/BA-SAM\n","authors":["Yiran Song","Qianyu Zhou","Xiangtai Li","Deng-Ping Fan","Xuequan Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02317v4.pdf","comment":"Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.11625v2","updated":"2024-03-20T02:01:58Z","published":"2024-03-18T09:56:48Z","title":"GaussNav: Gaussian Splatting for Visual Navigation","summary":"  In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to\nlocate a specific object depicted in a goal image within an unexplored\nenvironment. The primary difficulty of IIN stems from the necessity of\nrecognizing the target object across varying viewpoints and rejecting potential\ndistractors.\n  Existing map-based navigation methods largely adopt the representation form\nof Bird's Eye View (BEV) maps, which, however, lack the representation of\ndetailed textures in a scene.\n  To address the above issues, we propose a new Gaussian Splatting Navigation\n(abbreviated as GaussNav) framework for IIN task, which constructs a novel map\nrepresentation based on 3D Gaussian Splatting (3DGS).\n  The proposed framework enables the agent to not only memorize the geometry\nand semantic information of the scene, but also retain the textural features of\nobjects.\n  Our GaussNav framework demonstrates a significant leap in performance,\nevidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to\n0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.\n  Our code will be made publicly available.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.11625v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2403.13238v1","updated":"2024-03-20T01:59:43Z","published":"2024-03-20T01:59:43Z","title":"Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence\n  Generation","summary":"  Directly learning to model 4D content, including shape, color and motion, is\nchallenging. Existing methods depend on skeleton-based motion control and offer\nlimited continuity in detail. To address this, we propose a novel framework\nthat generates coherent 4D sequences with animation of 3D shapes under given\nconditions with dynamic evolution of shape and color over time through\nintegrative latent mapping. We first employ an integrative latent unified\nrepresentation to encode shape and color information of each detailed 3D\ngeometry frame. The proposed skeleton-free latent 4D sequence joint\nrepresentation allows us to leverage diffusion models in a low-dimensional\nspace to control the generation of 4D sequences. Finally, temporally coherent\n4D sequences are generated conforming well to the input images and text\nprompts. Extensive experiments on the ShapeNet, 3DBiCar and DeformingThings4D\ndatasets for several tasks demonstrate that our method effectively learns to\ngenerate quality 3D shapes with color and 4D mesh animations, improving over\nthe current state-of-the-art. Source code will be released.\n","authors":["Qitong Yang","Mingtao Feng","Zijie Wu","Shijie Sun","Weisheng Dong","Yaonan Wang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12267v2","updated":"2024-03-20T01:46:13Z","published":"2024-03-18T21:32:58Z","title":"Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data\n  Quality over Quantity","summary":"  Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption\ndatasets learns representations that can achieve remarkable zero-shot\ngeneralization. However, such models require a massive amount of pre-training\ndata. Improving the quality of the pre-training data has been shown to be much\nmore effective in improving CLIP's performance than increasing its volume.\nNevertheless, finding small subsets of training data that provably generalize\nthe best has remained an open question. In this work, we propose the first\ntheoretically rigorous data selection method for CLIP. We show that subsets\nthat closely preserve the cross-covariance of the images and captions of the\nfull data provably achieve a superior generalization performance. Our extensive\nexperiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that\nsubsets found by \\method\\ achieve over 2.7x and 1.4x the accuracy of the next\nbest baseline on ImageNet and its shifted versions. Moreover, we show that our\nsubsets obtain 1.5x the average accuracy across 11 downstream datasets, of the\nnext best baseline. The code is available at:\nhttps://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip.\n","authors":["Siddharth Joshi","Arnav Jain","Ali Payani","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2403.12267v2.pdf","comment":"AISTATS 2024, Code:\n  https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip"},{"id":"http://arxiv.org/abs/2403.11310v2","updated":"2024-03-20T01:34:35Z","published":"2024-03-17T19:10:07Z","title":"A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose\n  Estimation","summary":"  3D human pose data collected in controlled laboratory settings present\nchallenges for pose estimators that generalize across diverse scenarios. To\naddress this, domain generalization is employed. Current methodologies in\ndomain generalization for 3D human pose estimation typically utilize\nadversarial training to generate synthetic poses for training. Nonetheless,\nthese approaches exhibit several limitations. First, the lack of prior\ninformation about the target domain complicates the application of suitable\naugmentation through a single pose augmentor, affecting generalization on\ntarget domains. Moreover, adversarial training's discriminator tends to enforce\nsimilarity between source and synthesized poses, impeding the exploration of\nout-of-source distributions. Furthermore, the pose estimator's optimization is\nnot exposed to domain shifts, limiting its overall generalization ability.\n  To address these limitations, we propose a novel framework featuring two pose\naugmentors: the weak and the strong augmentors. Our framework employs\ndifferential strategies for generation and discrimination processes,\nfacilitating the preservation of knowledge related to source poses and the\nexploration of out-of-source distributions without prior information about\ntarget poses. Besides, we leverage meta-optimization to simulate domain shifts\nin the optimization process of the pose estimator, thereby improving its\ngeneralization ability. Our proposed approach significantly outperforms\nexisting methods, as demonstrated through comprehensive experiments on various\nbenchmark datasets.Our code will be released at\n\\url{https://github.com/davidpengucf/DAF-DG}.\n","authors":["Qucheng Peng","Ce Zheng","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11310v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11573v2","updated":"2024-03-20T01:13:48Z","published":"2024-03-18T08:50:04Z","title":"Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for\n  Resolving Class-imbalance Problem","summary":"  Typical LiDAR-based 3D object detection models are trained in a supervised\nmanner with real-world data collection, which is often imbalanced over classes\n(or long-tailed). To deal with it, augmenting minority-class examples by\nsampling ground truth (GT) LiDAR points from a database and pasting them into a\nscene of interest is often used, but challenges still remain: inflexibility in\nlocating GT samples and limited sample diversity. In this work, we propose to\nleverage pseudo-LiDAR point clouds generated (at a low cost) from videos\ncapturing a surround view of miniatures or real-world objects of minor classes.\nOur method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of\nthree main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D\nview synthesis model, (ii) object-level domain alignment with LiDAR intensity\nestimation and (iii) a hybrid context-aware placement method from ground and\nmap information. We demonstrate the superiority and generality of our method\nthrough performance improvements in extensive experiments conducted on three\npopular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the\ndatasets with large domain gaps captured by different LiDAR configurations. Our\ncode and data will be publicly available upon publication.\n","authors":["Mincheol Chang","Siyeong Lee","Jinkyu Kim","Namil Kim"],"pdf_url":"https://arxiv.org/pdf/2403.11573v2.pdf","comment":"28 pages, 12 figures, 11 tables"},{"id":"http://arxiv.org/abs/2311.16581v2","updated":"2024-03-20T00:59:44Z","published":"2023-11-28T07:55:25Z","title":"GeoScaler: Geometry and Rendering-Aware Downsampling of 3D Mesh Textures","summary":"  High-resolution texture maps are necessary for representing real-world\nobjects accurately with 3D meshes. The large sizes of textures can bottleneck\nthe real-time rendering of high-quality virtual 3D scenes on devices having low\ncomputational budgets and limited memory. Downsampling the texture maps\ndirectly addresses the issue, albeit at the cost of visual fidelity.\nTraditionally, downsampling of texture maps is performed using methods like\nbicubic interpolation and the Lanczos algorithm. These methods ignore the\ngeometric layout of the mesh and its UV parametrization and also do not account\nfor the rendering process used to obtain the final visualization that the users\nwill experience. Towards filling these gaps, we introduce GeoScaler, which is a\nmethod of downsampling texture maps of 3D meshes while incorporating geometric\ncues, and by maximizing the visual fidelity of the rendered views of the\ntextured meshes. We show that the textures generated by GeoScaler deliver\nsignificantly better quality rendered images compared to those generated by\ntraditional downsampling methods\n","authors":["Sai Karthikey Pentapati","Anshul Rai","Arkady Ten","Chaitanya Atluru","Alan Bovik"],"pdf_url":"https://arxiv.org/pdf/2311.16581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05892v4","updated":"2024-03-20T00:58:15Z","published":"2024-02-08T18:30:50Z","title":"Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data","summary":"  In recent years, Transformers have become the de-facto architecture for\nsequence modeling on text and a variety of multi-dimensional data, such as\nimages and video. However, the use of self-attention layers in a Transformer\nincurs prohibitive compute and memory complexity that scales quadratically\nw.r.t. the sequence length. A recent architecture, Mamba, based on state space\nmodels has been shown to achieve comparable performance for modeling text\nsequences, while scaling linearly with the sequence length. In this work, we\npresent Mamba-ND, a generalized design extending the Mamba architecture to\narbitrary multi-dimensional data. Our design alternatively unravels the input\ndata across different dimensions following row-major orderings. We provide a\nsystematic comparison of Mamba-ND with several other alternatives, based on\nprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.\nEmpirically, we show that Mamba-ND demonstrates performance competitive with\nthe state-of-the-art on a variety of multi-dimensional benchmarks, including\nImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather\nforecasting.\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2402.05892v4.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.13218v1","updated":"2024-03-20T00:37:19Z","published":"2024-03-20T00:37:19Z","title":"Self-Attention Based Semantic Decomposition in Vector Symbolic\n  Architectures","summary":"  Vector Symbolic Architectures (VSAs) have emerged as a novel framework for\nenabling interpretable machine learning algorithms equipped with the ability to\nreason and explain their decision processes. The basic idea is to represent\ndiscrete information through high dimensional random vectors. Complex data\nstructures can be built up with operations over vectors such as the \"binding\"\noperation involving element-wise vector multiplication, which associates data\ntogether. The reverse task of decomposing the associated elements is a\ncombinatorially hard task, with an exponentially large search space. The main\nalgorithm for performing this search is the resonator network, inspired by\nHopfield network-based memory search operations.\n  In this work, we introduce a new variant of the resonator network, based on\nself-attention based update rules in the iterative search problem. This update\nrule, based on the Hopfield network with log-sum-exp energy function and\nnorm-bounded states, is shown to substantially improve the performance and rate\nof convergence. As a result, our algorithm enables a larger capacity for\nassociative memory, enabling applications in many tasks like perception based\npattern recognition, scene decomposition, and object reasoning. We substantiate\nour algorithm with a thorough evaluation and comparisons to baselines.\n","authors":["Calvin Yeung","Prathyush Poduval","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2403.13218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13214v1","updated":"2024-03-20T00:23:42Z","published":"2024-03-20T00:23:42Z","title":"Nellie: Automated organelle segmentation, tracking, and hierarchical\n  feature extraction in 2D/3D live-cell microscopy","summary":"  The analysis of dynamic organelles remains a formidable challenge, though key\nto understanding biological processes. We introduce Nellie, an automated and\nunbiased pipeline for segmentation, tracking, and feature extraction of diverse\nintracellular structures. Nellie adapts to image metadata, eliminating user\ninput. Nellie's preprocessing pipeline enhances structural contrast on multiple\nintracellular scales allowing for robust hierarchical segmentation of\nsub-organellar regions. Internal motion capture markers are generated and\ntracked via a radius-adaptive pattern matching scheme, and used as guides for\nsub-voxel flow interpolation. Nellie extracts a plethora of features at\nmultiple hierarchical levels for deep and customizable analysis. Nellie\nfeatures a Napari-based GUI that allows for code-free operation and\nvisualization, while its modular open-source codebase invites customization by\nexperienced users. We demonstrate Nellie's wide variety of use cases with two\nexamples: unmixing multiple organelles from a single channel using\nfeature-based classification and training an unsupervised graph autoencoder on\nmitochondrial multi-mesh graphs to quantify latent space embedding changes\nfollowing ionomycin treatment.\n","authors":["Austin E. Y. T. Lefebvre","Gabriel Sturm","Ting-Yu Lin","Emily Stoops","Magdalena Preciado Lopez","Benjamin Kaufmann-Malaga","Kayley Hake"],"pdf_url":"https://arxiv.org/pdf/2403.13214v1.pdf","comment":"for associated code, see https://github.com/aelefebv/nellie; 82\n  pages, 5 main figures, 11 extended figures"},{"id":"http://arxiv.org/abs/2312.14115v2","updated":"2024-03-20T00:23:39Z","published":"2023-12-21T18:40:34Z","title":"LingoQA: Video Question Answering for Autonomous Driving","summary":"  Autonomous driving has long faced a challenge with public acceptance due to\nthe lack of explainability in the decision-making process. Video\nquestion-answering (QA) in natural language provides the opportunity for\nbridging this gap. Nonetheless, evaluating the performance of Video QA models\nhas proved particularly tough due to the absence of comprehensive benchmarks.\nTo fill this gap, we introduce LingoQA, a benchmark specifically for autonomous\ndriving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman\ncorrelation coefficient with human evaluations. We introduce a Video QA dataset\nof central London consisting of 419k samples that we release with the paper. We\nestablish a baseline vision-language model and run extensive ablation studies\nto understand its performance.\n","authors":["Ana-Maria Marcu","Long Chen","Jan Hünermann","Alice Karnsund","Benoit Hanotte","Prajwal Chidananda","Saurabh Nair","Vijay Badrinarayanan","Alex Kendall","Jamie Shotton","Elahe Arani","Oleg Sinavski"],"pdf_url":"https://arxiv.org/pdf/2312.14115v2.pdf","comment":"Benchmark and dataset are available at\n  https://github.com/wayveai/LingoQA/"},{"id":"http://arxiv.org/abs/2212.02477v3","updated":"2024-03-20T00:20:34Z","published":"2022-12-05T18:37:41Z","title":"Malaria Parasitic Detection using a New Deep Boosted and Ensemble\n  Learning Framework","summary":"  Malaria is a potentially fatal plasmodium parasite injected by female\nanopheles mosquitoes that infect red blood cells and millions worldwide yearly.\nHowever, specialists' manual screening in clinical practice is laborious and\nprone to error. Therefore, a novel Deep Boosted and Ensemble Learning (DBEL)\nframework, comprising the stacking of new Boosted-BR-STM convolutional neural\nnetworks (CNN) and the ensemble ML classifiers, is developed to screen malaria\nparasite images. The proposed Boosted-BR-STM is based on a new\ndilated-convolutional block-based split transform merge (STM) and feature-map\nSqueezing-Boosting (SB) ideas. Moreover, the new STM block uses regional and\nboundary operations to learn the malaria parasite's homogeneity, heterogeneity,\nand boundary with patterns. Furthermore, the diverse boosted channels are\nattained by employing Transfer Learning-based new feature-map SB in STM blocks\nat the abstract, medium, and conclusion levels to learn minute intensity and\ntexture variation of the parasitic pattern. The proposed DBEL framework\nimplicates the stacking of prominent and diverse boosted channels and provides\nthe generated discriminative features of the developed Boosted-BR-STM to the\nensemble of ML classifiers. The proposed framework improves the discrimination\nability and generalization of ensemble learning. Moreover, the deep feature\nspaces of the developed Boosted-BR-STM and customized CNNs are fed into ML\nclassifiers for comparative analysis. The proposed DBEL framework outperforms\nthe existing techniques on the NIH malaria dataset that are enhanced using\ndiscrete wavelet transform to enrich feature space. The proposed DBEL framework\nachieved Accuracy (98.50%), Sensitivity (0.9920), F-score (0.9850), and AUC\n(0.997), which suggest it to be utilized for malaria parasite screening.\n","authors":["Saddam Hussain Khan","Tahani Jaser Alahmadi"],"pdf_url":"https://arxiv.org/pdf/2212.02477v3.pdf","comment":"26 pages, 10 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2312.06071v2","updated":"2024-03-20T00:12:22Z","published":"2023-12-11T02:38:07Z","title":"Precipitation Downscaling with Spatiotemporal Video Diffusion","summary":"  In climate science and meteorology, high-resolution local precipitation (rain\nand snowfall) predictions are limited by the computational costs of\nsimulation-based methods. Statistical downscaling, or super-resolution, is a\ncommon workaround where a low-resolution prediction is improved using\nstatistical approaches. Unlike traditional computer vision tasks, weather and\nclimate applications require capturing the accurate conditional distribution of\nhigh-resolution given low-resolution patterns to assure reliable ensemble\naverages and unbiased estimates of extreme events, such as heavy rain. This\nwork extends recent video diffusion models to precipitation super-resolution,\nemploying a deterministic downscaler followed by a temporally-conditioned\ndiffusion model to capture noise characteristics and high-frequency patterns.\nWe test our approach on FV3GFS output, an established large-scale global\natmosphere model, and compare it against five state-of-the-art baselines. Our\nanalysis, capturing CRPS, MSE, precipitation distributions, and qualitative\naspects using California and the Himalayas as examples, establishes our method\nas a new standard for data-driven precipitation downscaling.\n","authors":["Prakhar Srivastava","Ruihan Yang","Gavin Kerrigan","Gideon Dresdner","Jeremy McGibbon","Christopher Bretherton","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2312.06071v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2403.13806v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS","summary":"  Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.\n","authors":["Michael Niemeyer","Fabian Manhardt","Marie-Julie Rakotosaona","Michael Oechsle","Daniel Duckworth","Rama Gosula","Keisuke Tateno","John Bates","Dominik Kaeser","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2403.13806v1.pdf","comment":"Project page at https://m-niemeyer.github.io/radsplat/"},{"id":"http://arxiv.org/abs/2312.06644v2","updated":"2024-03-20T17:58:05Z","published":"2023-12-11T18:56:37Z","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","summary":"  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n","authors":["Rao Fu","Zehao Wen","Zichen Liu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.06644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11868v2","updated":"2024-03-20T15:22:12Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16759v4","updated":"2024-03-20T05:00:06Z","published":"2023-05-26T09:21:56Z","title":"StyleHumanCLIP: Text-guided Garment Manipulation for StyleGAN-Human","summary":"  This paper tackles text-guided control of StyleGAN for editing garments in\nfull-body human images. Existing StyleGAN-based methods suffer from handling\nthe rich diversity of garments and body shapes and poses. We propose a\nframework for text-guided full-body human image synthesis via an\nattention-based latent code mapper, which enables more disentangled control of\nStyleGAN than existing mappers. Our latent code mapper adopts an attention\nmechanism that adaptively manipulates individual latent codes on different\nStyleGAN layers under text guidance. In addition, we introduce feature-space\nmasking at inference time to avoid unwanted changes caused by text inputs. Our\nquantitative and qualitative evaluations reveal that our method can control\ngenerated images more faithfully to given texts than existing methods.\n","authors":["Takato Yoshikawa","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2305.16759v4.pdf","comment":"VISIAPP 2024, project page:\n  https://www.cgg.cs.tsukuba.ac.jp/~yoshikawa/pub/style_human_clip/"},{"id":"http://arxiv.org/abs/2311.16581v2","updated":"2024-03-20T00:59:44Z","published":"2023-11-28T07:55:25Z","title":"GeoScaler: Geometry and Rendering-Aware Downsampling of 3D Mesh Textures","summary":"  High-resolution texture maps are necessary for representing real-world\nobjects accurately with 3D meshes. The large sizes of textures can bottleneck\nthe real-time rendering of high-quality virtual 3D scenes on devices having low\ncomputational budgets and limited memory. Downsampling the texture maps\ndirectly addresses the issue, albeit at the cost of visual fidelity.\nTraditionally, downsampling of texture maps is performed using methods like\nbicubic interpolation and the Lanczos algorithm. These methods ignore the\ngeometric layout of the mesh and its UV parametrization and also do not account\nfor the rendering process used to obtain the final visualization that the users\nwill experience. Towards filling these gaps, we introduce GeoScaler, which is a\nmethod of downsampling texture maps of 3D meshes while incorporating geometric\ncues, and by maximizing the visual fidelity of the rendered views of the\ntextured meshes. We show that the textures generated by GeoScaler deliver\nsignificantly better quality rendered images compared to those generated by\ntraditional downsampling methods\n","authors":["Sai Karthikey Pentapati","Anshul Rai","Arkady Ten","Chaitanya Atluru","Alan Bovik"],"pdf_url":"https://arxiv.org/pdf/2311.16581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05594v2","updated":"2024-03-20T20:39:27Z","published":"2024-03-07T04:33:42Z","title":"An Image-based Typology for Visualization","summary":"  We present and discuss the results of a qualitative analysis of visual\nrepresentations from images. We labeled each image's essential stimuli, the\nremoval of which would render a visualization uninterpretable. As a result, we\nderive a typology of 10 visualization types of defined groups. We describe the\ntypology derivation process in which we engaged. The resulting typology and\nimage analysis can serve a number of purposes: enabling researchers to study\nthe evolution of the community and its research output over time, facilitating\nthe categorization of visualization images for the purpose of research and\nteaching, allowing researchers and practitioners to identify visual design\nstyles to further align the quantification of any visual information processor,\nbe that a person or an algorithm observer, and it facilitates a discussion of\nstandardization in visualization. In addition to the visualization typology\nfrom images, we provide a dataset of 6,833 tagged images and an online tool\nthat can be used to explore and analyze the large set of labeled images. The\ntool and data set enable scholars to closely examine the diverse visual designs\nused and how they are published and communicated in our community. A\npre-registration, a free copy of this paper, and all supplemental materials are\navailable via osf.io/dxjwt.\n","authors":["Jian Chen","Petra Isenberg","Robert S. Laramee","Tobias Isenberg","Michael Sedlmair","Torsten Moeller","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2403.05594v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.07533"},{"id":"http://arxiv.org/abs/2403.13924v1","updated":"2024-03-20T18:54:52Z","published":"2024-03-20T18:54:52Z","title":"LFS-Aware Surface Reconstruction from Unoriented 3D Point Clouds","summary":"  We present a novel approach for generating isotropic surface triangle meshes\ndirectly from unoriented 3D point clouds, with mesh density adapting to the\nestimated local feature size (LFS). The popular reconstruction pipelines first\nreconstruct a dense mesh from the input point cloud and then apply remeshing to\nobtain the isotropic mesh. The sequential pipeline makes it hard to find a\nlower-density mesh while preserving more details. Instead, our approach\nreconstructs both an implicit function and an LFS-aware mesh sizing function\ndirectly from the input point cloud, which is then used to produce the final\nLFS-aware mesh without remeshing. We combine local curvature radius and shape\ndiameter to estimate the LFS directly from the input point clouds. Also, we\npropose a new mesh solver to solve an implicit function whose zero level set\ndelineates the surface without requiring normal orientation. The added value of\nour approach is generating isotropic meshes directly from 3D point clouds with\nan LFS-aware density, thus enabling flexible mesh quality control. Our\nexperiments demonstrate the robustness of our method to noise, outliers, and\nmissing data. Our method is also capable of preserving sharp features for CAD\npoint clouds.\n","authors":["Rao Fu","Kai Hormann","Pierre Alliez"],"pdf_url":"https://arxiv.org/pdf/2403.13924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13910v1","updated":"2024-03-20T18:30:12Z","published":"2024-03-20T18:30:12Z","title":"Augmented Reality Demonstrations for Scalable Robot Imitation Learning","summary":"  Robot Imitation Learning (IL) is a widely used method for training robots to\nperform manipulation tasks that involve mimicking human demonstrations to\nacquire skills. However, its practicality has been limited due to its\nrequirement that users be trained in operating real robot arms to provide\ndemonstrations. This paper presents an innovative solution: an Augmented\nReality (AR)-assisted framework for demonstration collection, empowering\nnon-roboticist users to produce demonstrations for robot IL using devices like\nthe HoloLens 2. Our framework facilitates scalable and diverse demonstration\ncollection for real-world tasks. We validate our approach with experiments on\nthree classical robotics tasks: reach, push, and pick-and-place. The real robot\nperforms each task successfully while replaying demonstrations collected via\nAR.\n","authors":["Yue Yang","Bryce Ikeda","Gedas Bertasius","Daniel Szafir"],"pdf_url":"https://arxiv.org/pdf/2403.13910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14723v1","updated":"2024-03-20T19:55:17Z","published":"2024-03-20T19:55:17Z","title":"GPolylla: Fully GPU-accelerated polygonal mesh generator","summary":"  This work presents a fully GPU-accelerated algorithm for the polygonal mesh\ngenerator known as Polylla. Polylla is a tri-to-polygon mesh generator, which\nbenefits from the half-edge data structure to manage any polygonal shape. The\nproposed parallel algorithm introduces a novel approach to modify\ntriangulations to get polygonal meshes using the half-edge data structure in\nparallel on the GPU. By changing the adjacency values of each half-edge, the\nalgorithm accomplish to unlink half-edges that are not used in the new\npolygonal mesh without the need neither removing nor allocating new memory in\nthe GPU. The experimental results show a speedup, reaching up to $\\times 83.2$\nwhen compared to the CPU sequential implementation. Additionally, the speedup\nis $\\times 746.8$ when the cost of copying the data structure from the host\ndevice and back is not included.\n","authors":["Sergio Salinas-Fernández","Nancy Hitschfeld-Kahler","Roberto Carrasco"],"pdf_url":"https://arxiv.org/pdf/2403.14723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15476v1","updated":"2024-03-20T17:29:58Z","published":"2024-03-20T17:29:58Z","title":"Learning to Infer Generative Template Programs for Visual Concepts","summary":"  People grasp flexible visual concepts from a few examples. We explore a\nneurosymbolic system that learns how to infer programs that capture visual\nconcepts in a domain-general fashion. We introduce Template Programs:\nprogrammatic expressions from a domain-specific language that specify\nstructural and parametric patterns common to an input concept. Our framework\nsupports multiple concept-related tasks, including few-shot generation and\nco-segmentation through parsing. We develop a learning paradigm that allows us\nto train networks that infer Template Programs directly from visual datasets\nthat contain concept groupings. We run experiments across multiple visual\ndomains: 2D layouts, Omniglot characters, and 3D shapes. We find that our\nmethod outperforms task-specific alternatives, and performs competitively\nagainst domain-specific approaches for the limited domains where they exist.\n","authors":["R. Kenny Jones","Siddhartha Chaudhuri","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2403.15476v1.pdf","comment":null}]},"2024-03-21T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.14626v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer","summary":"  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n","authors":["Tianye Ding","Hongyu Li","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14626v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2403.14605v1","updated":"2024-03-21T17:54:56Z","published":"2024-03-21T17:54:56Z","title":"SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under\n  Control Constraints","summary":"  The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR\nBRT), which is a multi-query algorithm for planning of dynamic systems under\nstochastic motion uncertainty and constraints on the control input with\nexplicit coverage guarantees. In contrast to existing roadmap-based\nprobabilistic planning methods that sample belief nodes randomly and draw edges\nbetween them \\cite{csbrm_tro2024}, under control constraints, the reachability\nof belief nodes needs to be explicitly established and is determined by\nchecking the feasibility of a non-convex program. Moreover, there is no\nexplicit consideration of coverage of the roadmap while adding nodes and edges\nduring the construction procedure for the existing methods. Our contribution is\na novel optimization formulation to add nodes and construct the corresponding\nedge controllers such that the generated roadmap results in provably maximal\ncoverage under control constraints as compared to any other method of adding\nnodes and edges. We characterize formally the notion of coverage of a roadmap\nin this stochastic domain via introduction of the h-$\\operatorname{BRS}$\n(Backward Reachable Set of Distributions) of a tree of distributions under\ncontrol constraints, and also support our method with extensive simulations on\na 6 DoF model.\n","authors":["Naman Aggarwal","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2403.14605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14597v1","updated":"2024-03-21T17:50:22Z","published":"2024-03-21T17:50:22Z","title":"Extended Reality for Enhanced Human-Robot Collaboration: a\n  Human-in-the-Loop Approach","summary":"  The rise of automation has provided an opportunity to achieve higher\nefficiency in manufacturing processes, yet it often compromises the flexibility\nrequired to promptly respond to evolving market needs and meet the demand for\ncustomization. Human-robot collaboration attempts to tackle these challenges by\ncombining the strength and precision of machines with human ingenuity and\nperceptual understanding. In this paper, we conceptualize and propose an\nimplementation framework for an autonomous, machine learning-based manipulator\nthat incorporates human-in-the-loop principles and leverages Extended Reality\n(XR) to facilitate intuitive communication and programming between humans and\nrobots. Furthermore, the conceptual framework foresees human involvement\ndirectly in the robot learning process, resulting in higher adaptability and\ntask generalization. The paper highlights key technologies enabling the\nproposed framework, emphasizing the importance of developing the digital\necosystem as a whole. Additionally, we review the existent implementation\napproaches of XR in human-robot collaboration, showcasing diverse perspectives\nand methodologies. The challenges and future outlooks are discussed, delving\ninto the major obstacles and potential research avenues of XR for more natural\nhuman-robot interaction and integration in the industrial landscape.\n","authors":["Yehor Karpichev","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2403.14597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14594v1","updated":"2024-03-21T17:49:26Z","published":"2024-03-21T17:49:26Z","title":"VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition","summary":"  Recent works on the global place recognition treat the task as a retrieval\nproblem, where an off-the-shelf global descriptor is commonly designed in\nimage-based and LiDAR-based modalities. However, it is non-trivial to perform\naccurate image-LiDAR global place recognition since extracting consistent and\nrobust global descriptors from different domains (2D images and 3D point\nclouds) is challenging. To address this issue, we propose a novel\nVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel\ncorrespondences in a self-supervised manner and brings them into a shared\nfeature space. Specifically, VXP is trained in a two-stage manner that first\nexplicitly exploits local feature correspondences and enforces similarity of\nglobal descriptors. Extensive experiments on the three benchmarks (Oxford\nRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the\nstate-of-the-art cross-modal retrieval by a large margin.\n","authors":["Yun-Jin Li","Mariia Gladkova","Yan Xia","Rui Wang","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2403.14594v1.pdf","comment":"Project page https://yunjinli.github.io/projects-vxp/"},{"id":"http://arxiv.org/abs/2403.14583v1","updated":"2024-03-21T17:37:43Z","published":"2024-03-21T17:37:43Z","title":"Co-Optimization of Environment and Policies for Decentralized\n  Multi-Agent Navigation","summary":"  This work views the multi-agent system and its surrounding environment as a\nco-evolving system, where the behavior of one affects the other. The goal is to\ntake both agent actions and environment configurations as decision variables,\nand optimize these two components in a coordinated manner to improve some\nmeasure of interest. Towards this end, we consider the problem of decentralized\nmulti-agent navigation in cluttered environments. By introducing two\nsub-objectives of multi-agent navigation and environment optimization, we\npropose an $\\textit{agent-environment co-optimization}$ problem and develop a\n$\\textit{coordinated algorithm}$ that alternates between these sub-objectives\nto search for an optimal synthesis of agent actions and obstacle configurations\nin the environment; ultimately, improving the navigation performance. Due to\nthe challenge of explicitly modeling the relation between agents, environment\nand performance, we leverage policy gradient to formulate a model-free learning\nmechanism within the coordinated framework. A formal convergence analysis shows\nthat our coordinated algorithm tracks the local minimum trajectory of an\nassociated time-varying non-convex optimization problem. Extensive numerical\nresults corroborate theoretical findings and show the benefits of\nco-optimization over baselines. Interestingly, the results also indicate that\noptimized environment configurations are able to offer structural guidance that\nis key to de-conflicting agents in motion.\n","authors":["Zhan Gao","Guang Yang","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2403.14583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01477v2","updated":"2024-03-21T17:31:00Z","published":"2024-02-02T15:06:00Z","title":"A Modular Aerial System Based on Homogeneous Quadrotors with\n  Fault-Tolerant Control","summary":"  The standard quadrotor is one of the most popular and widely used aerial\nvehicle of recent decades, offering great maneuverability with mechanical\nsimplicity. However, the under-actuation characteristic limits its\napplications, especially when it comes to generating desired wrench with six\ndegrees of freedom (DOF). Therefore, existing work often compromises between\nmechanical complexity and the controllable DOF of the aerial system. To take\nadvantage of the mechanical simplicity of a standard quadrotor, we propose a\nmodular aerial system, IdentiQuad, that combines only homogeneous\nquadrotor-based modules. Each IdentiQuad can be operated alone like a standard\nquadrotor, but at the same time allows task-specific assembly, increasing the\ncontrollable DOF of the system. Each module is interchangeable within its\nassembly. We also propose a general controller for different configurations of\nassemblies, capable of tolerating rotor failures and balancing the energy\nconsumption of each module. The functionality and robustness of the system and\nits controller are validated using physics-based simulations for different\nassembly configurations.\n","authors":["Mengguang Li","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2402.01477v2.pdf","comment":"ICRA2024"},{"id":"http://arxiv.org/abs/2403.14545v1","updated":"2024-03-21T16:44:49Z","published":"2024-03-21T16:44:49Z","title":"Learning Hierarchical Control For Constrained Dynamic Task Assignment","summary":"  This paper introduces a novel data-driven hierarchical control scheme for\nmanaging a fleet of nonlinear, capacity-constrained autonomous agents in an\niterative environment. We propose a control framework consisting of a\nhigh-level dynamic task assignment and routing layer and low-level motion\nplanning and tracking layer. Each layer of the control hierarchy uses a\ndata-driven MPC policy, maintaining bounded computational complexity at each\ncalculation of a new task assignment or actuation input. We utilize collected\ndata to iteratively refine estimates of agent capacity usage, and update MPC\npolicy parameters accordingly. Our approach leverages tools from iterative\nlearning control to integrate learning at both levels of the hierarchy, and\ncoordinates learning between levels in order to maintain closed-loop\nfeasibility and performance improvement of the connected architecture.\n","authors":["Charlott Vallon","Alessandro Pinto","Bartolomeo Stellato","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2403.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17587v2","updated":"2024-03-21T16:40:43Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).\nOur code will be made publicly available at https://github.com/XiaohanLei/IEVE.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14526v1","updated":"2024-03-21T16:26:19Z","published":"2024-03-21T16:26:19Z","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion\n  Descriptors","summary":"  Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp\n","authors":["Nikolaos Tsagkas","Jack Rome","Subramanian Ramamoorthy","Oisin Mac Aodha","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.14526v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14488v1","updated":"2024-03-21T15:36:26Z","published":"2024-03-21T15:36:26Z","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action\n  Selection in Robot Manipulation Tasks","summary":"  Safe and efficient object manipulation is a key enabler of many real-world\nrobot applications. However, this is challenging because robot operation must\nbe robust to a range of sensor and actuator uncertainties. In this paper, we\npresent a physics-informed causal-inference-based framework for a robot to\nprobabilistically reason about candidate actions in a block stacking task in a\npartially observable setting. We integrate a physics-based simulation of the\nrigid-body system dynamics with a causal Bayesian network (CBN) formulation to\ndefine a causal generative probabilistic model of the robot decision-making\nprocess. Using simulation-based Monte Carlo experiments, we demonstrate our\nframework's ability to successfully: (1) predict block tower stability with\nhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best\naction for the block stacking task, for execution by an integrated robot\nsystem, achieving 94.2% task success rate. We also demonstrate our framework's\nsuitability for real-world robot systems by demonstrating successful task\nexecutions with a domestic support robot, with perception and manipulation\nsub-system integration. Hence, we show that by embedding physics-based causal\nreasoning into robots' decision-making processes, we can make robot task\nexecution safer, more reliable, and more robust to various types of\nuncertainty.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v1.pdf","comment":"8 pages, 9 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2304.03133v2","updated":"2024-03-21T15:18:20Z","published":"2023-04-06T15:06:28Z","title":"Deep learning reduces sensor requirements for gust rejection on a small\n  uncrewed aerial vehicle morphing wing","summary":"  There is a growing need for uncrewed aerial vehicles (UAVs) to operate in\ncities. However, the uneven urban landscape and complex street systems cause\nlarge-scale wind gusts that challenge the safe and effective operation of UAVs.\nCurrent gust alleviation methods rely on traditional control surfaces and\ncomputationally expensive modeling to select a control action, leading to a\nslower response. Here, we used deep reinforcement learning to create an\nautonomous gust alleviation controller for a camber-morphing wing. This method\nreduced gust impact by 84%, directly from real-time, on-board pressure signals.\nNotably, we found that gust alleviation using signals from only three pressure\ntaps was statistically indistinguishable from using six signals. This\nreduced-sensor fly-by-feel control opens the door to UAV missions in previously\ninoperable locations.\n","authors":["Kevin PT. Haughn","Christina Harvey","Daniel J. Inman"],"pdf_url":"https://arxiv.org/pdf/2304.03133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12001v3","updated":"2024-03-21T15:07:30Z","published":"2023-09-21T12:16:32Z","title":"Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots","summary":"  As non-humanoid robots increasingly permeate various sectors, understanding\ntheir design implications for human acceptance becomes paramount. Despite their\nubiquity, studies on how to improve human interaction are sparse. Our\ninvestigation, conducted through two surveys, addresses this gap. The first\nsurvey emphasizes non-humanoid robots and human perceptions about gender\nattributions, suggesting that both design and perceived gender influence\nacceptance. Survey 2 investigates the effects of varying gender cues on robot\ndesigns and their consequent impacts on human-robot interactions. Our findings\nhighlighted that distinct gender cues can bolster or impede interaction\ncomfort.\n","authors":["Mahya Ramezani","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2309.12001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14449v1","updated":"2024-03-21T14:56:46Z","published":"2024-03-21T14:56:46Z","title":"Bringing Robots Home: The Rise of AI Robots in Consumer Electronics","summary":"  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose\nmultimodal generative AI model designed specifically for training humanoid\nrobots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid\nrobot on December 12, 2023, underscored the profound impact robotics is poised\nto have on reshaping various facets of our daily lives. While robots have long\ndominated industrial settings, their presence within our homes is a burgeoning\nphenomenon. This can be attributed, in part, to the complexities of domestic\nenvironments and the challenges of creating robots that can seamlessly\nintegrate into our daily routines.\n","authors":["Haiwei Dong","Yang Liu","Ted Chu","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.14449v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2403.14447v1","updated":"2024-03-21T14:53:50Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and \\spot, the quadruped robot\nmanufactured by Boston Dynamics. The key-novelty is the focus on the robot's\nperspective, i.e., on the data captured by the robot's sensors. These make 3D\nbody pose analysis challenging because being close to the ground captures\nhumans only partially. The scenario underlying HARPER includes 15 actions, of\nwhich 10 involve physical contact between the robot and users. The Corpus\ncontains not only the recordings of the built-in stereo cameras of Spot, but\nalso those of a 6-camera OptiTrack system (all recordings are synchronized).\nThis leads to ground-truth skeletal representations with a precision lower than\na millimeter. In addition, the Corpus includes reproducible benchmarks on 3D\nHuman Pose Estimation, Human Pose Forecasting, and Collision Prediction, all\nbased on publicly available baseline approaches. This enables future HARPER\nusers to rigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro. Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14414v1","updated":"2024-03-21T13:59:32Z","published":"2024-03-21T13:59:32Z","title":"Efficient Model Learning and Adaptive Tracking Control of Magnetic\n  Micro-Robots for Non-Contact Manipulation","summary":"  Magnetic microrobots can be navigated by an external magnetic field to\nautonomously move within living organisms with complex and unstructured\nenvironments. Potential applications include drug delivery, diagnostics, and\ntherapeutic interventions. Existing techniques commonly impart magnetic\nproperties to the target object,or drive the robot to contact and then\nmanipulate the object, both probably inducing physical damage. This paper\nconsiders a non-contact formulation, where the robot spins to generate a\nrepulsive field to push the object without physical contact. Under such a\nformulation, the main challenge is that the motion model between the input of\nthe magnetic field and the output velocity of the target object is commonly\nunknown and difficult to analyze. To deal with it, this paper proposes a\ndata-driven-based solution. A neural network is constructed to efficiently\nestimate the motion model. Then, an approximate model-based optimal control\nscheme is developed to push the object to track a time-varying trajectory,\nmaintaining the non-contact with distance constraints. Furthermore, a\nstraightforward planner is introduced to assess the adaptability of non-contact\nmanipulation in a cluttered unstructured environment. Experimental results are\npresented to show the tracking and navigation performance of the proposed\nscheme.\n","authors":["Yongyi Jia","Shu Miao","Junjian Zhou","Niandong Jiao","Lianqing Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.14414v1.pdf","comment":"7 pages, 6 figures, received by 2024 IEEE International Conference on\n  Robotics and Automation"},{"id":"http://arxiv.org/abs/2402.16348v2","updated":"2024-03-21T13:58:30Z","published":"2024-02-26T07:02:05Z","title":"Star-Searcher: A Complete and Efficient Aerial System for Autonomous\n  Target Search in Complex Unknown Environments","summary":"  This paper tackles the challenge of autonomous target search using unmanned\naerial vehicles (UAVs) in complex unknown environments. To fill the gap in\nsystematic approaches for this task, we introduce Star-Searcher, an aerial\nsystem featuring specialized sensor suites, mapping, and planning modules to\noptimize searching. Path planning challenges due to increased inspection\nrequirements are addressed through a hierarchical planner with a\nvisibility-based viewpoint clustering method. This simplifies planning by\nbreaking it into global and local sub-problems, ensuring efficient global and\nlocal path coverage in real-time. Furthermore, our global path planning employs\na history-aware mechanism to reduce motion inconsistency from frequent map\nchanges, significantly enhancing search efficiency. We conduct comparisons with\nstate-of-the-art methods in both simulation and the real world, demonstrating\nshorter flight paths, reduced time, and higher target search completeness. Our\napproach will be open-sourced for community benefit at\nhttps://github.com/SYSU-STAR/STAR-Searcher.\n","authors":["Yiming Luo","Zixuan Zhuang","Neng Pan","Chen Feng","Shaojie Shen","Fei Gao","Hui Cheng","Boyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.16348v2.pdf","comment":"Aceepted to IEEE RA-L. Code:\n  https://github.com/SYSU-STAR/STAR-Searcher. Video:\n  https://www.youtube.com/watch?v=08ll_oo_DtU"},{"id":"http://arxiv.org/abs/2401.15174v2","updated":"2024-03-21T13:16:13Z","published":"2024-01-26T19:39:33Z","title":"Large Language Models for Multi-Modal Human-Robot Interaction","summary":"  This paper presents an innovative large language model (LLM)-based robotic\nsystem for enhancing multi-modal human-robot interaction (HRI). Traditional HRI\nsystems relied on complex designs for intent estimation, reasoning, and\nbehavior generation, which were resource-intensive. In contrast, our system\nempowers researchers and practitioners to regulate robot behavior through three\nkey aspects: providing high-level linguistic guidance, creating \"atomics\" for\nactions and expressions the robot can use, and offering a set of examples.\nImplemented on a physical robot, it demonstrates proficiency in adapting to\nmulti-modal inputs and determining the appropriate manner of action to assist\nhumans with its arms, following researchers' defined guidelines.\nSimultaneously, it coordinates the robot's lid, neck, and ear movements with\nspeech output to produce dynamic, multi-modal expressions. This showcases the\nsystem's potential to revolutionize HRI by shifting from conventional, manual\nstate-and-flow design methods to an intuitive, guidance-based, and\nexample-driven approach.\n","authors":["Chao Wang","Stephan Hasler","Daniel Tanneberg","Felix Ocker","Frank Joublin","Antonello Ceravola","Joerg Deigmoeller","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2401.15174v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.16899v2","updated":"2024-03-21T12:56:24Z","published":"2024-01-30T11:05:43Z","title":"MAkEable: Memory-centered and Affordance-based Task Execution Framework\n  for Transferable Mobile Manipulation Skills","summary":"  To perform versatile mobile manipulation tasks in human-centered\nenvironments, the ability to efficiently transfer learned tasks and experiences\nfrom one robot to another or across different environments is key. In this\npaper, we present MAkEable, a versatile uni- and multi-manual mobile\nmanipulation framework that facilitates the transfer of capabilities and\nknowledge across different tasks, environments, and robots. Our framework\nintegrates an affordance-based task description into the memory-centric\ncognitive architecture of the ARMAR humanoid robot family, which supports the\nsharing of experiences and demonstrations for transfer learning. By\nrepresenting mobile manipulation actions through affordances, i.e., interaction\npossibilities of the robot with its environment, we provide a unifying\nframework for the autonomous uni- and multi-manual manipulation of known and\nunknown objects in various environments. We demonstrate the applicability of\nthe framework in real-world experiments for multiple robots, tasks, and\nenvironments. This includes grasping known and unknown objects, object placing,\nbimanual object grasping, memory-enabled skill transfer in a drawer opening\nscenario across two different humanoid robots, and a pouring task learned from\nhuman demonstration.\n","authors":["Christoph Pohl","Fabian Reister","Fabian Peller-Konrad","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2401.16899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14353v1","updated":"2024-03-21T12:28:44Z","published":"2024-03-21T12:28:44Z","title":"DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video\n  Analytics","summary":"  Deep neural network (DNN) video analytics is crucial for autonomous systems\nsuch as self-driving vehicles, unmanned aerial vehicles (UAVs), and security\nrobots. However, real-world deployment faces challenges due to their limited\ncomputational resources and battery power. To tackle these challenges,\ncontinuous learning exploits a lightweight \"student\" model at deployment\n(inference), leverages a larger \"teacher\" model for labeling sampled data\n(labeling), and continuously retrains the student model to adapt to changing\nscenarios (retraining). This paper highlights the limitations in\nstate-of-the-art continuous learning systems: (1) they focus on computations\nfor retraining, while overlooking the compute needs for inference and labeling,\n(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous\nsystems, and (3) they are located on a remote centralized server, intended for\nmulti-tenant scenarios, again unsuitable for autonomous systems due to privacy,\nnetwork availability, and latency concerns. We propose a hardware-algorithm\nco-designed solution for continuous learning, DaCapo, that enables autonomous\nsystems to perform concurrent executions of inference, labeling, and training\nin a performant and energy-efficient manner. DaCapo comprises (1) a\nspatially-partitionable and precision-flexible accelerator enabling parallel\nexecution of kernels on sub-accelerators at their respective precisions, and\n(2) a spatiotemporal resource allocation algorithm that strategically navigates\nthe resource-accuracy tradeoff space, facilitating optimal decisions for\nresource allocation to achieve maximal accuracy. Our evaluation shows that\nDaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based\ncontinuous learning systems, Ekya and EOMU, respectively, while consuming 254x\nless power.\n","authors":["Yoonsung Kim","Changhun Oh","Jinwoo Hwang","Wonung Kim","Seongryong Oh","Yubin Lee","Hardik Sharma","Amir Yazdanbakhsh","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2403.14353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14347v1","updated":"2024-03-21T12:24:01Z","published":"2024-03-21T12:24:01Z","title":"A Comparative Study of Real-Time Implementable Cooperative Aerial\n  Manipulation Systems","summary":"  This survey paper focuses on quadrotor- and multirotor- based cooperative\naerial manipulation. Emphasis is first given on comparing and evaluating\nprototype systems that have been implemented and tested in real-time in diverse\napplication environments. Underlying modeling and control approaches are also\ndiscussed and compared. The outcome of the survey allows for understanding the\nmotivation and rationale to develop such systems, their applicability and\nimplementability in diverse applications and also challenges that need to be\naddressed and overcome. Moreover, the survey provides a guide to develop the\nnext generation of prototype systems based on preferred characteristics,\nfunctionality, operability and application domain.\n","authors":["Stamatina C. Barakou","Costas S. Tzafestas","Kimon P. Valavanis"],"pdf_url":"https://arxiv.org/pdf/2403.14347v1.pdf","comment":"Submitted to MDPI Drones"},{"id":"http://arxiv.org/abs/2403.14344v1","updated":"2024-03-21T12:22:47Z","published":"2024-03-21T12:22:47Z","title":"Tell Me What You Want (What You Really, Really Want): Addressing the\n  Expectation Gap for Goal Conveyance from Humans to Robots","summary":"  Conveying human goals to autonomous systems (AS) occurs both when the system\nis being designed and when it is being operated. The design-step conveyance is\ntypically mediated by robotics and AI engineers, who must appropriately capture\nend-user requirements and concepts of operations, while the operation-step\nconveyance is mediated by the design, interfaces, and behavior of the AI.\nHowever, communication can be difficult during both these periods because of\nmismatches in the expectations and expertise of the end-user and the\nroboticist, necessitating more design cycles to resolve. We examine some of the\nbarriers in communicating system design requirements, and develop an\naugmentation for applied cognitive task analysis (ACTA) methods, that we call\nrobot task analysis (RTA), pertaining specifically to the development of\nautonomous systems. Further, we introduce a top-down view of an underexplored\narea of friction between requirements communication -- implied human\nexpectations -- utilizing a collection of work primarily from experimental\npsychology and social sciences. We show how such expectations can be used in\nconjunction with task-specific expectations and the system design process for\nAS to improve design team communication, alleviate barriers to user rejection,\nand reduce the number of design cycles.\n","authors":["Kevin Leahy","Ho Chit Siu"],"pdf_url":"https://arxiv.org/pdf/2403.14344v1.pdf","comment":"Presented at the End-User Development for Human-Robot Interaction\n  (EUD4HRI) workshop at HRI 2024"},{"id":"http://arxiv.org/abs/2403.12670v2","updated":"2024-03-21T12:14:14Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v2.pdf","comment":"Under review. For associated project page, see\n  https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2402.16068v3","updated":"2024-03-21T11:58:49Z","published":"2024-02-25T11:37:23Z","title":"ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot\n  Interaction Applications","summary":"  Deploying robots in human-shared spaces requires understanding interactions\namong nearby agents and objects. Modelling cause-and-effect relations through\ncausal inference aids in predicting human behaviours and anticipating robot\ninterventions. However, a critical challenge arises as existing causal\ndiscovery methods currently lack an implementation inside the ROS ecosystem,\nthe standard de facto in robotics, hindering effective utilisation in robotics.\nTo address this gap, this paper introduces ROS-Causal, a ROS-based framework\nfor onboard data collection and causal discovery in human-robot spatial\ninteractions. An ad-hoc simulator, integrated with ROS, illustrates the\napproach's effectiveness, showcasing the robot onboard generation of causal\nmodels during data collection. ROS-Causal is available on GitHub:\nhttps://github.com/lcastri/roscausal.git.\n","authors":["Luca Castri","Gloria Beraldo","Sariah Mghames","Marc Hanheide","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2402.16068v3.pdf","comment":"Accepted by the \"Causal-HRI: Causal Learning for Human-Robot\n  Interaction\" workshop at the 2024 ACM/IEEE International Conference on\n  Human-Robot Interaction (HRI)"},{"id":"http://arxiv.org/abs/2403.14328v1","updated":"2024-03-21T11:54:45Z","published":"2024-03-21T11:54:45Z","title":"Distilling Reinforcement Learning Policies for Interpretable Robot\n  Locomotion: Gradient Boosting Machines and Symbolic Regression","summary":"  Recent advancements in reinforcement learning (RL) have led to remarkable\nachievements in robot locomotion capabilities. However, the complexity and\n``black-box'' nature of neural network-based RL policies hinder their\ninterpretability and broader acceptance, particularly in applications demanding\nhigh levels of safety and reliability. This paper introduces a novel approach\nto distill neural RL policies into more interpretable forms using Gradient\nBoosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic\nRegression. By leveraging the inherent interpretability of generalized additive\nmodels, decision trees, and analytical expressions, we transform opaque neural\nnetwork policies into more transparent ``glass-box'' models. We train expert\nneural network policies using RL and subsequently distill them into (i) GBMs,\n(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution\nshift challenge of behavioral cloning, we propose to use the Dataset\nAggregation (DAgger) algorithm with a curriculum of episode-dependent\nalternation of actions between expert and distilled policies, to enable\nefficient distillation of feedback control policies. We evaluate our approach\non various robot locomotion gaits -- walking, trotting, bounding, and pacing --\nand study the importance of different observations in joint actions for\ndistilled policies using various methods. We train neural expert policies for\n205 hours of simulated experience and distill interpretable policies with only\n10 minutes of simulated interaction for each gait using the proposed method.\n","authors":["Fernando Acero","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2403.14328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14326v1","updated":"2024-03-21T11:50:00Z","published":"2024-03-21T11:50:00Z","title":"Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests","summary":"  Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\n","authors":["Haedam Oh","Nived Chebrolu","Matias Mattamala","Leonard Freißmuth","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14320v1","updated":"2024-03-21T11:41:39Z","published":"2024-03-21T11:41:39Z","title":"Exosense: A Vision-Centric Scene Understanding System For Safe\n  Exoskeleton Navigation","summary":"  Exoskeletons for daily use by those with mobility impairments are being\ndeveloped. They will require accurate and robust scene understanding systems.\nCurrent research has used vision to identify immediate terrain and geometric\nobstacles, however these approaches are constrained to detections directly in\nfront of the user and are limited to classifying a finite range of terrain\ntypes (e.g., stairs, ramps and level-ground). This paper presents Exosense, a\nvision-centric scene understanding system which is capable of generating rich,\nglobally-consistent elevation maps, incorporating both semantic and terrain\ntraversability information. It features an elastic Atlas mapping framework\nassociated with a visual SLAM pose graph, embedded with open-vocabulary room\nlabels from a Vision-Language Model (VLM). The device's design includes a wide\nfield-of-view (FoV) fisheye multi-camera system to mitigate the challenges\nintroduced by the exoskeleton walking pattern. We demonstrate the system's\nrobustness to the challenges of typical periodic walking gaits, and its ability\nto construct accurate semantically-rich maps in indoor settings. Additionally,\nwe showcase its potential for motion planning -- providing a step towards safe\nnavigation for exoskeletons.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Lintong Zhang","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14320v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.14305v1","updated":"2024-03-21T11:21:17Z","published":"2024-03-21T11:21:17Z","title":"Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic\n  Manipulation","summary":"  Sample efficient learning of manipulation skills poses a major challenge in\nrobotics. While recent approaches demonstrate impressive advances in the type\nof task that can be addressed and the sensing modalities that can be\nincorporated, they still require large amounts of training data. Especially\nwith regard to learning actions on robots in the real world, this poses a major\nproblem due to the high costs associated with both demonstrations and\nreal-world robot interactions. To address this challenge, we introduce\nBOpt-GMM, a hybrid approach that combines imitation learning with own\nexperience collection. We first learn a skill model as a dynamical system\nencoded in a Gaussian Mixture Model from a few demonstrations. We then improve\nthis model with Bayesian optimization building on a small number of autonomous\nskill executions in a sparse reward setting. We demonstrate the sample\nefficiency of our approach on multiple complex manipulation skills in both\nsimulations and real-world experiments. Furthermore, we make the code and\npre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Iman Nematollahi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.14305v1.pdf","comment":"7 pages, 5 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.14300v1","updated":"2024-03-21T11:16:28Z","published":"2024-03-21T11:16:28Z","title":"DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic\n  Supervision","summary":"  Learning dexterous locomotion policy for legged robots is becoming\nincreasingly popular due to its ability to handle diverse terrains and resemble\nintelligent behaviors. However, joint manipulation of moving objects and\nlocomotion with legs, such as playing soccer, receive scant attention in the\nlearning community, although it is natural for humans and smart animals. A key\nchallenge to solve this multitask problem is to infer the objectives of\nlocomotion from the states and targets of the manipulated objects. The implicit\nrelation between the object states and robot locomotion can be hard to capture\ndirectly from the training experience. We propose adding a feedback control\nblock to compute the necessary body-level movement accurately and using the\noutputs as dynamic joint-level locomotion supervision explicitly. We further\nutilize an improved ball dynamic model, an extended context-aided estimator,\nand a comprehensive ball observer to facilitate transferring policy learned in\nsimulation to the real world. We observe that our learning scheme can not only\nmake the policy network converge faster but also enable soccer robots to\nperform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a\ncapability that was lacking in previous methods. Video and code are available\nat https://github.com/SysCV/soccer-player\n","authors":["Yutong Hu","Kehan Wen","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14300v1.pdf","comment":"8 pages, 7 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2312.07214v3","updated":"2024-03-21T11:12:31Z","published":"2023-12-12T12:26:48Z","title":"Exploring Large Language Models to Facilitate Variable Autonomy for\n  Human-Robot Teaming","summary":"  In a rapidly evolving digital landscape autonomous tools and robots are\nbecoming commonplace. Recognizing the significance of this development, this\npaper explores the integration of Large Language Models (LLMs) like Generative\npre-trained transformer (GPT) into human-robot teaming environments to\nfacilitate variable autonomy through the means of verbal human-robot\ncommunication. In this paper, we introduce a novel framework for such a\nGPT-powered multi-robot testbed environment, based on a Unity Virtual Reality\n(VR) setting. This system allows users to interact with robot agents through\nnatural language, each powered by individual GPT cores. By means of OpenAI's\nfunction calling, we bridge the gap between unstructured natural language input\nand structure robot actions. A user study with 12 participants explores the\neffectiveness of GPT-4 and, more importantly, user strategies when being given\nthe opportunity to converse in natural language within a multi-robot\nenvironment. Our findings suggest that users may have preconceived expectations\non how to converse with robots and seldom try to explore the actual language\nand cognitive capabilities of their robot collaborators. Still, those users who\ndid explore where able to benefit from a much more natural flow of\ncommunication and human-like back-and-forth. We provide a set of lessons\nlearned for future research and technical implementations of similar systems.\n","authors":["Younes Lakhnati","Max Pascher","Jens Gerken"],"pdf_url":"https://arxiv.org/pdf/2312.07214v3.pdf","comment":"Frontiers in Robotics and AI, Variable Autonomy for Human-Robot\n  Teaming"},{"id":"http://arxiv.org/abs/2403.14293v1","updated":"2024-03-21T11:00:11Z","published":"2024-03-21T11:00:11Z","title":"Human Reactions to Incorrect Answers from Robots","summary":"  As robots grow more and more integrated into numerous industries, it is\ncritical to comprehend how humans respond to their failures. This paper\nsystematically studies how trust dynamics and system design are affected by\nhuman responses to robot failures. The three-stage survey used in the study\nprovides a thorough understanding of human-robot interactions. While the second\nstage concentrates on interaction details, such as robot precision and error\nacknowledgment, the first stage collects demographic data and initial levels of\ntrust. In the last phase, participants' perceptions are examined after the\nencounter, and trust dynamics, forgiveness, and propensity to suggest robotic\ntechnologies are evaluated. Results show that participants' trust in robotic\ntechnologies increased significantly when robots acknowledged their errors or\nlimitations to participants and their willingness to suggest robots for\nactivities in the future points to a favorable change in perception,\nemphasizing the role that direct engagement has in influencing trust dynamics.\nBy providing useful advice for creating more sympathetic, responsive, and\nreliable robotic systems, the study advances the science of human-robot\ninteraction and promotes a wider adoption of robotic technologies.\n","authors":["Ponkoj Chandra Shill","Md. Azizul Hakim","Muhammad Jahanzeb Khan","Bashira Akter Anima"],"pdf_url":"https://arxiv.org/pdf/2403.14293v1.pdf","comment":"6 pages, 6 figures, 1 table, Ro-Man 2024"},{"id":"http://arxiv.org/abs/2403.14281v1","updated":"2024-03-21T10:41:31Z","published":"2024-03-21T10:41:31Z","title":"UAV-Assisted Maritime Search and Rescue: A Holistic Approach","summary":"  In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs)\nin maritime search and rescue (mSAR) missions, focusing on medium-sized\nfixed-wing drones and quadcopters. We address the challenges and limitations\ninherent in operating some of the different classes of UAVs, particularly in\nsearch operations. Our research includes the development of a comprehensive\nsoftware framework designed to enhance the efficiency and efficacy of SAR\noperations. This framework combines preliminary detection onboard UAVs with\nadvanced object detection at ground stations, aiming to reduce visual strain\nand improve decision-making for operators. It will be made publicly available\nupon publication. We conduct experiments to evaluate various Region of Interest\n(RoI) proposal methods, especially by imposing simulated limited bandwidth on\nthem, an important consideration when flying remote or offshore operations.\nThis forces the algorithm to prioritize some predictions over others.\n","authors":["Martin Messmer","Benjamin Kiefer","Leon Amadeus Varga","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2403.14281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00823v2","updated":"2024-03-21T10:21:37Z","published":"2024-02-01T18:07:33Z","title":"SLIM: Skill Learning with Multiple Critics","summary":"  Self-supervised skill learning aims to acquire useful behaviors that leverage\nthe underlying dynamics of the environment. Latent variable models, based on\nmutual information maximization, have been successful in this task but still\nstruggle in the context of robotic manipulation. As it requires impacting a\npossibly large set of degrees of freedom composing the environment, mutual\ninformation maximization fails alone in producing useful and safe manipulation\nbehaviors. Furthermore, tackling this by augmenting skill discovery rewards\nwith additional rewards through a naive combination might fail to produce\ndesired behaviors. To address this limitation, we introduce SLIM, a\nmulti-critic learning approach for skill discovery with a particular focus on\nrobotic manipulation. Our main insight is that utilizing multiple critics in an\nactor-critic framework to gracefully combine multiple reward functions leads to\na significant improvement in latent-variable skill discovery for robotic\nmanipulation while overcoming possible interference occurring among rewards\nwhich hinders convergence to useful skills. Furthermore, in the context of\ntabletop manipulation, we demonstrate the applicability of our novel skill\ndiscovery approach to acquire safe and efficient motor primitives in a\nhierarchical reinforcement learning fashion and leverage them through planning,\nsignificantly surpassing baseline approaches for skill discovery.\n","authors":["David Emukpere","Bingbing Wu","Julien Perez","Jean-Michel Renders"],"pdf_url":"https://arxiv.org/pdf/2402.00823v2.pdf","comment":"Accepted at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11567v2","updated":"2024-03-21T10:04:26Z","published":"2024-03-18T08:41:36Z","title":"R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based\n  Robots Ecosystems via Proposal Refinement","summary":"  We introduce a novel approach for scalable domain adaptation in cloud\nrobotics scenarios where robots rely on third-party AI inference services\npowered by large pre-trained deep neural networks. Our method is based on a\ndownstream proposal-refinement stage running locally on the robots, exploiting\na new lightweight DNN architecture, R2SNet. This architecture aims to mitigate\nperformance degradation from domain shifts by adapting the object detection\nprocess to the target environment, focusing on relabeling, rescoring, and\nsuppression of bounding-box proposals. Our method allows for local execution on\nrobots, addressing the scalability challenges of domain adaptation without\nincurring significant computational costs. Real-world results on mobile service\nrobots performing door detection show the effectiveness of the proposed method\nin achieving scalable domain adaptation.\n","authors":["Michele Antonazzi","Matteo Luperto","N. Alberto Borghese","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09337v4","updated":"2024-03-21T09:34:49Z","published":"2022-03-17T14:13:19Z","title":"CoBRA: A Composable Benchmark for Robotics Applications","summary":"  Selecting an optimal robot, its base pose, and trajectory for a given task is\ncurrently mainly done by human expertise or trial and error. To evaluate\nautomatic approaches to this combined optimization problem, we introduce a\nbenchmark suite encompassing a unified format for robots, environments, and\ntask descriptions. Our benchmark suite is especially useful for modular robots,\nwhere the multitude of robots that can be assembled creates a host of\nadditional parameters to optimize. We include tasks such as machine tending and\nwelding in synthetic environments and 3D scans of real-world machine shops. All\nbenchmarks are accessible through https://cobra.cps.cit.tum.de, a platform to\nconveniently share, reference, and compare tasks, robot models, and solutions.\n","authors":["Matthias Mayer","Jonathan Külz","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2203.09337v4.pdf","comment":"7 pages, 5 Figures, 5 Tables Final version for IEEE ICRA'24"},{"id":"http://arxiv.org/abs/2308.03574v2","updated":"2024-03-21T09:13:17Z","published":"2023-08-07T13:25:48Z","title":"Generalized Early Stopping in Evolutionary Direct Policy Search","summary":"  Lengthy evaluation times are common in many optimization problems such as\ndirect policy search tasks, especially when they involve conducting evaluations\nin the physical world, e.g. in robotics applications. Often when evaluating\nsolution over a fixed time period it becomes clear that the objective value\nwill not increase with additional computation time (for example when a two\nwheeled robot continuously spins on the spot). In such cases, it makes sense to\nstop the evaluation early to save computation time. However, most approaches to\nstop the evaluation are problem specific and need to be specifically designed\nfor the task at hand. Therefore, we propose an early stopping method for direct\npolicy search. The proposed method only looks at the objective value at each\ntime step and requires no problem specific knowledge. We test the introduced\nstopping criterion in five direct policy search environments drawn from games,\nrobotics and classic control domains, and show that it can save up to 75% of\nthe computation time. We also compare it with problem specific stopping\ncriteria and show that it performs comparably, while being more generally\napplicable.\n","authors":["Etor Arza","Leni K. Le Goff","Emma Hart"],"pdf_url":"https://arxiv.org/pdf/2308.03574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08244v2","updated":"2024-03-21T08:02:59Z","published":"2023-11-14T15:29:52Z","title":"Language and Sketching: An LLM-driven Interactive Multimodal Multitask\n  Robot Navigation Framework","summary":"  The socially-aware navigation system has evolved to adeptly avoid various\nobstacles while performing multiple tasks, such as point-to-point navigation,\nhuman-following, and -guiding. However, a prominent gap persists: in\nHuman-Robot Interaction (HRI), the procedure of communicating commands to\nrobots demands intricate mathematical formulations. Furthermore, the transition\nbetween tasks does not quite possess the intuitive control and user-centric\ninteractivity that one would desire. In this work, we propose an LLM-driven\ninteractive multimodal multitask robot navigation framework, termed LIM2N, to\nsolve the above new challenge in the navigation field. We achieve this by first\nintroducing a multimodal interaction framework where language and hand-drawn\ninputs can serve as navigation constraints and control objectives. Next, a\nreinforcement learning agent is built to handle multiple tasks with the\nreceived information. Crucially, LIM2N creates smooth cooperation among the\nreasoning of multimodal input, multitask planning, and adaptation and\nprocessing of the intelligent sensing modules in the complicated system.\nExtensive experiments are conducted in both simulation and the real world\ndemonstrating that LIM2N has superior user needs understanding, alongside an\nenhanced interactive experience.\n","authors":["Weiqin Zu","Wenbin Song","Ruiqing Chen","Ze Guo","Fanglei Sun","Zheng Tian","Wei Pan","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14176v1","updated":"2024-03-21T06:57:28Z","published":"2024-03-21T06:57:28Z","title":"ReFeree: Radar-based efficient global descriptor using a Feature and\n  Free space for Place Recognition","summary":"  Radar is highlighted for robust sensing capabilities in adverse weather\nconditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can\ncover wide areas and penetrate small particles. Despite these advantages,\nRadar-based place recognition remains in the early stages compared to other\nsensors due to its unique characteristics such as low resolution, and\nsignificant noise. In this paper, we propose a Radarbased place recognition\nutilizing a descriptor called ReFeree using a feature and free space. Unlike\ntraditional methods, we overwhelmingly summarize the Radar image. Despite being\nlightweight, it contains semi-metric information and is also outstanding from\nthe perspective of place recognition performance. For concrete validation, we\ntest a single session from the MulRan dataset and a multi-session from the\nOxford Radar RobotCar and the Boreas dataset.\n","authors":["Byunghee Choi","Hogyun Kim","Younggun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.14176v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14173v1","updated":"2024-03-21T06:53:20Z","published":"2024-03-21T06:53:20Z","title":"HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous\n  Time Optimization for Compact Wearable Mapping System","summary":"  Compact wearable mapping system (WMS) has gained significant attention due to\ntheir convenience in various applications. Specifically, it provides an\nefficient way to collect prior maps for 3D structure inspection and robot-based\n\"last-mile delivery\" in complex environments. However, vibrations in human\nmotion and the uneven distribution of point cloud features in complex\nenvironments often lead to rapid drift, which is a prevalent issue when\napplying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To\naddress these limitations, we propose a novel LIO for WMSs based on Hybrid\nContinuous Time Optimization (HCTO) considering the optimality of Lidar\ncorrespondences. First, HCTO recognizes patterns in human motion\n(high-frequency part, low-frequency part, and constant velocity part) by\nanalyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors\naccording to different motion states, which enables robust and accurate\nestimation against vibration-induced noise in the IMU measurements. Third, the\nbest point correspondences are selected using optimal design to achieve\nreal-time performance and better odometry accuracy. We conduct experiments on\nhead-mounted WMS datasets to evaluate the performance of our system,\ndemonstrating significant advantages over state-of-the-art methods. Video\nrecordings of experiments can be found on the project page of HCTO:\n\\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.\n","authors":["Jianping Li","Shenghai Yuan","Muqing Cao","Thien-Minh Nguyen","Kun Cao","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.14173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14163v1","updated":"2024-03-21T06:32:36Z","published":"2024-03-21T06:32:36Z","title":"Leveraging Large Language Model-based Room-Object Relationships\n  Knowledge for Enhancing Multimodal-Input Object Goal Navigation","summary":"  Object-goal navigation is a crucial engineering task for the community of\nembodied navigation; it involves navigating to an instance of a specified\nobject category within unseen environments. Although extensive investigations\nhave been conducted on both end-to-end and modular-based, data-driven\napproaches, fully enabling an agent to comprehend the environment through\nperceptual knowledge and perform object-goal navigation as efficiently as\nhumans remains a significant challenge. Recently, large language models have\nshown potential in this task, thanks to their powerful capabilities for\nknowledge extraction and integration. In this study, we propose a data-driven,\nmodular-based approach, trained on a dataset that incorporates common-sense\nknowledge of object-to-room relationships extracted from a large language\nmodel. We utilize the multi-channel Swin-Unet architecture to conduct\nmulti-task learning incorporating with multimodal inputs. The results in the\nHabitat simulator demonstrate that our framework outperforms the baseline by an\naverage of 10.6% in the efficiency metric, Success weighted by Path Length\n(SPL). The real-world demonstration shows that the proposed approach can\nefficiently conduct this task by traversing several rooms. For more details and\nreal-world demonstrations, please check our project webpage\n(https://sunleyuan.github.io/ObjectNav).\n","authors":["Leyuan Sun","Asako Kanezaki","Guillaume Caron","Yusuke Yoshiyasu"],"pdf_url":"https://arxiv.org/pdf/2403.14163v1.pdf","comment":"will soon submit to the Elsevier journal, Advanced Engineering\n  Informatics"},{"id":"http://arxiv.org/abs/2310.11792v2","updated":"2024-03-21T06:26:50Z","published":"2023-10-18T08:33:08Z","title":"Real-time Perceptive Motion Control using Control Barrier Functions with\n  Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3","summary":"  To achieve safe legged locomotion, it is important to generate motion in\nreal-time considering various constraints in robots and environments. In this\nstudy, we propose a lightweight real-time perspective motion control system for\nthe newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the\nproposed method, analytically smoothed constraints including Smooth Separating\nAxis Theorem (Smooth SAT) as a novel higher order differentiable collision\ndetection for 3D shapes is applied to the Control Barrier Function (CBF). The\nproposed system integrating the CBF achieves online motion generation in a\nshort control cycle of 1 ms that satisfies joint limitations, environmental\ncollision avoidance and safe convex foothold constraints. The efficiency of\nSmooth SAT is shown from the collision detection time of 1 us or less and the\nCBF constraint computation time for Tachyon3 of several us. Furthermore, the\neffectiveness of the proposed system is verified through the stair-climbing\nmotion, integrating online recognition in a simulation and a real machine.\n","authors":["Noriaki Takasugi","Masaya Kinoshita","Yasuhisa Kamikawa","Ryoichi Tsuzaki","Atsushi Sakamoto","Toshimitsu Kai","Yasunori Kawanami"],"pdf_url":"https://arxiv.org/pdf/2310.11792v2.pdf","comment":"8 pages, 8 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.14161v1","updated":"2024-03-21T06:24:01Z","published":"2024-03-21T06:24:01Z","title":"Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on\n  Floor Plane And Object Segmentation","summary":"  Mobile robots equipped with multiple light detection and ranging (LiDARs) and\ncapable of recognizing their surroundings are increasing due to the\nminitualization and cost reduction of LiDAR. This paper proposes a target-less\nextrinsic calibration method of multiple LiDARs with non-overlapping field of\nview (FoV). The proposed method uses accumulated point clouds of floor plane\nand objects while in motion. It enables accurate calibration with challenging\nconfiguration of LiDARs that directed towards the floor plane, caused by biased\nfeature values. Additionally, the method includes a noise removal module that\nconsiders the scanning pattern to address bleeding points, which are noises of\nsignificant source of error in point cloud alignment using high-density LiDARs.\nEvaluations through simulation demonstrate that the proposed method achieved\nhigher accuracy extrinsic calibration with two and four LiDARs than\nconventional methods, regardless type of objects. Furthermore, the experiments\nusing a real mobile robot has shown that our proposed noise removal module can\neliminate noise more precisely than conventional methods, and the estimated\nextrinsic parameters have successfully created consistent 3D maps.\n","authors":["Shun Niijima","Atsushi Suzuki","Ryoichi Tsuzaki","Masaya Kinoshita"],"pdf_url":"https://arxiv.org/pdf/2403.14161v1.pdf","comment":"8pages, 10figures"},{"id":"http://arxiv.org/abs/2403.14160v1","updated":"2024-03-21T06:23:57Z","published":"2024-03-21T06:23:57Z","title":"Development of a Compact Robust Passive Transformable Omni-Ball for\n  Enhanced Step-Climbing and Vibration Reduction","summary":"  This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced\nomnidirectional wheel engineered to enhance step-climbing performance,\nincorporate built-in actuators, diminish vibrations, and fortify structural\nintegrity. By modifying the omni-ball's structure from two to three segments,\nwe have achieved improved in-wheel actuation and a reduction in vibrational\nfeedback. Additionally, we have implemented a sliding mechanism in the follower\nwheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm\ndiameter PTOB was constructed, which confirmed its functionality for\nomnidirectional movement and internal actuation. Compared to a traditional\nomni-wheel, the PTOB demonstrated a comparable level of vibration while\noffering superior capabilities. Extensive testing in varied settings showed\nthat the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35\n$\\%$ of the wheel's diameter, in both the forward and lateral directions. The\nPTOB showcased robust construction and proved to be versatile in navigating\nthrough environments with diverse obstacles.\n","authors":["Kazuo Hongo","Takashi Kito","Yasuhisa Kamikawa","Masaya Kinoshita","Yasunori Kawanami"],"pdf_url":"https://arxiv.org/pdf/2403.14160v1.pdf","comment":"8 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.14159v1","updated":"2024-03-21T06:23:38Z","published":"2024-03-21T06:23:38Z","title":"Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive\n  Control with Guard Saltation Matrix","summary":"  This paper presents a stochastic/robust nonlinear model predictive control\n(NMPC) to enhance the robustness of legged locomotion against contact\nuncertainties. We integrate the contact uncertainties into the covariance\npropagation of stochastic/robust NMPC framework by leveraging the guard\nsaltation matrix and an extended Kalman filter-like covariance update. We\nachieve fast stochastic/robust NMPC computation by utilizing the zero-order\nstochastic/robust NMPC algorithm with additional improvements in computational\nefficiency concerning the feedback gains. We conducted numerical experiments\nand demonstrate that the proposed method can accurately forecast future state\ncovariance and generate trajectories that satisfies constraints even in the\npresence of the contact uncertainties. Hardware experiments on the perceptive\nlocomotion of a wheeled-legged robot were also carried out, validating the\nfeasibility of the proposed method in a real-world system with limited on-board\ncomputation.\n","authors":["Sotaro Katayama","Noriaki Takasugi","Mitsuhisa Kaneko","Norio Nagatsuka","and Masaya Kinoshita"],"pdf_url":"https://arxiv.org/pdf/2403.14159v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.10678v2","updated":"2024-03-21T05:47:22Z","published":"2023-11-17T18:00:20Z","title":"Distilling and Retrieving Generalizable Knowledge for Robot Manipulation\n  via Language Corrections","summary":"  Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .\n","authors":["Lihan Zha","Yuchen Cui","Li-Heng Lin","Minae Kwon","Montserrat Gonzalez Arenas","Andy Zeng","Fei Xia","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2311.10678v2.pdf","comment":"8 pages, 4 figures, videos and code links on website\n  https://sites.google.com/stanford.edu/droc"},{"id":"http://arxiv.org/abs/2403.14138v1","updated":"2024-03-21T05:13:34Z","published":"2024-03-21T05:13:34Z","title":"Evidential Semantic Mapping in Off-road Environments with\n  Uncertainty-aware Bayesian Kernel Inference","summary":"  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in\ncreating semantic maps by effectively leveraging local spatial information.\nHowever, existing semantic mapping methods face challenges in constructing\nreliable maps in unstructured outdoor scenarios due to unreliable semantic\npredictions. To address this issue, we propose an evidential semantic mapping,\nwhich can enhance reliability in perceptually challenging off-road\nenvironments. We integrate Evidential Deep Learning into the semantic\nsegmentation network to obtain the uncertainty estimate of semantic prediction.\nSubsequently, this semantic uncertainty is incorporated into an\nuncertainty-aware BKI, tailored to prioritize more confident semantic\npredictions when accumulating semantic information. By adaptively handling\nsemantic uncertainties, the proposed framework constructs robust\nrepresentations of the surroundings even in previously unseen environments.\nComprehensive experiments across various off-road datasets demonstrate that our\nframework enhances accuracy and robustness, consistently outperforming existing\nmethods in scenes with high perceptual uncertainties.\n","authors":["Junyoung Kim","Junwon Seo","Jihong Min"],"pdf_url":"https://arxiv.org/pdf/2403.14138v1.pdf","comment":"Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping"},{"id":"http://arxiv.org/abs/2303.03757v3","updated":"2024-03-21T05:11:08Z","published":"2023-03-07T09:33:49Z","title":"Deep Learning for Inertial Positioning: A Survey","summary":"  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT\ndevices, playing a crucial role in enabling ubiquitous and reliable\nlocalization. Inertial sensor-based positioning is essential in various\napplications, including personal navigation, location-based security, and\nhuman-device interaction. However, low-cost MEMS inertial sensors' measurements\nare inevitably corrupted by various error sources, leading to unbounded drifts\nwhen integrated doubly in traditional inertial navigation algorithms,\nsubjecting inertial positioning to the problem of error drifts. In recent\nyears, with the rapid increase in sensor data and computational power, deep\nlearning techniques have been developed, sparking significant research into\naddressing the problem of inertial positioning. Relevant literature in this\nfield spans across mobile computing, robotics, and machine learning. In this\narticle, we provide a comprehensive review of deep learning-based inertial\npositioning and its applications in tracking pedestrians, drones, vehicles, and\nrobots. We connect efforts from different fields and discuss how deep learning\ncan be applied to address issues such as sensor calibration, positioning error\ndrift reduction, and multi-sensor fusion. This article aims to attract readers\nfrom various backgrounds, including researchers and practitioners interested in\nthe potential of deep learning-based techniques to solve inertial positioning\nproblems. Our review demonstrates the exciting possibilities that deep learning\nbrings to the table and provides a roadmap for future research in this field.\n","authors":["Changhao Chen","Xianfei Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03757v3.pdf","comment":"Accepted by IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2403.13331v2","updated":"2024-03-21T04:01:10Z","published":"2024-03-20T06:22:37Z","title":"AMP: Autoregressive Motion Prediction Revisited with Next Token\n  Prediction for Autonomous Driving","summary":"  As an essential task in autonomous driving (AD), motion prediction aims to\npredict the future states of surround objects for navigation. One natural\nsolution is to estimate the position of other agents in a step-by-step manner\nwhere each predicted time-step is conditioned on both observed time-steps and\npreviously predicted time-steps, i.e., autoregressive prediction. Pioneering\nworks like SocialLSTM and MFP design their decoders based on this intuition.\nHowever, almost all state-of-the-art works assume that all predicted time-steps\nare independent conditioned on observed time-steps, where they use a single\nlinear layer to generate positions of all time-steps simultaneously. They\ndominate most motion prediction leaderboards due to the simplicity of training\nMLPs compared to autoregressive networks.\n  In this paper, we introduce the GPT style next token prediction into motion\nforecasting. In this way, the input and output could be represented in a\nunified space and thus the autoregressive prediction becomes more feasible.\nHowever, different from language data which is composed of homogeneous units\n-words, the elements in the driving scene could have complex spatial-temporal\nand semantic relations. To this end, we propose to adopt three factorized\nattention modules with different neighbors for information aggregation and\ndifferent position encoding styles to capture their relations, e.g., encoding\nthe transformation between coordinate systems for spatial relativity while\nadopting RoPE for temporal relativity. Empirically, by equipping with the\naforementioned tailored designs, the proposed method achieves state-of-the-art\nperformance in the Waymo Open Motion and Waymo Interaction datasets. Notably,\nAMP outperforms other recent autoregressive motion prediction methods: MotionLM\nand StateTransformer, which demonstrates the effectiveness of the proposed\ndesigns.\n","authors":["Xiaosong Jia","Shaoshuai Shi","Zijun Chen","Li Jiang","Wenlong Liao","Tao He","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.13331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.13122v2","updated":"2024-03-21T01:15:02Z","published":"2023-07-24T20:40:18Z","title":"Redundancy parameterization and inverse kinematics of 7-DOF revolute\n  manipulators","summary":"  Seven degree-of-freedom (DOF) robot arms have one redundant DOF which does\nnot change the motion of the end effector. The redundant DOF offers greater\nmanipulability of the arm configuration to avoid obstacles and singularities,\nbut it must be parameterized to fully specify the joint angles for a given end\neffector pose. For 7-DOF revolute (7R) manipulators, we introduce a new concept\nof generalized shoulder-elbow-wrist (SEW) angle, a generalization of the\nconventional SEW angle but with an arbitrary choice of the reference direction\nfunction. The SEW angle is widely used and easy for human operators to\nvisualize as a rotation of the elbow about the shoulder-wrist line. Since other\nredundancy parameterizations including the conventional SEW angle encounter an\nalgorithmic singularity along a line in the workspace, we introduce a special\nchoice of the reference direction function called the stereographic SEW angle\nwhich has a singularity only along a half-line, which can be placed out of\nreach. We prove that such a singularity is unavoidable for any\nparameterization. We also include expressions for the SEW angle Jacobian along\nwith singularity analysis. Finally, we provide efficient and singularity-robust\ninverse kinematics solutions for most known 7R manipulators using the general\nSEW angle and the subproblem decomposition method. These solutions are often\nclosed-form but may sometimes involve a 1D or 2D search in the general case.\nSearch-based solutions may be converted to finding zeros of a high-order\npolynomial. Inverse kinematics solutions, examples, and evaluations are\navailable in a publicly accessible repository.\n","authors":["Alexander J. Elias","John T. Wen"],"pdf_url":"https://arxiv.org/pdf/2307.13122v2.pdf","comment":"22 pages, 14 figures. Update: Sawyer IK using polynomial method, two\n  video extensions, expanded related literature"},{"id":"http://arxiv.org/abs/2403.14056v1","updated":"2024-03-21T00:59:35Z","published":"2024-03-21T00:59:35Z","title":"Semantics from Space: Satellite-Guided Thermal Semantic Segmentation\n  Annotation for Aerial Field Robots","summary":"  We present a new method to automatically generate semantic segmentation\nannotations for thermal imagery captured from an aerial vehicle by utilizing\nsatellite-derived data products alongside onboard global positioning and\nattitude estimates. This new capability overcomes the challenge of developing\nthermal semantic perception algorithms for field robots due to the lack of\nannotated thermal field datasets and the time and costs of manual annotation,\nenabling precise and rapid annotation of thermal data from field collection\nefforts at a massively-parallelizable scale. By incorporating a\nthermal-conditioned refinement step with visual foundation models, our approach\ncan produce highly-precise semantic segmentation labels using low-resolution\nsatellite land cover data for little-to-no cost. It achieves 98.5% of the\nperformance from using costly high-resolution options and demonstrates between\n70-160% improvement over popular zero-shot semantic segmentation methods based\non large vision-language models currently used for generating annotations for\nRGB imagery. Code will be available at:\nhttps://github.com/connorlee77/aerial-auto-segment.\n","authors":["Connor Lee","Saraswati Soedarmadji","Matthew Anderson","Anthony J. Clark","Soon-Jo Chung"],"pdf_url":"https://arxiv.org/pdf/2403.14056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08864v5","updated":"2024-03-21T00:48:52Z","published":"2023-10-13T05:20:40Z","title":"Open X-Embodiment: Robotic Learning Datasets and RT-X Models","summary":"  Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.\n","authors":["Open X-Embodiment Collaboration","Abby O'Neill","Abdul Rehman","Abhiram Maddukuri","Abhishek Gupta","Abhishek Padalkar","Abraham Lee","Acorn Pooley","Agrim Gupta","Ajay Mandlekar","Ajinkya Jain","Albert Tung","Alex Bewley","Alex Herzog","Alex Irpan","Alexander Khazatsky","Anant Rai","Anchit Gupta","Andrew Wang","Anikait Singh","Animesh Garg","Aniruddha Kembhavi","Annie Xie","Anthony Brohan","Antonin Raffin","Archit Sharma","Arefeh Yavary","Arhan Jain","Ashwin Balakrishna","Ayzaan Wahid","Ben Burgess-Limerick","Beomjoon Kim","Bernhard Schölkopf","Blake Wulfe","Brian Ichter","Cewu Lu","Charles Xu","Charlotte Le","Chelsea Finn","Chen Wang","Chenfeng Xu","Cheng Chi","Chenguang Huang","Christine Chan","Christopher Agia","Chuer Pan","Chuyuan Fu","Coline Devin","Danfei Xu","Daniel Morton","Danny Driess","Daphne Chen","Deepak Pathak","Dhruv Shah","Dieter Büchler","Dinesh Jayaraman","Dmitry Kalashnikov","Dorsa Sadigh","Edward Johns","Ethan Foster","Fangchen Liu","Federico Ceola","Fei Xia","Feiyu Zhao","Freek Stulp","Gaoyue Zhou","Gaurav S. Sukhatme","Gautam Salhotra","Ge Yan","Gilbert Feng","Giulio Schiavi","Glen Berseth","Gregory Kahn","Guanzhi Wang","Hao Su","Hao-Shu Fang","Haochen Shi","Henghui Bao","Heni Ben Amor","Henrik I Christensen","Hiroki Furuta","Homer Walke","Hongjie Fang","Huy Ha","Igor Mordatch","Ilija Radosavovic","Isabel Leal","Jacky Liang","Jad Abou-Chakra","Jaehyung Kim","Jaimyn Drake","Jan Peters","Jan Schneider","Jasmine Hsu","Jeannette Bohg","Jeffrey Bingham","Jeffrey Wu","Jensen Gao","Jiaheng Hu","Jiajun Wu","Jialin Wu","Jiankai Sun","Jianlan Luo","Jiayuan Gu","Jie Tan","Jihoon Oh","Jimmy Wu","Jingpei Lu","Jingyun Yang","Jitendra Malik","João Silvério","Joey Hejna","Jonathan Booher","Jonathan Tompson","Jonathan Yang","Jordi Salvador","Joseph J. Lim","Junhyek Han","Kaiyuan Wang","Kanishka Rao","Karl Pertsch","Karol Hausman","Keegan Go","Keerthana Gopalakrishnan","Ken Goldberg","Kendra Byrne","Kenneth Oslund","Kento Kawaharazuka","Kevin Black","Kevin Lin","Kevin Zhang","Kiana Ehsani","Kiran Lekkala","Kirsty Ellis","Krishan Rana","Krishnan Srinivasan","Kuan Fang","Kunal Pratap Singh","Kuo-Hao Zeng","Kyle Hatch","Kyle Hsu","Laurent Itti","Lawrence Yunliang Chen","Lerrel Pinto","Li Fei-Fei","Liam Tan","Linxi \"Jim\" Fan","Lionel Ott","Lisa Lee","Luca Weihs","Magnum Chen","Marion Lepert","Marius Memmel","Masayoshi Tomizuka","Masha Itkina","Mateo Guaman Castro","Max Spero","Maximilian Du","Michael Ahn","Michael C. Yip","Mingtong Zhang","Mingyu Ding","Minho Heo","Mohan Kumar Srirama","Mohit Sharma","Moo Jin Kim","Naoaki Kanazawa","Nicklas Hansen","Nicolas Heess","Nikhil J Joshi","Niko Suenderhauf","Ning Liu","Norman Di Palo","Nur Muhammad Mahi Shafiullah","Oier Mees","Oliver Kroemer","Osbert Bastani","Pannag R Sanketi","Patrick \"Tree\" Miller","Patrick Yin","Paul Wohlhart","Peng Xu","Peter David Fagan","Peter Mitrano","Pierre Sermanet","Pieter Abbeel","Priya Sundaresan","Qiuyu Chen","Quan Vuong","Rafael Rafailov","Ran Tian","Ria Doshi","Roberto Martín-Martín","Rohan Baijal","Rosario Scalise","Rose Hendrix","Roy Lin","Runjia Qian","Ruohan Zhang","Russell Mendonca","Rutav Shah","Ryan Hoque","Ryan Julian","Samuel Bustamante","Sean Kirmani","Sergey Levine","Shan Lin","Sherry Moore","Shikhar Bahl","Shivin Dass","Shubham Sonawani","Shuran Song","Sichun Xu","Siddhant Haldar","Siddharth Karamcheti","Simeon Adebola","Simon Guist","Soroush Nasiriany","Stefan Schaal","Stefan Welker","Stephen Tian","Subramanian Ramamoorthy","Sudeep Dasari","Suneel Belkhale","Sungjae Park","Suraj Nair","Suvir Mirchandani","Takayuki Osa","Tanmay Gupta","Tatsuya Harada","Tatsuya Matsushima","Ted Xiao","Thomas Kollar","Tianhe Yu","Tianli Ding","Todor Davchev","Tony Z. Zhao","Travis Armstrong","Trevor Darrell","Trinity Chung","Vidhi Jain","Vincent Vanhoucke","Wei Zhan","Wenxuan Zhou","Wolfram Burgard","Xi Chen","Xiaolong Wang","Xinghao Zhu","Xinyang Geng","Xiyuan Liu","Xu Liangwei","Xuanlin Li","Yao Lu","Yecheng Jason Ma","Yejin Kim","Yevgen Chebotar","Yifan Zhou","Yifeng Zhu","Yilin Wu","Ying Xu","Yixuan Wang","Yonatan Bisk","Yoonyoung Cho","Youngwoon Lee","Yuchen Cui","Yue Cao","Yueh-Hua Wu","Yujin Tang","Yuke Zhu","Yunchu Zhang","Yunfan Jiang","Yunshuang Li","Yunzhu Li","Yusuke Iwasawa","Yutaka Matsuo","Zehan Ma","Zhuo Xu","Zichen Jeff Cui","Zichen Zhang","Zipeng Lin"],"pdf_url":"https://arxiv.org/pdf/2310.08864v5.pdf","comment":"Project website: https://robotics-transformer-x.github.io"},{"id":"http://arxiv.org/abs/2403.14049v1","updated":"2024-03-21T00:14:53Z","published":"2024-03-21T00:14:53Z","title":"A Roadmap Towards Automated and Regulated Robotic Systems","summary":"  The rapid development of generative technology opens up possibility for\nhigher level of automation, and artificial intelligence (AI) embodiment in\nrobotic systems is imminent. However, due to the blackbox nature of the\ngenerative technology, the generation of the knowledge and workflow scheme is\nuncontrolled, especially in a dynamic environment and a complex scene. This\nposes challenges to regulations in safety-demanding applications such as\nmedical scenes. We argue that the unregulated generative processes from AI is\nfitted for low level end tasks, but intervention in the form of manual or\nautomated regulation should happen post-workflow-generation and\npre-robotic-execution. To address this, we propose a roadmap that can lead to\nfully automated and regulated robotic systems. In this paradigm, the high level\npolicies are generated as structured graph data, enabling regulatory oversight\nand reusability, while the code base for lower level tasks is generated by\ngenerative models. Our approach aims the transitioning from expert knowledge to\nregulated action, akin to the iterative processes of study, practice, scrutiny,\nand execution in human tasks. We identify the generative and deterministic\nprocesses in a design cycle, where generative processes serve as a text-based\nworld simulator and the deterministic processes generate the executable system.\nWe propose State Machine Seralization Language (SMSL) to be the conversion\npoint between text simulator and executable workflow control. From there, we\nanalyze the modules involved based on the current literature, and discuss human\nin the loop. As a roadmap, this work identifies the current possible\nimplementation and future work. This work does not provide an implemented\nsystem but envisions to inspire the researchers working on the direction in the\nroadmap. We implement the SMSL and D-SFO paradigm that serve as the starting\npoint of the roadmap.\n","authors":["Yihao Liu","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2403.14049v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.14887v1","updated":"2024-03-21T23:44:42Z","published":"2024-03-21T23:44:42Z","title":"GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile\n  Sensing and Proprioception","summary":"  Compared to fully-actuated robotic end-effectors, underactuated ones are\ngenerally more adaptive, robust, and cost-effective. However, state estimation\nfor underactuated hands is usually more challenging. Vision-based tactile\nsensors, like Gelsight, can mitigate this issue by providing high-resolution\ntactile sensing and accurate proprioceptive sensing. As such, we present\nGelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,\nhigh-resolution vision-based tactile sensing and proprioceptive sensing\ncapabilities. In order to reduce the amount of embedded hardware, i.e. the\ncameras and motors, we optimize the linkage transmission with a planar linkage\nmechanism simulator and develop a planar reflection simulator to simplify the\ntactile sensing hardware. As a result, GelLink only requires one motor to\nactuate the three phalanges, and one camera to capture tactile signals along\nthe entire finger. Overall, GelLink is a compact robotic finger that shows\nadaptability and robustness when performing grasping tasks. The integration of\nvision-based tactile sensors can significantly enhance the capabilities of\nunderactuated fingers and potentially broaden their future usage.\n","authors":["Yuxiang Ma"," Jialiang"," Zhao","Edward Adelson"],"pdf_url":"https://arxiv.org/pdf/2403.14887v1.pdf","comment":"Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7\n  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and\n  Automation)"},{"id":"http://arxiv.org/abs/2403.14879v1","updated":"2024-03-21T23:00:10Z","published":"2024-03-21T23:00:10Z","title":"Learning to Change: Choreographing Mixed Traffic Through Lateral Control\n  and Hierarchical Reinforcement Learning","summary":"  The management of mixed traffic that consists of robot vehicles (RVs) and\nhuman-driven vehicles (HVs) at complex intersections presents a multifaceted\nchallenge. Traditional signal controls often struggle to adapt to dynamic\ntraffic conditions and heterogeneous vehicle types. Recent advancements have\nturned to strategies based on reinforcement learning (RL), leveraging its\nmodel-free nature, real-time operation, and generalizability over different\nscenarios. We introduce a hierarchical RL framework to manage mixed traffic\nthrough precise longitudinal and lateral control of RVs. Our proposed\nhierarchical framework combines the state-of-the-art mixed traffic control\nalgorithm as a high level decision maker to improve the performance and\nrobustness of the whole system. Our experiments demonstrate that the framework\ncan reduce the average waiting time by up to 54% compared to the\nstate-of-the-art mixed traffic control method. When the RV penetration rate\nexceeds 60%, our technique consistently outperforms conventional traffic signal\ncontrol programs in terms of the average waiting time for all vehicles at the\nintersection.\n","authors":["Dawei Wang","Weizi Li","Lei Zhu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.14879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14877v1","updated":"2024-03-21T22:54:08Z","published":"2024-03-21T22:54:08Z","title":"TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path\n  Planning Across City-Scale Wind Fields","summary":"  Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their\nmaneuverability and flexibility, offer a promising alternative to our\ntransportation system. However, the operational effectiveness of these aircraft\nfaces many challenges, such as the delicate balance between energy and time\nefficiency, stemming from unpredictable environmental factors, including wind\nfields. Mathematical modeling-based approaches have been adopted to plan\naircraft flight path in urban wind fields with the goal to save energy and time\ncosts. While effective, they are limited in adapting to dynamic and complex\nenvironments. To optimize energy and time efficiency in eVTOL's flight through\ndynamic wind fields, we introduce a novel path planning method leveraging deep\nreinforcement learning. We assess our method with extensive experiments,\ncomparing it to Dijkstra's algorithm -- the theoretically optimal approach for\ndetermining shortest paths in a weighted graph, where weights represent either\nenergy or time cost. The results show that our method achieves a graceful\nbalance between energy and time efficiency, closely resembling the\ntheoretically optimal values for both objectives.\n","authors":["Songyang Liu","Shuai Li","Haochen Li","Weizi Li","Jindong Tan"],"pdf_url":"https://arxiv.org/pdf/2403.14877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14864v1","updated":"2024-03-21T22:18:59Z","published":"2024-03-21T22:18:59Z","title":"Learning Quadruped Locomotion Using Differentiable Simulation","summary":"  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n","authors":["Yunlong Song","Sangbae Kim","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07869v2","updated":"2024-03-21T19:57:46Z","published":"2024-03-12T17:58:01Z","title":"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile\n  Manipulation","summary":"  A critical bottleneck limiting imitation learning in robotics is the lack of\ndata. This problem is more severe in mobile manipulation, where collecting\ndemonstrations is harder than in stationary manipulation due to the lack of\navailable and easy-to-use teleoperation interfaces. In this work, we\ndemonstrate TeleMoMa, a general and modular interface for whole-body\nteleoperation of mobile manipulators. TeleMoMa unifies multiple human\ninterfaces including RGB and depth cameras, virtual reality controllers,\nkeyboard, joysticks, etc., and any combination thereof. In its more accessible\nversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering\nthe entry bar for humans to provide mobile manipulation demonstrations. We\ndemonstrate the versatility of TeleMoMa by teleoperating several existing\nmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and\nthe real world. We demonstrate the quality of the demonstrations collected with\nTeleMoMa by training imitation learning policies for mobile manipulation tasks\ninvolving synchronized whole-body motion. Finally, we also show that TeleMoMa's\nteleoperation channel enables teleoperation on site, looking at the robot, or\nremote, sending commands and observations through a computer network, and\nperform user studies to evaluate how easy it is for novice users to learn to\ncollect demonstrations with different combinations of human interfaces enabled\nby our system. We hope TeleMoMa becomes a helpful tool for the community\nenabling researchers to collect whole-body mobile manipulation demonstrations.\nFor more information and video results,\nhttps://robin-lab.cs.utexas.edu/telemoma-web.\n","authors":["Shivin Dass","Wensi Ai","Yuqian Jiang","Samik Singh","Jiaheng Hu","Ruohan Zhang","Peter Stone","Ben Abbatematteo","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2403.07869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14780v1","updated":"2024-03-21T18:50:33Z","published":"2024-03-21T18:50:33Z","title":"Multi-agent Task-Driven Exploration via Intelligent Map Compression and\n  Sharing","summary":"  This paper investigates the task-driven exploration of unknown environments\nwith mobile sensors communicating compressed measurements. The sensors explore\nthe area and transmit their compressed data to another robot, assisting it in\nreaching a goal location. We propose a novel communication framework and a\ntractable multi-agent exploration algorithm to select the sensors' actions. The\nalgorithm uses a task-driven measure of uncertainty, resulting from map\ncompression, as a reward function. We validate the efficacy of our algorithm\nthrough numerical simulations conducted on a realistic map and compare it with\ntwo alternative approaches. The results indicate that the proposed algorithm\neffectively decreases the time required for the robot to reach its target\nwithout causing excessive load on the communication network.\n","authors":["Evangelos Psomiadis","Dipankar Maity","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2403.14780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10698v2","updated":"2024-03-21T18:20:14Z","published":"2023-09-19T15:37:24Z","title":"OASIS: Optimal Arrangements for Sensing in SLAM","summary":"  The number and arrangement of sensors on mobile robot dramatically influence\nits perception capabilities. Ensuring that sensors are mounted in a manner that\nenables accurate detection, localization, and mapping is essential for the\nsuccess of downstream control tasks. However, when designing a new robotic\nplatform, researchers and practitioners alike usually mimic standard\nconfigurations or maximize simple heuristics like field-of-view (FOV) coverage\nto decide where to place exteroceptive sensors. In this work, we conduct an\ninformation-theoretic investigation of this overlooked element of robotic\nperception in the context of simultaneous localization and mapping (SLAM). We\nshow how to formalize the sensor arrangement problem as a form of subset\nselection under the E-optimality performance criterion. While this formulation\nis NP-hard in general, we show that a combination of greedy sensor selection\nand fast convex relaxation-based post-hoc verification enables the efficient\nrecovery of certifiably optimal sensor designs in practice. Results from\nsynthetic experiments reveal that sensors placed with OASIS outperform\nbenchmarks in terms of mean squared error of visual SLAM estimates.\n","authors":["Pushyami Kaveti","Matthew Giamou","Hanumant Singh","David M. Rosen"],"pdf_url":"https://arxiv.org/pdf/2309.10698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15494v1","updated":"2024-03-21T17:36:53Z","published":"2024-03-21T17:36:53Z","title":"Multiple and Gyro-Free Inertial Datasets","summary":"  An inertial navigation system (INS) utilizes three orthogonal accelerometers\nand gyroscopes to determine platform position, velocity, and orientation. There\nare countless applications for INS, including robotics, autonomous platforms,\nand the internet of things. Recent research explores the integration of\ndata-driven methods with INS, highlighting significant innovations, improving\naccuracy and efficiency. Despite the growing interest in this field and the\navailability of INS datasets, no datasets are available for gyro-free INS\n(GFINS) and multiple inertial measurement unit (MIMU) architectures. To fill\nthis gap and to stimulate further research in this field, we designed and\nrecorded GFINS and MIMU datasets using 54 inertial sensors grouped in nine\ninertial measurement units. These sensors can be used to define and evaluate\ndifferent types of MIMU and GFINS architectures. The inertial sensors were\narranged in three different sensor configurations and mounted on a mobile robot\nand a passenger car. In total, the dataset contains 35 hours of inertial data\nand corresponding ground truth trajectories. The data and code are freely\naccessible through our GitHub repository.\n","authors":["Zeev Yampolsky","Yair Stolero","Nitzan Pri-Hadash","Dan Solodar","Shira Massas","Itai Savin","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2403.15494v1.pdf","comment":"10 pages, 16 figures, 6 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.14628v1","updated":"2024-03-21T17:59:59Z","published":"2024-03-21T17:59:59Z","title":"Zero-Shot Multi-Object Shape Completion","summary":"  We present a 3D shape completion method that recovers the complete geometry\nof multiple objects in complex scenes from a single RGB-D image. Despite\nnotable advancements in single object 3D shape completion, high-quality\nreconstructions in highly cluttered real-world multi-object scenes remains a\nchallenge. To address this issue, we propose OctMAE, an architecture that\nleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near\nreal-time multi-object shape completion through both local and global geometric\nreasoning. Because a na\\\"ive 3D MAE can be computationally intractable and\nmemory intensive even in the latent space, we introduce a novel occlusion\nmasking strategy and adopt 3D rotary embeddings, which significantly improves\nthe runtime and shape completion quality. To generalize to a wide range of\nobjects in diverse scenes, we create a large-scale photorealistic dataset,\nfeaturing a diverse set of 12K 3D object models from the Objaverse dataset\nwhich are rendered in multi-object scenes with physics-based positioning. Our\nmethod outperforms the current state-of-the-art on both synthetic and\nreal-world datasets and demonstrates a strong zero-shot capability.\n","authors":["Shun Iwase","Katherine Liu","Vitor Guizilini","Adrien Gaidon","Kris Kitani","Rares Ambrus","Sergey Zakharov"],"pdf_url":"https://arxiv.org/pdf/2403.14628v1.pdf","comment":"21 pages, 8 figues"},{"id":"http://arxiv.org/abs/2403.14627v1","updated":"2024-03-21T17:59:58Z","published":"2024-03-21T17:59:58Z","title":"MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images","summary":"  We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model\nlearned from sparse multi-view images. To accurately localize the Gaussian\ncenters, we propose to build a cost volume representation via plane sweeping in\nthe 3D space, where the cross-view feature similarities stored in the cost\nvolume can provide valuable geometry cues to the estimation of depth. We learn\nthe Gaussian primitives' opacities, covariances, and spherical harmonics\ncoefficients jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussian Splatting models via extensive\nexperimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,\nour model achieves state-of-the-art performance with the fastest feed-forward\ninference speed (22 fps). Compared to the latest state-of-the-art method\npixelSplat, our model uses $10\\times $ fewer parameters and infers more than\n$2\\times$ faster while providing higher appearance and geometry quality as well\nas better cross-dataset generalization.\n","authors":["Yuedong Chen","Haofei Xu","Chuanxia Zheng","Bohan Zhuang","Marc Pollefeys","Andreas Geiger","Tat-Jen Cham","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14627v1.pdf","comment":"Project page: https://donydchen.github.io/mvsplat Code:\n  https://github.com/donydchen/mvsplat"},{"id":"http://arxiv.org/abs/2403.14625v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT\n  Descriptors","summary":"  We present a simple self-supervised method to enhance the performance of ViT\nfeatures for dense downstream tasks. Our Lightweight Feature Transform (LiFT)\nis a straightforward and compact postprocessing network that can be applied to\nenhance the features of any pre-trained ViT backbone. LiFT is fast and easy to\ntrain with a self-supervised objective, and it boosts the density of ViT\nfeatures for minimal extra inference cost. Furthermore, we demonstrate that\nLiFT can be applied with approaches that use additional task-specific\ndownstream modules, as we integrate LiFT with ViTDet for COCO detection and\nsegmentation. Despite the simplicity of LiFT, we find that it is not simply\nlearning a more complex version of bilinear interpolation. Instead, our LiFT\ntraining protocol leads to several desirable emergent properties that benefit\nViT features in dense downstream tasks. This includes greater scale invariance\nfor features, and better object boundary maps. By simply training LiFT for a\nfew epochs, we show improved performance on keypoint correspondence, detection,\nsegmentation, and object discovery tasks. Overall, LiFT provides an easy way to\nunlock the benefits of denser feature arrays for a fraction of the\ncomputational cost. For more details, refer to our project page at\nhttps://www.cs.umd.edu/~sakshams/LiFT/.\n","authors":["Saksham Suri","Matthew Walmer","Kamal Gupta","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2403.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14626v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer","summary":"  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n","authors":["Tianye Ding","Hongyu Li","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14626v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2403.14623v1","updated":"2024-03-21T17:59:41Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14622v1","updated":"2024-03-21T17:59:35Z","published":"2024-03-21T17:59:35Z","title":"Language Repository for Long Video Understanding","summary":"  Language has become a prominent modality in computer vision with the rise of\nmulti-modal LLMs. Despite supporting long context-lengths, their effectiveness\nin handling long-term information gradually declines with input length. This\nbecomes critical, especially in applications such as long-form video\nunderstanding. In this paper, we introduce a Language Repository (LangRepo) for\nLLMs, that maintains concise and structured information as an interpretable\n(i.e., all-textual) representation. Our repository is updated iteratively based\non multi-scale video chunks. We introduce write and read operations that focus\non pruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.\n","authors":["Kumara Kahatapitiya","Kanchana Ranasinghe","Jongwoo Park","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2403.14622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14621v1","updated":"2024-03-21T17:59:34Z","published":"2024-03-21T17:59:34Z","title":"GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n  and Generation","summary":"  We introduce GRM, a large-scale reconstructor capable of recovering a 3D\nasset from sparse-view images in around 0.1s. GRM is a feed-forward\ntransformer-based model that efficiently incorporates multi-view information to\ntranslate the input pixels into pixel-aligned Gaussians, which are unprojected\nto create a set of densely distributed 3D Gaussians representing a scene.\nTogether, our transformer architecture and the use of 3D Gaussians unlock a\nscalable and efficient reconstruction framework. Extensive experimental results\ndemonstrate the superiority of our method over alternatives regarding both\nreconstruction quality and efficiency. We also showcase the potential of GRM in\ngenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it with\nexisting multi-view diffusion models. Our project website is at:\nhttps://justimyhxu.github.io/projects/grm/.\n","authors":["Yinghao Xu","Zifan Shi","Wang Yifan","Hansheng Chen","Ceyuan Yang","Sida Peng","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2403.14621v1.pdf","comment":"Project page: https://justimyhxu.github.io/projects/grm/ Code:\n  https://github.com/justimyhxu/GRM"},{"id":"http://arxiv.org/abs/2403.14619v1","updated":"2024-03-21T17:59:16Z","published":"2024-03-21T17:59:16Z","title":"ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition","summary":"  3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.\n","authors":["Tianhao Wu","Chuanxia Zheng","Tat-Jen Cham","Qianyi Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14619v1.pdf","comment":"Project Page: https://sm0kywu.github.io/ClusteringSDF/"},{"id":"http://arxiv.org/abs/2403.14617v1","updated":"2024-03-21T17:59:03Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14616v1","updated":"2024-03-21T17:58:56Z","published":"2024-03-21T17:58:56Z","title":"Hierarchical Text-to-Vision Self Supervised Alignment for Improved\n  Histopathology Representation Learning","summary":"  Self-supervised representation learning has been highly promising for\nhistopathology image analysis with numerous approaches leveraging their\npatient-slide-patch hierarchy to learn better representations. In this paper,\nwe explore how the combination of domain specific natural language information\nwith such hierarchical visual representations can benefit rich representation\nlearning for medical image tasks. Building on automated language description\ngeneration for features visible in histopathology images, we present a novel\nlanguage-tied self-supervised learning framework, Hierarchical Language-tied\nSelf-Supervision (HLSS) for histopathology images. We explore contrastive\nobjectives and granular language description based text alignment at multiple\nhierarchies to inject language modality information into the visual\nrepresentations. Our resulting model achieves state-of-the-art performance on\ntwo medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also\nprovides better interpretability with our language aligned representation\nspace. Code is available at https://github.com/Hasindri/HLSS.\n","authors":["Hasindri Watawana","Kanchana Ranasinghe","Tariq Mahmood","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14616v1.pdf","comment":"13 pages and 5 figures"},{"id":"http://arxiv.org/abs/2403.14614v1","updated":"2024-03-21T17:58:14Z","published":"2024-03-21T17:58:14Z","title":"AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and\n  Modulation","summary":"  In the image acquisition process, various forms of degradation, including\nnoise, haze, and rain, are frequently introduced. These degradations typically\narise from the inherent limitations of cameras or unfavorable ambient\nconditions. To recover clean images from degraded versions, numerous\nspecialized restoration methods have been developed, each targeting a specific\ntype of degradation. Recently, all-in-one algorithms have garnered significant\nattention by addressing different types of degradations within a single model\nwithout requiring prior information of the input degradation type. However,\nthese methods purely operate in the spatial domain and do not delve into the\ndistinct frequency variations inherent to different degradation types. To\naddress this gap, we propose an adaptive all-in-one image restoration network\nbased on frequency mining and modulation. Our approach is motivated by the\nobservation that different degradation types impact the image content on\ndifferent frequency subbands, thereby requiring different treatments for each\nrestoration task. Specifically, we first mine low- and high-frequency\ninformation from the input features, guided by the adaptively decoupled spectra\nof the degraded image. The extracted features are then modulated by a\nbidirectional operator to facilitate interactions between different frequency\ncomponents. Finally, the modulated features are merged into the original input\nfor a progressively guided restoration. With this approach, the model achieves\nadaptive reconstruction by accentuating the informative frequency subbands\naccording to different input degradations. Extensive experiments demonstrate\nthat the proposed method achieves state-of-the-art performance on different\nimage restoration tasks, including denoising, dehazing, deraining, motion\ndeblurring, and low-light image enhancement. Our code is available at\nhttps://github.com/c-yn/AdaIR.\n","authors":["Yuning Cui","Syed Waqas Zamir","Salman Khan","Alois Knoll","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14614v1.pdf","comment":"28 pages,15 figures"},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2403.14611v1","updated":"2024-03-21T17:57:31Z","published":"2024-03-21T17:57:31Z","title":"Explorative Inbetweening of Time and Space","summary":"  We introduce bounded generation as a generalized task to control video\ngeneration to synthesize arbitrary camera and subject motion based only on a\ngiven start and end frame. Our objective is to fully leverage the inherent\ngeneralization capability of an image-to-video model without additional\ntraining or fine-tuning of the original model. This is achieved through the\nproposed new sampling strategy, which we call Time Reversal Fusion, that fuses\nthe temporally forward and backward denoising paths conditioned on the start\nand end frame, respectively. The fused path results in a video that smoothly\nconnects the two frames, generating inbetweening of faithful subject motion,\nnovel views of static scenes, and seamless video looping when the two bounding\nframes are identical. We curate a diverse evaluation dataset of image pairs and\ncompare against the closest existing methods. We find that Time Reversal Fusion\noutperforms related work on all subtasks, exhibiting the ability to generate\ncomplex motions and 3D-consistent views guided by bounded frames. See project\npage at https://time-reversal.github.io.\n","authors":["Haiwen Feng","Zheng Ding","Zhihao Xia","Simon Niklaus","Victoria Abrevaya","Michael J. Black","Xuaner Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14611v1.pdf","comment":"project page at https://time-reversal.github.io"},{"id":"http://arxiv.org/abs/2403.14610v1","updated":"2024-03-21T17:57:03Z","published":"2024-03-21T17:57:03Z","title":"T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy","summary":"  We present T-Rex2, a highly practical model for open-set object detection.\nPrevious open-set object detection methods relying on text prompts effectively\nencapsulate the abstract concept of common objects, but struggle with rare or\ncomplex object representation due to data scarcity and descriptive limitations.\nConversely, visual prompts excel in depicting novel objects through concrete\nvisual examples, but fall short in conveying the abstract concept of objects as\neffectively as text prompts. Recognizing the complementary strengths and\nweaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes\nboth prompts within a single model through contrastive learning. T-Rex2 accepts\ninputs in diverse formats, including text prompts, visual prompts, and the\ncombination of both, so that it can handle different scenarios by switching\nbetween the two prompt modalities. Comprehensive experiments demonstrate that\nT-Rex2 exhibits remarkable zero-shot object detection capabilities across a\nwide spectrum of scenarios. We show that text prompts and visual prompts can\nbenefit from each other within the synergy, which is essential to cover massive\nand complicated real-world scenarios and pave the way towards generic object\ndetection. Model API is now available at\n\\url{https://github.com/IDEA-Research/T-Rex}.\n","authors":["Qing Jiang","Feng Li","Zhaoyang Zeng","Tianhe Ren","Shilong Liu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14610v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2403.14602v1","updated":"2024-03-21T17:52:08Z","published":"2024-03-21T17:52:08Z","title":"ReNoise: Real Image Inversion Through Iterative Noising","summary":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n","authors":["Daniel Garibi","Or Patashnik","Andrey Voynov","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14602v1.pdf","comment":"project page at: https://garibida.github.io/ReNoise-Inversion/"},{"id":"http://arxiv.org/abs/2403.14599v1","updated":"2024-03-21T17:51:01Z","published":"2024-03-21T17:51:01Z","title":"MyVLM: Personalizing VLMs for User-Specific Queries","summary":"  Recent large-scale vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding and generating textual descriptions for visual\ncontent. However, these models lack an understanding of user-specific concepts.\nIn this work, we take a first step toward the personalization of VLMs, enabling\nthem to learn and reason over user-provided concepts. For example, we explore\nwhether these models can learn to recognize you in an image and communicate\nwhat you are doing, tailoring the model to reflect your personal experiences\nand relationships. To effectively recognize a variety of user-specific\nconcepts, we augment the VLM with external concept heads that function as\ntoggles for the model, enabling the VLM to identify the presence of specific\ntarget concepts in a given image. Having recognized the concept, we learn a new\nconcept embedding in the intermediate feature space of the VLM. This embedding\nis tasked with guiding the language model to naturally integrate the target\nconcept in its generated response. We apply our technique to BLIP-2 and LLaVA\nfor personalized image captioning and further show its applicability for\npersonalized visual question-answering. Our experiments demonstrate our ability\nto generalize to unseen images of learned concepts while preserving the model\nbehavior on unrelated inputs.\n","authors":["Yuval Alaluf","Elad Richardson","Sergey Tulyakov","Kfir Aberman","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14599v1.pdf","comment":"Project page: https://snap-research.github.io/MyVLM/"},{"id":"http://arxiv.org/abs/2403.14598v1","updated":"2024-03-21T17:50:47Z","published":"2024-03-21T17:50:47Z","title":"PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model","summary":"  PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address\nthe segmentation task challenges. To overcome the limitation of the LMM being\nlimited to textual output, PSALM incorporates a mask decoder and a\nwell-designed input schema to handle a variety of segmentation tasks. This\nschema includes images, task instructions, conditional prompts, and mask\ntokens, which enable the model to generate and classify segmentation masks\neffectively. The flexible design of PSALM supports joint training across\nmultiple datasets and tasks, leading to improved performance and task\ngeneralization. PSALM achieves superior results on several benchmarks, such as\nRefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,\nand further exhibits zero-shot capabilities on unseen tasks, such as\nopen-vocabulary segmentation, generalized referring expression segmentation and\nvideo object segmentation, making a significant step towards a GPT moment in\ncomputer vision. Through extensive experiments, PSALM demonstrates its\npotential to transform the domain of image segmentation, leveraging the robust\nvisual understanding capabilities of LMMs as seen in natural language\nprocessing. Code and models are available at https://github.com/zamling/PSALM.\n","authors":["Zheng Zhang","Yeyao Ma","Enming Zhang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2403.14598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14594v1","updated":"2024-03-21T17:49:26Z","published":"2024-03-21T17:49:26Z","title":"VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition","summary":"  Recent works on the global place recognition treat the task as a retrieval\nproblem, where an off-the-shelf global descriptor is commonly designed in\nimage-based and LiDAR-based modalities. However, it is non-trivial to perform\naccurate image-LiDAR global place recognition since extracting consistent and\nrobust global descriptors from different domains (2D images and 3D point\nclouds) is challenging. To address this issue, we propose a novel\nVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel\ncorrespondences in a self-supervised manner and brings them into a shared\nfeature space. Specifically, VXP is trained in a two-stage manner that first\nexplicitly exploits local feature correspondences and enforces similarity of\nglobal descriptors. Extensive experiments on the three benchmarks (Oxford\nRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the\nstate-of-the-art cross-modal retrieval by a large margin.\n","authors":["Yun-Jin Li","Mariia Gladkova","Yan Xia","Rui Wang","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2403.14594v1.pdf","comment":"Project page https://yunjinli.github.io/projects-vxp/"},{"id":"http://arxiv.org/abs/2402.19150v2","updated":"2024-03-21T17:26:47Z","published":"2024-02-29T13:31:56Z","title":"Unveiling Typographic Deceptions: Insights of the Typographic\n  Vulnerability in Large Vision-Language Model","summary":"  Large Vision-Language Models (LVLMs) rely on vision encoders and Large\nLanguage Models (LLMs) to exhibit remarkable capabilities on various\nmulti-modal tasks in the joint space of vision and language. However, the\nTypographic Attack, which disrupts vision-language models (VLMs) such as\nContrastive Language-Image Pretraining (CLIP), has also been expected to be a\nsecurity threat to LVLMs. Firstly, we verify typographic attacks on current\nwell-known commercial and open-source LVLMs and uncover the widespread\nexistence of this threat. Secondly, to better assess this vulnerability, we\npropose the most comprehensive and largest-scale Typographic Dataset to date.\nThe Typographic Dataset not only considers the evaluation of typographic\nattacks under various multi-modal tasks but also evaluates the effects of\ntypographic attacks, influenced by texts generated with diverse factors. Based\non the evaluation results, we investigate the causes why typographic attacks\nmay impact VLMs and LVLMs, leading to three highly insightful discoveries. By\nthe examination of our discoveries and experimental validation in the\nTypographic Dataset, we reduce the performance degradation from $42.07\\%$ to\n$13.90\\%$ when LVLMs confront typographic attacks.\n","authors":["Hao Cheng","Erjia Xiao","Jindong Gu","Le Yang","Jinhao Duan","Jize Zhang","Jiahang Cao","Kaidi Xu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2402.19150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19474v2","updated":"2024-03-21T17:25:52Z","published":"2024-02-29T18:59:17Z","title":"The All-Seeing Project V2: Towards General Relation Comprehension of the\n  Open World","summary":"  We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.\n","authors":["Weiyun Wang","Yiming Ren","Haowen Luo","Tiantong Li","Chenxiang Yan","Zhe Chen","Wenhai Wang","Qingyun Li","Lewei Lu","Xizhou Zhu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2402.19474v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2403.11085v3","updated":"2024-03-21T17:25:23Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14572v1","updated":"2024-03-21T17:20:21Z","published":"2024-03-21T17:20:21Z","title":"Implicit Style-Content Separation using B-LoRA","summary":"  Image stylization involves manipulating the visual appearance and texture\n(style) of an image while preserving its underlying objects, structures, and\nconcepts (content). The separation of style and content is essential for\nmanipulating the image's style independently from its content, ensuring a\nharmonious and visually pleasing result. Achieving this separation requires a\ndeep understanding of both the visual and semantic characteristics of images,\noften necessitating the training of specialized models or employing heavy\noptimization. In this paper, we introduce B-LoRA, a method that leverages LoRA\n(Low-Rank Adaptation) to implicitly separate the style and content components\nof a single image, facilitating various image stylization tasks. By analyzing\nthe architecture of SDXL combined with LoRA, we find that jointly learning the\nLoRA weights of two specific blocks (referred to as B-LoRAs) achieves\nstyle-content separation that cannot be achieved by training each B-LoRA\nindependently. Consolidating the training into only two blocks and separating\nstyle and content allows for significantly improving style manipulation and\novercoming overfitting issues often associated with model fine-tuning. Once\ntrained, the two B-LoRAs can be used as independent components to allow various\nimage stylization tasks, including image style transfer, text-based image\nstylization, consistent style generation, and style-content mixing.\n","authors":["Yarden Frenkel","Yael Vinker","Ariel Shamir","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13444v2","updated":"2024-03-21T17:19:25Z","published":"2024-03-20T09:40:11Z","title":"MedCycle: Unpaired Medical Report Generation via Cycle-Consistency","summary":"  Generating medical reports for X-ray images presents a significant challenge,\nparticularly in unpaired scenarios where access to paired image-report data for\ntraining is unavailable. Previous works have typically learned a joint\nembedding space for images and reports, necessitating a specific labeling\nschema for both. We introduce an innovative approach that eliminates the need\nfor consistent labeling schemas, thereby enhancing data accessibility and\nenabling the use of incompatible datasets. This approach is based on\ncycle-consistent mapping functions that transform image embeddings into report\nembeddings, coupled with report auto-encoding for medical report generation.\nOur model and objectives consider intricate local details and the overarching\nsemantic context within images and reports. This approach facilitates the\nlearning of effective mapping functions, resulting in the generation of\ncoherent reports. It outperforms state-of-the-art results in unpaired chest\nX-ray report generation, demonstrating improvements in both language and\nclinical metrics.\n","authors":["Elad Hirsch","Gefen Dawidowicz","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2403.13444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06860v2","updated":"2024-03-21T17:06:49Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14559v1","updated":"2024-03-21T16:59:45Z","published":"2024-03-21T16:59:45Z","title":"Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation","summary":"  Localizing predefined 3D keypoints in a 2D image is an effective way to\nestablish 3D-2D correspondences for 6DoF object pose estimation. However,\nunreliable localization results of invisible keypoints degrade the quality of\ncorrespondences. In this paper, we address this issue by localizing the\nimportant keypoints in terms of visibility. Since keypoint visibility\ninformation is currently missing in dataset collection process, we propose an\nefficient way to generate binary visibility labels from available object-level\nannotations, for keypoints of both asymmetric objects and symmetric objects. We\nfurther derive real-valued visibility-aware importance from binary labels based\non PageRank algorithm. Taking advantage of the flexibility of our\nvisibility-aware importance, we construct VAPO (Visibility-Aware POse\nestimator) by integrating the visibility-aware importance with a\nstate-of-the-art pose estimation algorithm, along with additional positional\nencoding. Extensive experiments are conducted on popular pose estimation\nbenchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show\nthat, VAPO improves both the keypoint correspondences and final estimated\nposes, and clearly achieves state-of-the-art performances.\n","authors":["Ruyi Lian","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2403.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16274v2","updated":"2024-03-21T16:58:06Z","published":"2023-12-26T15:00:35Z","title":"Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face\n  Synthesis","summary":"  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n","authors":["Jingjing Ren","Cheng Xu","Haoyu Chen","Xinran Qin","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.16274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14554v1","updated":"2024-03-21T16:53:03Z","published":"2024-03-21T16:53:03Z","title":"Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering","summary":"  We propose Gaussian Frosting, a novel mesh-based representation for\nhigh-quality rendering and editing of complex 3D effects in real-time. Our\napproach builds on the recent 3D Gaussian Splatting framework, which optimizes\na set of 3D Gaussians to approximate a radiance field from images. We propose\nfirst extracting a base mesh from Gaussians during optimization, then building\nand refining an adaptive layer of Gaussians with a variable thickness around\nthe mesh to better capture the fine details and volumetric effects near the\nsurface, such as hair or grass. We call this layer Gaussian Frosting, as it\nresembles a coating of frosting on a cake. The fuzzier the material, the\nthicker the frosting. We also introduce a parameterization of the Gaussians to\nenforce them to stay inside the frosting layer and automatically adjust their\nparameters when deforming, rescaling, editing or animating the mesh. Our\nrepresentation allows for efficient rendering using Gaussian splatting, as well\nas editing and animation by modifying the base mesh. We demonstrate the\neffectiveness of our method on various synthetic and real scenes, and show that\nit outperforms existing surface-based approaches. We will release our code and\na web-based viewer as additional contributions. Our project page is the\nfollowing: https://anttwo.github.io/frosting/\n","authors":["Antoine Guédon","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2403.14554v1.pdf","comment":"Project Webpage: https://anttwo.github.io/frosting/"},{"id":"http://arxiv.org/abs/2403.14552v1","updated":"2024-03-21T16:52:27Z","published":"2024-03-21T16:52:27Z","title":"Token Transformation Matters: Towards Faithful Post-hoc Explanation for\n  Vision Transformer","summary":"  While Transformers have rapidly gained popularity in various computer vision\napplications, post-hoc explanations of their internal mechanisms remain largely\nunexplored. Vision Transformers extract visual information by representing\nimage regions as transformed tokens and integrating them via attention weights.\nHowever, existing post-hoc explanation methods merely consider these attention\nweights, neglecting crucial information from the transformed tokens, which\nfails to accurately illustrate the rationales behind the models' predictions.\nTo incorporate the influence of token transformation into interpretation, we\npropose TokenTM, a novel post-hoc explanation method that utilizes our\nintroduced measurement of token transformation effects. Specifically, we\nquantify token transformation effects by measuring changes in token lengths and\ncorrelations in their directions pre- and post-transformation. Moreover, we\ndevelop initialization and aggregation rules to integrate both attention\nweights and token transformation effects across all layers, capturing holistic\ntoken contributions throughout the model. Experimental results on segmentation\nand perturbation tests demonstrate the superiority of our proposed TokenTM\ncompared to state-of-the-art Vision Transformer explanation methods.\n","authors":["Junyi Wu","Bin Duan","Weitai Kang","Hao Tang","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.14552v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.03849v2","updated":"2024-03-21T16:49:20Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nhave been widely used to classify various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevents them from\neffectively extracting features in medical images, while Transformers are\nhampered by their quadratic computational complexity. Recent research has shown\nthat the state space model (SSM) represented by Mamba can efficiently model\nlong-range interactions while maintaining linear computational complexity.\nInspired by this, we propose Vision Mamba for medical image classification\n(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM\ncombines the local feature extraction ability of convolutional layers with the\nability of SSM to capture long-range dependency, thereby modeling medical\nimages with different modalities. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 14 publicly available medical datasets\nwith different imaging techniques and two private datasets built by ourselves.\nExtensive experimental results demonstrate that the proposed MedMamba performs\nwell in detecting lesions in various medical images. To the best of our\nknowledge, this is the first Vision Mamba tailored for medical image\nclassification. The purpose of this work is to establish a new baseline for\nmedical image classification tasks and provide valuable insights for the future\ndevelopment of more efficient and effective SSM-based artificial intelligence\nalgorithms and application systems in the medical. Source code has been\navailable at https://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14548v1","updated":"2024-03-21T16:49:20Z","published":"2024-03-21T16:49:20Z","title":"DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single\n  Video","summary":"  We present DINO-Tracker -- a new framework for long-term dense tracking in\nvideo. The pillar of our approach is combining test-time training on a single\nvideo, with the powerful localized semantic features learned by a pre-trained\nDINO-ViT model. Specifically, our framework simultaneously adopts DINO's\nfeatures to fit to the motion observations of the test video, while training a\ntracker that directly leverages the refined features. The entire framework is\ntrained end-to-end using a combination of self-supervised losses, and\nregularization that allows us to retain and benefit from DINO's semantic prior.\nExtensive evaluation demonstrates that our method achieves state-of-the-art\nresults on known benchmarks. DINO-tracker significantly outperforms\nself-supervised methods and is competitive with state-of-the-art supervised\ntrackers, while outperforming them in challenging cases of tracking under\nlong-term occlusions.\n","authors":["Narek Tumanyan","Assaf Singer","Shai Bagon","Tali Dekel"],"pdf_url":"https://arxiv.org/pdf/2403.14548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14547v1","updated":"2024-03-21T16:48:45Z","published":"2024-03-21T16:48:45Z","title":"Estimating Physical Information Consistency of Channel Data Augmentation\n  for Remote Sensing Images","summary":"  The application of data augmentation for deep learning (DL) methods plays an\nimportant role in achieving state-of-the-art results in supervised,\nsemi-supervised, and self-supervised image classification. In particular,\nchannel transformations (e.g., solarize, grayscale, brightness adjustments) are\nintegrated into data augmentation pipelines for remote sensing (RS) image\nclassification tasks. However, contradicting beliefs exist about their proper\napplications to RS images. A common point of critique is that the application\nof channel augmentation techniques may lead to physically inconsistent spectral\ndata (i.e., pixel signatures). To shed light on the open debate, we propose an\napproach to estimate whether a channel augmentation technique affects the\nphysical information of RS images. To this end, the proposed approach estimates\na score that measures the alignment of a pixel signature within a time series\nthat can be naturally subject to deviations caused by factors such as\nacquisition conditions or phenological states of vegetation. We compare the\nscores associated with original and augmented pixel signatures to evaluate the\nphysical consistency. Experimental results on a multi-label image\nclassification task show that channel augmentations yielding a score that\nexceeds the expected deviation of original pixel signatures can not improve the\nperformance of a baseline model trained without augmentation.\n","authors":["Tom Burgert","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2403.14547v1.pdf","comment":"Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium"},{"id":"http://arxiv.org/abs/2402.17587v2","updated":"2024-03-21T16:40:43Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).\nOur code will be made publicly available at https://github.com/XiaohanLei/IEVE.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14539v1","updated":"2024-03-21T16:40:10Z","published":"2024-03-21T16:40:10Z","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild","summary":"  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n","authors":["Junhyeong Cho","Kim Youwang","Hunmin Yang","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2403.14539v1.pdf","comment":"Project Page: https://ObjectDR.github.io"},{"id":"http://arxiv.org/abs/2403.12167v2","updated":"2024-03-21T16:38:33Z","published":"2024-03-18T18:35:32Z","title":"Generalizing deep learning models for medical image classification","summary":"  Numerous Deep Learning (DL) models have been developed for a large spectrum\nof medical image analysis applications, which promises to reshape various\nfacets of medical practice. Despite early advances in DL model validation and\nimplementation, which encourage healthcare institutions to adopt them, some\nfundamental questions remain: are the DL models capable of generalizing? What\ncauses a drop in DL model performances? How to overcome the DL model\nperformance drop? Medical data are dynamic and prone to domain shift, due to\nmultiple factors such as updates to medical equipment, new imaging workflow,\nand shifts in patient demographics or populations can induce this drift over\ntime. In this paper, we review recent developments in generalization methods\nfor DL-based classification models. We also discuss future challenges,\nincluding the need for improved evaluation protocols and benchmarks, and\nenvisioned future developments to achieve robust, generalized models for\nmedical image classification.\n","authors":["Matta Sarah","Lamard Mathieu","Zhang Philippe","Alexandre Le Guilcher","Laurent Borderie","Béatrice Cochener","Gwenolé Quellec"],"pdf_url":"https://arxiv.org/pdf/2403.12167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14534v1","updated":"2024-03-21T16:36:40Z","published":"2024-03-21T16:36:40Z","title":"Transfer Learning for Cross-dataset Isolated Sign Language Recognition\n  in Under-Resourced Datasets","summary":"  Sign language recognition (SLR) has recently achieved a breakthrough in\nperformance thanks to deep neural networks trained on large annotated sign\ndatasets. Of the many different sign languages, these annotated datasets are\nonly available for a select few. Since acquiring gloss-level labels on sign\nlanguage videos is difficult, learning by transferring knowledge from existing\nannotated sources is useful for recognition in under-resourced sign languages.\nThis study provides a publicly available cross-dataset transfer learning\nbenchmark from two existing public Turkish SLR datasets. We use a temporal\ngraph convolution-based sign language recognition approach to evaluate five\nsupervised transfer learning approaches and experiment with closed-set and\npartial-set cross-dataset transfer learning. Experiments demonstrate that\nimprovement over finetuning based transfer learning is possible with\nspecialized supervised transfer learning methods.\n","authors":["Ahmet Alp Kindiroglu","Ozgur Kara","Ogulcan Ozdemir","Lale Akarun"],"pdf_url":"https://arxiv.org/pdf/2403.14534v1.pdf","comment":"Accepted to The 18th IEEE International Conference on Automatic Face\n  and Gesture Recognition 2024, Code available in\n  https://github.com/alpk/tid-supervised-transfer-learning-dataset"},{"id":"http://arxiv.org/abs/2403.14530v1","updated":"2024-03-21T16:28:58Z","published":"2024-03-21T16:28:58Z","title":"HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To address this, we make use of the relations between the\nunorganized anchors and the structured hash grid, leveraging their mutual\ninformation for context modeling, and propose a Hash-grid Assisted Context\n(HAC) framework for highly compact 3DGS representation. Our approach introduces\na binary hash grid to establish continuous spatial consistencies, allowing us\nto unveil the inherent spatial relations of anchors through a carefully\ndesigned context model. To facilitate entropy coding, we utilize Gaussian\ndistributions to accurately estimate the probability of each quantized\nattribute, where an adaptive quantization module is proposed to enable\nhigh-precision quantization of these attributes for improved fidelity\nrestoration. Additionally, we incorporate an adaptive masking strategy to\neliminate invalid Gaussians and anchors. Importantly, our work is the pioneer\nto explore context-based compression for 3DGS representation, resulting in a\nremarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while\nsimultaneously improving fidelity, and achieving over $11\\times$ size reduction\nover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:\nhttps://github.com/YihangChen-ee/HAC\n","authors":["Yihang Chen","Qianyi Wu","Jianfei Cai","Mehrtash Harandi","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2403.14530v1.pdf","comment":"Project Page: https://yihangchen-ee.github.io/project_hac/ Code:\n  https://github.com/YihangChen-ee/HAC"},{"id":"http://arxiv.org/abs/2403.12966v2","updated":"2024-03-21T16:26:44Z","published":"2024-03-19T17:59:52Z","title":"Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language\n  Models","summary":"  In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot\n","authors":["Zuyan Liu","Yuhao Dong","Yongming Rao","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2403.12966v2.pdf","comment":"Project Page: https://sites.google.com/view/chain-of-spot/"},{"id":"http://arxiv.org/abs/2403.14526v1","updated":"2024-03-21T16:26:19Z","published":"2024-03-21T16:26:19Z","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion\n  Descriptors","summary":"  Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp\n","authors":["Nikolaos Tsagkas","Jack Rome","Subramanian Ramamoorthy","Oisin Mac Aodha","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.14526v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14523v1","updated":"2024-03-21T16:23:25Z","published":"2024-03-21T16:23:25Z","title":"Invisible Needle Detection in Ultrasound: Leveraging Mechanism-Induced\n  Vibration","summary":"  In clinical applications that involve ultrasound-guided intervention, the\nvisibility of the needle can be severely impeded due to steep insertion and\nstrong distractors such as speckle noise and anatomical occlusion. To address\nthis challenge, we propose VibNet, a learning-based framework tailored to\nenhance the robustness and accuracy of needle detection in ultrasound images,\neven when the target becomes invisible to the naked eye. Inspired by Eulerian\nVideo Magnification techniques, we utilize an external step motor to induce\nlow-amplitude periodic motion on the needle. These subtle vibrations offer the\npotential to generate robust frequency features for detecting the motion\npatterns around the needle. To robustly and precisely detect the needle\nleveraging these vibrations, VibNet integrates learning-based\nShort-Time-Fourier-Transform and Hough-Transform modules to achieve successive\nsub-goals, including motion feature extraction in the spatiotemporal space,\nfrequency feature aggregation, and needle detection in the Hough space. Based\non the results obtained on distinct ex vivo porcine and bovine tissue samples,\nthe proposed algorithm exhibits superior detection performance with efficient\ncomputation and generalization capability.\n","authors":["Chenyang Li","Dianye Huang","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14520v1","updated":"2024-03-21T16:17:57Z","published":"2024-03-21T16:17:57Z","title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference","summary":"  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, \\textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster\nspeed due to Cobra's linear sequential modeling. (2) Interestingly, the results\nof closed-set challenging prediction benchmarks show that Cobra performs well\nin overcoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n","authors":["Han Zhao","Min Zhang","Wei Zhao","Pengxiang Ding","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17797v3","updated":"2024-03-21T16:11:23Z","published":"2024-02-26T22:00:59Z","title":"Neural Radiance Fields in Medical Imaging: Challenges and Next Steps","summary":"  Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,\noffer great potential to revolutionize medical imaging by synthesizing\nthree-dimensional representations from the projected two-dimensional image\ndata. However, they face unique challenges when applied to medical\napplications. This paper presents a comprehensive examination of applications\nof NeRFs in medical imaging, highlighting four imminent challenges, including\nfundamental imaging principles, inner structure requirement, object boundary\ndefinition, and color density significance. We discuss current methods on\ndifferent organs and discuss related limitations. We also review several\ndatasets and evaluation metrics and propose several promising directions for\nfuture research.\n","authors":["Xin Wang","Shu Hu","Heng Fan","Hongtu Zhu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2402.17797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14513v1","updated":"2024-03-21T16:08:21Z","published":"2024-03-21T16:08:21Z","title":"View-decoupled Transformer for Person Re-identification under\n  Aerial-ground Camera Network","summary":"  Existing person re-identification methods have achieved remarkable advances\nin appearance-based identity association across homogeneous cameras, such as\nground-ground matching. However, as a more practical scenario, aerial-ground\nperson re-identification (AGPReID) among heterogeneous cameras has received\nminimal attention. To alleviate the disruption of discriminative identity\nrepresentation by dramatic view discrepancy as the most significant challenge\nin AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet\neffective framework. Two major components are designed in VDT to decouple\nview-related and view-unrelated features, namely hierarchical subtractive\nseparation and orthogonal loss, where the former separates these two features\ninside the VDT, and the latter constrains these two to be independent. In\naddition, we contribute a large-scale AGPReID dataset called CARGO, consisting\nof five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.\nExperiments on two datasets show that VDT is a feasible and effective solution\nfor AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on\nCARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational\ncomplexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID\n","authors":["Quan Zhang","Lei Wang","Vishal M. Patel","Xiaohua Xie","Jianhuang Lai"],"pdf_url":"https://arxiv.org/pdf/2403.14513v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.10217v2","updated":"2024-03-21T16:06:17Z","published":"2023-12-15T21:30:49Z","title":"T-MAE: Temporal Masked Autoencoders for Point Cloud Representation\n  Learning","summary":"  The scarcity of annotated data in LiDAR point cloud understanding hinders\neffective representation learning. Consequently, scholars have been actively\ninvestigating efficacious self-supervised pre-training paradigms. Nevertheless,\ntemporal information, which is inherent in the LiDAR point cloud sequence, is\nconsistently disregarded. To better utilize this property, we propose an\neffective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),\nwhich takes as input temporally adjacent frames and learns temporal dependency.\nA SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention\n(WCA) module, is established for the two-frame input. Considering that the\nmovement of an ego-vehicle alters the view of the same instance, temporal\nmodeling also serves as a robust and natural data augmentation, enhancing the\ncomprehension of target objects. SiamWCA is a powerful architecture but heavily\nrelies on annotated data. Our T-MAE pre-training strategy alleviates its demand\nfor annotated data. Comprehensive experiments demonstrate that T-MAE achieves\nthe best performance on both Waymo and ONCE datasets among competitive\nself-supervised approaches.\n","authors":["Weijie Wei","Fatemeh Karimi Nejadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10217v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2312.09641v2","updated":"2024-03-21T15:57:28Z","published":"2023-12-15T09:30:47Z","title":"Ins-HOI: Instance Aware Human-Object Interactions Recovery","summary":"  Accurately modeling detailed interactions between human/hand and object is an\nappealing yet challenging task. Current multi-view capture systems are only\ncapable of reconstructing multiple subjects into a single, unified mesh, which\nfails to model the states of each instance individually during interactions. To\naddress this, previous methods use template-based representations to track\nhuman/hand and object. However, the quality of the reconstructions is limited\nby the descriptive capabilities of the templates so that these methods are\ninherently struggle with geometry details, pressing deformations and invisible\ncontact surfaces. In this work, we propose an end-to-end Instance-aware\nHuman-Object Interactions recovery (Ins-HOI) framework by introducing an\ninstance-level occupancy field representation. However, the real-captured data\nis presented as a holistic mesh, unable to provide instance-level supervision.\nTo address this, we further propose a complementary training strategy that\nleverages synthetic data to introduce instance-level shape priors, enabling the\ndisentanglement of occupancy fields for different instances. Specifically,\nsynthetic data, created by randomly combining individual scans of humans/hands\nand objects, guides the network to learn a coarse prior of instances.\nMeanwhile, real-captured data helps in learning the overall geometry and\nrestricting interpenetration in contact areas. As demonstrated in experiments,\nour method Ins-HOI supports instance-level reconstruction and provides\nreasonable and realistic invisible contact surfaces even in cases of extremely\nclose interaction. To facilitate the research of this task, we collect a\nlarge-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans\nwith real-world human-chair and hand-object interactions. The code and data\nwill be public for research purposes.\n","authors":["Jiajun Zhang","Yuxiang Zhang","Hongwen Zhang","Xiao Zhou","Boyao Zhou","Ruizhi Shao","Zonghai Hu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09641v2.pdf","comment":"Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and\n  Dataset Page: https://github.com/jiajunzhang16/ins-hoi"},{"id":"http://arxiv.org/abs/2403.14499v1","updated":"2024-03-21T15:52:05Z","published":"2024-03-21T15:52:05Z","title":"Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting","summary":"  Monitoring diseases that affect the brain's structural integrity requires\nautomated analysis of magnetic resonance (MR) images, e.g., for the evaluation\nof volumetric changes. However, many of the evaluation tools are optimized for\nanalyzing healthy tissue. To enable the evaluation of scans containing\npathological tissue, it is therefore required to restore healthy tissue in the\npathological areas. In this work, we explore and extend denoising diffusion\nmodels for consistent inpainting of healthy 3D brain tissue. We modify\nstate-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as\nwell as 3D latent and 3D wavelet diffusion models, and train them to synthesize\nhealthy brain tissue. Our evaluation shows that the pseudo-3D model performs\nbest regarding the structural-similarity index, peak signal-to-noise ratio, and\nmean squared error. To emphasize the clinical relevance, we fine-tune this\nmodel on data containing synthetic MS lesions and evaluate it on a downstream\nbrain tissue segmentation task, whereby it outperforms the established FMRIB\nSoftware Library (FSL) lesion-filling method.\n","authors":["Alicia Durrer","Julia Wolleb","Florentin Bieder","Paul Friedrich","Lester Melie-Garcia","Mario Ocampo-Pineda","Cosmin I. Bercea","Ibrahim E. Hamamci","Benedikt Wiestler","Marie Piraud","Özgür Yaldizli","Cristina Granziera","Bjoern H. Menze","Philippe C. Cattin","Florian Kofler"],"pdf_url":"https://arxiv.org/pdf/2403.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14497v1","updated":"2024-03-21T15:46:19Z","published":"2024-03-21T15:46:19Z","title":"MULDE: Multiscale Log-Density Estimation via Denoising Score Matching\n  for Video Anomaly Detection","summary":"  We propose a novel approach to video anomaly detection: we treat feature\nvectors extracted from videos as realizations of a random variable with a fixed\ndistribution and model this distribution with a neural network. This lets us\nestimate the likelihood of test videos and detect video anomalies by\nthresholding the likelihood estimates. We train our video anomaly detector\nusing a modification of denoising score matching, a method that injects\ntraining data with noise to facilitate modeling its distribution. To eliminate\nhyperparameter selection, we model the distribution of noisy video features\nacross a range of noise levels and introduce a regularizer that tends to align\nthe models for different levels of noise. At test time, we combine anomaly\nindications at multiple noise scales with a Gaussian mixture model. Running our\nvideo anomaly detector induces minimal delays as inference requires merely\nextracting the features and forward-propagating them through a shallow neural\nnetwork and a Gaussian mixture model. Our experiments on five popular video\nanomaly detection benchmarks demonstrate state-of-the-art performance, both in\nthe object-centric and in the frame-centric setup.\n","authors":["Jakub Micorek","Horst Possegger","Dominik Narnhofer","Horst Bischof","Mateusz Kozinski"],"pdf_url":"https://arxiv.org/pdf/2403.14497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02116v3","updated":"2024-03-21T15:45:29Z","published":"2023-12-04T18:48:02Z","title":"GIVT: Generative Infinite-Vocabulary Transformers","summary":"  We introduce generative infinite-vocabulary transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a\n$\\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and\nimproved variants thereof) as well as MaskGIT, and achieves performance\ncompetitive with recent latent diffusion models. Finally, we obtain strong\nresults outside of image generation when applying GIVT to panoptic segmentation\nand depth estimation with a VAE variant of the UViM framework\n","authors":["Michael Tschannen","Cian Eastwood","Fabian Mentzer"],"pdf_url":"https://arxiv.org/pdf/2312.02116v3.pdf","comment":"v2: add related NLP work, loss details. v3: Improved GMM formulation,\n  added adapter module, larger models, better image generation results. Code\n  and model checkpoints are available at:\n  https://github.com/google-research/big_vision"},{"id":"http://arxiv.org/abs/2403.14494v1","updated":"2024-03-21T15:42:17Z","published":"2024-03-21T15:42:17Z","title":"Learning to Project for Cross-Task Knowledge Distillation","summary":"  Traditional knowledge distillation (KD) relies on a proficient teacher\ntrained on the target task, which is not always available. In this setting,\ncross-task distillation can be used, enabling the use of any teacher model\ntrained on a different task. However, many KD methods prove ineffective when\napplied to this cross-task setting. To address this limitation, we propose a\nsimple modification: the use of an inverted projection. We show that this\ndrop-in replacement for a standard projector is effective by learning to\ndisregard any task-specific features which might degrade the student's\nperformance. We find that this simple modification is sufficient for extending\nmany KD methods to the cross-task setting, where the teacher and student tasks\ncan be very different. In doing so, we obtain up to a 1.9% improvement in the\ncross-task setting compared to the traditional projection, at no additional\ncost. Our method can obtain significant performance improvements (up to 7%)\nwhen using even a randomly-initialised teacher on various tasks such as depth\nestimation, image translation, and semantic segmentation, despite the lack of\nany learned knowledge to transfer. To provide conceptual and analytical\ninsights into this result, we show that using an inverted projection allows the\ndistillation loss to be decomposed into a knowledge transfer and a spectral\nregularisation component. Through this analysis we are additionally able to\npropose a novel regularisation loss that allows teacher-free distillation,\nenabling performance improvements of up to 8.57% on ImageNet with no additional\ntraining costs.\n","authors":["Dylan Auty","Roy Miles","Benedikt Kolbeinsson","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2403.14494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10132v2","updated":"2024-03-21T15:42:06Z","published":"2023-12-15T17:02:19Z","title":"Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against\n  Query-Based Attacks","summary":"  Although promising, existing defenses against query-based attacks share a\ncommon limitation: they offer increased robustness against attacks at the price\nof a considerable accuracy drop on clean samples. In this work, we show how to\nefficiently establish, at test-time, a solid tradeoff between robustness and\naccuracy when mitigating query-based attacks. Given that these attacks\nnecessarily explore low-confidence regions, our insight is that activating\ndedicated defenses, such as random noise defense and random image\ntransformations, only for low-confidence inputs is sufficient to prevent them.\nOur approach is independent of training and supported by theory. We verify the\neffectiveness of our approach for various existing defenses by conducting\nextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm\nthat our proposal can indeed enhance these defenses by providing better\ntradeoffs between robustness and accuracy when compared to state-of-the-art\napproaches while being completely training-free.\n","authors":["Pascal Zimmer","Sébastien Andreina","Giorgia Azzurra Marson","Ghassan Karame"],"pdf_url":"https://arxiv.org/pdf/2312.10132v2.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2024"},{"id":"http://arxiv.org/abs/2403.13261v2","updated":"2024-03-21T15:40:16Z","published":"2024-03-20T02:58:45Z","title":"Self-Supervised Class-Agnostic Motion Prediction with Spatial and\n  Temporal Consistency Regularizations","summary":"  The perception of motion behavior in a dynamic environment holds significant\nimportance for autonomous driving systems, wherein class-agnostic motion\nprediction methods directly predict the motion of the entire point cloud. While\nmost existing methods rely on fully-supervised learning, the manual labeling of\npoint cloud data is laborious and time-consuming. Therefore, several\nannotation-efficient methods have been proposed to address this challenge.\nAlthough effective, these methods rely on weak annotations or additional\nmulti-modal data like images, and the potential benefits inherent in the point\ncloud sequence are still underexplored. To this end, we explore the feasibility\nof self-supervised motion prediction with only unlabeled LiDAR point clouds.\nInitially, we employ an optimal transport solver to establish coarse\ncorrespondences between current and future point clouds as the coarse pseudo\nmotion labels. Training models directly using such coarse labels leads to\nnoticeable spatial and temporal prediction inconsistencies. To mitigate these\nissues, we introduce three simple spatial and temporal regularization losses,\nwhich facilitate the self-supervised training process effectively. Experimental\nresults demonstrate the significant superiority of our approach over the\nstate-of-the-art self-supervised methods.\n","authors":["Kewei Wang","Yizheng Wu","Jun Cen","Zhiyu Pan","Xingyi Li","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13261v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.14489v1","updated":"2024-03-21T15:37:37Z","published":"2024-03-21T15:37:37Z","title":"Adversary-Robust Graph-Based Learning of WSIs","summary":"  Enhancing the robustness of deep learning models against adversarial attacks\nis crucial, especially in critical domains like healthcare where significant\nfinancial interests heighten the risk of such attacks. Whole slide images\n(WSIs) are high-resolution, digitized versions of tissue samples mounted on\nglass slides, scanned using sophisticated imaging equipment. The digital\nanalysis of WSIs presents unique challenges due to their gigapixel size and\nmulti-resolution storage format. In this work, we aim at improving the\nrobustness of cancer Gleason grading classification systems against adversarial\nattacks, addressing challenges at both the image and graph levels. As regards\nthe proposed algorithm, we develop a novel and innovative graph-based model\nwhich utilizes GNN to extract features from the graph representation of WSIs. A\ndenoising module, along with a pooling layer is incorporated to manage the\nimpact of adversarial attacks on the WSIs. The process concludes with a\ntransformer module that classifies various grades of prostate cancer based on\nthe processed data. To assess the effectiveness of the proposed method, we\nconducted a comparative analysis using two scenarios. Initially, we trained and\ntested the model without the denoiser using WSIs that had not been exposed to\nany attack. We then introduced a range of attacks at either the image or graph\nlevel and processed them through the proposed network. The performance of the\nmodel was evaluated in terms of accuracy and kappa scores. The results from\nthis comparison showed a significant improvement in cancer diagnosis accuracy,\nhighlighting the robustness and efficiency of the proposed method in handling\nadversarial challenges in the context of medical imaging.\n","authors":["Saba Heidari Gheshlaghi","Milan Aryal","Nasim Yahyasoltani","Masoud Ganji"],"pdf_url":"https://arxiv.org/pdf/2403.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14487v1","updated":"2024-03-21T15:35:42Z","published":"2024-03-21T15:35:42Z","title":"DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified &\n  Accurate Image Editing","summary":"  Recently, how to achieve precise image editing has attracted increasing\nattention, especially given the remarkable success of text-to-image generation\nmodels. To unify various spatial-aware image editing abilities into one\nframework, we adopt the concept of layers from the design domain to manipulate\nobjects flexibly with various operations. The key insight is to transform the\nspatial-aware image editing task into a combination of two sub-tasks:\nmulti-layered latent decomposition and multi-layered latent fusion. First, we\nsegment the latent representations of the source images into multiple layers,\nwhich include several object layers and one incomplete background layer that\nnecessitates reliable inpainting. To avoid extra tuning, we further explore the\ninner inpainting ability within the self-attention mechanism. We introduce a\nkey-masking self-attention scheme that can propagate the surrounding context\ninformation into the masked region while mitigating its impact on the regions\noutside the mask. Second, we propose an instruction-guided latent fusion that\npastes the multi-layered latent representations onto a canvas latent. We also\nintroduce an artifact suppression scheme in the latent space to enhance the\ninpainting quality. Due to the inherent modular advantages of such\nmulti-layered representations, we can achieve accurate image editing, and we\ndemonstrate that our approach consistently surpasses the latest spatial editing\nmethods, including Self-Guidance and DiffEditor. Last, we show that our\napproach is a unified framework that supports various accurate image editing\ntasks on more than six different editing tasks.\n","authors":["Yueru Jia","Yuhui Yuan","Aosong Cheng","Chuke Wang","Ji Li","Huizhu Jia","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14487v1.pdf","comment":"technical report, 15 pages, webpage: https://design-edit.github.io/"},{"id":"http://arxiv.org/abs/2312.02015v2","updated":"2024-03-21T15:32:35Z","published":"2023-12-04T16:38:16Z","title":"ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy","summary":"  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEX\nscores. Qualitatively, our reconstruction visualizations show much clearer\ntextures and more accurate geometric details. These sufficiently demonstrate\nour superior performance over the state-of-the-art methods.\n","authors":["Yufei Shi","Beijia Lu","Jia-Wei Liu","Ming Li","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.02015v2.pdf","comment":"for Project Page, see https://showlab.github.io/ColonNeRF/"},{"id":"http://arxiv.org/abs/2403.14484v1","updated":"2024-03-21T15:31:28Z","published":"2024-03-21T15:31:28Z","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n","authors":["Mehul Arora","Chirag Shantilal Jain","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju Surampudi"],"pdf_url":"https://arxiv.org/pdf/2403.14484v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2309.15627v2","updated":"2024-03-21T15:17:10Z","published":"2023-09-27T12:58:18Z","title":"Neuromorphic Imaging and Classification with Graph Learning","summary":"  Bio-inspired neuromorphic cameras asynchronously record pixel brightness\nchanges and generate sparse event streams. They can capture dynamic scenes with\nlittle motion blur and more details in extreme illumination conditions. Due to\nthe multidimensional address-event structure, most existing vision algorithms\ncannot properly handle asynchronous event streams. While several event\nrepresentations and processing methods have been developed to address such an\nissue, they are typically driven by a large number of events, leading to\nsubstantial overheads in runtime and memory. In this paper, we propose a new\ngraph representation of the event data and couple it with a Graph Transformer\nto perform accurate neuromorphic classification. Extensive experiments show\nthat our approach leads to better results and excels at the challenging\nrealistic situations where only a small number of events and limited\ncomputational resources are available, paving the way for neuromorphic\napplications embedded into mobile facilities.\n","authors":["Pei Zhang","Chutian Wang","Edmund Y. Lam"],"pdf_url":"https://arxiv.org/pdf/2309.15627v2.pdf","comment":"15 pages, 4 figures, and 7 tables. Accepted by Elsevier\n  Neurocomputing"},{"id":"http://arxiv.org/abs/2403.08262v2","updated":"2024-03-21T15:15:28Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14468v1","updated":"2024-03-21T15:15:00Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Huan Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.14465v1","updated":"2024-03-21T15:13:36Z","published":"2024-03-21T15:13:36Z","title":"CathFlow: Self-Supervised Segmentation of Catheters in Interventional\n  Ultrasound Using Optical Flow and Transformers","summary":"  In minimally invasive endovascular procedures, contrast-enhanced angiography\nremains the most robust imaging technique. However, it is at the expense of the\npatient and clinician's health due to prolonged radiation exposure. As an\nalternative, interventional ultrasound has notable benefits such as being\nradiation-free, fast to deploy, and having a small footprint in the operating\nroom. Yet, ultrasound is hard to interpret, and highly prone to artifacts and\nnoise. Additionally, interventional radiologists must undergo extensive\ntraining before they become qualified to diagnose and treat patients\neffectively, leading to a shortage of staff, and a lack of open-source\ndatasets. In this work, we seek to address both problems by introducing a\nself-supervised deep learning architecture to segment catheters in longitudinal\nultrasound images, without demanding any labeled data. The network architecture\nbuilds upon AiAReSeg, a segmentation transformer built with the Attention in\nAttention mechanism, and is capable of learning feature changes across time and\nspace. To facilitate training, we used synthetic ultrasound data based on\nphysics-driven catheter insertion simulations, and translated the data into a\nunique CT-Ultrasound common domain, CACTUSS, to improve the segmentation\nperformance. We generated ground truth segmentation masks by computing the\noptical flow between adjacent frames using FlowNet2, and performed thresholding\nto obtain a binary map estimate. Finally, we validated our model on a test\ndataset, consisting of unseen synthetic data and images collected from silicon\naorta phantoms, thus demonstrating its potential for applications to clinical\ndata in the future.\n","authors":["Alex Ranne","Liming Kuang","Yordanka Velikova","Nassir Navab","Ferdinando Rodriguez y Baena"],"pdf_url":"https://arxiv.org/pdf/2403.14465v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2309.00903v2","updated":"2024-03-21T15:12:36Z","published":"2023-09-02T10:46:05Z","title":"An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition","summary":"  Explainable AI is crucial in medical imaging. In the challenging field of\nneuroscience, visual topics present a high level of complexity, particularly\nwithin three-dimensional space. The application of neuroscience, which involves\nidentifying brain sulcal features from MRI, faces significant hurdles due to\nvarying annotation protocols among experts and the intricate three-dimension\nfunctionality of the brain. Consequently, traditional explainability approaches\nfall short in effectively validating and evaluating these networks. To address\nthis, we first present a mathematical formulation delineating various\ncategories of explanation needs across diverse computer vision tasks,\ncategorized into self-explanatory, semi-explanatory, non-explanatory, and\nnew-pattern learning applications based on the reliability of the validation\nprotocol. With respect to this mathematical formulation, we propose a 3D\nexplainability framework aimed at validating the outputs of deep learning\nnetworks in detecting the paracingulate sulcus an essential brain anatomical\nfeature. The framework integrates local 3D explanations, global explanations\nthrough dimensionality reduction, concatenated global explanations, and\nstatistical shape features, unveiling new insights into pattern learning. We\ntrained and tested two advanced 3D deep learning networks on the challenging\nTOP-OSLO dataset, significantly improving sulcus detection accuracy,\nparticularly on the left hemisphere. During evaluation with diverse annotation\nprotocols for this dataset, we highlighted the crucial role of an unbiased\nannotation process in achieving precise predictions and effective pattern\nlearning within our proposed 3D framework. The proposed framework not only\nannotates the variable sulcus but also uncovers hidden AI knowledge, promising\nto advance our understanding of brain anatomy and function.\n","authors":["Michail Mamalakis","Heloise de Vareilles","Atheer AI-Manea","Samantha C. Mitchell","Ingrid Arartz","Lynn Egeland Morch-Johnsen","Jane Garrison","Jon Simons","Pietro Lio","John Suckling","Graham Murray"],"pdf_url":"https://arxiv.org/pdf/2309.00903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02041v2","updated":"2024-03-21T14:59:13Z","published":"2024-03-04T13:47:30Z","title":"A Generative Approach for Wikipedia-Scale Visual Entity Recognition","summary":"  In this paper, we address web-scale visual entity recognition, specifically\nthe task of mapping a given query image to one of the 6 million existing\nentities in Wikipedia. One way of approaching a problem of such scale is using\ndual-encoder models (eg CLIP), where all the entity names and query images are\nembedded into a unified space, paving the way for an approximate k-NN search.\nAlternatively, it is also possible to re-purpose a captioning model to directly\ngenerate the entity names for a given image. In contrast, we introduce a novel\nGenerative Entity Recognition (GER) framework, which given an input image\nlearns to auto-regressively decode a semantic and discriminative ``code''\nidentifying the target entity. Our experiments demonstrate the efficacy of this\nGER paradigm, showcasing state-of-the-art performance on the challenging OVEN\nbenchmark. GER surpasses strong captioning, dual-encoder, visual matching and\nhierarchical classification baselines, affirming its advantage in tackling the\ncomplexities of web-scale recognition.\n","authors":["Mathilde Caron","Ahmet Iscen","Alireza Fathi","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2403.02041v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2401.00463v2","updated":"2024-03-21T14:57:25Z","published":"2023-12-31T11:38:50Z","title":"Analyzing Local Representations of Self-supervised Vision Transformers","summary":"  In this paper, we present a comparative analysis of various self-supervised\nVision Transformers (ViTs), focusing on their local representative power.\nInspired by large language models, we examine the abilities of ViTs to perform\nvarious computer vision tasks with little to no fine-tuning. We design\nevaluation framework to analyze the quality of local, i.e.\\ patch-level,\nrepresentations in the context of few-shot semantic segmentation, instance\nidentification, object retrieval and tracking. We discover that contrastive\nlearning based methods like DINO produce more universal patch representations\nthat can be immediately applied for downstream tasks with no parameter tuning,\ncompared to masked image modeling. The embeddings learned using the latter\napproach, e.g. in masked autoencoders, have high variance features that harm\ndistance-based algorithms, such as k-NN, and do not contain useful information\nfor most downstream tasks. Furthermore, we demonstrate that removing these\nhigh-variance features enhances k-NN for MAE, as well as for its recent\nextension Scale-MAE. Finally, we find an object instance retrieval setting\nwhere DINOv2, a model pretrained on two orders of magnitude more data, falls\nshort of its less compute intensive counterpart DINO.\n","authors":["Ani Vanyan","Alvard Barseghyan","Hakob Tamazyan","Vahan Huroyan","Hrant Khachatrian","Martin Danelljan"],"pdf_url":"https://arxiv.org/pdf/2401.00463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14447v1","updated":"2024-03-21T14:53:50Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and \\spot, the quadruped robot\nmanufactured by Boston Dynamics. The key-novelty is the focus on the robot's\nperspective, i.e., on the data captured by the robot's sensors. These make 3D\nbody pose analysis challenging because being close to the ground captures\nhumans only partially. The scenario underlying HARPER includes 15 actions, of\nwhich 10 involve physical contact between the robot and users. The Corpus\ncontains not only the recordings of the built-in stereo cameras of Spot, but\nalso those of a 6-camera OptiTrack system (all recordings are synchronized).\nThis leads to ground-truth skeletal representations with a precision lower than\na millimeter. In addition, the Corpus includes reproducible benchmarks on 3D\nHuman Pose Estimation, Human Pose Forecasting, and Collision Prediction, all\nbased on publicly available baseline approaches. This enables future HARPER\nusers to rigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro. Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07208v2","updated":"2024-03-21T14:52:55Z","published":"2024-01-14T06:07:07Z","title":"Enhanced Few-Shot Class-Incremental Learning via Ensemble Models","summary":"  Few-shot class-incremental learning (FSCIL) aims to continually fit new\nclasses with limited training data, while maintaining the performance of\npreviously learned classes. The main challenges are overfitting the rare new\ntraining samples and forgetting old classes. While catastrophic forgetting has\nbeen extensively studied, the overfitting problem has attracted less attention\nin FSCIL. To tackle overfitting challenge, we design a new ensemble model\nframework cooperated with data augmentation to boost generalization. In this\nway, the enhanced model works as a library storing abundant features to\nguarantee fast adaptation to downstream tasks. Specifically, the multi-input\nmulti-output ensemble structure is applied with a spatial-aware data\naugmentation strategy, aiming at diversifying the feature extractor and\nalleviating overfitting in incremental sessions. Moreover, self-supervised\nlearning is also integrated to further improve the model generalization.\nComprehensive experimental results show that the proposed method can indeed\nmitigate the overfitting problem in FSCIL, and outperform the state-of-the-art\nmethods.\n","authors":["Mingli Zhu","Zihao Zhu","Sihong Chen","Chen Chen","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14442v1","updated":"2024-03-21T14:47:12Z","published":"2024-03-21T14:47:12Z","title":"RoDLA: Benchmarking the Robustness of Document Layout Analysis Models","summary":"  Before developing a Document Layout Analysis (DLA) model in real-world\napplications, conducting comprehensive robustness testing is essential.\nHowever, the robustness of DLA models remains underexplored in the literature.\nTo address this, we are the first to introduce a robustness benchmark for DLA\nmodels, which includes 450K document images of three datasets. To cover\nrealistic corruptions, we propose a perturbation taxonomy with 36 common\ndocument perturbations inspired by real-world document processing.\nAdditionally, to better understand document perturbation impacts, we propose\ntwo metrics, Mean Perturbation Effect (mPE) for perturbation assessment and\nMean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we\nintroduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA),\nwhich improves attention mechanisms to boost extraction of robust features.\nExperiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and\nM$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of\n115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA\nachieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.\n","authors":["Yufan Chen","Jiaming Zhang","Kunyu Peng","Junwei Zheng","Ruiping Liu","Philip Torr","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.14442v1.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://yufanchen96.github.io/projects/RoDLA"},{"id":"http://arxiv.org/abs/2403.14440v1","updated":"2024-03-21T14:45:54Z","published":"2024-03-21T14:45:54Z","title":"Analysing Diffusion Segmentation for Medical Images","summary":"  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n","authors":["Mathias Öttl","Siyuan Mei","Frauke Wilm","Jana Steenpass","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14439v1","updated":"2024-03-21T14:45:41Z","published":"2024-03-21T14:45:41Z","title":"Raw Instinct: Trust Your Classifiers and Skip the Conversion","summary":"  Using RAW-images in computer vision problems is surprisingly underexplored\nconsidering that converting from RAW to RGB does not introduce any new capture\ninformation. In this paper, we show that a sufficiently advanced classifier can\nyield equivalent results on RAW input compared to RGB and present a new public\ndataset consisting of RAW images and the corresponding converted RGB images.\nClassifying images directly from RAW is attractive, as it allows for skipping\nthe conversion to RGB, lowering computation time significantly. Two CNN\nclassifiers are used to classify the images in both formats, confirming that\nclassification performance can indeed be preserved. We furthermore show that\nthe total computation time from RAW image data to classification results for\nRAW images can be up to 8.46 times faster than RGB. These results contribute to\nthe evidence found in related works, that using RAW images as direct input to\ncomputer vision algorithms looks very promising.\n","authors":["Christos Kantas","Bjørk Antoniussen","Mathias V. Andersen","Rasmus Munksø","Shobhit Kotnala","Simon B. Jensen","Andreas Møgelmose","Lau Nørgaard","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2403.14439v1.pdf","comment":"https://www.kaggle.com/datasets/mathiasviborg/raw-instinct"},{"id":"http://arxiv.org/abs/2403.14435v1","updated":"2024-03-21T14:41:58Z","published":"2024-03-21T14:41:58Z","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","summary":"  To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes\n","authors":["Xinyi Zhang","Johanna Sophie Bieri","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2403.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14430v1","updated":"2024-03-21T14:37:50Z","published":"2024-03-21T14:37:50Z","title":"Ranking Distillation for Open-Ended Video Question Answering with\n  Insufficient Labels","summary":"  This paper focuses on open-ended video question answering, which aims to find\nthe correct answers from a large answer set in response to a video-related\nquestion. This is essentially a multi-label classification task, since a\nquestion may have multiple answers. However, due to annotation costs, the\nlabels in existing benchmarks are always extremely insufficient, typically one\nanswer per question. As a result, existing works tend to directly treat all the\nunlabeled answers as negative labels, leading to limited ability for\ngeneralization. In this work, we introduce a simple yet effective ranking\ndistillation framework (RADI) to mitigate this problem without additional\nmanual annotation. RADI employs a teacher model trained with incomplete labels\nto generate rankings for potential answers, which contain rich knowledge about\nlabel priority as well as label-associated visual cues, thereby enriching the\ninsufficient labeling information. To avoid overconfidence in the imperfect\nteacher model, we further present two robust and parameter-free ranking\ndistillation approaches: a pairwise approach which introduces adaptive soft\nmargins to dynamically refine the optimization constraints on various pairwise\nrankings, and a listwise approach which adopts sampling-based partial listwise\nlearning to resist the bias in teacher ranking. Extensive experiments on five\npopular benchmarks consistently show that both our pairwise and listwise RADIs\noutperform state-of-the-art methods. Further analysis demonstrates the\neffectiveness of our methods on the insufficient labeling problem.\n","authors":["Tianming Liang","Chaolei Tan","Beihao Xia","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2403.14430v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14429v1","updated":"2024-03-21T14:36:59Z","published":"2024-03-21T14:36:59Z","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation","summary":"  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n","authors":["Mathias Öttl","Frauke Wilm","Jana Steenpass","Jingna Qiu","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Bernhard Kainz","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18467v3","updated":"2024-03-21T14:33:33Z","published":"2024-02-28T16:43:27Z","title":"Separate and Conquer: Decoupling Co-occurrence via Decomposition and\n  Representation for Weakly Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) with image-level labels aims\nto achieve segmentation tasks without dense annotations. However, attributed to\nthe frequent coupling of co-occurring objects and the limited supervision from\nimage-level labels, the challenging co-occurrence problem is widely present and\nleads to false activation of objects in WSSS. In this work, we devise a\n'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of\nimage space and feature space. In the image space, we propose to 'separate' the\nco-occurring objects with image decomposition by subdividing images into\npatches. Importantly, we assign each patch a category tag from Class Activation\nMaps (CAMs), which spatially helps remove the co-context bias and guide the\nsubsequent representation. In the feature space, we propose to 'conquer' the\nfalse activation by enhancing semantic representation with multi-granularity\nknowledge contrast. To this end, a dual-teacher-single-student architecture is\ndesigned and tag-guided contrast is conducted, which guarantee the correctness\nof knowledge and further facilitate the discrepancy among co-contexts. We\nstreamline the multi-staged WSSS pipeline end-to-end and tackle this issue\nwithout external supervision. Extensive experiments are conducted, validating\nthe efficiency of our method and the superiority over previous single-staged\nand even multi-staged competitors on PASCAL VOC and MS COCO. Code is available\nat https://github.com/zwyang6/SeCo.git.\n","authors":["Zhiwei Yang","Kexue Fu","Minghong Duan","Linhao Qu","Shuo Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2402.18467v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2308.08325v2","updated":"2024-03-21T14:31:56Z","published":"2023-08-16T12:39:39Z","title":"Visually-Aware Context Modeling for News Image Captioning","summary":"  News Image Captioning aims to create captions from news articles and images,\nemphasizing the connection between textual context and visual elements.\nRecognizing the significance of human faces in news images and the face-name\nco-occurrence pattern in existing datasets, we propose a face-naming module for\nlearning better name embeddings. Apart from names, which can be directly linked\nto an image area (faces), news image captions mostly contain context\ninformation that can only be found in the article. We design a retrieval\nstrategy using CLIP to retrieve sentences that are semantically close to the\nimage, mimicking human thought process of linking articles to images.\nFurthermore, to tackle the problem of the imbalanced proportion of article\ncontext and image context in captions, we introduce a simple yet effective\nmethod Contrasting with Language Model backbone (CoLaM) to the training\npipeline. We conduct extensive experiments to demonstrate the efficacy of our\nframework. We out-perform the previous state-of-the-art (without external data)\nby 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at\nhttps://github.com/tingyu215/VACNIC.\n","authors":["Tingyu Qu","Tinne Tuytelaars","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2308.08325v2.pdf","comment":"Accepted at NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.07570v2","updated":"2024-03-21T14:28:15Z","published":"2024-03-12T11:58:37Z","title":"An Active Contour Model Driven By the Hybrid Signed Pressure Function","summary":"  Due to the influence of imaging equipment and complex imaging environments,\nmost images in daily life have features of intensity inhomogeneity and noise.\nTherefore, many scholars have designed many image segmentation algorithms to\naddress these issues. Among them, the active contour model is one of the most\neffective image segmentation algorithms.This paper proposes an active contour\nmodel driven by the hybrid signed pressure function that combines global and\nlocal information construction. Firstly, a new global region-based signed\npressure function is introduced by combining the average intensity of the inner\nand outer regions of the curve with the median intensity of the inner region of\nthe evolution curve. Then, the paper uses the energy differences between the\ninner and outer regions of the curve in the local region to design the signed\npressure function of the local term. Combine the two SPF function to obtain a\nnew signed pressure function and get the evolution equation of the new model.\nFinally, experiments and numerical analysis show that the model has excellent\nsegmentation performance for both intensity inhomogeneous images and noisy\nimages.\n","authors":["Jing Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.07570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15980v2","updated":"2024-03-21T14:21:58Z","published":"2023-11-27T16:26:54Z","title":"Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion","summary":"  Recent advances in generative AI have unveiled significant potential for the\ncreation of 3D content. However, current methods either apply a pre-trained 2D\ndiffusion model with the time-consuming score distillation sampling (SDS), or a\ndirect 3D diffusion model trained on limited 3D data losing generation\ndiversity. In this work, we approach the problem by employing a multi-view 2.5D\ndiffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D\ndiffusion directly models the structural distribution of 3D data, while still\nmaintaining the strong generalization ability of the original 2D diffusion\nmodel, filling the gap between 2D diffusion-based and direct 3D diffusion-based\nmethods for 3D content generation. During inference, multi-view normal maps are\ngenerated using the 2.5D diffusion, and a novel differentiable rasterization\nscheme is introduced to fuse the almost consistent multi-view normal maps into\na consistent 3D model. We further design a normal-conditioned multi-view image\ngeneration module for fast appearance generation given the 3D geometry. Our\nmethod is a one-pass diffusion process and does not require any SDS\noptimization as post-processing. We demonstrate through extensive experiments\nthat, our direct 2.5D generation with the specially-designed fusion scheme can\nachieve diverse, mode-seeking-free, and high-fidelity 3D content generation in\nonly 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.\n","authors":["Yuanxun Lu","Jingyang Zhang","Shiwei Li","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan","Xun Cao","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2311.15980v2.pdf","comment":"CVPR 2024 camera ready, including more evaluations and discussions.\n  Project webpage: https://nju-3dv.github.io/projects/direct25"},{"id":"http://arxiv.org/abs/2403.14421v1","updated":"2024-03-21T14:17:28Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15583v7","updated":"2024-03-21T14:16:39Z","published":"2023-05-24T21:39:27Z","title":"Alleviating Exposure Bias in Diffusion Models through Sampling with\n  Shifted Time Steps","summary":"  Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\nsynthesis of high-quality images. However, their inference process\ncharacteristically requires numerous, potentially hundreds, of iterative steps,\nwhich could exaggerate the problem of exposure bias due to the training and\ninference discrepancy. Previous work has attempted to mitigate this issue by\nperturbing inputs during training, which consequently mandates the retraining\nof the DPM. In this work, we conduct a systematic study of exposure bias in DPM\nand, intriguingly, we find that the exposure bias could be alleviated with a\nnovel sampling method that we propose, without retraining the model. We\nempirically and theoretically show that, during inference, for each backward\ntime step $t$ and corresponding state $\\hat{x}_t$, there might exist another\ntime step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\nthis finding, we introduce a sampling method named Time-Shift Sampler. Our\nframework can be seamlessly integrated to existing sampling algorithms, such as\nDDPM, DDIM and other high-order solvers, inducing merely minimal additional\ncomputations. Experimental results show our method brings significant and\nconsistent improvements in FID scores on different datasets and sampling\nmethods. For example, integrating Time-Shift Sampler to F-PNDM yields a\nFID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\nwith 10 sampling steps, which is more performant than the vanilla DDIM with 100\nsampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.\n","authors":["Mingxiao Li","Tingyu Qu","Ruicong Yao","Wei Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2305.15583v7.pdf","comment":"Accepted at International Conference on Learning Representations\n  (ICLR2024)"},{"id":"http://arxiv.org/abs/2403.14418v1","updated":"2024-03-21T14:06:38Z","published":"2024-03-21T14:06:38Z","title":"OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation","summary":"  The booming of 3D recognition in the 2020s began with the introduction of\npoint cloud transformers. They quickly overwhelmed sparse CNNs and became\nstate-of-the-art models, especially in 3D semantic segmentation. However,\nsparse CNNs are still valuable networks, due to their efficiency treasure, and\nease of application. In this work, we reexamine the design distinctions and\ntest the limits of what a sparse CNN can achieve. We discover that the key\ncredit to the performance difference is adaptivity. Specifically, we propose\ntwo key components, i.e., adaptive receptive fields (spatially) and adaptive\nrelation, to bridge the gap. This exploration led to the creation of\nOmni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a\nlightweight module to greatly enhance the adaptivity of sparse CNNs at minimal\ncomputational cost. Without any self-attention modules, OA-CNNs favorably\nsurpass point transformers in terms of accuracy in both indoor and outdoor\nscenes, with much less latency and memory cost. Notably, it achieves 76.1%,\n78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation\nbenchmarks respectively, while maintaining at most 5x better speed than\ntransformer counterparts. This revelation highlights the potential of pure\nsparse CNNs to outperform transformer-related networks.\n","authors":["Bohao Peng","Xiaoyang Wu","Li Jiang","Yukang Chen","Hengshuang Zhao","Zhuotao Tian","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.14418v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.11504v2","updated":"2024-03-21T14:02:48Z","published":"2024-02-18T08:26:22Z","title":"To use or not to use proprietary street view images in (health and\n  place) research? That is the question","summary":"  Computer vision-based analysis of street view imagery has transformative\nimpacts on environmental assessments. Interactive web services, particularly\nGoogle Street View, play an ever-important role in making imagery data\nubiquitous. Despite the technical ease of harnessing millions of Google Street\nView images, this article questions the current practices in using this\nproprietary data source from a European viewpoint. Our concern lies with\nGoogle's terms of service, which restrict bulk image downloads and the\ngeneration of street view image-based indices. To reconcile the challenge of\nadvancing society through groundbreaking research while maintaining data\nlicense agreements and legal integrity, we believe it is crucial to 1) include\nan author's statement on using proprietary street view data and the directives\nit entails, 2) negotiate academic-specific license to democratize Google Street\nView data access, and 3) adhere to open data principles and utilize open image\nsources for future research.\n","authors":["Marco Helbich","Matthew Danish","SM Labib","Britta Ricker"],"pdf_url":"https://arxiv.org/pdf/2402.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14412v1","updated":"2024-03-21T13:59:00Z","published":"2024-03-21T13:59:00Z","title":"CombiNeRF: A Combination of Regularization Techniques for Few-Shot\n  Neural Radiance Field View Synthesis","summary":"  Neural Radiance Fields (NeRFs) have shown impressive results for novel view\nsynthesis when a sufficiently large amount of views are available. When dealing\nwith few-shot settings, i.e. with a small set of input views, the training\ncould overfit those views, leading to artifacts and geometric and chromatic\ninconsistencies in the resulting rendering. Regularization is a valid solution\nthat helps NeRF generalization. On the other hand, each of the most recent NeRF\nregularization techniques aim to mitigate a specific rendering problem.\nStarting from this observation, in this paper we propose CombiNeRF, a framework\nthat synergically combines several regularization techniques, some of them\nnovel, in order to unify the benefits of each. In particular, we regularize\nsingle and neighboring rays distributions and we add a smoothness term to\nregularize near geometries. After these geometric approaches, we propose to\nexploit Lipschitz regularization to both NeRF density and color networks and to\nuse encoding masks for input features regularization. We show that CombiNeRF\noutperforms the state-of-the-art methods with few-shot settings in several\npublicly available datasets. We also present an ablation study on the LLFF and\nNeRF-Synthetic datasets that support the choices made. We release with this\npaper the open-source implementation of our framework.\n","authors":["Matteo Bonotto","Luigi Sarrocco","Daniele Evangelista","Marco Imperoli","Alberto Pretto"],"pdf_url":"https://arxiv.org/pdf/2403.14412v1.pdf","comment":"This paper has been accepted for publication at the 2024\n  International Conference on 3D Vision (3DV)"},{"id":"http://arxiv.org/abs/2403.14410v1","updated":"2024-03-21T13:57:45Z","published":"2024-03-21T13:57:45Z","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning","summary":"  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Röhrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14410v1.pdf","comment":"This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\""},{"id":"http://arxiv.org/abs/2312.02914v3","updated":"2024-03-21T13:53:48Z","published":"2023-12-05T17:39:19Z","title":"Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training","summary":"  In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.\n","authors":["Arun Reddy","William Paul","Corban Rivera","Ketul Shah","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2312.02914v3.pdf","comment":"Accepted at CVPR 2024. 13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.10938v2","updated":"2024-03-21T13:51:10Z","published":"2022-11-20T10:30:58Z","title":"AI-KD: Adversarial learning and Implicit regularization for\n  self-Knowledge Distillation","summary":"  We present a novel adversarial penalized self-knowledge distillation method,\nnamed adversarial learning and implicit regularization for self-knowledge\ndistillation (AI-KD), which regularizes the training procedure by adversarial\nlearning and implicit distillations. Our model not only distills the\ndeterministic and progressive knowledge which are from the pre-trained and\nprevious epoch predictive probabilities but also transfers the knowledge of the\ndeterministic predictive distributions using adversarial learning. The\nmotivation is that the self-knowledge distillation methods regularize the\npredictive probabilities with soft targets, but the exact distributions may be\nhard to predict. Our method deploys a discriminator to distinguish the\ndistributions between the pre-trained and student models while the student\nmodel is trained to fool the discriminator in the trained procedure. Thus, the\nstudent model not only can learn the pre-trained model's predictive\nprobabilities but also align the distributions between the pre-trained and\nstudent models. We demonstrate the effectiveness of the proposed method with\nnetwork architectures on multiple datasets and show the proposed method\nachieves better performance than state-of-the-art methods.\n","authors":["Hyungmin Kim","Sungho Suh","Sunghyun Baek","Daehwan Kim","Daun Jeong","Hansang Cho","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2211.10938v2.pdf","comment":"Accepted to KBS"},{"id":"http://arxiv.org/abs/2403.14401v1","updated":"2024-03-21T13:49:42Z","published":"2024-03-21T13:49:42Z","title":"Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination","summary":"  Multi-modal Large Language Models (MLLMs) demonstrate remarkable success\nacross various vision-language tasks. However, they suffer from visual\nhallucination, where the generated responses diverge from the provided image.\nAre MLLMs completely oblivious to accurate visual cues when they hallucinate?\nOur investigation reveals that the visual branch may simultaneously advocate\nboth accurate and non-existent content. To address this issue, we propose\nPensieve, a training-free method inspired by our observation that analogous\nvisual hallucinations can arise among images sharing common semantic and\nappearance characteristics. During inference, Pensieve enables MLLMs to\nretrospect relevant images as references and compare them with the test image.\nThis paradigm assists MLLMs in downgrading hallucinatory content mistakenly\nsupported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA\nBench demonstrate the efficacy of Pensieve in mitigating visual hallucination,\nsurpassing other advanced decoding strategies. Additionally, Pensieve aids\nMLLMs in identifying details in the image and enhancing the specificity of\nimage descriptions.\n","authors":["Dingchen Yang","Bowen Cao","Guang Chen","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14392v1","updated":"2024-03-21T13:33:00Z","published":"2024-03-21T13:33:00Z","title":"A Bag of Tricks for Few-Shot Class-Incremental Learning","summary":"  We present a bag of tricks framework for few-shot class-incremental learning\n(FSCIL), which is a challenging form of continual learning that involves\ncontinuous adaptation to new tasks with limited samples. FSCIL requires both\nstability and adaptability, i.e., preserving proficiency in previously learned\ntasks while learning new ones. Our proposed bag of tricks brings together eight\nkey and highly influential techniques that improve stability, adaptability, and\noverall performance under a unified framework for FSCIL. We organize these\ntricks into three categories: stability tricks, adaptability tricks, and\ntraining tricks. Stability tricks aim to mitigate the forgetting of previously\nlearned classes by enhancing the separation between the embeddings of learned\nclasses and minimizing interference when learning new ones. On the other hand,\nadaptability tricks focus on the effective learning of new classes. Finally,\ntraining tricks improve the overall performance without compromising stability\nor adaptability. We perform extensive experiments on three benchmark datasets,\nCIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed\nframework. Our detailed analysis shows that our approach substantially improves\nboth stability and adaptability, establishing a new state-of-the-art by\noutperforming prior works in the area. We believe our method provides a go-to\nsolution and establishes a robust baseline for future research in this area.\n","authors":["Shuvendu Roy","Chunjong Park","Aldi Fahrezi","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2403.14392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12648v3","updated":"2024-03-21T13:23:44Z","published":"2024-01-23T10:56:01Z","title":"Consistency Enhancement-Based Deep Multiview Clustering via Contrastive\n  Learning","summary":"  Multiview clustering (MVC) segregates data samples into meaningful clusters\nby synthesizing information across multiple views. Moreover, deep\nlearning-based methods have demonstrated their strong feature learning\ncapabilities in MVC scenarios. However, effectively generalizing feature\nrepresentations while maintaining consistency is still an intractable problem.\nIn addition, most existing deep clustering methods based on contrastive\nlearning overlook the consistency of the clustering representations during the\nclustering process. In this paper, we show how the above problems can be\novercome and propose a consistent enhancement-based deep MVC method via\ncontrastive learning (CCEC). Specifically, semantic connection blocks are\nincorporated into a feature representation to preserve the consistent\ninformation among multiple views. Furthermore, the representation process for\nclustering is enhanced through spectral clustering, and the consistency across\nmultiple views is improved. Experiments conducted on five datasets demonstrate\nthe effectiveness and superiority of our method in comparison with the\nstate-of-the-art (SOTA) methods. The code for this method can be accessed at\nhttps://anonymous.4open.science/r/CCEC-E84E/.\n","authors":["Hao Yang","Hua Mao","Wai Lok Woo","Jie Chen","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2401.12648v3.pdf","comment":"There are multiple errors that need to be corrected, including some\n  formulas and concept descriptions. We will re upload the paper after the\n  modifications are completed"},{"id":"http://arxiv.org/abs/2403.14379v1","updated":"2024-03-21T13:12:33Z","published":"2024-03-21T13:12:33Z","title":"Tensor network compressibility of convolutional models","summary":"  Convolutional neural networks (CNNs) represent one of the most widely used\nneural network architectures, showcasing state-of-the-art performance in\ncomputer vision tasks. Although larger CNNs generally exhibit higher accuracy,\ntheir size can be effectively reduced by \"tensorization\" while maintaining\naccuracy. Tensorization consists of replacing the convolution kernels with\ncompact decompositions such as Tucker, Canonical Polyadic decompositions, or\nquantum-inspired decompositions such as matrix product states, and directly\ntraining the factors in the decompositions to bias the learning towards\nlow-rank decompositions. But why doesn't tensorization seem to impact the\naccuracy adversely? We explore this by assessing how truncating the convolution\nkernels of dense (untensorized) CNNs impact their accuracy. Specifically, we\ntruncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50\npre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We\nfound that kernels (especially those inside deeper layers) could often be\ntruncated along several cuts resulting in significant loss in kernel norm but\nnot in classification accuracy. This suggests that such ``correlation\ncompression'' (underlying tensorization) is an intrinsic feature of how\ninformation is encoded in dense CNNs. We also found that aggressively truncated\nmodels could often recover the pre-truncation accuracy after only a few epochs\nof re-training, suggesting that compressing the internal correlations of\nconvolution layers does not often transport the model to a worse minimum. Our\nresults can be applied to tensorize and compress CNN models more effectively.\n","authors":["Sukhbinder Singh","Saeed S. Jahromi","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2403.14379v1.pdf","comment":"20 pages, 21 images"},{"id":"http://arxiv.org/abs/2403.14376v1","updated":"2024-03-21T13:06:57Z","published":"2024-03-21T13:06:57Z","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space\n  Complexity","summary":"  The conventional mesh-based Level of Detail (LoD) technique, exemplified by\napplications such as Google Earth and many game engines, exhibits the\ncapability to holistically represent a large scene even the Earth, and achieves\nrendering with a space complexity of O(log n). This constrained data\nrequirement not only enhances rendering efficiency but also facilitates dynamic\ndata fetching, thereby enabling a seamless 3D navigation experience for users.\nIn this work, we extend this proven LoD technique to Neural Radiance Fields\n(NeRF) by introducing an octree structure to represent the scenes in different\nscales. This innovative approach provides a mathematically simple and elegant\nrepresentation with a rendering space complexity of O(log n), aligned with the\nefficiency of mesh-based LoD techniques. We also present a novel training\nstrategy that maintains a complexity of O(n). This strategy allows for parallel\ntraining with minimal overhead, ensuring the scalability and efficiency of our\nproposed method. Our contribution is not only in extending the capabilities of\nexisting techniques but also in establishing a foundation for scalable and\nefficient large-scale scene representation using NeRF and octree structures.\n","authors":["Jiabin Liang","Lanqing Zhang","Zhuoran Zhao","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13307v2","updated":"2024-03-21T13:06:49Z","published":"2024-03-20T05:11:10Z","title":"LaserHuman: Language-guided Scene-aware Human Motion Generation in Free\n  Environment","summary":"  Language-guided scene-aware human motion generation has great significance\nfor entertainment and robotics. In response to the limitations of existing\ndatasets, we introduce LaserHuman, a pioneering dataset engineered to\nrevolutionize Scene-Text-to-Motion research. LaserHuman stands out with its\ninclusion of genuine human motions within 3D environments, unbounded free-form\nnatural language descriptions, a blend of indoor and outdoor scenarios, and\ndynamic, ever-changing scenes. Diverse modalities of capture data and rich\nannotations present great opportunities for the research of conditional motion\ngeneration, and can also facilitate the development of real-life applications.\nMoreover, to generate semantically consistent and physically plausible human\nmotions, we propose a multi-conditional diffusion model, which is simple but\neffective, achieving state-of-the-art performance on existing datasets.\n","authors":["Peishan Cong","Ziyi Wang","Zhiyang Dou","Yiming Ren","Wei Yin","Kai Cheng","Yujing Sun","Xiaoxiao Long","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10404v6","updated":"2024-03-21T12:59:04Z","published":"2023-10-16T13:49:46Z","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph\n  Generation","summary":"  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently\nemerged as an alternative to the fully-supervised approach that heavily relies\non costly annotations. In this regard, studies on WSSGG have utilized image\ncaptions to obtain unlocalized triplets while primarily focusing on grounding\nthe unlocalized triplets over image regions. However, they have overlooked the\ntwo issues involved in the triplet formation process from the captions: 1)\nSemantic over-simplification issue arises when extracting triplets from\ncaptions, where fine-grained predicates in captions are undesirably converted\ninto coarse-grained predicates, resulting in a long-tailed predicate\ndistribution, and 2) Low-density scene graph issue arises when aligning the\ntriplets in the caption with entity/predicate classes of interest, where many\ntriplets are discarded and not used in training, leading to insufficient\nsupervision. To tackle the two issues, we propose a new approach, i.e., Large\nLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two\nissues by leveraging the LLM's in-depth understanding of language and reasoning\nability during the extraction of triplets from captions and alignment of\nentity/predicate classes with target data. To further engage the LLM in these\nprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shot\nlearning strategy. To validate the effectiveness of LLM4SGG, we conduct\nextensive experiments on Visual Genome and GQA datasets, showing significant\nimprovements in both Recall@K and mean Recall@K compared to the\nstate-of-the-art WSSGG methods. A further appeal is that LLM4SGG is\ndata-efficient, enabling effective model training with a small amount of\ntraining images.\n","authors":["Kibum Kim","Kanghoon Yoon","Jaehyeong Jeon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2310.10404v6.pdf","comment":"8 pages; CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14370v1","updated":"2024-03-21T12:57:30Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v1.pdf","comment":"Project page: https://synctweedies.github.io/"},{"id":"http://arxiv.org/abs/2312.12274v2","updated":"2024-03-21T12:51:31Z","published":"2023-12-19T15:56:19Z","title":"Intrinsic Image Diffusion for Indoor Single-view Material Estimation","summary":"  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Peter Kocsis","Vincent Sitzmann","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.12274v2.pdf","comment":"Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/\n  Video: https://youtu.be/lz0meJlj5cA"},{"id":"http://arxiv.org/abs/2403.14368v1","updated":"2024-03-21T12:50:15Z","published":"2024-03-21T12:50:15Z","title":"Enabling Visual Composition and Animation in Unsupervised Video\n  Generation","summary":"  In this work we propose a novel method for unsupervised controllable video\ngeneration. Once trained on a dataset of unannotated videos, at inference our\nmodel is capable of both composing scenes of predefined object parts and\nanimating them in a plausible and controlled way. This is achieved by\nconditioning video generation on a randomly selected subset of local\npre-trained self-supervised features during training. We call our model CAGE\nfor visual Composition and Animation for video GEneration. We conduct a series\nof experiments to demonstrate capabilities of CAGE in various settings. Project\nwebsite: https://araachie.github.io/cage.\n","authors":["Aram Davtyan","Sepehr Sameni","Björn Ommer","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2403.14368v1.pdf","comment":"Project website: https://araachie.github.io/cage"},{"id":"http://arxiv.org/abs/2403.14366v1","updated":"2024-03-21T12:49:32Z","published":"2024-03-21T12:49:32Z","title":"SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance\n  Field","summary":"  Vision-centric 3D environment understanding is both vital and challenging for\nautonomous driving systems. Recently, object-free methods have attracted\nconsiderable attention. Such methods perceive the world by predicting the\nsemantics of discrete voxel grids but fail to construct continuous and accurate\nobstacle surfaces. To this end, in this paper, we propose SurroundSDF to\nimplicitly predict the signed distance field (SDF) and semantic field for the\ncontinuous perception from surround images. Specifically, we introduce a\nquery-based approach and utilize SDF constrained by the Eikonal formulation to\naccurately describe the surfaces of obstacles. Furthermore, considering the\nabsence of precise SDF ground truth, we propose a novel weakly supervised\nparadigm for SDF, referred to as the Sandwich Eikonal formulation, which\nemphasizes applying correct and dense constraints on both sides of the surface,\nthereby enhancing the perceptual accuracy of the surface. Experiments suggest\nthat our method achieves SOTA for both occupancy prediction and 3D scene\nreconstruction tasks on the nuScenes dataset.\n","authors":["Lizhe Liu","Bohua Wang","Hongwei Xie","Daqi Liu","Li Liu","Zhiqiang Tian","Kuiyuan Yang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14362v1","updated":"2024-03-21T12:45:01Z","published":"2024-03-21T12:45:01Z","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics","summary":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n","authors":["Jiaqi Yue","Jiancheng Zhao","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.14362v1.pdf","comment":"This work is submitted to IEEE TNNLS and is subject to IEEE copyright"},{"id":"http://arxiv.org/abs/2311.14758v2","updated":"2024-03-21T12:43:32Z","published":"2023-11-23T15:57:41Z","title":"Point2RBox: Combine Knowledge from Synthetic Visual Patterns for\n  End-to-end Oriented Object Detection with Single Point Supervision","summary":"  With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning rotated box\n(RBox) from the horizontal box (HBox) has attracted more and more attention. In\nthis paper, we explore a more challenging yet label-efficient setting, namely\nsingle point-supervised OOD, and present our approach called Point2RBox.\nSpecifically, we propose to leverage two principles: 1) Synthetic pattern\nknowledge combination: By sampling around each labeled point on the image, we\nspread the object feature to synthetic visual patterns with known boxes to\nprovide the knowledge for box regression. 2) Transform self-supervision: With a\ntransformed input image (e.g. scaled/rotated), the output RBoxes are trained to\nfollow the same transformation so that the network can perceive the relative\nsize/rotation between objects. The detector is further enhanced by a few\ndevised techniques to cope with peripheral issues, e.g. the anchor/layer\nassignment as the size of the object is not available in our point supervision\nsetting. To our best knowledge, Point2RBox is the first end-to-end solution for\npoint-supervised OOD. In particular, our method uses a lightweight paradigm,\nyet it achieves a competitive performance among point-supervised alternatives,\n41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.\n","authors":["Yi Yu","Xue Yang","Qingyun Li","Feipeng Da","Jifeng Dai","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.14758v2.pdf","comment":"10 pages, 3 figures, 5 tables, code:\n  https://github.com/yuyi1005/point2rbox-mmrotate"},{"id":"http://arxiv.org/abs/2403.14359v1","updated":"2024-03-21T12:40:41Z","published":"2024-03-21T12:40:41Z","title":"Varroa destructor detection on honey bees using hyperspectral imagery","summary":"  Hyperspectral (HS) imagery in agriculture is becoming increasingly common.\nThese images have the advantage of higher spectral resolution. Advanced\nspectral processing techniques are required to unlock the information potential\nin these HS images. The present paper introduces a method rooted in\nmultivariate statistics designed to detect parasitic Varroa destructor mites on\nthe body of western honey bee Apis mellifera, enabling easier and continuous\nmonitoring of the bee hives. The methodology explores unsupervised (K-means++)\nand recently developed supervised (Kernel Flows - Partial Least-Squares,\nKF-PLS) methods for parasitic identification. Additionally, in light of the\nemergence of custom-band multispectral cameras, the present research outlines a\nstrategy for identifying the specific wavelengths necessary for effective\nbee-mite separation, suitable for implementation in a custom-band camera.\nIllustrated with a real-case dataset, our findings demonstrate that as few as\nfour spectral bands are sufficient for accurate parasite identification.\n","authors":["Zina-Sabrina Duma","Tomas Zemcik","Simon Bilik","Tuomas Sihvonen","Peter Honec","Satu-Pia Reinikainen","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2403.14359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14354v1","updated":"2024-03-21T12:29:26Z","published":"2024-03-21T12:29:26Z","title":"LDTR: Transformer-based Lane Detection with Anchor-chain Representation","summary":"  Despite recent advances in lane detection methods, scenarios with limited- or\nno-visual-clue of lanes due to factors such as lighting conditions and\nocclusion remain challenging and crucial for automated driving. Moreover,\ncurrent lane representations require complex post-processing and struggle with\nspecific instances. Inspired by the DETR architecture, we propose LDTR, a\ntransformer-based model to address these issues. Lanes are modeled with a novel\nanchor-chain, regarding a lane as a whole from the beginning, which enables\nLDTR to handle special lanes inherently. To enhance lane instance perception,\nLDTR incorporates a novel multi-referenced deformable attention module to\ndistribute attention around the object. Additionally, LDTR incorporates two\nline IoU algorithms to improve convergence efficiency and employs a Gaussian\nheatmap auxiliary branch to enhance model representation capability during\ntraining. To evaluate lane detection models, we rely on Frechet distance,\nparameterized F1-score, and additional synthetic metrics. Experimental results\ndemonstrate that LDTR achieves state-of-the-art performance on well-known\ndatasets.\n","authors":["Zhongyu Yang","Chen Shen","Wei Shao","Tengfei Xing","Runbo Hu","Pengfei Xu","Hua Chai","Ruini Xue"],"pdf_url":"https://arxiv.org/pdf/2403.14354v1.pdf","comment":"Accepted by CVM 2024 and CVMJ. 16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.14350v1","updated":"2024-03-21T12:25:17Z","published":"2024-03-21T12:25:17Z","title":"Annotation-Efficient Polyp Segmentation via Active Learning","summary":"  Deep learning-based techniques have proven effective in polyp segmentation\ntasks when provided with sufficient pixel-wise labeled data. However, the high\ncost of manual annotation has created a bottleneck for model generalization. To\nminimize annotation costs, we propose a deep active learning framework for\nannotation-efficient polyp segmentation. In practice, we measure the\nuncertainty of each sample by examining the similarity between features masked\nby the prediction map of the polyp and the background area. Since the\nsegmentation model tends to perform weak in samples with indistinguishable\nfeatures of foreground and background areas, uncertainty sampling facilitates\nthe fitting of under-learning data. Furthermore, clustering image-level\nfeatures weighted by uncertainty identify samples that are both uncertain and\nrepresentative. To enhance the selectivity of the active selection strategy, we\npropose a novel unsupervised feature discrepancy learning mechanism. The\nselection strategy and feature optimization work in tandem to achieve optimal\nperformance with a limited annotation budget. Extensive experimental results\nhave demonstrated that our proposed method achieved state-of-the-art\nperformance compared to other competitors on both a public dataset and a\nlarge-scale in-house dataset.\n","authors":["Duojun Huang","Xinyu Xiong","De-Jun Fan","Feng Gao","Xiao-Jian Wu","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.14350v1.pdf","comment":"2024 IEEE 21th International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2403.14349v1","updated":"2024-03-21T12:24:53Z","published":"2024-03-21T12:24:53Z","title":"On the Concept Trustworthiness in Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs), which break down the reasoning process into\nthe input-to-concept mapping and the concept-to-label prediction, have garnered\nsignificant attention due to their remarkable interpretability achieved by the\ninterpretable concept bottleneck. However, despite the transparency of the\nconcept-to-label prediction, the mapping from the input to the intermediate\nconcept remains a black box, giving rise to concerns about the trustworthiness\nof the learned concepts (i.e., these concepts may be predicted based on\nspurious cues). The issue of concept untrustworthiness greatly hampers the\ninterpretability of CBMs, thereby hindering their further advancement. To\nconduct a comprehensive analysis on this issue, in this study we establish a\nbenchmark to assess the trustworthiness of concepts in CBMs. A pioneering\nmetric, referred to as concept trustworthiness score, is proposed to gauge\nwhether the concepts are derived from relevant regions. Additionally, an\nenhanced CBM is introduced, enabling concept predictions to be made\nspecifically from distinct parts of the feature map, thereby facilitating the\nexploration of their related regions. Besides, we introduce three modules,\nnamely the cross-layer alignment (CLA) module, the cross-image alignment (CIA)\nmodule, and the prediction alignment (PA) module, to further enhance the\nconcept trustworthiness within the elaborated CBM. The experiments on five\ndatasets across ten architectures demonstrate that without using any concept\nlocalization annotations during training, our model improves the concept\ntrustworthiness by a large margin, meanwhile achieving superior accuracy to the\nstate-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.\n","authors":["Qihan Huang","Jie Song","Jingwen Hu","Haofei Zhang","Yong Wang","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2403.14349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14346v1","updated":"2024-03-21T12:23:29Z","published":"2024-03-21T12:23:29Z","title":"Towards Efficient Information Fusion: Concentric Dual Fusion Attention\n  Based Multiple Instance Learning for Whole Slide Images","summary":"  In the realm of digital pathology, multi-magnification Multiple Instance\nLearning (multi-mag MIL) has proven effective in leveraging the hierarchical\nstructure of Whole Slide Images (WSIs) to reduce information loss and redundant\ndata. However, current methods fall short in bridging the domain gap between\npretrained models and medical imaging, and often fail to account for spatial\nrelationships across different magnifications. Addressing these challenges, we\nintroduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which\ninnovatively combines point-to-area feature-colum attention and point-to-point\nconcentric-row attention using concentric patch. This approach is designed to\neffectively fuse correlated information, enhancing feature representation and\nproviding stronger correlation guidance for WSI analysis. CDFA-MIL\ndistinguishes itself by offering a robust fusion strategy that leads to\nsuperior WSI recognition. Its application has demonstrated exceptional\nperformance, significantly surpassing existing MIL methods in accuracy and F1\nscores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically,\nCDFA-MIL achieved an average accuracy and F1-score of 93.7\\% and 94.1\\%\nrespectively on these datasets, marking a notable advancement over traditional\nMIL approaches.\n","authors":["Yujian Liu","Ruoxuan Wu","Xinjie Shen","Zihuang Lu","Lingyu Liang","Haiyu Zhou","Shipu Xu","Shaoai Cai","Shidang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14346v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.12670v2","updated":"2024-03-21T12:14:14Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v2.pdf","comment":"Under review. For associated project page, see\n  https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2403.14339v1","updated":"2024-03-21T12:11:26Z","published":"2024-03-21T12:11:26Z","title":"$\\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning","summary":"  Machine Unlearning, the process of selectively eliminating the influence of\ncertain data examples used during a model's training, has gained significant\nattention as a means for practitioners to comply with recent data protection\nregulations. However, existing unlearning methods face critical drawbacks,\nincluding their prohibitively high cost, often associated with a large number\nof hyperparameters, and the limitation of forgetting only relatively small data\nportions. This often makes retraining the model from scratch a quicker and more\neffective solution. In this study, we introduce Gradient-based and\nTask-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework\ndesigned to remove the influence of a subset of training data efficiently. It\napplies adaptive gradient ascent to the data to be forgotten while using\nstandard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple\nbenefits over existing approaches. It enables the unlearning of large sections\nof the training dataset (up to 30%). It is versatile, supporting various\nunlearning tasks (such as subset forgetting or class removal) and applicable\nacross different domains (images, text, etc.). Importantly, $\\nabla \\tau$\nrequires no hyperparameter adjustments, making it a more appealing option than\nretraining the model from scratch. We evaluate our framework's effectiveness\nusing a set of well-established Membership Inference Attack metrics,\ndemonstrating up to 10% enhancements in performance compared to\nstate-of-the-art methods without compromising the original model's accuracy.\n","authors":["Daniel Trippa","Cesare Campagnano","Maria Sofia Bucarelli","Gabriele Tolomei","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2403.14339v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.09780v3","updated":"2024-03-21T12:05:47Z","published":"2023-03-17T05:27:16Z","title":"Mpox-AISM: AI-Mediated Super Monitoring for Mpox and Like-Mpox","summary":"  The key to preventing the spread of mpox (monkeypox) lies in timely,\nconvenient, and accurate diagnosis for earlier-stage infected individuals.\nUnfortunately, the resemblances between common skin diseases and mpox and the\nneed for professional diagnosis inevitably deteriorated the diagnosis of\nearlier-stage patients with Mpox and contributed to its widespread outbreak in\ncrowded areas. Here, we proposed a real-time visualization strategy called\n\"Super Monitoring\" using artificial intelligence and Internet technology,\nthereby performing a low-cost, convenient, timely, and unspecialized diagnosis\nfor earlier-stage mpox. Specifically, such AI-mediated \"super monitoring\"\n(Mpox-AISM) invokes a framework assembled by deep learning models, data\naugmentation, self-supervised learning, and cloud services. Verified by\npublicly available datasets, the Precision, Recall, Specificity, and F1-score\nof Mpox-AISM in diagnosing mpox achieved 99.3%, 94.1%, 99.9%, and 96.6%,\nrespectively. Furthermore, Mpox-AISM's overall accuracy reaches 94.51% in\ndiagnosing mpox, six like-mpox skin diseases, and normal skin. We also employed\ngradient-weighted class activation mapping to explain the decision-making\nprocess of Mpox-AISM, thus handily understanding the specific characteristics\nthat may indicate the mpox's onset and improving its reliability. With the help\nof the Internet and communication terminal, Mpox-AISM can perform a real-time,\nlow-cost, and convenient diagnosis for earlier-stage mpox in various real-world\nsettings, thereby effectively curbing the spread of mpox virus.\n","authors":["Yubiao Yue","Minghua Jiang","Xinyue Zhang","Jialong Xu","Huacong Ye","Fan Zhang","Zhenzhang Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2303.09780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14335v1","updated":"2024-03-21T12:01:54Z","published":"2024-03-21T12:01:54Z","title":"FFT-based Selection and Optimization of Statistics for Robust\n  Recognition of Severely Corrupted Images","summary":"  Improving model robustness in case of corrupted images is among the key\nchallenges to enable robust vision systems on smart devices, such as robotic\nagents. Particularly, robust test-time performance is imperative for most of\nthe applications. This paper presents a novel approach to improve robustness of\nany classification model, especially on severely corrupted images. Our method\n(FROST) employs high-frequency features to detect input image corruption type,\nand select layer-wise feature normalization statistics. FROST provides the\nstate-of-the-art results for different models and datasets, outperforming\ncompetitors on ImageNet-C by up to 37.1% relative gain, improving baseline of\n40.9% mCE on severe corruptions.\n","authors":["Elena Camuffo","Umberto Michieli","Jijoong Moon","Daehyun Kim","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2403.14335v1.pdf","comment":"ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other"},{"id":"http://arxiv.org/abs/2403.14333v1","updated":"2024-03-21T11:58:50Z","published":"2024-03-21T11:58:50Z","title":"CFPL-FAS: Class Free Prompt Learning for Generalizable Face\n  Anti-spoofing","summary":"  Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the\nmodel's performance on unseen domains. Existing methods either rely on domain\nlabels to align domain-invariant feature spaces, or disentangle generalizable\nfeatures from the whole sample, which inevitably lead to the distortion of\nsemantic feature structures and achieve limited generalization. In this work,\nwe make use of large-scale VLMs like CLIP and leverage the textual feature to\ndynamically adjust the classifier's weights for exploring generalizable visual\nfeatures. Specifically, we propose a novel Class Free Prompt Learning (CFPL)\nparadigm for DG FAS, which utilizes two lightweight transformers, namely\nContent Q-Former (CQF) and Style Q-Former (SQF), to learn the different\nsemantic prompts conditioned on content and style features by using a set of\nlearnable query vectors, respectively. Thus, the generalizable prompt can be\nlearned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is\nintroduced to ensure CQF learns visual representation that is most informative\nof the content description. (2) A Diversified Style Prompt (DSP) technology is\nproposed to diversify the learning of style prompts by mixing feature\nstatistics between instance-specific styles. Finally, the learned text features\nmodulate visual features to generalization through the designed Prompt\nModulation (PM). Extensive experiments show that the CFPL is effective and\noutperforms the state-of-the-art methods on several cross-domain datasets.\n","authors":["Ajian Liu","Shuai Xue","Jianwen Gan","Jun Wan","Yanyan Liang","Jiankang Deng","Sergio Escalera","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2403.14333v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14324v1","updated":"2024-03-21T11:44:25Z","published":"2024-03-21T11:44:25Z","title":"Neural Network-Based Processing and Reconstruction of Compromised\n  Biophotonic Image Data","summary":"  The integration of deep learning techniques with biophotonic setups has\nopened new horizons in bioimaging. A compelling trend in this field involves\ndeliberately compromising certain measurement metrics to engineer better\nbioimaging tools in terms of cost, speed, and form-factor, followed by\ncompensating for the resulting defects through the utilization of deep learning\nmodels trained on a large amount of ideal, superior or alternative data. This\nstrategic approach has found increasing popularity due to its potential to\nenhance various aspects of biophotonic imaging. One of the primary motivations\nfor employing this strategy is the pursuit of higher temporal resolution or\nincreased imaging speed, critical for capturing fine dynamic biological\nprocesses. This approach also offers the prospect of simplifying hardware\nrequirements/complexities, thereby making advanced imaging standards more\naccessible in terms of cost and/or size. This article provides an in-depth\nreview of the diverse measurement aspects that researchers intentionally impair\nin their biophotonic setups, including the point spread function,\nsignal-to-noise ratio, sampling density, and pixel resolution. By deliberately\ncompromising these metrics, researchers aim to not only recuperate them through\nthe application of deep learning networks, but also bolster in return other\ncrucial parameters, such as the field-of-view, depth-of-field, and\nspace-bandwidth product. Here, we discuss various biophotonic methods that have\nsuccessfully employed this strategic approach. These techniques span broad\napplications and showcase the versatility and effectiveness of deep learning in\nthe context of compromised biophotonic data. Finally, by offering our\nperspectives on the future possibilities of this rapidly evolving concept, we\nhope to motivate our readers to explore novel ways of balancing hardware\ncompromises with compensation via AI.\n","authors":["Michael John Fanous","Paloma Casteleiro Costa","Cagatay Isil","Luzhe Huang","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2403.14324v1.pdf","comment":"17 Pages, 4 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2403.14320v1","updated":"2024-03-21T11:41:39Z","published":"2024-03-21T11:41:39Z","title":"Exosense: A Vision-Centric Scene Understanding System For Safe\n  Exoskeleton Navigation","summary":"  Exoskeletons for daily use by those with mobility impairments are being\ndeveloped. They will require accurate and robust scene understanding systems.\nCurrent research has used vision to identify immediate terrain and geometric\nobstacles, however these approaches are constrained to detections directly in\nfront of the user and are limited to classifying a finite range of terrain\ntypes (e.g., stairs, ramps and level-ground). This paper presents Exosense, a\nvision-centric scene understanding system which is capable of generating rich,\nglobally-consistent elevation maps, incorporating both semantic and terrain\ntraversability information. It features an elastic Atlas mapping framework\nassociated with a visual SLAM pose graph, embedded with open-vocabulary room\nlabels from a Vision-Language Model (VLM). The device's design includes a wide\nfield-of-view (FoV) fisheye multi-camera system to mitigate the challenges\nintroduced by the exoskeleton walking pattern. We demonstrate the system's\nrobustness to the challenges of typical periodic walking gaits, and its ability\nto construct accurate semantically-rich maps in indoor settings. Additionally,\nwe showcase its potential for motion planning -- providing a step towards safe\nnavigation for exoskeletons.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Lintong Zhang","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14320v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.14318v1","updated":"2024-03-21T11:40:51Z","published":"2024-03-21T11:40:51Z","title":"A Lightweight Attention-based Deep Network via Multi-Scale Feature\n  Fusion for Multi-View Facial Expression Recognition","summary":"  Convolutional neural networks (CNNs) and their variations have shown\neffectiveness in facial expression recognition (FER). However, they face\nchallenges when dealing with high computational complexity and multi-view head\nposes in real-world scenarios. We introduce a lightweight attentional network\nincorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For\nthe first challenge, we have carefully designed a lightweight fully\nconvolutional network (FCN). We address the second challenge by presenting two\nnovel components, namely mass attention (MassAtt) and point wise feature\nselection (PWFS) blocks. The MassAtt block simultaneously generates channel and\nspatial attention maps to recalibrate feature maps by emphasizing important\nfeatures while suppressing irrelevant ones. On the other hand, the PWFS block\nemploys a feature selection mechanism that discards less meaningful features\nprior to the fusion process. This mechanism distinguishes it from previous\nmethods that directly fuse multi-scale features. Our proposed approach achieved\nresults comparable to state-of-the-art methods in terms of parameter counts and\nrobustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on\nFER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at\nhttps://github.com/AE-1129/LANMSFF.\n","authors":["Ali Ezati","Mohammadreza Dezyani","Rajib Rana","Roozbeh Rajabi","Ahmad Ayatollahi"],"pdf_url":"https://arxiv.org/pdf/2403.14318v1.pdf","comment":"9 pages, two-column, submitted to journal"},{"id":"http://arxiv.org/abs/2403.14302v1","updated":"2024-03-21T11:16:42Z","published":"2024-03-21T11:16:42Z","title":"SpikingResformer: Bridging ResNet and Vision Transformer in Spiking\n  Neural Networks","summary":"  The remarkable success of Vision Transformers in Artificial Neural Networks\n(ANNs) has led to a growing interest in incorporating the self-attention\nmechanism and transformer-based architecture into Spiking Neural Networks\n(SNNs). While existing methods propose spiking self-attention mechanisms that\nare compatible with SNNs, they lack reasonable scaling methods, and the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting local features. To address these challenges, we propose a novel\nspiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a\nreasonable scaling method. Based on DSSA, we propose a novel spiking Vision\nTransformer architecture called SpikingResformer, which combines the\nResNet-based multi-stage architecture with our proposed DSSA to improve both\nperformance and energy efficiency while reducing parameters. Experimental\nresults show that SpikingResformer achieves higher accuracy with fewer\nparameters and lower energy consumption than other spiking Vision Transformer\ncounterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on\nImageNet with 4 time-steps, which is the state-of-the-art result in the SNN\nfield.\n","authors":["Xinyu Shi","Zecheng Hao","Zhaofei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14302v1.pdf","comment":"To be published in the 2024 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.14297v1","updated":"2024-03-21T11:03:56Z","published":"2024-03-21T11:03:56Z","title":"Impact Assessment of Missing Data in Model Predictions for Earth\n  Observation Applications","summary":"  Earth observation (EO) applications involving complex and heterogeneous data\nsources are commonly approached with machine learning models. However, there is\na common assumption that data sources will be persistently available. Different\nsituations could affect the availability of EO sources, like noise, clouds, or\nsatellite mission failures. In this work, we assess the impact of missing\ntemporal and static EO sources in trained models across four datasets with\nclassification and regression tasks. We compare the predictive quality of\ndifferent methods and find that some are naturally more robust to missing data.\nThe Ensemble strategy, in particular, achieves a prediction robustness up to\n100%. We evidence that missing scenarios are significantly more challenging in\nregression than classification tasks. Finally, we find that the optical view is\nthe most critical view when it is missing individually.\n","authors":["Francisco Mena","Diego Arenas","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.14297v1.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2403.14292v1","updated":"2024-03-21T10:59:44Z","published":"2024-03-21T10:59:44Z","title":"HySim: An Efficient Hybrid Similarity Measure for Patch Matching in\n  Image Inpainting","summary":"  Inpainting, for filling missing image regions, is a crucial task in various\napplications, such as medical imaging and remote sensing. Trending data-driven\napproaches efficiency, for image inpainting, often requires extensive data\npreprocessing. In this sense, there is still a need for model-driven approaches\nin case of application constrained with data availability and quality,\nespecially for those related for time series forecasting using image inpainting\ntechniques. This paper proposes an improved modeldriven approach relying on\npatch-based techniques. Our approach deviates from the standard Sum of Squared\nDifferences (SSD) similarity measure by introducing a Hybrid Similarity\n(HySim), which combines both strengths of Chebychev and Minkowski distances.\nThis hybridization enhances patch selection, leading to high-quality inpainting\nresults with reduced mismatch errors. Experimental results proved the\neffectiveness of our approach against other model-driven techniques, such as\ndiffusion or patch-based approaches, showcasing its effectiveness in achieving\nvisually pleasing restorations.\n","authors":["Saad Noufel","Nadir Maaroufi","Mehdi Najib","Mohamed Bakhouya"],"pdf_url":"https://arxiv.org/pdf/2403.14292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14291v1","updated":"2024-03-21T10:56:12Z","published":"2024-03-21T10:56:12Z","title":"Open-Vocabulary Attention Maps with Token Optimization for Semantic\n  Segmentation in Diffusion Models","summary":"  Diffusion models represent a new paradigm in text-to-image generation. Beyond\ngenerating high-quality images from text prompts, models such as Stable\nDiffusion have been successfully extended to the joint generation of semantic\nsegmentation pseudo-masks. However, current extensions primarily rely on\nextracting attentions linked to prompt words used for image synthesis. This\napproach limits the generation of segmentation masks derived from word tokens\nnot contained in the text prompt. In this work, we introduce Open-Vocabulary\nAttention Maps (OVAM)-a training-free method for text-to-image diffusion models\nthat enables the generation of attention maps for any word. In addition, we\npropose a lightweight optimization process based on OVAM for finding tokens\nthat generate accurate attention maps for an object class with a single\nannotation. We evaluate these tokens within existing state-of-the-art Stable\nDiffusion extensions. The best-performing model improves its mIoU from 52.1 to\n86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized\ntokens are an efficient way to improve the performance of existing methods\nwithout architectural changes or retraining.\n","authors":["Pablo Marcos-Manchón","Roberto Alcover-Couso","Juan C. SanMiguel","Jose M. Martínez"],"pdf_url":"https://arxiv.org/pdf/2403.14291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14290v1","updated":"2024-03-21T10:54:21Z","published":"2024-03-21T10:54:21Z","title":"Exploring Green AI for Audio Deepfake Detection","summary":"  The state-of-the-art audio deepfake detectors leveraging deep neural networks\nexhibit impressive recognition performance. Nonetheless, this advantage is\naccompanied by a significant carbon footprint. This is mainly due to the use of\nhigh-performance computing with accelerators and high training time. Studies\nshow that average deep NLP model produces around 626k lbs of\nCO\\textsubscript{2} which is equivalent to five times of average US car\nemission at its lifetime. This is certainly a massive threat to the\nenvironment. To tackle this challenge, this study presents a novel framework\nfor audio deepfake detection that can be seamlessly trained using standard CPU\nresources. Our proposed framework utilizes off-the-shelve self-supervised\nlearning (SSL) based models which are pre-trained and available in public\nrepositories. In contrast to existing methods that fine-tune SSL models and\nemploy additional deep neural networks for downstream tasks, we exploit\nclassical machine learning algorithms such as logistic regression and shallow\nneural networks using the SSL embeddings extracted using the pre-trained model.\nOur approach shows competitive results compared to the commonly used\nhigh-carbon footprint approaches. In experiments with the ASVspoof 2019 LA\ndataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable\nmodel parameters. To encourage further research in this direction and support\nreproducible results, the Python code will be made publicly accessible\nfollowing acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-\n","authors":["Subhajit Saha","Md Sahidullah","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2403.14290v1.pdf","comment":"This manuscript is under review in a conference"},{"id":"http://arxiv.org/abs/2403.14287v1","updated":"2024-03-21T10:51:19Z","published":"2024-03-21T10:51:19Z","title":"Enhancing Historical Image Retrieval with Compositional Cues","summary":"  In analyzing vast amounts of digitally stored historical image data, existing\ncontent-based retrieval methods often overlook significant non-semantic\ninformation, limiting their effectiveness for flexible exploration across\nvaried themes. To broaden the applicability of image retrieval methods for\ndiverse purposes and uncover more general patterns, we innovatively introduce a\ncrucial factor from computational aesthetics, namely image composition, into\nthis topic. By explicitly integrating composition-related information extracted\nby CNN into the designed retrieval model, our method considers both the image's\ncomposition rules and semantic information. Qualitative and quantitative\nexperiments demonstrate that the image retrieval network guided by composition\ninformation outperforms those relying solely on content information,\nfacilitating the identification of images in databases closer to the target\nimage in human perception. Please visit https://github.com/linty5/CCBIR to try\nour codes.\n","authors":["Tingyu Lin","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2403.14287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14286v1","updated":"2024-03-21T10:49:54Z","published":"2024-03-21T10:49:54Z","title":"Assessing the Robustness of Spectral Clustering for Deep Speaker\n  Diarization","summary":"  Clustering speaker embeddings is crucial in speaker diarization but hasn't\nreceived as much focus as other components. Moreover, the robustness of speaker\ndiarization across various datasets hasn't been explored when the development\nand evaluation data are from different domains. To bridge this gap, this study\nthoroughly examines spectral clustering for both same-domain and cross-domain\nspeaker diarization. Our extensive experiments on two widely used corpora, AMI\nand DIHARD, reveal the performance trend of speaker diarization in the presence\nof domain mismatch. We observe that the performance difference between two\ndifferent domain conditions can be attributed to the role of spectral\nclustering. In particular, keeping other modules unchanged, we show that\ndifferences in optimal tuning parameters as well as speaker count estimation\noriginates due to the mismatch. This study opens several future directions for\nspeaker diarization research.\n","authors":["Nikhil Raghav","Md Sahidullah"],"pdf_url":"https://arxiv.org/pdf/2403.14286v1.pdf","comment":"Manuscript Under Review"},{"id":"http://arxiv.org/abs/2403.14279v1","updated":"2024-03-21T10:38:18Z","published":"2024-03-21T10:38:18Z","title":"Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D\n  Pose Estimation","summary":"  Estimating the pose of objects through vision is essential to make robotic\nplatforms interact with the environment. Yet, it presents many challenges,\noften related to the lack of flexibility and generalizability of\nstate-of-the-art solutions. Diffusion models are a cutting-edge neural\narchitecture transforming 2D and 3D computer vision, outlining remarkable\nperformances in zero-shot novel-view synthesis. Such a use case is particularly\nintriguing for reconstructing 3D objects. However, localizing objects in\nunstructured environments is rather unexplored. To this end, this work presents\nZero123-6D to demonstrate the utility of Diffusion Model-based\nnovel-view-synthesizers in enhancing RGB 6D pose estimation at category-level\nby integrating them with feature extraction techniques. The outlined method\nexploits such a novel view synthesizer to expand a sparse set of RGB-only\nreference views for the zero-shot 6D pose estimation task. Experiments are\nquantitatively analyzed on the CO3D dataset, showcasing increased performance\nover baselines, a substantial reduction in data requirements, and the removal\nof the necessity of depth information.\n","authors":["Francesco Di Felice","Alberto Remus","Stefano Gasperini","Benjamin Busam","Lionel Ott","Federico Tombari","Roland Siegwart","Carlo Alberto Avizzano"],"pdf_url":"https://arxiv.org/pdf/2403.14279v1.pdf","comment":"6 pages, 2 reference pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14264v1","updated":"2024-03-21T09:59:53Z","published":"2024-03-21T09:59:53Z","title":"A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity\n  Identification","summary":"  Portrait stylization is a challenging task involving the transformation of an\ninput portrait image into a specific style while preserving its inherent\ncharacteristics. The recent introduction of Stable Diffusion (SD) has\nsignificantly improved the quality of outcomes in this field. However, a\npractical stylization framework that can effectively filter harmful input\ncontent and preserve the distinct characteristics of an input, such as\nskin-tone, while maintaining the quality of stylization remains lacking. These\nchallenges have hindered the wide deployment of such a framework. To address\nthese issues, this study proposes a portrait stylization framework that\nincorporates a nudity content identification module (NCIM) and a\nskin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM\nshowed good performance in enhancing explicit content filtering, and STAPSM\naccurately represented a diverse range of skin tones. Our proposed framework\nhas been successfully deployed in practice, and it has effectively satisfied\ncritical requirements of real-world applications.\n","authors":["Seungkwon Kim","Sangyeon Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2403.14264v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.14262v1","updated":"2024-03-21T09:50:39Z","published":"2024-03-21T09:50:39Z","title":"Diffusion Models with Ensembled Structure-Based Anomaly Scoring for\n  Unsupervised Anomaly Detection","summary":"  Supervised deep learning techniques show promise in medical image analysis.\nHowever, they require comprehensive annotated data sets, which poses\nchallenges, particularly for rare diseases. Consequently, unsupervised anomaly\ndetection (UAD) emerges as a viable alternative for pathology segmentation, as\nonly healthy data is required for training. However, recent UAD anomaly scoring\nfunctions often focus on intensity only and neglect structural differences,\nwhich impedes the segmentation performance. This work investigates the\npotential of Structural Similarity (SSIM) to bridge this gap. SSIM captures\nboth intensity and structural disparities and can be advantageous over the\nclassical $l1$ error. However, we show that there is more than one optimal\nkernel size for the SSIM calculation for different pathologies. Therefore, we\ninvestigate an adaptive ensembling strategy for various kernel sizes to offer a\nmore pathology-agnostic scoring mechanism. We demonstrate that this ensembling\nstrategy can enhance the performance of DMs and mitigate the sensitivity to\ndifferent kernel sizes across varying pathologies, highlighting its promise for\nbrain MRI anomaly detection.\n","authors":["Finn Behrendt","Debayan Bhattacharya","Lennart Maack","Julia Krüger","Roland Opfer","Robin Mieling","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2403.14262v1.pdf","comment":"Accepted at IEEE ISBI 2024"},{"id":"http://arxiv.org/abs/2311.11241v2","updated":"2024-03-21T09:49:50Z","published":"2023-11-19T06:00:39Z","title":"Open-Vocabulary Camouflaged Object Segmentation","summary":"  Recently, the emergence of the large-scale vision-language model (VLM), such\nas CLIP, has opened the way towards open-world object perception. Many works\nhave explored the utilization of pre-trained VLM for the challenging\nopen-vocabulary dense prediction task that requires perceiving diverse objects\nwith novel classes at inference time. Existing methods construct experiments\nbased on the public datasets of related tasks, which are not tailored for open\nvocabulary and rarely involve imperceptible objects camouflaged in complex\nscenes due to data collection bias and annotation costs. To fill in the gaps,\nwe introduce a new task, open-vocabulary camouflaged object segmentation\n(OVCOS), and construct a large-scale complex scene dataset (\\textbf{OVCamo})\ncontaining 11,483 hand-selected images with fine annotations and corresponding\nobject classes. Further, we build a strong single-stage open-vocabulary\n\\underline{c}amouflaged \\underline{o}bject \\underline{s}egmentation\ntransform\\underline{er} baseline \\textbf{OVCoser} attached to the\nparameter-fixed CLIP with iterative semantic guidance and structure\nenhancement. By integrating the guidance of class semantic knowledge and the\nsupplement of visual structure cues from the edge and depth information, the\nproposed method can efficiently capture camouflaged objects. Moreover, this\neffective framework also surpasses previous state-of-the-arts of\nopen-vocabulary semantic image segmentation by a large margin on our OVCamo\ndataset. With the proposed dataset and baseline, we hope that this new task\nwith more practical value can further expand the research on open-vocabulary\ndense prediction tasks. The code and data will be available in the future.\n","authors":["Youwei Pang","Xiaoqi Zhao","Jiaming Zuo","Lihe Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.11241v2.pdf","comment":"Update the style and add details"},{"id":"http://arxiv.org/abs/2308.09951v2","updated":"2024-03-21T09:28:34Z","published":"2023-08-19T09:12:13Z","title":"Semantics Meets Temporal Correspondence: Self-supervised Object-centric\n  Learning in Videos","summary":"  Self-supervised methods have shown remarkable progress in learning high-level\nsemantics and low-level temporal correspondence. Building on these results, we\ntake one step further and explore the possibility of integrating these two\nfeatures to enhance object-centric representations. Our preliminary experiments\nindicate that query slot attention can extract different semantic components\nfrom the RGB feature map, while random sampling based slot attention can\nexploit temporal correspondence cues between frames to assist instance\nidentification. Motivated by this, we propose a novel semantic-aware masked\nslot attention on top of the fused semantic features and correspondence maps.\nIt comprises two slot attention stages with a set of shared learnable Gaussian\ndistributions. In the first stage, we use the mean vectors as slot\ninitialization to decompose potential semantics and generate semantic\nsegmentation masks through iterative attention. In the second stage, for each\nsemantics, we randomly sample slots from the corresponding Gaussian\ndistribution and perform masked feature aggregation within the semantic area to\nexploit temporal correspondence patterns for instance identification. We adopt\nsemantic- and instance-level temporal consistency as self-supervision to\nencourage temporally coherent object-centric representations. Our model\neffectively identifies multiple object instances with semantic structure,\nreaching promising results on unsupervised video object discovery. Furthermore,\nwe achieve state-of-the-art performance on dense label propagation tasks,\ndemonstrating the potential for object-centric analysis. The code is released\nat https://github.com/shvdiwnkozbw/SMTC.\n","authors":["Rui Qian","Shuangrui Ding","Xian Liu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2308.09951v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2403.14252v1","updated":"2024-03-21T09:25:24Z","published":"2024-03-21T09:25:24Z","title":"LayoutLLM: Large Language Model Instruction Tuning for Visually Rich\n  Document Understanding","summary":"  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n","authors":["Masato Fujitake"],"pdf_url":"https://arxiv.org/pdf/2403.14252v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14250v1","updated":"2024-03-21T09:22:23Z","published":"2024-03-21T09:22:23Z","title":"Safeguarding Medical Image Segmentation Datasets against Unauthorized\n  Training via Contour- and Texture-Aware Perturbations","summary":"  The widespread availability of publicly accessible medical images has\nsignificantly propelled advancements in various research and clinical fields.\nNonetheless, concerns regarding unauthorized training of AI systems for\ncommercial purposes and the duties of patient privacy protection have led\nnumerous institutions to hesitate to share their images. This is particularly\ntrue for medical image segmentation (MIS) datasets, where the processes of\ncollection and fine-grained annotation are time-intensive and laborious.\nRecently, Unlearnable Examples (UEs) methods have shown the potential to\nprotect images by adding invisible shortcuts. These shortcuts can prevent\nunauthorized deep neural networks from generalizing. However, existing UEs are\ndesigned for natural image classification and fail to protect MIS datasets\nimperceptibly as their protective perturbations are less learnable than\nimportant prior knowledge in MIS, e.g., contour and texture features. To this\nend, we propose an Unlearnable Medical image generation method, termed UMed.\nUMed integrates the prior knowledge of MIS by injecting contour- and\ntexture-aware perturbations to protect images. Given that our target is to only\npoison features critical to MIS, UMed requires only minimal perturbations\nwithin the ROI and its contour to achieve greater imperceptibility (average\nPSNR is 50.03) and protective performance (clean average DSC degrades from\n82.18% to 6.80%).\n","authors":["Xun Lin","Yi Yu","Song Xia","Jue Jiang","Haoran Wang","Zitong Yu","Yizhong Liu","Ying Fu","Shuai Wang","Wenzhong Tang","Alex Kot"],"pdf_url":"https://arxiv.org/pdf/2403.14250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17351v2","updated":"2024-03-21T09:20:54Z","published":"2024-02-27T09:41:59Z","title":"ICP-Flow: LiDAR Scene Flow Estimation with ICP","summary":"  Scene flow characterizes the 3D motion between two LiDAR scans captured by an\nautonomous vehicle at nearby timesteps. Prevalent methods consider scene flow\nas point-wise unconstrained flow vectors that can be learned by either\nlarge-scale training beforehand or time-consuming optimization at inference.\nHowever, these methods do not take into account that objects in autonomous\ndriving often move rigidly. We incorporate this rigid-motion assumption into\nour design, where the goal is to associate objects over scans and then estimate\nthe locally rigid transformations. We propose ICP-Flow, a learning-free flow\nestimator. The core of our design is the conventional Iterative Closest Point\n(ICP) algorithm, which aligns the objects over time and outputs the\ncorresponding rigid transformations. Crucially, to aid ICP, we propose a\nhistogram-based initialization that discovers the most likely translation, thus\nproviding a good starting point for ICP. The complete scene flow is then\nrecovered from the rigid transformations. We outperform state-of-the-art\nbaselines, including supervised models, on the Waymo dataset and perform\ncompetitively on Argoverse-v2 and nuScenes. Further, we train a feedforward\nneural network, supervised by the pseudo labels from our model, and achieve top\nperformance among all models capable of real-time inference. We validate the\nadvantage of our model on scene flow estimation with longer temporal gaps, up\nto 0.4 seconds where other models fail to deliver meaningful results.\n","authors":["Yancong Lin","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2402.17351v2.pdf","comment":"CVPR 2024, camera-ready. Code: https://github.com/yanconglin/ICP-Flow"},{"id":"http://arxiv.org/abs/2312.10103v3","updated":"2024-03-21T09:20:49Z","published":"2023-12-15T02:54:31Z","title":"GSVA: Generalized Segmentation via Multimodal Large Language Models","summary":"  Generalized Referring Expression Segmentation (GRES) extends the scope of\nclassic RES to refer to multiple objects in one expression or identify the\nempty targets absent in the image. GRES poses challenges in modeling the\ncomplex spatial relationships of the instances in the image and identifying\nnon-existing referents. Multimodal Large Language Models (MLLMs) have recently\nshown tremendous progress in these complicated vision-language tasks.\nConnecting Large Language Models (LLMs) and vision models, MLLMs are proficient\nin understanding contexts with visual inputs. Among them, LISA, as a\nrepresentative, adopts a special [SEG] token to prompt a segmentation mask\ndecoder, e.g., SAM, to enable MLLMs in the RES task. However, existing\nsolutions to GRES remain unsatisfactory since current segmentation MLLMs cannot\ncorrectly handle the cases where users might reference multiple subjects in a\nsingular prompt or provide descriptions incongruent with any image target. In\nthis paper, we propose Generalized Segmentation Vision Assistant (GSVA) to\naddress this gap. Specifically, GSVA reuses the [SEG] token to prompt the\nsegmentation model towards supporting multiple mask references simultaneously\nand innovatively learns to generate a [REJ] token to reject the null targets\nexplicitly. Experiments validate GSVA's efficacy in resolving the GRES issue,\nmarking a notable enhancement and setting a new record on the GRES benchmark\ngRefCOCO dataset. GSVA also proves effective across various classic referring\nsegmentation and comprehension tasks.\n","authors":["Zhuofan Xia","Dongchen Han","Yizeng Han","Xuran Pan","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.10103v3.pdf","comment":"Accepted by CVPR2024 (19 pages, 9 figures, 11 tables)"},{"id":"http://arxiv.org/abs/2309.17189v4","updated":"2024-03-21T09:19:18Z","published":"2023-09-29T12:38:00Z","title":"RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual\n  Speech Separation","summary":"  Audio-visual speech separation methods aim to integrate different modalities\nto generate high-quality separated speech, thereby enhancing the performance of\ndownstream tasks such as speech recognition. Most existing state-of-the-art\n(SOTA) models operate in the time domain. However, their overly simplistic\napproach to modeling acoustic features often necessitates larger and more\ncomputationally intensive models in order to achieve SOTA performance. In this\npaper, we present a novel time-frequency domain audio-visual speech separation\nmethod: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies\nits algorithms on the complex time-frequency bins yielded by the Short-Time\nFourier Transform. We model and capture the time and frequency dimensions of\nthe audio independently using a multi-layered RNN along each dimension.\nFurthermore, we introduce a unique attention-based fusion technique for the\nefficient integration of audio and visual information, and a new mask\nseparation approach that takes advantage of the intrinsic spectral nature of\nthe acoustic features for a clearer separation. RTFS-Net outperforms the prior\nSOTA method in both inference speed and separation quality while reducing the\nnumber of parameters by 90% and MACs by 83%. This is the first time-frequency\ndomain audio-visual speech separation method to outperform all contemporary\ntime-domain counterparts.\n","authors":["Samuel Pegg","Kai Li","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2309.17189v4.pdf","comment":"Accepted by The Twelfth International Conference on Learning\n  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr"},{"id":"http://arxiv.org/abs/2312.08007v2","updated":"2024-03-21T09:09:52Z","published":"2023-12-13T09:29:45Z","title":"Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring\n  Expression Segmentation","summary":"  Referring expression segmentation (RES) aims at segmenting the foreground\nmasks of the entities that match the descriptive natural language expression.\nPrevious datasets and methods for classic RES task heavily rely on the prior\nassumption that one expression must refer to object-level targets. In this\npaper, we take a step further to finer-grained part-level RES task. To promote\nthe object-level RES task towards finer-grained vision-language understanding,\nwe put forward a new multi-granularity referring expression segmentation (MRES)\ntask and construct an evaluation benchmark called RefCOCOm by manual\nannotations. By employing our automatic model-assisted data engine, we build\nthe largest visual grounding dataset namely MRES-32M, which comprises over\n32.2M high-quality masks and captions on the provided 1M images. Besides, a\nsimple yet strong model named UniRES is designed to accomplish the unified\nobject-level and part-level grounding task. Extensive experiments on our\nRefCOCOm for MRES and three datasets (i.e., RefCOCO(+/g) for classic RES task\ndemonstrate the superiority of our method over previous state-of-the-art\nmethods. To foster future research into fine-grained visual grounding, our\nbenchmark RefCOCOm, the MRES-32M dataset and model UniRES will be publicly\navailable at https://github.com/Rubics-Xuan/MRES\n","authors":["Wenxuan Wang","Tongtian Yue","Yisi Zhang","Longteng Guo","Xingjian He","Xinlong Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2312.08007v2.pdf","comment":"This work is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14248v1","updated":"2024-03-21T09:07:28Z","published":"2024-03-21T09:07:28Z","title":"ResNet101 and DAE for Enhance Quality and Classification Accuracy in\n  Skin Cancer Imaging","summary":"  Skin cancer is a crucial health issue that requires timely detection for\nhigher survival rates. Traditional computer vision techniques face challenges\nin addressing the advanced variability of skin lesion features, a gap partially\nbridged by convolutional neural networks (CNNs). To overcome the existing\nissues, we introduce an innovative convolutional ensemble network approach\nnamed deep autoencoder (DAE) with ResNet101. This method utilizes\nconvolution-based deep neural networks for the detection of skin cancer. The\nISIC-2018 public data taken from the source is used for experimental results,\nwhich demonstrate remarkable performance with the different in terms of\nperformance metrics. The methods result in 96.03% of accuracy, 95.40 % of\nprecision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC.\n","authors":["Sibasish Dhibar"],"pdf_url":"https://arxiv.org/pdf/2403.14248v1.pdf","comment":"6 Pages; 14 figures; 3 tables"},{"id":"http://arxiv.org/abs/2403.14244v1","updated":"2024-03-21T09:02:31Z","published":"2024-03-21T09:02:31Z","title":"Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering","summary":"  The 3D Gaussian splatting method has drawn a lot of attention, thanks to its\nhigh performance in training and high quality of the rendered image. However,\nit uses anisotropic Gaussian kernels to represent the scene. Although such\nanisotropic kernels have advantages in representing the geometry, they lead to\ndifficulties in terms of computation, such as splitting or merging two kernels.\nIn this paper, we propose to use isotropic Gaussian kernels to avoid such\ndifficulties in the computation, leading to a higher performance method. The\nexperiments confirm that the proposed method is about {\\bf 100X} faster without\nlosing the geometry representation accuracy. The proposed method can be applied\nin a large range applications where the radiance field is needed, such as 3D\nreconstruction, view synthesis, and dynamic object modeling.\n","authors":["Yuanhao Gong","Lantao Yu","Guanghui Yue"],"pdf_url":"https://arxiv.org/pdf/2403.14244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14243v1","updated":"2024-03-21T09:02:17Z","published":"2024-03-21T09:02:17Z","title":"Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large\n  Language Models with Machine Learning in tele-dermatology","summary":"  The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.\n","authors":["Dimitrios P. Panagoulias","Evridiki Tsoureli-Nikita","Maria Virvou","George A. Tsihrintzis"],"pdf_url":"https://arxiv.org/pdf/2403.14243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14240v1","updated":"2024-03-21T09:01:21Z","published":"2024-03-21T09:01:21Z","title":"Weak Supervision with Arbitrary Single Frame for Micro- and\n  Macro-expression Spotting","summary":"  Frame-level micro- and macro-expression spotting methods require\ntime-consuming frame-by-frame observation during annotation. Meanwhile,\nvideo-level spotting lacks sufficient information about the location and number\nof expressions during training, resulting in significantly inferior performance\ncompared with fully-supervised spotting. To bridge this gap, we propose a\npoint-level weakly-supervised expression spotting (PWES) framework, where each\nexpression requires to be annotated with only one random frame (i.e., a point).\nTo mitigate the issue of sparse label distribution, the prevailing solution is\npseudo-label mining, which, however, introduces new problems: localizing\ncontextual background snippets results in inaccurate boundaries and discarding\nforeground snippets leads to fragmentary predictions. Therefore, we design the\nstrategies of multi-refined pseudo label generation (MPLG) and\ndistribution-guided feature contrastive learning (DFCL) to address these\nproblems. Specifically, MPLG generates more reliable pseudo labels by merging\nclass-specific probabilities, attention scores, fused features, and point-level\nlabels. DFCL is utilized to enhance feature similarity for the same categories\nand feature variability for different categories while capturing global\nrepresentations across the entire datasets. Extensive experiments on the\nCAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising\nperformance comparable to that of recent fully-supervised methods.\n","authors":["Wang-Wang Yu","Xian-Shi Zhang","Fu-Ya Luo","Yijun Cao","Kai-Fu Yang","Hong-Mei Yan","Yong-Jie Li"],"pdf_url":"https://arxiv.org/pdf/2403.14240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13507v2","updated":"2024-03-21T08:54:27Z","published":"2024-03-20T11:05:07Z","title":"FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based\n  LLMs","summary":"  Despite the remarkable performance of video-based large language models\n(LLMs), their adversarial threat remains unexplored. To fill this gap, we\npropose the first adversarial attack tailored for video-based LLMs by crafting\nflow-based multi-modal adversarial perturbations on a small fraction of frames\nwithin a video, dubbed FMM-Attack. Extensive experiments show that our attack\ncan effectively induce video-based LLMs to generate incorrect answers when\nvideos are added with imperceptible adversarial perturbations. Intriguingly,\nour FMM-Attack can also induce garbling in the model output, prompting\nvideo-based LLMs to hallucinate. Overall, our observations inspire a further\nunderstanding of multi-modal robustness and safety-related feature alignment\nacross different modalities, which is of great importance for various large\nmulti-modal models. Our code is available at\nhttps://github.com/THU-Kingmin/FMM-Attack.\n","authors":["Jinmin Li","Kuofeng Gao","Yang Bai","Jingyun Zhang","Shu-tao Xia","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14235v1","updated":"2024-03-21T08:52:39Z","published":"2024-03-21T08:52:39Z","title":"RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU\n  Pilot Survey","summary":"  We present source detection and catalogue construction pipelines to build the\nfirst catalogue of radio galaxies from the 270 $\\rm deg^2$ pilot survey of the\nEvolutionary Map of the Universe (EMU-PS) conducted with the Australian Square\nKilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses\nGal-DINO computer-vision networks (Gupta et al., 2024) to predict the\ncategories of radio morphology and bounding boxes for radio sources, as well as\ntheir potential infrared host positions. The Gal-DINO network is trained and\nevaluated on approximately 5,000 visually inspected radio galaxies and their\ninfrared hosts, encompassing both compact and extended radio morphologies. We\nfind that the Intersection over Union (IoU) for the predicted and ground truth\nbounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of\npredicted host positions are within $3^{\\prime \\prime}$ of the ground truth\ninfrared host in the evaluation set. The catalogue construction pipeline uses\nthe predictions of the trained network on the radio and infrared image cutouts\nbased on the catalogue of radio components identified using the Selavy source\nfinder algorithm. Confidence scores of the predictions are then used to\nprioritize Selavy components with higher scores and incorporate them first into\nthe catalogue. This results in identifications for a total of 211,625 radio\nsources, with 201,211 classified as compact and unresolved. The remaining\n10,414 are categorized as extended radio morphologies, including 582 FR-I,\n5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak\nresolved) radio galaxies, and 361 with peculiar and other rare morphologies. We\ncross-match the radio sources in the catalogue with the infrared and optical\ncatalogues, finding infrared cross-matches for 73% and photometric redshifts\nfor 36% of the radio galaxies.\n","authors":["Nikhel Gupta","Ray P. Norris","Zeeshan Hayder","Minh Huynh","Lars Petersson","X. Rosalind Wang","Andrew M. Hopkins","Heinz Andernach","Yjan Gordon","Simone Riggi","Miranda Yew","Evan J. Crawford","Bärbel Koribalski","Miroslav D. Filipović","Anna D. Kapinśka","Stanislav Shabala","Tessa Vernstrom","Joshua R. Marvil"],"pdf_url":"https://arxiv.org/pdf/2403.14235v1.pdf","comment":"Accepted for publication in PASA. The paper has 22 pages, 12 figures\n  and 5 tables"},{"id":"http://arxiv.org/abs/2212.02340v3","updated":"2024-03-21T08:50:54Z","published":"2022-12-05T15:15:27Z","title":"CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text\n  Detection","summary":"  Recently, segmentation-based methods are quite popular in scene text\ndetection, which mainly contain two steps: text kernel segmentation and\nexpansion. However, the segmentation process only considers each pixel\nindependently, and the expansion process is difficult to achieve a favorable\naccuracy-speed trade-off. In this paper, we propose a Context-aware and\nBoundary-guided Network (CBN) to tackle these problems. In CBN, a basic text\ndetector is firstly used to predict initial segmentation results. Then, we\npropose a context-aware module to enhance text kernel feature representations,\nwhich considers both global and local contexts. Finally, we introduce a\nboundary-guided module to expand enhanced text kernels adaptively with only the\npixels on the contours, which not only obtains accurate text boundaries but\nalso keeps high speed, especially on high-resolution output maps. In\nparticular, with a lightweight backbone, the basic detector equipped with our\nproposed CBN achieves state-of-the-art results on several popular benchmarks,\nand our proposed CBN can be plugged into several segmentation-based methods.\nCode is available at https://github.com/XiiZhao/cbn.pytorch.\n","authors":["Xi Zhao","Wei Feng","Zheng Zhang","Jingjing Lv","Xin Zhu","Zhangang Lin","Jinghe Hu","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2212.02340v3.pdf","comment":"Accepted by IJCV 2024. Code is available at this https URL:\n  https://github.com/XiiZhao/cbn.pytorch"},{"id":"http://arxiv.org/abs/2403.14233v1","updated":"2024-03-21T08:49:34Z","published":"2024-03-21T08:49:34Z","title":"SoftPatch: Unsupervised Anomaly Detection with Noisy Data","summary":"  Although mainstream unsupervised anomaly detection (AD) algorithms perform\nwell in academic datasets, their performance is limited in practical\napplication due to the ideal experimental setting of clean training data.\nTraining with noisy data is an inevitable problem in real-world anomaly\ndetection but is seldom discussed. This paper considers label-level noise in\nimage sensory anomaly detection for the first time. To solve this problem, we\nproposed a memory-based unsupervised AD method, SoftPatch, which efficiently\ndenoises the data at the patch level. Noise discriminators are utilized to\ngenerate outlier scores for patch-level noise elimination before coreset\nconstruction. The scores are then stored in the memory bank to soften the\nanomaly detection boundary. Compared with existing methods, SoftPatch maintains\na strong modeling ability of normal data and alleviates the overconfidence\nproblem in coreset. Comprehensive experiments in various noise scenes\ndemonstrate that SoftPatch outperforms the state-of-the-art AD methods on the\nMVTecAD and BTAD benchmarks and is comparable to those methods under the\nsetting without noise.\n","authors":["Xi Jiang","Ying Chen","Qiang Nie","Yong Liu","Jianlin Liu","Bin-Bin Gao","Jun Liu","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.14233v1.pdf","comment":"36th Conference on Neural Information Processing Systems"},{"id":"http://arxiv.org/abs/2402.03631v2","updated":"2024-03-21T08:36:15Z","published":"2024-02-06T02:00:18Z","title":"Conditional Tuning Network for Few-Shot Adaptation of Segmentation\n  Anything Model","summary":"  The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot\ncapability and flexible geometric prompting in general image segmentation.\nHowever, SAM often struggles when handling various unconventional images, such\nas aerial, medical, and non-RGB images. This paper presents CAT-SAM, a\nConditionAl Tuning network that adapts SAM toward various unconventional target\ntasks with just few-shot target samples. CAT-SAM freezes the entire SAM and\nadapts its mask decoder and image encoder simultaneously with a small number of\nlearnable parameters. The core design is a prompt bridge structure that enables\ndecoder-conditioned joint tuning of the heavyweight image encoder and the\nlightweight mask decoder. The bridging maps the prompt token of the mask\ndecoder to the image encoder, fostering synergic adaptation of the encoder and\nthe decoder with mutual benefits. We develop two representative tuning\nstrategies for the image encoder which leads to two CAT-SAM variants: one\ninjecting learnable prompt tokens in the input space and the other inserting\nlightweight adapter networks. Extensive experiments over 11 unconventional\ntasks show that both CAT-SAM variants achieve superior target segmentation\nperformance consistently even under the very challenging one-shot adaptation\nsetup. Project page: https://xiaoaoran.github.io/projects/CAT-SAM\n","authors":["Aoran Xiao","Weihao Xuan","Heli Qi","Yun Xing","Ruijie Ren","Xiaoqin Zhang","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2402.03631v2.pdf","comment":"Project page: https://xiaoaoran.github.io/projects/CAT-SAM"},{"id":"http://arxiv.org/abs/2309.06670v4","updated":"2024-03-21T08:30:54Z","published":"2023-09-13T02:15:29Z","title":"ShaDocFormer: A Shadow-Attentive Threshold Detector With Cascaded Fusion\n  Refiner for Document Shadow Removal","summary":"  Document shadow is a common issue that arises when capturing documents using\nmobile devices, which significantly impacts readability. Current methods\nencounter various challenges, including inaccurate detection of shadow masks\nand estimation of illumination. In this paper, we propose ShaDocFormer, a\nTransformer-based architecture that integrates traditional methodologies and\ndeep learning techniques to tackle the problem of document shadow removal. The\nShaDocFormer architecture comprises two components: the Shadow-attentive\nThreshold Detector (STD) and the Cascaded Fusion Refiner (CFR). The STD module\nemploys a traditional thresholding technique and leverages the attention\nmechanism of the Transformer to gather global information, thereby enabling\nprecise detection of shadow masks. The cascaded and aggregative structure of\nthe CFR module facilitates a coarse-to-fine restoration process for the entire\nimage. As a result, ShaDocFormer excels in accurately detecting and capturing\nvariations in both shadow and illumination, thereby enabling effective removal\nof shadows. Extensive experiments demonstrate that ShaDocFormer outperforms\ncurrent state-of-the-art methods in both qualitative and quantitative\nmeasurements.\n","authors":["Weiwen Chen","Yingtie Lei","Shenghong Luo","Ziyang Zhou","Mingxian Li","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2309.06670v4.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2312.07485v2","updated":"2024-03-21T08:19:05Z","published":"2023-12-12T18:21:36Z","title":"MinD-3D: Reconstruct High-quality 3D objects in Human Brain","summary":"  In this paper, we introduce Recon3DMind, an innovative task aimed at\nreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)\nsignals, marking a significant advancement in the fields of cognitive\nneuroscience and computer vision. To support this pioneering task, we present\nthe fMRI-Shape dataset, which includes data from 14 participants and features\n360-degree videos of 3D objects to enable comprehensive fMRI signal capture\nacross various settings, thereby laying a foundation for future research.\nFurthermore, we propose MinD-3D, a novel and effective three-stage framework\nspecifically designed to decode the brain's 3D visual information from fMRI\nsignals, demonstrating the feasibility of this challenging task. The framework\nbegins by extracting and aggregating features from fMRI frames through a\nneuro-fusion encoder, subsequently employs a feature bridge diffusion model to\ngenerate visual features, and ultimately recovers the 3D object via a\ngenerative transformer decoder. We assess the performance of MinD-3D using a\nsuite of semantic and structural metrics and analyze the correlation between\nthe features extracted by our model and the visual regions of interest (ROIs)\nin fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3D\nobjects with high semantic relevance and spatial similarity but also\nsignificantly enhances our understanding of the human brain's capabilities in\nprocessing 3D visual information. Project page at:\nhttps://jianxgao.github.io/MinD-3D.\n","authors":["Jianxiong Gao","Yuqian Fu","Yun Wang","Xuelin Qian","Jianfeng Feng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07485v2.pdf","comment":"26 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.14213v1","updated":"2024-03-21T08:08:31Z","published":"2024-03-21T08:08:31Z","title":"Toward Multi-class Anomaly Detection: Exploring Class-aware Unified\n  Model against Inter-class Interference","summary":"  In the context of high usability in single-class anomaly detection models,\nrecent academic research has become concerned about the more complex\nmulti-class anomaly detection. Although several papers have designed unified\nmodels for this task, they often overlook the utility of class labels, a potent\ntool for mitigating inter-class interference. To address this issue, we\nintroduce a Multi-class Implicit Neural representation Transformer for unified\nAnomaly Detection (MINT-AD), which leverages the fine-grained category\ninformation in the training stage. By learning the multi-class distributions,\nthe model generates class-aware query embeddings for the transformer decoder,\nmitigating inter-class interference within the reconstruction model. Utilizing\nsuch an implicit neural representation network, MINT-AD can project category\nand position information into a feature embedding space, further supervised by\nclassification and prior probability loss functions. Experimental results on\nmultiple datasets demonstrate that MINT-AD outperforms existing unified\ntraining models.\n","authors":["Xi Jiang","Ying Chen","Qiang Nie","Jianlin Liu","Yong Liu","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.14213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14203v1","updated":"2024-03-21T07:56:09Z","published":"2024-03-21T07:56:09Z","title":"Unsupervised Audio-Visual Segmentation with Modality Alignment","summary":"  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the\nobject in a visual scene that produces a given sound. Current AVS methods rely\non costly fine-grained annotations of mask-audio pairs, making them impractical\nfor scalability. To address this, we introduce unsupervised AVS, eliminating\nthe need for such expensive annotation. To tackle this more challenging\nproblem, we propose an unsupervised learning method, named Modality\nCorrespondence Alignment (MoCA), which seamlessly integrates off-the-shelf\nfoundation models like DINO, SAM, and ImageBind. This approach leverages their\nknowledge complementarity and optimizes their joint usage for multi-modality\nassociation. Initially, we estimate positive and negative image pairs in the\nfeature space. For pixel-level association, we introduce an audio-visual\nadapter and a novel pixel matching aggregation strategy within the image-level\ncontrastive learning framework. This allows for a flexible connection between\nobject appearance and audio signal at the pixel level, with tolerance to\nimaging variations such as translation and rotation. Extensive experiments on\nthe AVSBench (single and multi-object splits) and AVSS datasets demonstrate\nthat our MoCA outperforms strongly designed baseline methods and approaches\nsupervised counterparts, particularly in complex scenarios with multiple\nauditory objects. Notably when comparing mIoU, MoCA achieves a substantial\nimprovement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and\nAVSS (+19.23%) audio-visual segmentation challenges.\n","authors":["Swapnil Bhosale","Haosen Yang","Diptesh Kanojia","Jiangkang Deng","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11193v2","updated":"2024-03-21T07:53:23Z","published":"2024-03-17T12:40:46Z","title":"Neural Markov Random Field for Stereo Matching","summary":"  Stereo matching is a core task for many computer vision and robotics\napplications. Despite their dominance in traditional stereo methods, the\nhand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy\ncompared to end-to-end deep models. While deep learning representations have\ngreatly improved the unary terms of the MRF models, the overall accuracy is\nstill severely limited by the hand-crafted pairwise terms and message passing.\nTo address these issues, we propose a neural MRF model, where both potential\nfunctions and message passing are designed using data-driven neural networks.\nOur fully data-driven model is built on the foundation of variational inference\ntheory, to prevent convergence issues and retain stereo MRF's graph inductive\nbias. To make the inference tractable and scale well to high-resolution images,\nwe also propose a Disparity Proposal Network (DPN) to adaptively prune the\nsearch space of disparity. The proposed approach ranks $1^{st}$ on both KITTI\n2012 and 2015 leaderboards among all published methods while running faster\nthan 100 ms. This approach significantly outperforms prior global methods,\ne.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our\nmethod exhibits strong cross-domain generalization and can recover sharp edges.\nThe codes at https://github.com/aeolusguan/NMRF\n","authors":["Tongfan Guan","Chen Wang","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11193v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.11178v3","updated":"2024-03-21T07:52:03Z","published":"2023-11-18T22:42:16Z","title":"Active Prompt Learning in Vision Language Models","summary":"  Pre-trained Vision Language Models (VLMs) have demonstrated notable progress\nin various zero-shot tasks, such as classification and retrieval. Despite their\nperformance, because improving performance on new tasks requires task-specific\nknowledge, their adaptation is essential. While labels are needed for the\nadaptation, acquiring them is typically expensive. To overcome this challenge,\nactive learning, a method of achieving a high performance by obtaining labels\nfor a small number of samples from experts, has been studied. Active learning\nprimarily focuses on selecting unlabeled samples for labeling and leveraging\nthem to train models. In this study, we pose the question, \"how can the\npre-trained VLMs be adapted under the active learning framework?\" In response\nto this inquiry, we observe that (1) simply applying a conventional active\nlearning framework to pre-trained VLMs even may degrade performance compared to\nrandom selection because of the class imbalance in labeling candidates, and (2)\nthe knowledge of VLMs can provide hints for achieving the balance before\nlabeling. Based on these observations, we devise a novel active learning\nframework for VLMs, denoted as PCB. To assess the effectiveness of our\napproach, we conduct experiments on seven different real-world datasets, and\nthe results demonstrate that PCB surpasses conventional active learning and\nrandom sampling methods. Code will be available in\nhttps://github.com/kaist-dmlab/pcb .\n","authors":["Jihwan Bang","Sumyeong Ahn","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2311.11178v3.pdf","comment":"accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14200v1","updated":"2024-03-21T07:50:45Z","published":"2024-03-21T07:50:45Z","title":"Debiasing surgeon: fantastic weights and how to find them","summary":"  Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic\nbiases that can lead to unfair models, emerges. Several debiasing approaches\nhave been proposed in the realm of deep learning, employing more or less\nsophisticated approaches to discourage these models from massively employing\nthese biases. However, a question emerges: is this extra complexity really\nnecessary? Is a vanilla-trained model already embodying some ``unbiased\nsub-networks'' that can be used in isolation and propose a solution without\nrelying on the algorithmic biases? In this work, we show that such a\nsub-network typically exists, and can be extracted from a vanilla-trained model\nwithout requiring additional training. We further validate that such specific\narchitecture is incapable of learning a specific bias, suggesting that there\nare possible architectural countermeasures to the problem of biases in deep\nneural networks.\n","authors":["Rémi Nahon","Ivan Luiz De Moura Matos","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2403.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14198v1","updated":"2024-03-21T07:48:35Z","published":"2024-03-21T07:48:35Z","title":"Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization","summary":"  This paper investigates the effective utilization of unlabeled data for\nlarge-area cross-view geo-localization (CVGL), encompassing both unsupervised\nand semi-supervised settings. Common approaches to CVGL rely on\nground-satellite image pairs and employ label-driven supervised training.\nHowever, the cost of collecting precise cross-view image pairs hinders the\ndeployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more\nchallenging to handle the significant imaging and spatial gaps between ground\nand satellite images. To this end, we propose an unsupervised framework\nincluding a cross-view projection to guide the model for retrieving initial\npseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by\nleveraging the fact that ``the perfectly paired ground-satellite image is\nlocated in a unique and identical scene\". The framework exhibits competitive\nperformance compared with supervised works on three open-source benchmarks. Our\ncode and models will be released on https://github.com/liguopeng0923/UCVGL.\n","authors":["Guopeng Li","Ming Qian","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2403.14198v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2311.15876v2","updated":"2024-03-21T07:38:51Z","published":"2023-11-27T14:49:06Z","title":"LMM-Assisted Breast Cancer Treatment Target Segmentation with\n  Consistency Embedding","summary":"  Recent advancements in Artificial Intelligence (AI) have profoundly\ninfluenced medical fields, by providing tools to reduce clinical workloads.\nHowever, most AI models are constrained to execute unimodal tasks, in stark\ncontrast to the comprehensive approaches utilized by medical professionals. To\naddress this, here we present RO-LMM, a multi-purpose large multimodal model\n(LMM) tailored for the field of radiation oncology. This model covers series of\ntasks within clinical workflow, adept at clinical report summarization,\nradiation treatment plan suggestion, and plan-guided target volume\nsegmentation. In particular, to perform consecutive clinical tasks, we further\npresent a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which\nboosts LMM's robustness to noisy inputs while preserving the capability of\nhandling clean inputs, and transform this concept into LMM-driven segmentation\nframework as Consistency Embedding Segmentation~(CESEG). Experimental results\non multi-centre cohorts demonstrate our RO-LMM's promising performance for\nmultiple clinical tasks with generalization capabilities.\n","authors":["Kwanyoung Kim","Yujin Oh","Sangjoon Park","Hwa Kyung Byun","Jin Sung Kim","Yong Bae Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2311.15876v2.pdf","comment":"30 pages, 16 table, 5 figures"},{"id":"http://arxiv.org/abs/2403.14191v1","updated":"2024-03-21T07:34:31Z","published":"2024-03-21T07:34:31Z","title":"PECI-Net: Bolus segmentation from video fluoroscopic swallowing study\n  images using preprocessing ensemble and cascaded inference","summary":"  Bolus segmentation is crucial for the automated detection of swallowing\ndisorders in videofluoroscopic swallowing studies (VFSS). However, it is\ndifficult for the model to accurately segment a bolus region in a VFSS image\nbecause VFSS images are translucent, have low contrast and unclear region\nboundaries, and lack color information. To overcome these challenges, we\npropose PECI-Net, a network architecture for VFSS image analysis that combines\ntwo novel techniques: the preprocessing ensemble network (PEN) and the cascaded\ninference network (CIN). PEN enhances the sharpness and contrast of the VFSS\nimage by combining multiple preprocessing algorithms in a learnable way. CIN\nreduces ambiguity in bolus segmentation by using context from other regions\nthrough cascaded inference. Moreover, CIN prevents undesirable side effects\nfrom unreliably segmented regions by referring to the context in an asymmetric\nway. In experiments, PECI-Net exhibited higher performance than four recently\ndeveloped baseline models, outperforming TernausNet, the best among the\nbaseline models, by 4.54\\% and the widely used UNet by 10.83\\%. The results of\nthe ablation studies confirm that CIN and PEN are effective in improving bolus\nsegmentation performance.\n","authors":["Dougho Park","Younghun Kim","Harim Kang","Junmyeoung Lee","Jinyoung Choi","Taeyeon Kim","Sangeok Lee","Seokil Son","Minsol Kim","Injung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14191v1.pdf","comment":"20 pages, 8 figures,"},{"id":"http://arxiv.org/abs/2402.17159v2","updated":"2024-03-21T07:29:34Z","published":"2024-02-27T02:47:09Z","title":"NocPlace: Nocturnal Visual Place Recognition via Generative and\n  Inherited Knowledge Transfer","summary":"  Visual Place Recognition (VPR) is crucial in computer vision, aiming to\nretrieve database images similar to a query image from an extensive collection\nof known images. However, like many vision tasks, VPR always degrades at night\ndue to the scarcity of nighttime images. Moreover, VPR needs to address the\ncross-domain problem of night-to-day rather than just the issue of a single\nnighttime domain. In response to these issues, we present NocPlace, which\nleverages generative and inherited knowledge transfer to embed resilience\nagainst dazzling lights and extreme darkness in the global descriptor. First,\nwe establish a day-night urban scene dataset called NightCities, capturing\ndiverse lighting variations and dark scenarios across 60 cities globally. Then,\nan image generation network is trained on this dataset and processes a\nlarge-scale VPR dataset, obtaining its nighttime version. Finally, VPR models\nare fine-tuned using descriptors inherited from themselves and night-style\nimages, which builds explicit cross-domain contrastive relationships.\nComprehensive experiments on various datasets demonstrate our contributions and\nthe superiority of NocPlace. Without adding any real-time computing resources,\nNocPlace improves the performance of Eigenplaces by 7.6% on Tokyo 24/7 Night\nand 16.8% on SVOX Night.\n","authors":["Bingxi Liu","Yiqun Wang","Huaqi Tao","Tingjun Huang","Fulin Tang","Yihong Wu","Jinqiang Cui","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17159v2.pdf","comment":"28 pages,9 figures"},{"id":"http://arxiv.org/abs/2403.14186v1","updated":"2024-03-21T07:21:51Z","published":"2024-03-21T07:21:51Z","title":"StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained\n  StyleGAN","summary":"  We propose a method that can generate cinemagraphs automatically from a still\nlandscape image using a pre-trained StyleGAN. Inspired by the success of recent\nunconditional video generation, we leverage a powerful pre-trained image\ngenerator to synthesize high-quality cinemagraphs. Unlike previous approaches\nthat mainly utilize the latent space of a pre-trained StyleGAN, our approach\nutilizes its deep feature space for both GAN inversion and cinemagraph\ngeneration. Specifically, we propose multi-scale deep feature warping (MSDFW),\nwhich warps the intermediate features of a pre-trained StyleGAN at different\nresolutions. By using MSDFW, the generated cinemagraphs are of high resolution\nand exhibit plausible looping animation. We demonstrate the superiority of our\nmethod through user studies and quantitative comparisons with state-of-the-art\ncinemagraph generation methods and a video generation method that uses a\npre-trained StyleGAN.\n","authors":["Jongwoo Choi","Kwanggyoon Seo","Amirsaman Ashtari","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.14186v1.pdf","comment":"Project website: https://jeolpyeoni.github.io/stylecinegan_project/"},{"id":"http://arxiv.org/abs/2310.02712v2","updated":"2024-03-21T07:20:35Z","published":"2023-10-04T10:28:38Z","title":"ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space\n  NeRF","summary":"  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n","authors":["Jangho Park","Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2310.02712v2.pdf","comment":"ICLR 2024; Project Page: https://jhq1234.github.io/ed-nerf.github.io/"},{"id":"http://arxiv.org/abs/2403.14183v1","updated":"2024-03-21T07:15:37Z","published":"2024-03-21T07:15:37Z","title":"OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic\n  Segmentation","summary":"  The recent success of CLIP has demonstrated promising results in zero-shot\nsemantic segmentation by transferring muiltimodal knowledge to pixel-level\nclassification. However, leveraging pre-trained CLIP knowledge to closely align\ntext embeddings with pixel embeddings still has limitations in existing\napproaches. To address this issue, we propose OTSeg, a novel multimodal\nattention mechanism aimed at enhancing the potential of multiple text prompts\nfor matching associated pixel embeddings. We first propose Multi-Prompts\nSinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads\nmultiple text prompts to selectively focus on various semantic features within\nimage pixels. Moreover, inspired by the success of Sinkformers in unimodal\nsettings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn\nAttention (MPSA), which effectively replaces cross-attention mechanisms within\nTransformer framework in multimodal settings. Through extensive experiments, we\ndemonstrate that OTSeg achieves state-of-the-art (SOTA) performance with\nsignificant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three\nbenchmark datasets.\n","authors":["Kwanyoung Kim","Yujin Oh","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.14183v1.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.14174v1","updated":"2024-03-21T06:53:40Z","published":"2024-03-21T06:53:40Z","title":"Unified Static and Dynamic Network: Efficient Temporal Filtering for\n  Video Grounding","summary":"  Inspired by the activity-silent and persistent activity mechanisms in human\nvisual perception biology, we design a Unified Static and Dynamic Network\n(UniSDNet), to learn the semantic association between the video and text/audio\nqueries in a cross-modal environment for efficient video grounding. For static\nmodeling, we devise a novel residual structure (ResMLP) to boost the global\ncomprehensive interaction between the video segments and queries, achieving\nmore effective semantic enhancement/supplement. For dynamic modeling, we\neffectively exploit three characteristics of the persistent activity mechanism\nin our network design for a better video context comprehension. Specifically,\nwe construct a diffusely connected video clip graph on the basis of 2D sparse\ntemporal masking to reflect the \"short-term effect\" relationship. We\ninnovatively consider the temporal distance and relevance as the joint\n\"auxiliary evidence clues\" and design a multi-kernel Temporal Gaussian Filter\nto expand the context clue into high-dimensional space, simulating the \"complex\nvisual perception\", and then conduct element level filtering convolution\noperations on neighbour clip nodes in message passing stage for finally\ngenerating and ranking the candidate proposals. Our UniSDNet is applicable to\nboth Natural Language Video Grounding (NLVG) and Spoken Language Video\nGrounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely\nused datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new\nrecords at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on\nTACoS. To facilitate this field, we collect two new datasets (Charades-STA\nSpeech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our\nUniSDNet is 1.56$\\times$ faster than the strong multi-query benchmark. Code is\navailable at: https://github.com/xian-sh/UniSDNet.\n","authors":["Jingjing Hu","Dan Guo","Kun Li","Zhan Si","Xun Yang","Xiaojun Chang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09559v2","updated":"2024-03-21T06:51:16Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07798v2","updated":"2024-03-21T06:45:53Z","published":"2024-03-12T16:35:32Z","title":"A Fourier Transform Framework for Domain Adaptation","summary":"  By using unsupervised domain adaptation (UDA), knowledge can be transferred\nfrom a label-rich source domain to a target domain that contains relevant\ninformation but lacks labels. Many existing UDA algorithms suffer from directly\nusing raw images as input, resulting in models that overly focus on redundant\ninformation and exhibit poor generalization capability. To address this issue,\nwe attempt to improve the performance of unsupervised domain adaptation by\nemploying the Fourier method (FTF).Specifically, FTF is inspired by the\namplitude of Fourier spectra, which primarily preserves low-level statistical\ninformation. In FTF, we effectively incorporate low-level information from the\ntarget domain into the source domain by fusing the amplitudes of both domains\nin the Fourier domain. Additionally, we observe that extracting features from\nbatches of images can eliminate redundant information while retaining\nclass-specific features relevant to the task. Building upon this observation,\nwe apply the Fourier Transform at the data stream level for the first time. To\nfurther align multiple sources of data, we introduce the concept of correlation\nalignment. To evaluate the effectiveness of our FTF method, we conducted\nevaluations on four benchmark datasets for domain adaptation, including\nOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\ndemonstrate superior performance.\n","authors":["Le Luo","Bingrong Xu","Qingyong Zhang","Cheng Lian","Jie Luo"],"pdf_url":"https://arxiv.org/pdf/2403.07798v2.pdf","comment":"The paper contains significant errors and the experimental\n  methodology is not rigorous. The experimental section and methodology need to\n  be rewritten"},{"id":"http://arxiv.org/abs/2308.13223v2","updated":"2024-03-21T06:45:32Z","published":"2023-08-25T07:39:26Z","title":"EfficientDreamer: High-Fidelity and Robust 3D Creation via\n  Orthogonal-view Diffusion Prior","summary":"  While image diffusion models have made significant progress in text-driven 3D\ncontent creation, they often fail to accurately capture the intended meaning of\ntext prompts, especially for view information. This limitation leads to the\nJanus problem, where multi-faced 3D models are generated under the guidance of\nsuch diffusion models. In this paper, we propose a robust high-quality 3D\ncontent generation pipeline by exploiting orthogonal-view image guidance.\nFirst, we introduce a novel 2D diffusion model that generates an image\nconsisting of four orthogonal-view sub-images based on the given text prompt.\nThen, the 3D content is created using this diffusion model. Notably, the\ngenerated orthogonal-view image provides strong geometric structure priors and\nthus improves 3D consistency. As a result, it effectively resolves the Janus\nproblem and significantly enhances the quality of 3D content creation.\nAdditionally, we present a 3D synthesis fusion network that can further improve\nthe details of the generated 3D contents. Both quantitative and qualitative\nevaluations demonstrate that our method surpasses previous text-to-3D\ntechniques. Project page: https://efficientdreamer.github.io.\n","authors":["Zhipeng Hu","Minda Zhao","Chaoyi Zhao","Xinyue Liang","Lincheng Li","Zeng Zhao","Changjie Fan","Xiaowei Zhou","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2308.13223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14166v1","updated":"2024-03-21T06:34:46Z","published":"2024-03-21T06:34:46Z","title":"Mini-Splatting: Representing Scenes with a Constrained Number of\n  Gaussians","summary":"  In this study, we explore the challenge of efficiently representing scenes\nwith a constrained number of Gaussians. Our analysis shifts from traditional\ngraphics and 2D computer vision to the perspective of point clouds,\nhighlighting the inefficient spatial distribution of Gaussian representation as\na key limitation in model performance. To address this, we introduce strategies\nfor densification including blur split and depth reinitialization, and\nsimplification through Gaussian binarization and sampling. These techniques\nreorganize the spatial positions of the Gaussians, resulting in significant\nimprovements across various datasets and benchmarks in terms of rendering\nquality, resource consumption, and storage compression. Our proposed\nMini-Splatting method integrates seamlessly with the original rasterization\npipeline, providing a strong baseline for future research in\nGaussian-Splatting-based works.\n","authors":["Guangchi Fang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14163v1","updated":"2024-03-21T06:32:36Z","published":"2024-03-21T06:32:36Z","title":"Leveraging Large Language Model-based Room-Object Relationships\n  Knowledge for Enhancing Multimodal-Input Object Goal Navigation","summary":"  Object-goal navigation is a crucial engineering task for the community of\nembodied navigation; it involves navigating to an instance of a specified\nobject category within unseen environments. Although extensive investigations\nhave been conducted on both end-to-end and modular-based, data-driven\napproaches, fully enabling an agent to comprehend the environment through\nperceptual knowledge and perform object-goal navigation as efficiently as\nhumans remains a significant challenge. Recently, large language models have\nshown potential in this task, thanks to their powerful capabilities for\nknowledge extraction and integration. In this study, we propose a data-driven,\nmodular-based approach, trained on a dataset that incorporates common-sense\nknowledge of object-to-room relationships extracted from a large language\nmodel. We utilize the multi-channel Swin-Unet architecture to conduct\nmulti-task learning incorporating with multimodal inputs. The results in the\nHabitat simulator demonstrate that our framework outperforms the baseline by an\naverage of 10.6% in the efficiency metric, Success weighted by Path Length\n(SPL). The real-world demonstration shows that the proposed approach can\nefficiently conduct this task by traversing several rooms. For more details and\nreal-world demonstrations, please check our project webpage\n(https://sunleyuan.github.io/ObjectNav).\n","authors":["Leyuan Sun","Asako Kanezaki","Guillaume Caron","Yusuke Yoshiyasu"],"pdf_url":"https://arxiv.org/pdf/2403.14163v1.pdf","comment":"will soon submit to the Elsevier journal, Advanced Engineering\n  Informatics"},{"id":"http://arxiv.org/abs/2403.14158v1","updated":"2024-03-21T06:14:46Z","published":"2024-03-21T06:14:46Z","title":"Volumetric Environment Representation for Vision-Language Navigation","summary":"  Vision-language navigation (VLN) requires an agent to navigate through an 3D\nenvironment based on visual observations and natural language instructions. It\nis clear that the pivotal factor for successful navigation lies in the\ncomprehensive scene understanding. Previous VLN agents employ monocular\nframeworks to extract 2D features of perspective views directly. Though\nstraightforward, they struggle for capturing 3D geometry and semantics, leading\nto a partial and incomplete environment representation. To achieve a\ncomprehensive 3D representation with fine-grained details, we introduce a\nVolumetric Environment Representation (VER), which voxelizes the physical world\ninto structured 3D cells. For each cell, VER aggregates multi-view 2D features\ninto such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature\nextraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D\nroom layout, and 3D bounding boxes jointly. Based on online collected VERs, our\nagent performs volume state estimation and builds episodic memory for\npredicting the next step. Experimental results show our environment\nrepresentations from multi-task learning lead to evident performance gains on\nVLN. Our model achieves state-of-the-art performance across VLN benchmarks\n(R2R, REVERIE, and R4R).\n","authors":["Rui Liu","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.14158v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14155v1","updated":"2024-03-21T06:03:51Z","published":"2024-03-21T06:03:51Z","title":"Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image\n  Customization","summary":"  In a surge of text-to-image (T2I) models and their customization methods that\ngenerate new images of a user-provided subject, current works focus on\nalleviating the costs incurred by a lengthy per-subject optimization. These\nzero-shot customization methods encode the image of a specified subject into a\nvisual embedding which is then utilized alongside the textual embedding for\ndiffusion guidance. The visual embedding incorporates intrinsic information\nabout the subject, while the textual embedding provides a new, transient\ncontext. However, the existing methods often 1) are significantly affected by\nthe input images, eg., generating images with the same pose, and 2) exhibit\ndeterioration in the subject's identity. We first pin down the problem and show\nthat redundant pose information in the visual embedding interferes with the\ntextual embedding containing the desired pose information. To address this\nissue, we propose orthogonal visual embedding which effectively harmonizes with\nthe given textual embedding. We also adopt the visual-only embedding and inject\nthe subject's clear features utilizing a self-attention swap. Our results\ndemonstrate the effectiveness and robustness of our method, which offers highly\nflexible zero-shot generation while effectively maintaining the subject's\nidentity.\n","authors":["Yeji Song","Jimyeong Kim","Wonhark Park","Wonsik Shin","Wonjong Rhee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2403.14155v1.pdf","comment":"Project page: https://ldynx.github.io/harmony-zero-t2i/"},{"id":"http://arxiv.org/abs/2307.12429v2","updated":"2024-03-21T05:59:17Z","published":"2023-07-23T20:55:11Z","title":"SwIPE: Efficient and Robust Medical Image Segmentation with Implicit\n  Patch Embeddings","summary":"  Modern medical image segmentation methods primarily use discrete\nrepresentations in the form of rasterized masks to learn features and generate\npredictions. Although effective, this paradigm is spatially inflexible, scales\npoorly to higher-resolution images, and lacks direct understanding of object\nshapes. To address these limitations, some recent works utilized implicit\nneural representations (INRs) to learn continuous representations for\nsegmentation. However, these methods often directly adopted components designed\nfor 3D shape reconstruction. More importantly, these formulations were also\nconstrained to either point-based or global contexts, lacking contextual\nunderstanding or local fine-grained details, respectively--both critical for\naccurate segmentation. To remedy this, we propose a novel approach, SwIPE\n(Segmentation with Implicit Patch Embeddings), that leverages the advantages of\nINRs and predicts shapes at the patch level--rather than at the point level or\nimage level--to enable both accurate local boundary delineation and global\nshape coherence. Extensive evaluations on two tasks (2D polyp segmentation and\n3D abdominal organ segmentation) show that SwIPE significantly improves over\nrecent implicit approaches and outperforms state-of-the-art discrete methods\nwith over 10x fewer parameters. Our method also demonstrates superior data\nefficiency and improved robustness to data shifts across image resolutions and\ndatasets. Code is available on Github\n(https://github.com/charzharr/miccai23-swipe-implicit-segmentation).\n","authors":["Yejia Zhang","Pengfei Gu","Nishchal Sapkota","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2307.12429v2.pdf","comment":"Accepted to the 2023 International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI'23)"},{"id":"http://arxiv.org/abs/2403.14148v1","updated":"2024-03-21T05:48:48Z","published":"2024-03-21T05:48:48Z","title":"Efficient Video Diffusion Models via Content-Frame Motion-Latent\n  Decomposition","summary":"  Video diffusion models have recently made great progress in generation\nquality, but are still limited by the high memory and computational\nrequirements. This is because current video diffusion models often attempt to\nprocess high-dimensional videos directly. To tackle this issue, we propose\ncontent-motion latent diffusion model (CMD), a novel efficient extension of\npretrained image diffusion models for video generation. Specifically, we\npropose an autoencoder that succinctly encodes a video as a combination of a\ncontent frame (like an image) and a low-dimensional motion latent\nrepresentation. The former represents the common content, and the latter\nrepresents the underlying motion in the video, respectively. We generate the\ncontent frame by fine-tuning a pretrained image diffusion model, and we\ngenerate the motion latent representation by training a new lightweight\ndiffusion model. A key innovation here is the design of a compact latent space\nthat can directly utilizes a pretrained image diffusion model, which has not\nbeen done in previous latent video diffusion models. This leads to considerably\nbetter quality generation and reduced computational costs. For instance, CMD\ncan sample a video 7.7$\\times$ faster than prior approaches by generating a\nvideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD\nachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous\nstate-of-the-art of 292.4.\n","authors":["Sihyun Yu","Weili Nie","De-An Huang","Boyi Li","Jinwoo Shin","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2403.14148v1.pdf","comment":"ICLR 2024. Project page: https://sihyun.me/CMD"},{"id":"http://arxiv.org/abs/2301.12831v3","updated":"2024-03-21T05:39:44Z","published":"2023-01-30T12:37:04Z","title":"M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing\n  System","summary":"  Face presentation attacks (FPA), also known as face spoofing, have brought\nincreasing concerns to the public through various malicious applications, such\nas financial fraud and privacy leakage. Therefore, safeguarding face\nrecognition systems against FPA is of utmost importance. Although existing\nlearning-based face anti-spoofing (FAS) models can achieve outstanding\ndetection performance, they lack generalization capability and suffer\nsignificant performance drops in unforeseen environments. Many methodologies\nseek to use auxiliary modality data (e.g., depth and infrared maps) during the\npresentation attack detection (PAD) to address this limitation. However, these\nmethods can be limited since (1) they require specific sensors such as depth\nand infrared cameras for data capture, which are rarely available on commodity\nmobile devices, and (2) they cannot work properly in practical scenarios when\neither modality is missing or of poor quality. In this paper, we devise an\naccurate and robust MultiModal Mobile Face Anti-Spoofing system named M3FAS to\novercome the issues above. The primary innovation of this work lies in the\nfollowing aspects: (1) To achieve robust PAD, our system combines visual and\nauditory modalities using three commonly available sensors: camera, speaker,\nand microphone; (2) We design a novel two-branch neural network with three\nhierarchical feature aggregation modules to perform cross-modal feature fusion;\n(3). We propose a multi-head training strategy, allowing the model to output\npredictions from the vision, acoustic, and fusion heads, resulting in a more\nflexible PAD. Extensive experiments have demonstrated the accuracy, robustness,\nand flexibility of M3FAS under various challenging experimental settings. The\nsource code and dataset are available at: https://github.com/ChenqiKONG/M3FAS/\n","authors":["Chenqi Kong","Kexin Zheng","Yibing Liu","Shiqi Wang","Anderson Rocha","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2301.12831v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14141v1","updated":"2024-03-21T05:36:25Z","published":"2024-03-21T05:36:25Z","title":"Empowering Segmentation Ability to Multi-modal Large Language Models","summary":"  Multi-modal large language models (MLLMs) can understand image-language\nprompts and demonstrate impressive reasoning ability. In this paper, we extend\nMLLMs' output by empowering MLLMs with the segmentation ability. The extended\nMLLMs can both output language responses to the image-language prompts and\nsegment the regions that the complex question or query in the language prompts\nfocuses on. To this end, the existing work, LISA, enlarges the original word\nembeddings with an additional segment token and fine-tunes dialogue generation\nand query-focused segmentation together, where the feature of the segment token\nis used to prompt the segment-anything model. Although they achieve superior\nsegmentation performance, we observe that the dialogue ability decreases by a\nlarge margin compared to the original MLLMs. To maintain the original MLLMs'\ndialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which\nleverages a chain-of-thought prompting strategy to instruct the MLLMs to\nsegment the target region queried by the user. The MLLMs are first prompted to\nreason about the simple description of the target region from the complicated\nuser query, then extract the visual attributes of the target region according\nto the understanding of MLLMs to the image. These visual attributes, such as\ncolor and relative locations, are utilized to prompt the downstream\nsegmentation model. Experiments show that the proposed method keeps the\noriginal dialogue ability and equips the MLLMs' model with strong reasoning\nsegmentation ability. The code is available at\nhttps://github.com/YuqiYang213/LLaVASeg.\n","authors":["Yuqi Yang","Peng-Tao Jiang","Jing Wang","Hao Zhang","Kai Zhao","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.14141v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13438v2","updated":"2024-03-21T05:35:24Z","published":"2024-03-18T17:38:29Z","title":"See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single\n  Image","summary":"  Humans can not only recognize and understand the world in its current state\nbut also envision future scenarios that extend beyond immediate perception. To\nresemble this profound human capacity, we introduce zero-shot task\nhallucination -- given a single RGB image of any scene comprising unknown\nenvironments and objects, our model can identify potential tasks and imagine\ntheir execution in a vivid narrative, realized as a video. We develop a modular\npipeline that progressively enhances scene decomposition, comprehension, and\nreconstruction, incorporating VLM for dynamic interaction and 3D motion\nplanning for object trajectories. Our model can discover diverse tasks, with\nthe generated task videos demonstrating realistic and compelling visual\noutcomes that are understandable by both machines and humans. Project Page:\nhttps://dannymcy.github.io/zeroshot_task_hallucination/\n","authors":["Chenyang Ma","Kai Lu","Ta-Ying Cheng","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2403.13438v2.pdf","comment":"Project Page: https://dannymcy.github.io/zeroshot_task_hallucination/"},{"id":"http://arxiv.org/abs/2403.14140v1","updated":"2024-03-21T05:33:49Z","published":"2024-03-21T05:33:49Z","title":"Learning Decomposable and Debiased Representations via Attribute-Centric\n  Information Bottlenecks","summary":"  Biased attributes, spuriously correlated with target labels in a dataset, can\nproblematically lead to neural networks that learn improper shortcuts for\nclassifications and limit their capabilities for out-of-distribution (OOD)\ngeneralization. Although many debiasing approaches have been proposed to ensure\ncorrect predictions from biased datasets, few studies have considered learning\nlatent embedding consisting of intrinsic and biased attributes that contribute\nto improved performance and explain how the model pays attention to attributes.\nIn this paper, we propose a novel debiasing framework, Debiasing Global\nWorkspace, introducing attention-based information bottlenecks for learning\ncompositional representations of attributes without defining specific bias\ntypes. Based on our observation that learning shape-centric representation\nhelps robust performance on OOD datasets, we adopt those abilities to learn\nrobust and generalizable representations of decomposable latent embeddings\ncorresponding to intrinsic and biasing attributes. We conduct comprehensive\nevaluations on biased datasets, along with both quantitative and qualitative\nanalyses, to showcase our approach's efficacy in attribute-centric\nrepresentation learning and its ability to differentiate between intrinsic and\nbias-related features.\n","authors":["Jinyung Hong","Eun Som Jeon","Changhoon Kim","Keun Hee Park","Utkarsh Nath","Yezhou Yang","Pavan Turaga","Theodore P. Pavlic"],"pdf_url":"https://arxiv.org/pdf/2403.14140v1.pdf","comment":"24 pages, 16 figures, 3 tables"},{"id":"http://arxiv.org/abs/2309.06030v4","updated":"2024-03-21T05:32:37Z","published":"2023-09-12T08:04:56Z","title":"Federated Learning for Large-Scale Scene Modeling with Neural Radiance\n  Fields","summary":"  We envision a system to continuously build and maintain a map based on\nearth-scale neural radiance fields (NeRF) using data collected from vehicles\nand drones in a lifelong learning manner. However, existing large-scale\nmodeling by NeRF has problems in terms of scalability and maintainability when\nmodeling earth-scale environments. Therefore, to address these problems, we\npropose a federated learning pipeline for large-scale modeling with NeRF. We\ntailor the model aggregation pipeline in federated learning for NeRF, thereby\nallowing local updates of NeRF. In the aggregation step, the accuracy of the\nclients' global pose is critical. Thus, we also propose global pose alignment\nto align the noisy global pose of clients before the aggregation step. In\nexperiments, we show the effectiveness of the proposed pose alignment and the\nfederated learning pipeline on the large-scale scene dataset, Mill19.\n","authors":["Teppei Suzuki"],"pdf_url":"https://arxiv.org/pdf/2309.06030v4.pdf","comment":"Our subsequent work is available at arXiv:2403.11460"},{"id":"http://arxiv.org/abs/2311.08046v2","updated":"2024-03-21T05:28:06Z","published":"2023-11-14T10:11:36Z","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models\n  with Image and Video Understanding","summary":"  Large language models have demonstrated impressive universal capabilities\nacross a wide range of open-ended tasks and have extended their utility to\nencompass multimodal conversations. However, existing methods encounter\nchallenges in effectively handling both image and video understanding,\nparticularly with limited visual tokens. In this work, we introduce Chat-UniVi,\na Unified Vision-language model capable of comprehending and engaging in\nconversations involving images and videos through a unified visual\nrepresentation. Specifically, we employ a set of dynamic visual tokens to\nuniformly represent images and videos. This representation framework empowers\nthe model to efficiently utilize a limited number of visual tokens to\nsimultaneously capture the spatial details necessary for images and the\ncomprehensive temporal relationship required for videos. Moreover, we leverage\na multi-scale representation, enabling the model to perceive both high-level\nsemantic concepts and low-level visual details. Notably, Chat-UniVi is trained\non a mixed dataset containing both images and videos, allowing direct\napplication to tasks involving both mediums without requiring any\nmodifications. Extensive experimental results demonstrate that Chat-UniVi\nconsistently outperforms even existing methods exclusively designed for either\nimages or videos. Code is available at\nhttps://github.com/PKU-YuanGroup/Chat-UniVi.\n","authors":["Peng Jin","Ryuichi Takanobu","Wancai Zhang","Xiaochun Cao","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2311.08046v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14138v1","updated":"2024-03-21T05:13:34Z","published":"2024-03-21T05:13:34Z","title":"Evidential Semantic Mapping in Off-road Environments with\n  Uncertainty-aware Bayesian Kernel Inference","summary":"  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in\ncreating semantic maps by effectively leveraging local spatial information.\nHowever, existing semantic mapping methods face challenges in constructing\nreliable maps in unstructured outdoor scenarios due to unreliable semantic\npredictions. To address this issue, we propose an evidential semantic mapping,\nwhich can enhance reliability in perceptually challenging off-road\nenvironments. We integrate Evidential Deep Learning into the semantic\nsegmentation network to obtain the uncertainty estimate of semantic prediction.\nSubsequently, this semantic uncertainty is incorporated into an\nuncertainty-aware BKI, tailored to prioritize more confident semantic\npredictions when accumulating semantic information. By adaptively handling\nsemantic uncertainties, the proposed framework constructs robust\nrepresentations of the surroundings even in previously unseen environments.\nComprehensive experiments across various off-road datasets demonstrate that our\nframework enhances accuracy and robustness, consistently outperforming existing\nmethods in scenes with high perceptual uncertainties.\n","authors":["Junyoung Kim","Junwon Seo","Jihong Min"],"pdf_url":"https://arxiv.org/pdf/2403.14138v1.pdf","comment":"Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping"},{"id":"http://arxiv.org/abs/2403.14137v1","updated":"2024-03-21T05:13:12Z","published":"2024-03-21T05:13:12Z","title":"Improving Image Classification Accuracy through Complementary\n  Intra-Class and Inter-Class Mixup","summary":"  MixUp and its variants, such as Manifold MixUp, have two key limitations in\nimage classification tasks. First, they often neglect mixing within the same\nclass (intra-class mixup), leading to an underutilization of the relationships\namong samples within the same class. Second, although these methods effectively\nenhance inter-class separability by mixing between different classes\n(inter-class mixup), they fall short in improving intra-class cohesion through\ntheir mixing operations, limiting their classification performance. To tackle\nthese issues, we propose a novel mixup method and a comprehensive integrated\nsolution.Our mixup approach specifically targets intra-class mixup, an aspect\ncommonly overlooked, to strengthen intra-class cohesion-a feature not provided\nby current mixup techniques.For each mini-batch, our method utilizes feature\nrepresentations of unaugmented original images from each class within the\nmini-batch to generate a single synthesized feature representation through\nrandom linear interpolation. All synthesized representations for this\nmini-batch are then fed into the classification and loss layers to calculate an\naverage classification loss that can markedly enhance intra-class cohesion.\nMoreover, our integrated solution seamlessly combines our intra-class mixup\nmethod with an existing mixup approach such as MixUp or Manifold MixUp. This\ncomprehensive solution incorporates inter- and intra-class mixup in a balanced\nmanner while concurrently improving intra-class cohesion and inter-class\nseparability. Experimental results on six public datasets demonstrate that our\nintegrated solution achieves a 0.1% to 3.43% higher accuracy than the best of\neither MixUp or our intra-class mixup method, averaging a 1.16% gain. It also\noutperforms the better performer of either Manifold MixUp or our intra-class\nmixup method by 0.12% to 5.16%, with an average gain of 1.11%.\n","authors":["Ye Xu","Ya Gao","Xiaorong Qiu","Yang Chen","Ying Ji"],"pdf_url":"https://arxiv.org/pdf/2403.14137v1.pdf","comment":"25 pages,12 figures"},{"id":"http://arxiv.org/abs/2303.03757v3","updated":"2024-03-21T05:11:08Z","published":"2023-03-07T09:33:49Z","title":"Deep Learning for Inertial Positioning: A Survey","summary":"  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT\ndevices, playing a crucial role in enabling ubiquitous and reliable\nlocalization. Inertial sensor-based positioning is essential in various\napplications, including personal navigation, location-based security, and\nhuman-device interaction. However, low-cost MEMS inertial sensors' measurements\nare inevitably corrupted by various error sources, leading to unbounded drifts\nwhen integrated doubly in traditional inertial navigation algorithms,\nsubjecting inertial positioning to the problem of error drifts. In recent\nyears, with the rapid increase in sensor data and computational power, deep\nlearning techniques have been developed, sparking significant research into\naddressing the problem of inertial positioning. Relevant literature in this\nfield spans across mobile computing, robotics, and machine learning. In this\narticle, we provide a comprehensive review of deep learning-based inertial\npositioning and its applications in tracking pedestrians, drones, vehicles, and\nrobots. We connect efforts from different fields and discuss how deep learning\ncan be applied to address issues such as sensor calibration, positioning error\ndrift reduction, and multi-sensor fusion. This article aims to attract readers\nfrom various backgrounds, including researchers and practitioners interested in\nthe potential of deep learning-based techniques to solve inertial positioning\nproblems. Our review demonstrates the exciting possibilities that deep learning\nbrings to the table and provides a roadmap for future research in this field.\n","authors":["Changhao Chen","Xianfei Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03757v3.pdf","comment":"Accepted by IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2403.14135v1","updated":"2024-03-21T05:10:26Z","published":"2024-03-21T05:10:26Z","title":"Powerful Lossy Compression for Noisy Images","summary":"  Image compression and denoising represent fundamental challenges in image\nprocessing with many real-world applications. To address practical demands,\ncurrent solutions can be categorized into two main strategies: 1) sequential\nmethod; and 2) joint method. However, sequential methods have the disadvantage\nof error accumulation as there is information loss between multiple individual\nmodels. Recently, the academic community began to make some attempts to tackle\nthis problem through end-to-end joint methods. Most of them ignore that\ndifferent regions of noisy images have different characteristics. To solve\nthese problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware\njoint solution exploits local and non-local features for image compression and\ndenoising simultaneously. We design an end-to-end trainable network, which\nincludes the main encoder branch, the guidance branch, and the signal-to-noise\nratio~(SNR) aware branch. We conducted extensive experiments on both synthetic\nand real-world datasets, demonstrating that our joint solution outperforms\nexisting state-of-the-art methods.\n","authors":["Shilv Cai","Xiaoguo Liang","Shuning Cao","Luxin Yan","Sheng Zhong","Liqun Chen","Xu Zou"],"pdf_url":"https://arxiv.org/pdf/2403.14135v1.pdf","comment":"Accpeted by ICME 2024"},{"id":"http://arxiv.org/abs/2403.14133v1","updated":"2024-03-21T05:04:52Z","published":"2024-03-21T05:04:52Z","title":"3D Object Detection from Point Cloud via Voting Step Diffusion","summary":"  3D object detection is a fundamental task in scene understanding. Numerous\nresearch efforts have been dedicated to better incorporate Hough voting into\nthe 3D object detection pipeline. However, due to the noisy, cluttered, and\npartial nature of real 3D scans, existing voting-based methods often receive\nvotes from the partial surfaces of individual objects together with severe\nnoises, leading to sub-optimal detection performance. In this work, we focus on\nthe distributional properties of point clouds and formulate the voting process\nas generating new points in the high-density region of the distribution of\nobject centers. To achieve this, we propose a new method to move random 3D\npoints toward the high-density region of the distribution by estimating the\nscore function of the distribution with a noise conditioned score network.\nSpecifically, we first generate a set of object center proposals to coarsely\nidentify the high-density region of the object center distribution. To estimate\nthe score function, we perturb the generated object center proposals by adding\nnormalized Gaussian noise, and then jointly estimate the score function of all\nperturbed distributions. Finally, we generate new votes by moving random 3D\npoints to the high-density region of the object center distribution according\nto the estimated score function. Extensive experiments on two large scale\nindoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority\nof our proposed method. The code will be released at\nhttps://github.com/HHrEtvP/DiffVote.\n","authors":["Haoran Hou","Mingtao Feng","Zijie Wu","Weisheng Dong","Qing Zhu","Yaonan Wang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.14133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16387v3","updated":"2024-03-21T04:52:57Z","published":"2023-10-25T05:59:25Z","title":"Frequency-Aware Transformer for Learned Image Compression","summary":"  Learned image compression (LIC) has gained traction as an effective solution\nfor image storage and transmission in recent years. However, existing LIC\nmethods are redundant in latent representation due to limitations in capturing\nanisotropic frequency components and preserving directional details. To\novercome these challenges, we propose a novel frequency-aware transformer (FAT)\nblock that for the first time achieves multiscale directional ananlysis for\nLIC. The FAT block comprises frequency-decomposition window attention (FDWA)\nmodules to capture multiscale and directional frequency components of natural\nimages. Additionally, we introduce frequency-modulation feed-forward network\n(FMFFN) to adaptively modulate different frequency components, improving\nrate-distortion performance. Furthermore, we present a transformer-based\nchannel-wise autoregressive (T-CA) model that effectively exploits channel\ndependencies. Experiments show that our method achieves state-of-the-art\nrate-distortion performance compared to existing LIC methods, and evidently\noutperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in\nBD-rate on the Kodak, Tecnick, and CLIC datasets.\n","authors":["Han Li","Shaohui Li","Wenrui Dai","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16387v3.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2310.16226v3","updated":"2024-03-21T04:47:27Z","published":"2023-10-24T22:41:14Z","title":"TiC-CLIP: Continual Training of CLIP Models","summary":"  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps.\nTiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text\npairs spanning 9 years (2014-2022). We first use our benchmarks to curate\nvarious dynamic evaluations to measure temporal robustness of existing models.\nWe show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$\nzero-shot accuracy on our curated retrieval task from 2021-2022 compared with\nmore recently trained models in OpenCLIP repository. We then study how to\nefficiently train models on time-continuous data. We demonstrate that a simple\nrehearsal-based approach that continues training from the last checkpoint and\nreplays old data reduces compute by $2.5\\times$ when compared to the standard\npractice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.\n","authors":["Saurabh Garg","Mehrdad Farajtabar","Hadi Pouransari","Raviteja Vemulapalli","Sachin Mehta","Oncel Tuzel","Vaishaal Shankar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2310.16226v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.14124v1","updated":"2024-03-21T04:34:24Z","published":"2024-03-21T04:34:24Z","title":"Soft Masked Transformer for Point Cloud Processing with Skip\n  Attention-Based Upsampling","summary":"  Point cloud processing methods leverage local and global point features %at\nthe feature level to cater to downstream tasks, yet they often overlook the\ntask-level context inherent in point clouds during the encoding stage. We argue\nthat integrating task-level information into the encoding stage significantly\nenhances performance. To that end, we propose SMTransformer which incorporates\ntask-level information into a vector-based transformer by utilizing a soft mask\ngenerated from task-level queries and keys to learn the attention weights.\nAdditionally, to facilitate effective communication between features from the\nencoding and decoding layers in high-level tasks such as segmentation, we\nintroduce a skip-attention-based up-sampling block. This block dynamically\nfuses features from various resolution points across the encoding and decoding\nlayers. To mitigate the increase in network parameters and training time\nresulting from the complexity of the aforementioned blocks, we propose a novel\nshared position encoding strategy. This strategy allows various transformer\nblocks to share the same position information over the same resolution points,\nthereby reducing network parameters and training time without compromising\naccuracy.Experimental comparisons with existing methods on multiple datasets\ndemonstrate the efficacy of SMTransformer and skip-attention-based up-sampling\nfor point cloud processing tasks, including semantic segmentation and\nclassification. In particular, we achieve state-of-the-art semantic\nsegmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN\ndataset\n","authors":["Yong He","Hongshan Yu","Muhammad Ibrahim","Xiaoyan Liu","Tongjia Chen","Anwaar Ulhaq","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.14124v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.14121v1","updated":"2024-03-21T04:24:49Z","published":"2024-03-21T04:24:49Z","title":"External Knowledge Enhanced 3D Scene Generation from Sketch","summary":"  Generating realistic 3D scenes is challenging due to the complexity of room\nlayouts and object geometries.We propose a sketch based knowledge enhanced\ndiffusion architecture (SEK) for generating customized, diverse, and plausible\n3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the\ntarget scene and cues from an object relationship knowledge base. We first\nconstruct an external knowledge base containing object relationships and then\nleverage knowledge enhanced graph reasoning to assist our model in\nunderstanding hand-drawn sketches. A scene is represented as a combination of\n3D objects and their relationships, and then incrementally diffused to reach a\nGaussian distribution.We propose a 3D denoising scene transformer that learns\nto reverse the diffusion process, conditioned by a hand-drawn sketch along with\nknowledge cues, to regressively generate the scene including the 3D object\ninstances as well as their layout. Experiments on the 3D-FRONT dataset show\nthat our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and\nFID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest\ncompetitor DiffuScene.\n","authors":["Zijie Wu","Mingtao Feng","Yaonan Wang","He Xie","Weisheng Dong","Bo Miao","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.14121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10159v4","updated":"2024-03-21T04:17:26Z","published":"2023-06-16T20:02:51Z","title":"Vision-Language Models can Identify Distracted Driver Behavior from\n  Naturalistic Videos","summary":"  Recognizing the activities causing distraction in real-world driving\nscenarios is critical for ensuring the safety and reliability of both drivers\nand pedestrians on the roadways. Conventional computer vision techniques are\ntypically data-intensive and require a large volume of annotated training data\nto detect and classify various distracted driving behaviors, thereby limiting\ntheir efficiency and scalability. We aim to develop a generalized framework\nthat showcases robust performance with access to limited or no annotated\ntraining data. Recently, vision-language models have offered large-scale\nvisual-textual pretraining that can be adapted to task-specific learning like\ndistracted driving activity recognition. Vision-language pretraining models,\nsuch as CLIP, have shown significant promise in learning natural\nlanguage-guided visual representations. This paper proposes a CLIP-based driver\nactivity recognition approach that identifies driver distraction from\nnaturalistic driving images and videos. CLIP's vision embedding offers\nzero-shot transfer and task-based finetuning, which can classify distracted\nactivities from driving video data. Our results show that this framework offers\nstate-of-the-art performance on zero-shot transfer and video-based CLIP for\npredicting the driver's state on two public datasets. We propose both\nframe-based and video-based frameworks developed on top of the CLIP's visual\nrepresentation for distracted driving detection and classification tasks and\nreport the results.\n","authors":["Md Zahid Hasan","Jiajing Chen","Jiyang Wang","Mohammed Shaiqur Rahman","Ameya Joshi","Senem Velipasalar","Chinmay Hegde","Anuj Sharma","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2306.10159v4.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.14119v1","updated":"2024-03-21T04:08:29Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration-a crucial aspect\nfor quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.08977v2","updated":"2024-03-21T04:04:25Z","published":"2023-12-14T14:26:57Z","title":"Weighted Ensemble Models Are Strong Continual Learners","summary":"  In this work, we study the problem of continual learning (CL) where the goal\nis to learn a model on a sequence of tasks, such that the data from the\nprevious tasks becomes unavailable while learning on the current task data. CL\nis essentially a balancing act between being able to learn on the new task\n(i.e., plasticity) and maintaining the performance on the previously learned\nconcepts (i.e., stability). Intending to address the stability-plasticity\ntrade-off, we propose to perform weight-ensembling of the model parameters of\nthe previous and current tasks. This weighted-ensembled model, which we call\nContinual Model Averaging (or CoMA), attains high accuracy on the current task\nby leveraging plasticity, while not deviating too far from the previous weight\nconfiguration, ensuring stability. We also propose an improved variant of CoMA,\nnamed Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively\nweighs each parameter in the weights ensemble by leveraging the Fisher\ninformation of the weights of the model. Both variants are conceptually simple,\neasy to implement, and effective in attaining state-of-the-art performance on\nseveral standard CL benchmarks. Code is available at:\nhttps://github.com/IemProg/CoFiMA.\n","authors":["Imad Eddine Marouf","Subhankar Roy","Enzo Tartaglione","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.08977v2.pdf","comment":"Code: https://github.com/IemProg/CoFiMA"},{"id":"http://arxiv.org/abs/2403.14115v1","updated":"2024-03-21T04:01:26Z","published":"2024-03-21T04:01:26Z","title":"Training point-based deep learning networks for forest segmentation with\n  synthetic data","summary":"  Remote sensing through unmanned aerial systems (UAS) has been increasing in\nforestry in recent years, along with using machine learning for data\nprocessing. Deep learning architectures, extensively applied in natural\nlanguage and image processing, have recently been extended to the point cloud\ndomain. However, the availability of point cloud datasets for training and\ntesting remains limited. Creating forested environment point cloud datasets is\nexpensive, requires high-precision sensors, and is time-consuming as manual\npoint classification is required. Moreover, forest areas could be inaccessible\nor dangerous for humans, further complicating data collection. Then, a question\narises whether it is possible to use synthetic data to train deep learning\nnetworks without the need to rely on large volumes of real forest data. To\nanswer this question, we developed a realistic simulator that procedurally\ngenerates synthetic forest scenes. Thanks to this, we have conducted a\ncomparative study of different state-of-the-art point-based deep learning\nnetworks for forest segmentation. Using created datasets, we determined the\nfeasibility of using synthetic data to train deep learning networks to classify\npoint clouds from real forest datasets. Both the simulator and the datasets are\nreleased as part of this work.\n","authors":["Francisco Raverta Capua","Juan Schandin","Pablo De Cristóforis"],"pdf_url":"https://arxiv.org/pdf/2403.14115v1.pdf","comment":"15 pages, 4 figures. Submitted to the International Conference on\n  Pattern Recognition (ICPR) 2024"},{"id":"http://arxiv.org/abs/2403.13331v2","updated":"2024-03-21T04:01:10Z","published":"2024-03-20T06:22:37Z","title":"AMP: Autoregressive Motion Prediction Revisited with Next Token\n  Prediction for Autonomous Driving","summary":"  As an essential task in autonomous driving (AD), motion prediction aims to\npredict the future states of surround objects for navigation. One natural\nsolution is to estimate the position of other agents in a step-by-step manner\nwhere each predicted time-step is conditioned on both observed time-steps and\npreviously predicted time-steps, i.e., autoregressive prediction. Pioneering\nworks like SocialLSTM and MFP design their decoders based on this intuition.\nHowever, almost all state-of-the-art works assume that all predicted time-steps\nare independent conditioned on observed time-steps, where they use a single\nlinear layer to generate positions of all time-steps simultaneously. They\ndominate most motion prediction leaderboards due to the simplicity of training\nMLPs compared to autoregressive networks.\n  In this paper, we introduce the GPT style next token prediction into motion\nforecasting. In this way, the input and output could be represented in a\nunified space and thus the autoregressive prediction becomes more feasible.\nHowever, different from language data which is composed of homogeneous units\n-words, the elements in the driving scene could have complex spatial-temporal\nand semantic relations. To this end, we propose to adopt three factorized\nattention modules with different neighbors for information aggregation and\ndifferent position encoding styles to capture their relations, e.g., encoding\nthe transformation between coordinate systems for spatial relativity while\nadopting RoPE for temporal relativity. Empirically, by equipping with the\naforementioned tailored designs, the proposed method achieves state-of-the-art\nperformance in the Waymo Open Motion and Waymo Interaction datasets. Notably,\nAMP outperforms other recent autoregressive motion prediction methods: MotionLM\nand StateTransformer, which demonstrates the effectiveness of the proposed\ndesigns.\n","authors":["Xiaosong Jia","Shaoshuai Shi","Zijun Chen","Li Jiang","Wenlong Liao","Tao He","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.13331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14114v1","updated":"2024-03-21T03:58:27Z","published":"2024-03-21T03:58:27Z","title":"Test-time Similarity Modification for Person Re-identification toward\n  Temporal Distribution Shift","summary":"  Person re-identification (re-id), which aims to retrieve images of the same\nperson in a given image from a database, is one of the most practical image\nrecognition applications. In the real world, however, the environments that the\nimages are taken from change over time. This causes a distribution shift\nbetween training and testing and degrades the performance of re-id. To maintain\nre-id performance, models should continue adapting to the test environment's\ntemporal changes. Test-time adaptation (TTA), which aims to adapt models to the\ntest environment with only unlabeled test data, is a promising way to handle\nthis problem because TTA can adapt models instantly in the test environment.\nHowever, the previous TTA methods are designed for classification and cannot be\ndirectly applied to re-id. This is because the set of people's identities in\nthe dataset differs between training and testing in re-id, whereas the set of\nclasses is fixed in the current TTA methods designed for classification. To\nimprove re-id performance in changing test environments, we propose TEst-time\nsimilarity Modification for Person re-identification (TEMP), a novel TTA method\nfor re-id. TEMP is the first fully TTA method for re-id, which does not require\nany modification to pre-training. Inspired by TTA methods that refine the\nprediction uncertainty in classification, we aim to refine the uncertainty in\nre-id. However, the uncertainty cannot be computed in the same way as\nclassification in re-id since it is an open-set task, which does not share\nperson labels between training and testing. Hence, we propose re-id entropy, an\nalternative uncertainty measure for re-id computed based on the similarity\nbetween the feature vectors. Experiments show that the re-id entropy can\nmeasure the uncertainty on re-id and TEMP improves the performance of re-id in\nonline settings where the distribution changes over time.\n","authors":["Kazuki Adachi","Shohei Enomoto","Taku Sasaki","Shin'ya Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.14114v1.pdf","comment":"Accepted to IJCNN2024"},{"id":"http://arxiv.org/abs/2403.14113v1","updated":"2024-03-21T03:56:24Z","published":"2024-03-21T03:56:24Z","title":"Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity\n  Recognition","summary":"  Panoramic Activity Recognition (PAR) seeks to identify diverse human\nactivities across different scales, from individual actions to social group and\nglobal activities in crowded panoramic scenes. PAR presents two major\nchallenges: 1) recognizing the nuanced interactions among numerous individuals\nand 2) understanding multi-granular human activities. To address these, we\npropose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key\ndesign principles. First, while previous works often focus on spatial distance\namong individuals within an image, we argue to consider the spatio-temporal\nproximity. It is crucial for individual relation encoding to correctly\nunderstand social dynamics. Secondly, deviating from existing hierarchical\napproaches (individual-to-social-to-global activity), we introduce a dual-path\narchitecture for multi-granular activity recognition. This architecture\ncomprises individual-to-global and individual-to-social paths, mutually\nreinforcing each other's task with global-local context through multiple\nlayers. Through extensive experiments, we validate the effectiveness of the\nspatio-temporal proximity among individuals and the dual-path architecture in\nPAR. Furthermore, SPDP-Net achieves new state-of-the-art performance with\n46.5\\% of overall F1 score on JRDB-PAR dataset.\n","authors":["Sumin Lee","Yooseung Wang","Sangmin Woo","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14104v1","updated":"2024-03-21T03:34:18Z","published":"2024-03-21T03:34:18Z","title":"Existence Is Chaos: Enhancing 3D Human Motion Prediction with\n  Uncertainty Consideration","summary":"  Human motion prediction is consisting in forecasting future body poses from\nhistorically observed sequences. It is a longstanding challenge due to motion's\ncomplex dynamics and uncertainty. Existing methods focus on building up\ncomplicated neural networks to model the motion dynamics. The predicted results\nare required to be strictly similar to the training samples with L2 loss in\ncurrent training pipeline. However, little attention has been paid to the\nuncertainty property which is crucial to the prediction task. We argue that the\nrecorded motion in training data could be an observation of possible future,\nrather than a predetermined result. In addition, existing works calculate the\npredicted error on each future frame equally during training, while recent work\nindicated that different frames could play different roles. In this work, a\nnovel computationally efficient encoder-decoder model with uncertainty\nconsideration is proposed, which could learn proper characteristics for future\nframes by a dynamic function. Experimental results on benchmark datasets\ndemonstrate that our uncertainty consideration approach has obvious advantages\nboth in quantity and quality. Moreover, the proposed method could produce\nmotion sequences with much better quality that avoids the intractable shaking\nartefacts. We believe our work could provide a novel perspective to consider\nthe uncertainty quality for the general motion prediction task and encourage\nthe studies in this field. The code will be available in\nhttps://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.\n","authors":["Zhihao Wang","Yulin Zhou","Ningyu Zhang","Xiaosong Yang","Jun Xiao","Zhao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14104v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.14103v1","updated":"2024-03-21T03:28:24Z","published":"2024-03-21T03:28:24Z","title":"MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical\n  Image Segmentation","summary":"  Segment Anything Model~(SAM), a prompt-driven foundation model for natural\nimage segmentation, has demonstrated impressive zero-shot performance. However,\nSAM does not work when directly applied to medical image segmentation tasks,\nsince SAM lacks the functionality to predict semantic labels for predicted\nmasks and needs to provide extra prompts, such as points or boxes, to segment\ntarget regions. Meanwhile, there is a huge gap between 2D natural images and 3D\nmedical images, so the performance of SAM is imperfect for medical image\nsegmentation tasks. Following the above issues, we propose MaskSAM, a novel\nmask classification prompt-free SAM adaptation framework for medical image\nsegmentation. We design a prompt generator combined with the image encoder in\nSAM to generate a set of auxiliary classifier tokens, auxiliary binary masks,\nand auxiliary bounding boxes. Each pair of auxiliary mask and box prompts,\nwhich can solve the requirements of extra prompts, is associated with class\nlabel predictions by the sum of the auxiliary classifier token and the\nlearnable global classifier tokens in the mask decoder of SAM to solve the\npredictions of semantic labels. Meanwhile, we design a 3D depth-convolution\nadapter for image embeddings and a 3D depth-MLP adapter for prompt embeddings.\nWe inject one of them into each transformer block in the image encoder and mask\ndecoder to enable pre-trained 2D SAM models to extract 3D information and adapt\nto 3D medical images. Our method achieves state-of-the-art performance on\nAMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method\nsurpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.\n","authors":["Bin Xie","Hao Tang","Bin Duan","Dawen Cai","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.14103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14101v1","updated":"2024-03-21T03:24:01Z","published":"2024-03-21T03:24:01Z","title":"Text-Enhanced Data-free Approach for Federated Class-Incremental\n  Learning","summary":"  Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal\nissue, involving the dynamic addition of new classes in the context of\nfederated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a\ncrucial role in addressing catastrophic forgetting and data privacy problems.\nHowever, prior approaches lack the crucial synergy between DFKT and the model\ntraining phases, causing DFKT to encounter difficulties in generating\nhigh-quality data from a non-anchored latent space of the old task model. In\nthis paper, we introduce LANDER (Label Text Centered Data-Free Knowledge\nTransfer) to address this issue by utilizing label text embeddings (LTE)\nproduced by pretrained language models. Specifically, during the model training\nphase, our approach treats LTE as anchor points and constrains the feature\nembeddings of corresponding training samples around them, enriching the\nsurrounding area with more meaningful information. In the DFKT phase, by using\nthese LTE anchors, LANDER can synthesize more meaningful samples, thereby\neffectively addressing the forgetting problem. Additionally, instead of tightly\nconstraining embeddings toward the anchor, the Bounding Loss is introduced to\nencourage sample embeddings to remain flexible within a defined radius. This\napproach preserves the natural differences in sample embeddings and mitigates\nthe embedding overlap caused by heterogeneous federated settings. Extensive\nexperiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that\nLANDER significantly outperforms previous methods and achieves state-of-the-art\nperformance in FCIL. The code is available at\nhttps://github.com/tmtuan1307/lander.\n","authors":["Minh-Tuan Tran","Trung Le","Xuan-May Le","Mehrtash Harandi","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.14101v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2104.02857v2","updated":"2024-03-21T03:21:34Z","published":"2021-04-07T02:04:12Z","title":"Soft-Label Anonymous Gastric X-ray Image Distillation","summary":"  This paper presents a soft-label anonymous gastric X-ray image distillation\nmethod based on a gradient descent approach. The sharing of medical data is\ndemanded to construct high-accuracy computer-aided diagnosis (CAD) systems.\nHowever, the large size of the medical dataset and privacy protection are\nremaining problems in medical data sharing, which hindered the research of CAD\nsystems. The idea of our distillation method is to extract the valid\ninformation of the medical dataset and generate a tiny distilled dataset that\nhas a different data distribution. Different from model distillation, our\nmethod aims to find the optimal distilled images, distilled labels and the\noptimized learning rate. Experimental results show that the proposed method can\nnot only effectively compress the medical dataset but also anonymize medical\nimages to protect the patient's private information. The proposed approach can\nimprove the efficiency and security of medical data sharing.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2104.02857v2.pdf","comment":"The first paper to explore real-world dataset distillation; Work was\n  done in 2019 and published as a conference paper at ICIP 2020"},{"id":"http://arxiv.org/abs/2309.06255v3","updated":"2024-03-21T03:21:24Z","published":"2023-09-12T14:16:34Z","title":"Enhancing Multimodal Cooperation via Fine-grained Modality Valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However, most models often\nsuffer from unsatisfactory multimodal cooperation, which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality, but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence, it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities, especially when facing realistic scenarios\nwhere the modality discrepancy could vary across different samples. To this\nend, we introduce a sample-level modality valuation metric to evaluate the\ncontribution of each modality for each sample. Via modality valuation, we\nobserve that modality discrepancy indeed could be different at sample-level,\nbeyond the global contribution discrepancy at dataset-level. We further analyze\nthis issue and improve cooperation between modalities at sample-level by\nenhancing the discriminative ability of low-contributing modalities in a\ntargeted manner. Overall, our methods reasonably observe the fine-grained\nuni-modal contribution and achieve considerable improvement. The source code\nand dataset are available at\n\\url{https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation}.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.19286v2","updated":"2024-03-21T03:14:59Z","published":"2024-02-29T15:51:14Z","title":"PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology\n  Segmentation","summary":"  Understanding the anatomy of renal pathology is crucial for advancing disease\ndiagnostics, treatment evaluation, and clinical research. The complex kidney\nsystem comprises various components across multiple levels, including regions\n(cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes,\nmesangial cells in glomerulus). Prior studies have predominantly overlooked the\nintricate spatial interrelations among objects from clinical knowledge. In this\nresearch, we introduce a novel universal proposition learning approach, called\npanoramic renal pathology segmentation (PrPSeg), designed to segment\ncomprehensively panoramic structures within kidney by integrating extensive\nknowledge of kidney anatomy.\n  In this paper, we propose (1) the design of a comprehensive universal\nproposition matrix for renal pathology, facilitating the incorporation of\nclassification and spatial relationships into the segmentation process; (2) a\ntoken-based dynamic head single network architecture, with the improvement of\nthe partial label image segmentation and capability for future data\nenlargement; and (3) an anatomy loss function, quantifying the inter-object\nrelationships across the kidney.\n","authors":["Ruining Deng","Quan Liu","Can Cui","Tianyuan Yao","Jialin Yue","Juming Xiong","Lining Yu","Yifei Wu","Mengmeng Yin","Yu Wang","Shilin Zhao","Yucheng Tang","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2402.19286v2.pdf","comment":"IEEE / CVF Computer Vision and Pattern Recognition Conference 2024"},{"id":"http://arxiv.org/abs/2403.14093v1","updated":"2024-03-21T03:01:25Z","published":"2024-03-21T03:01:25Z","title":"Science based AI model certification for untrained operational\n  environments with application in traffic state estimation","summary":"  The expanding role of Artificial Intelligence (AI) in diverse engineering\ndomains highlights the challenges associated with deploying AI models in new\noperational environments, involving substantial investments in data collection\nand model training. Rapid application of AI necessitates evaluating the\nfeasibility of utilizing pre-trained models in unobserved operational settings\nwith minimal or no additional data. However, interpreting the opaque nature of\nAI's black-box models remains a persistent challenge. Addressing this issue,\nthis paper proposes a science-based certification methodology to assess the\nviability of employing pre-trained data-driven models in untrained operational\nenvironments. The methodology advocates a profound integration of domain\nknowledge, leveraging theoretical and analytical models from physics and\nrelated disciplines, with data-driven AI models. This novel approach introduces\ntools to facilitate the development of secure engineering systems, providing\ndecision-makers with confidence in the trustworthiness and safety of AI-based\nmodels across diverse environments characterized by limited training data and\ndynamic, uncertain conditions. The paper demonstrates the efficacy of this\nmethodology in real-world safety-critical scenarios, particularly in the\ncontext of traffic state estimation. Through simulation results, the study\nillustrates how the proposed methodology efficiently quantifies physical\ninconsistencies exhibited by pre-trained AI models. By utilizing analytical\nmodels, the methodology offers a means to gauge the applicability of\npre-trained AI models in new operational environments. This research\ncontributes to advancing the understanding and deployment of AI models,\noffering a robust certification framework that enhances confidence in their\nreliability and safety across a spectrum of operational conditions.\n","authors":["Daryl Mupupuni","Anupama Guntu","Liang Hong","Kamrul Hasan","Leehyun Keel"],"pdf_url":"https://arxiv.org/pdf/2403.14093v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11708v2","updated":"2024-03-21T02:48:22Z","published":"2024-03-18T12:12:45Z","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n","authors":["Kaijie Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11708v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14089v1","updated":"2024-03-21T02:45:16Z","published":"2024-03-21T02:45:16Z","title":"Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced\n  Training","summary":"  Unsupervised intrinsic image decomposition (IID) is the process of separating\na natural image into albedo and shade without these ground truths. A recent\nmodel employing light detection and ranging (LiDAR) intensity demonstrated\nimpressive performance, though the necessity of LiDAR intensity during\ninference restricts its practicality. Thus, IID models employing only a single\nimage during inference while keeping as high IID quality as the one with an\nimage plus LiDAR intensity are highly desired. To address this challenge, we\npropose a novel approach that utilizes only an image during inference while\nutilizing an image and LiDAR intensity during training. Specifically, we\nintroduce a partially-shared model that accepts an image and LiDAR intensity\nindividually using a different specific encoder but processes them together in\nspecific components to learn shared representations. In addition, to enhance\nIID quality, we propose albedo-alignment loss and image-LiDAR conversion (ILC)\npaths. Albedo-alignment loss aligns the gray-scale albedo from an image to that\ninferred from LiDAR intensity, thereby reducing cast shadows in albedo from an\nimage due to the absence of cast shadows in LiDAR intensity. Furthermore, to\ntranslate the input image into albedo and shade style while keeping the image\ncontents, the input image is separated into style code and content code by\nencoders. The ILC path mutually translates the image and LiDAR intensity, which\nshare content but differ in style, contributing to the distinct differentiation\nof style from content. Consequently, LIET achieves comparable IID quality to\nthe existing model with LiDAR intensity, while utilizing only an image without\nLiDAR intensity during inference.\n","authors":["Shogo Sato","Takuhiro Kaneko","Kazuhiko Murasaki","Taiga Yoshida","Ryuichi Tanida","Akisato Kimura"],"pdf_url":"https://arxiv.org/pdf/2403.14089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13535v2","updated":"2024-03-21T02:31:58Z","published":"2024-03-20T12:13:04Z","title":"IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models","summary":"  Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images.\n","authors":["Siying Cui","Jia Guo","Xiang An","Jiankang Deng","Yongle Zhao","Xinyu Wei","Ziyong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13535v2.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.13800v4","updated":"2024-03-21T02:31:39Z","published":"2023-03-24T04:45:45Z","title":"Aligning Step-by-Step Instructional Diagrams to Video Demonstrations","summary":"  Multimodal alignment facilitates the retrieval of instances from one modality\nwhen queried using another. In this paper, we consider a novel setting where\nsuch an alignment is between (i) instruction steps that are depicted as\nassembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video\nsegments from in-the-wild videos; these videos comprising an enactment of the\nassembly actions in the real world. To learn this alignment, we introduce a\nnovel supervised contrastive learning method that learns to align videos with\nthe subtle details in the assembly diagrams, guided by a set of novel losses.\nTo study this problem and demonstrate the effectiveness of our method, we\nintroduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183\nhours of videos from diverse furniture assembly collections and nearly 8,300\nillustrations from their associated instruction manuals and annotated for their\nground truth alignments. We define two tasks on this dataset: First, nearest\nneighbor retrieval between video segments and illustrations, and, second,\nalignment of instruction steps and the segments for each video. Extensive\nexperiments on IAW demonstrate superior performances of our approach against\nalternatives.\n","authors":["Jiahao Zhang","Anoop Cherian","Yanbin Liu","Yizhak Ben-Shabat","Cristian Rodriguez","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.13800v4.pdf","comment":"Project website:\n  https://academic.davidz.cn/en/publication/zhang-cvpr-2023/"},{"id":"http://arxiv.org/abs/2403.14085v1","updated":"2024-03-21T02:31:17Z","published":"2024-03-21T02:31:17Z","title":"Surface Reconstruction from Point Clouds via Grid-based Intersection\n  Prediction","summary":"  Surface reconstruction from point clouds is a crucial task in the fields of\ncomputer vision and computer graphics. SDF-based methods excel at\nreconstructing smooth meshes with minimal error and artifacts but struggle with\nrepresenting open surfaces. On the other hand, UDF-based methods can\neffectively represent open surfaces but often introduce noise near the surface,\nleading to artifacts in the mesh. In this work, we propose a novel approach\nthat directly predicts the intersection points between sampled line segments of\npoint pairs and implicit surfaces. This method not only preserves the ability\nto represent open surfaces but also eliminates artifacts in the mesh. Our\napproach demonstrates state-of-the-art performance on three datasets: ShapeNet,\nMGN, and ScanNet. The code will be made available upon acceptance.\n","authors":["Hui Tian","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14082v1","updated":"2024-03-21T02:19:54Z","published":"2024-03-21T02:19:54Z","title":"EventDance: Unsupervised Source-free Cross-modal Adaptation for\n  Event-based Object Recognition","summary":"  In this paper, we make the first attempt at achieving the cross-modal (i.e.,\nimage-to-events) adaptation for event-based object recognition without\naccessing any labeled source image data owning to privacy and commercial\nissues. Tackling this novel problem is non-trivial due to the novelty of event\ncameras and the distinct modality gap between images and events. In particular,\nas only the source model is available, a hurdle is how to extract the knowledge\nfrom the source model by only using the unlabeled target event data while\nachieving knowledge transfer. To this end, we propose a novel framework, dubbed\nEventDance for this unsupervised source-free cross-modal adaptation problem.\nImportantly, inspired by event-to-video reconstruction methods, we propose a\nreconstruction-based modality bridging (RMB) module, which reconstructs\nintensity frames from events in a self-supervised manner. This makes it\npossible to build up the surrogate images to extract the knowledge (i.e.,\nlabels) from the source model. We then propose a multi-representation knowledge\nadaptation (MKA) module that transfers the knowledge to target models learning\nevents with multiple representation types for fully exploring the\nspatiotemporal information of events. The two modules connecting the source and\ntarget models are mutually updated so as to achieve the best performance.\nExperiments on three benchmark datasets with two adaption settings show that\nEventDance is on par with prior methods utilizing the source data.\n","authors":["Xu Zheng","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14082v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2312.05826v2","updated":"2024-03-21T02:08:18Z","published":"2023-12-10T08:59:43Z","title":"R2Human: Real-Time 3D Human Appearance Rendering from a Single Image","summary":"  Rendering 3D human appearance in different views is crucial for achieving\nholographic communication and immersive VR/AR. Existing methods either rely on\nmulti-camera setups or have low-quality rendered images from a single image. In\nthis paper, we propose R2Human, the first approach for real-time inference and\nrendering of photorealistic 3D human appearance from a single image. The core\nof our approach is to combine the strengths of implicit texture fields and\nexplicit neural rendering with our novel representation, namely Z-map. Based on\nthis, we present an end-to-end network that performs high-fidelity color\nreconstruction of visible areas and provides reliable color inference for\noccluded regions. To further enhance the 3D perception ability of our network,\nwe leverage the Fourier occupancy field as a prior for generating the texture\nfield and providing a sampling surface in the rendering stage. We also propose\na consistency loss and a spatio-temporal fusion strategy to ensure the\nmulti-view coherence. Experimental results show that our method outperforms the\nstate-of-the-art methods on both synthetic data and challenging real-world\nimages, in real time.\n","authors":["Yuanwang Yang","Qiao Feng","Yu-Kun Lai","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2312.05826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14070v1","updated":"2024-03-21T01:37:50Z","published":"2024-03-21T01:37:50Z","title":"QSMDiff: Unsupervised 3D Diffusion Models for Quantitative\n  Susceptibility Mapping","summary":"  Quantitative Susceptibility Mapping (QSM) dipole inversion is an ill-posed\ninverse problem for quantifying magnetic susceptibility distributions from MRI\ntissue phases. While supervised deep learning methods have shown success in\nspecific QSM tasks, their generalizability across different acquisition\nscenarios remains constrained. Recent developments in diffusion models have\ndemonstrated potential for solving 2D medical imaging inverse problems.\nHowever, their application to 3D modalities, such as QSM, remains challenging\ndue to high computational demands. In this work, we developed a 3D image\npatch-based diffusion model, namely QSMDiff, for robust QSM reconstruction\nacross different scan parameters, alongside simultaneous super-resolution and\nimage-denoising tasks. QSMDiff adopts unsupervised 3D image patch training and\nfull-size measurement guidance during inference for controlled image\ngeneration. Evaluation on simulated and in-vivo human brains, using\ngradient-echo and echo-planar imaging sequences across different acquisition\nparameters, demonstrates superior performance. The method proposed in QSMDiff\nalso holds promise for impacting other 3D medical imaging applications beyond\nQSM.\n","authors":["Zhuang Xiong","Wei Jiang","Yang Gao","Feng Liu","Hongfu Sun"],"pdf_url":"https://arxiv.org/pdf/2403.14070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.09213v6","updated":"2024-03-21T16:24:05Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v6.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.14886v1","updated":"2024-03-21T23:43:30Z","published":"2024-03-21T23:43:30Z","title":"DSGG: Dense Relation Transformer for an End-to-end Scene Graph\n  Generation","summary":"  Scene graph generation aims to capture detailed spatial and semantic\nrelationships between objects in an image, which is challenging due to\nincomplete labelling, long-tailed relationship categories, and relational\nsemantic overlap. Existing Transformer-based methods either employ distinct\nqueries for objects and predicates or utilize holistic queries for relation\ntriplets and hence often suffer from limited capacity in learning low-frequency\nrelationships. In this paper, we present a new Transformer-based method, called\nDSGG, that views scene graph detection as a direct graph prediction problem\nbased on a unique set of graph-aware queries. In particular, each graph-aware\nquery encodes a compact representation of both the node and all of its\nrelations in the graph, acquired through the utilization of a relaxed sub-graph\nmatching during the training process. Moreover, to address the problem of\nrelational semantic overlap, we utilize a strategy for relation distillation,\naiming to efficiently learn multiple instances of semantic relationships.\nExtensive experiments on the VG and the PSG datasets show that our model\nachieves state-of-the-art results, showing a significant improvement of 3.5\\%\nand 6.7\\% in mR@50 and mR@100 for the scene-graph generation task and achieves\nan even more substantial improvement of 8.5\\% and 10.3\\% in mR@50 and mR@100\nfor the panoptic scene graph generation task. Code is available at\n\\url{https://github.com/zeeshanhayder/DSGG}.\n","authors":["Zeeshan Hayder","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2403.14886v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14874v1","updated":"2024-03-21T22:46:27Z","published":"2024-03-21T22:46:27Z","title":"WeatherProof: Leveraging Language Guidance for Semantic Segmentation in\n  Adverse Weather","summary":"  We propose a method to infer semantic segmentation maps from images captured\nunder adverse weather conditions. We begin by examining existing models on\nimages degraded by weather conditions such as rain, fog, or snow, and found\nthat they exhibit a large performance drop as compared to those captured under\nclear weather. To control for changes in scene structures, we propose\nWeatherProof, the first semantic segmentation dataset with accurate clear and\nadverse weather image pairs that share an underlying scene. Through this\ndataset, we analyze the error modes in existing models and found that they were\nsensitive to the highly complex combination of different weather effects\ninduced on the image during capture. To improve robustness, we propose a way to\nuse language as guidance by identifying contributions of adverse weather\nconditions and injecting that as \"side information\". Models trained using our\nlanguage guidance exhibit performance gains by up to 10.2% in mIoU on\nWeatherProof, up to 8.44% in mIoU on the widely used ACDC dataset compared to\nstandard training techniques, and up to 6.21% in mIoU on the ACDC dataset as\ncompared to previous SOTA methods.\n","authors":["Blake Gella","Howard Zhang","Rishi Upadhyay","Tiffany Chang","Nathan Wei","Matthew Waliman","Yunhao Bao","Celso de Melo","Alex Wong","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2403.14874v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.09534"},{"id":"http://arxiv.org/abs/2403.14870v1","updated":"2024-03-21T22:36:24Z","published":"2024-03-21T22:36:24Z","title":"VidLA: Video-Language Alignment at Scale","summary":"  In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.\n","authors":["Mamshad Nayeem Rizve","Fan Fei","Jayakrishnan Unnikrishnan","Son Tran","Benjamin Z. Yao","Belinda Zeng","Mubarak Shah","Trishul Chilimbi"],"pdf_url":"https://arxiv.org/pdf/2403.14870v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14863v1","updated":"2024-03-21T22:18:25Z","published":"2024-03-21T22:18:25Z","title":"Distribution-informed and wavelength-flexible data-driven photoacoustic\n  oximetry","summary":"  Significance: Photoacoustic imaging (PAI) promises to measure\nspatially-resolved blood oxygen saturation, but suffers from a lack of accurate\nand robust spectral unmixing methods to deliver on this promise. Accurate blood\noxygenation estimation could have important clinical applications, from cancer\ndetection to quantifying inflammation.\n  Aim: This study addresses the inflexibility of existing data-driven methods\nfor estimating blood oxygenation in PAI by introducing a recurrent neural\nnetwork architecture.\n  Approach: We created 25 simulated training dataset variations to assess\nneural network performance. We used a long short-term memory network to\nimplement a wavelength-flexible network architecture and proposed the\nJensen-Shannon divergence to predict the most suitable training dataset.\n  Results: The network architecture can handle arbitrary input wavelengths and\noutperforms linear unmixing and the previously proposed learned spectral\ndecolouring method. Small changes in the training data significantly affect the\naccuracy of our method, but we find that the Jensen-Shannon divergence\ncorrelates with the estimation error and is thus suitable for predicting the\nmost appropriate training datasets for any given application.\n  Conclusions: A flexible data-driven network architecture combined with the\nJensen-Shannon Divergence to predict the best training data set provides a\npromising direction that might enable robust data-driven photoacoustic oximetry\nfor clinical use cases.\n","authors":["Janek Gröhl","Kylie Yeung","Kevin Gu","Thomas R. Else","Monika Golinska","Ellie V. Bunce","Lina Hacker","Sarah E. Bohndiek"],"pdf_url":"https://arxiv.org/pdf/2403.14863v1.pdf","comment":"37 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.06729v2","updated":"2024-03-21T22:11:31Z","published":"2023-12-11T09:12:35Z","title":"RGNet: A Unified Clip Retrieval and Grounding Network for Long Videos","summary":"  Locating specific moments within long videos (20-120 minutes) presents a\nsignificant challenge, akin to finding a needle in a haystack. Adapting\nexisting short video (5-30 seconds) grounding methods to this problem yields\npoor performance. Since most real life videos, such as those on YouTube and\nAR/VR, are lengthy, addressing this issue is crucial. Existing methods\ntypically operate in two stages: clip retrieval and grounding. However, this\ndisjoint process limits the retrieval module's fine-grained event\nunderstanding, crucial for specific moment detection. We propose RGNet which\ndeeply integrates clip retrieval and grounding into a single network capable of\nprocessing long videos into multiple granular levels, e.g., clips and frames.\nIts core component is a novel transformer encoder, RG-Encoder, that unifies the\ntwo stages through shared features and mutual optimization. The encoder\nincorporates a sparse attention mechanism and an attention loss to model both\ngranularity jointly. Moreover, we introduce a contrastive clip sampling\ntechnique to mimic the long video paradigm closely during training. RGNet\nsurpasses prior methods, showcasing state-of-the-art performance on long video\ntemporal grounding (LVTG) datasets MAD and Ego4D.\n","authors":["Tanveer Hannan","Md Mohaiminul Islam","Thomas Seidl","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2312.06729v2.pdf","comment":"The code is released at https://github.com/Tanveer81/RGNet"},{"id":"http://arxiv.org/abs/2403.11905v2","updated":"2024-03-21T21:57:13Z","published":"2024-03-18T16:06:30Z","title":"Tur[k]ingBench: A Challenge Benchmark for Web Agents","summary":"  Recent chatbots have demonstrated impressive ability to understand and\ncommunicate in raw-text form. However, there is more to the world than raw\ntext. For example, humans spend long hours of their time on web pages, where\ntext is intertwined with other modalities and tasks are accomplished in the\nform of various complex interactions. Can state-of-the-art multi-modal models\ngeneralize to such complex domains?\n  To address this question, we introduce TurkingBench, a benchmark of tasks\nformulated as web pages containing textual instructions with multi-modal\ncontext. Unlike existing work which employs artificially synthesized web pages,\nhere we use natural HTML pages that were originally designed for crowdsourcing\nworkers for various annotation purposes. The HTML instructions of each task are\nalso instantiated with various values (obtained from the crowdsourcing tasks)\nto form new instances of the task. This benchmark contains 32.2K instances\ndistributed across 158 tasks.\n  Additionally, to facilitate the evaluation on TurkingBench, we develop an\nevaluation framework that connects the responses of chatbots to modifications\non web pages (modifying a text box, checking a radio, etc.). We evaluate the\nperformance of state-of-the-art models, including language-only, vision-only,\nand layout-only models, and their combinations, on this benchmark. Our findings\nreveal that these models perform significantly better than random chance, yet\nconsiderable room exists for improvement. We hope this benchmark will help\nfacilitate the evaluation and development of web-based agents.\n","authors":["Kevin Xu","Yeganeh Kordi","Kate Sanders","Yizhong Wang","Adam Byerly","Jack Zhang","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2403.11905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14852v1","updated":"2024-03-21T21:56:09Z","published":"2024-03-21T21:56:09Z","title":"KeyPoint Relative Position Encoding for Face Recognition","summary":"  In this paper, we address the challenge of making ViT models more robust to\nunseen affine transformations. Such robustness becomes useful in various\nrecognition tasks such as face recognition when image alignment failures occur.\nWe propose a novel method called KP-RPE, which leverages key points\n(e.g.~facial landmarks) to make ViT more resilient to scale, translation, and\npose variations. We begin with the observation that Relative Position Encoding\n(RPE) is a good way to bring affine transform generalization to ViTs. RPE,\nhowever, can only inject the model with prior knowledge that nearby pixels are\nmore important than far pixels. Keypoint RPE (KP-RPE) is an extension of this\nprinciple, where the significance of pixels is not solely dictated by their\nproximity but also by their relative positions to specific keypoints within the\nimage. By anchoring the significance of pixels around keypoints, the model can\nmore effectively retain spatial relationships, even when those relationships\nare disrupted by affine transformations. We show the merit of KP-RPE in face\nand gait recognition. The experimental results demonstrate the effectiveness in\nimproving face recognition performance from low-quality images, particularly\nwhere alignment is prone to failure. Code and pre-trained models are available.\n","authors":["Minchul Kim","Yiyang Su","Feng Liu","Anil Jain","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14852v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2402.10045v3","updated":"2024-03-21T21:37:50Z","published":"2024-01-11T03:36:47Z","title":"Short-Form Videos and Mental Health: A Knowledge-Guided Neural Topic\n  Model","summary":"  While short-form videos head to reshape the entire social media landscape,\nexperts are exceedingly worried about their depressive impacts on viewers, as\nevidenced by medical studies. To prevent widespread consequences, platforms are\neager to predict these videos' impact on viewers' mental health. Subsequently,\nthey can take intervention measures, such as revising recommendation algorithms\nand displaying viewer discretion. Nevertheless, applicable predictive methods\nlack relevance to well-established medical knowledge, which outlines clinically\nproven external and environmental factors of depression. To account for such\nmedical knowledge, we resort to an emergent methodological discipline, seeded\nNeural Topic Models (NTMs). However, existing seeded NTMs suffer from the\nlimitations of single-origin topics, unknown topic sources, unclear seed\nsupervision, and suboptimal convergence. To address those challenges, we\ndevelop a novel Knowledge-guided Multimodal NTM to predict a short-form video's\ndepressive impact on viewers. Extensive empirical analyses using TikTok and\nDouyin datasets prove that our method outperforms state-of-the-art benchmarks.\nOur method also discovers medically relevant topics from videos that are linked\nto depressive impact. We contribute to IS with a novel video analytics method\nthat is generalizable to other video classification problems. Practically, our\nmethod can help platforms understand videos' mental impacts, thus adjusting\nrecommendations and video topic disclosure.\n","authors":["Jiaheng Xie","Ruicheng Liang","Yidong Chai","Yang Liu","Daniel Zeng"],"pdf_url":"https://arxiv.org/pdf/2402.10045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02362v2","updated":"2024-03-21T21:28:37Z","published":"2023-12-04T21:43:00Z","title":"PointNeRF++: A multi-scale, point-based Neural Radiance Field","summary":"  Point clouds offer an attractive source of information to complement images\nin neural scene representations, especially when few images are available.\nNeural rendering methods based on point clouds do exist, but they do not\nperform well when the point cloud quality is low -- e.g., sparse or incomplete,\nwhich is often the case with real-world data. We overcome these problems with a\nsimple representation that aggregates point clouds at multiple scale levels\nwith sparse voxel grids at different resolutions. To deal with point cloud\nsparsity, we average across multiple scale levels -- but only among those that\nare valid, i.e., that have enough neighboring points in proximity to the ray of\na pixel. To help model areas without points, we add a global voxel at the\ncoarsest scale, thus unifying ``classical'' and point-based NeRF formulations.\nWe validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,\noutperforming the state of the art, with a significant gap compared to other\nNeRF-based methods, especially on more challenging scenes.\n","authors":["Weiwei Sun","Eduard Trulls","Yang-Che Tseng","Sneha Sambandam","Gopal Sharma","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2312.02362v2.pdf","comment":"Project website: https://pointnerfpp.github.io/"},{"id":"http://arxiv.org/abs/2403.14839v1","updated":"2024-03-21T21:18:08Z","published":"2024-03-21T21:18:08Z","title":"Hyperspectral Neural Radiance Fields","summary":"  Hyperspectral Imagery (HSI) has been used in many applications to\nnon-destructively determine the material and/or chemical compositions of\nsamples. There is growing interest in creating 3D hyperspectral\nreconstructions, which could provide both spatial and spectral information\nwhile also mitigating common HSI challenges such as non-Lambertian surfaces and\ntranslucent objects. However, traditional 3D reconstruction with HSI is\ndifficult due to technological limitations of hyperspectral cameras. In recent\nyears, Neural Radiance Fields (NeRFs) have seen widespread success in creating\nhigh quality volumetric 3D representations of scenes captured by a variety of\ncamera models. Leveraging recent advances in NeRFs, we propose computing a\nhyperspectral 3D reconstruction in which every point in space and view\ndirection is characterized by wavelength-dependent radiance and transmittance\nspectra. To evaluate our approach, a dataset containing nearly 2000\nhyperspectral images across 8 scenes and 2 cameras was collected. We perform\ncomparisons against traditional RGB NeRF baselines and apply ablation testing\nwith alternative spectra representations. Finally, we demonstrate the potential\nof hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor\nsimulation. We show that our hyperspectral NeRF approach enables creating fast,\naccurate volumetric 3D hyperspectral scenes and enables several new\napplications and areas for future study.\n","authors":["Gerry Chen","Sunil Kumar Narayanan","Thomas Gautier Ottou","Benjamin Missaoui","Harsh Muriki","Cédric Pradalier","Yongsheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14839v1.pdf","comment":"Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6\n  pages"},{"id":"http://arxiv.org/abs/2403.14837v1","updated":"2024-03-21T21:13:53Z","published":"2024-03-21T21:13:53Z","title":"Osmosis: RGBD Diffusion Prior for Underwater Image Restoration","summary":"  Underwater image restoration is a challenging task because of strong water\neffects that increase dramatically with distance. This is worsened by lack of\nground truth data of clean scenes without water. Diffusion priors have emerged\nas strong image restoration priors. However, they are often trained with a\ndataset of the desired restored output, which is not available in our case. To\novercome this critical issue, we show how to leverage in-air images to train\ndiffusion priors for underwater restoration. We also observe that only color\ndata is insufficient, and augment the prior with a depth channel. We train an\nunconditional diffusion model prior on the joint space of color and depth,\nusing standard RGBD datasets of natural outdoor scenes in air. Using this prior\ntogether with a novel guidance method based on the underwater image formation\nmodel, we generate posterior samples of clean images, removing the water\neffects. Even though our prior did not see any underwater images during\ntraining, our method outperforms state-of-the-art baselines for image\nrestoration on very challenging scenes. Data, models and code are published in\nthe project page.\n","authors":["Opher Bar Nathan","Deborah Levy","Tali Treibitz","Dan Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.14837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14836v1","updated":"2024-03-21T21:11:23Z","published":"2024-03-21T21:11:23Z","title":"Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis","summary":"  This paper presents the use of panoramic 3D estimation in lighting\nsimulation. Conventional lighting simulation necessitates detailed modeling as\ninput, resulting in significant labor effort and time cost. The 3D layout\nestimation method directly takes a single panorama as input and generates a\nlighting simulation model with room geometry and window aperture. We evaluate\nthe simulation results by comparing the luminance errors between on-site High\nDynamic Range (HDR) photographs, 3D estimation model, and detailed model in\npanoramic representation and fisheye perspective. Given the selected scene, the\nresults demonstrate the estimated room layout is reliable for lighting\nsimulation.\n","authors":["Zining Cheng","Guanzhou Ji"],"pdf_url":"https://arxiv.org/pdf/2403.14836v1.pdf","comment":"Annual Modeling and Simulation Conference (ANNSIM), May 20-23, 2024,\n  Washington D.C., USA"},{"id":"http://arxiv.org/abs/2403.14828v1","updated":"2024-03-21T20:43:10Z","published":"2024-03-21T20:43:10Z","title":"Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing","summary":"  Fashion illustration is a crucial medium for designers to convey their\ncreative vision and transform design concepts into tangible representations\nthat showcase the interplay between clothing and the human body. In the context\nof fashion design, computer vision techniques have the potential to enhance and\nstreamline the design process. Departing from prior research primarily focused\non virtual try-on, this paper tackles the task of multimodal-conditioned\nfashion image editing. Our approach aims to generate human-centric fashion\nimages guided by multimodal prompts, including text, human body poses, garment\nsketches, and fabric textures. To address this problem, we propose extending\nlatent diffusion models to incorporate these multiple modalities and modifying\nthe structure of the denoising network, taking multimodal prompts as input. To\ncondition the proposed architecture on fabric textures, we employ textual\ninversion techniques and let diverse cross-attention layers of the denoising\nnetwork attend to textual and texture information, thus incorporating different\ngranularity conditioning details. Given the lack of datasets for the task, we\nextend two existing fashion datasets, Dress Code and VITON-HD, with multimodal\nannotations. Experimental evaluations demonstrate the effectiveness of our\nproposed approach in terms of realism and coherence concerning the provided\nmultimodal inputs.\n","authors":["Alberto Baldrati","Davide Morelli","Marcella Cornia","Marco Bertini","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2403.14828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03077v3","updated":"2024-03-21T20:37:54Z","published":"2024-03-05T16:01:55Z","title":"MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding","summary":"  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n","authors":["Chun-Peng Chang","Shaoxiang Wang","Alain Pagani","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2403.03077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14821v1","updated":"2024-03-21T20:28:22Z","published":"2024-03-21T20:28:22Z","title":"Learning Gaussian Representation for Eye Fixation Prediction","summary":"  Existing eye fixation prediction methods perform the mapping from input\nimages to the corresponding dense fixation maps generated from raw fixation\npoints. However, due to the stochastic nature of human fixation, the generated\ndense fixation maps may be a less-than-ideal representation of human fixation.\nTo provide a robust fixation model, we introduce Gaussian Representation for\neye fixation modeling. Specifically, we propose to model the eye fixation map\nas a mixture of probability distributions, namely a Gaussian Mixture Model. In\nthis new representation, we use several Gaussian distribution components as an\nalternative to the provided fixation map, which makes the model more robust to\nthe randomness of fixation. Meanwhile, we design our framework upon some\nlightweight backbones to achieve real-time fixation prediction. Experimental\nresults on three public fixation prediction datasets (SALICON, MIT1003,\nTORONTO) demonstrate that our method is fast and effective.\n","authors":["Peipei Song","Jing Zhang","Piotr Koniusz","Nick Barnes"],"pdf_url":"https://arxiv.org/pdf/2403.14821v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.14800v1","updated":"2024-03-21T19:28:17Z","published":"2024-03-21T19:28:17Z","title":"Deep Active Learning: A Reality Check","summary":"  We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.\n","authors":["Edrina Gashi","Jiankang Deng","Ismail Elezi"],"pdf_url":"https://arxiv.org/pdf/2403.14800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14797v1","updated":"2024-03-21T19:20:29Z","published":"2024-03-21T19:20:29Z","title":"Preventing Catastrophic Forgetting through Memory Networks in Continuous\n  Detection","summary":"  Modern pre-trained architectures struggle to retain previous information\nwhile undergoing continuous fine-tuning on new tasks. Despite notable progress\nin continual classification, systems designed for complex vision tasks such as\ndetection or segmentation still struggle to attain satisfactory performance. In\nthis work, we introduce a memory-based detection transformer architecture to\nadapt a pre-trained DETR-style detector to new tasks while preserving knowledge\nfrom previous tasks. We propose a novel localized query function for efficient\ninformation retrieval from memory units, aiming to minimize forgetting.\nFurthermore, we identify a fundamental challenge in continual detection\nreferred to as background relegation. This arises when object categories from\nearlier tasks reappear in future tasks, potentially without labels, leading\nthem to be implicitly treated as background. This is an inevitable issue in\ncontinual detection or segmentation. The introduced continual optimization\ntechnique effectively tackles this challenge. Finally, we assess the\nperformance of our proposed system on continual detection benchmarks and\ndemonstrate that our approach surpasses the performance of existing\nstate-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on\nthe task of continual detection.\n","authors":["Gaurav Bhatt","James Ross","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2403.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03187v2","updated":"2024-03-21T19:14:04Z","published":"2023-12-05T23:33:49Z","title":"FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction","summary":"  Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically annotate user\npreferences from their spontaneous facial expression reaction to the generated\nimages. We collect a dataset of Facial Expression Reaction to Generated Images\n(FERGI) and show that the activations of multiple facial action units (AUs) are\nhighly correlated with user evaluations of the generated images. Specifically,\nAU4 (brow lowerer) is reflective of negative evaluations of the generated image\nwhereas AU12 (lip corner puller) is reflective of positive evaluations. These\ncan be useful in two ways. Firstly, we can automatically annotate user\npreferences between image pairs with substantial difference in these AU\nresponses with an accuracy significantly outperforming state-of-the-art scoring\nmodels. Secondly, directly integrating the AU responses with the scoring models\nimproves their consistency with human preferences. Finally, this method of\nautomatic annotation with facial expression analysis can be potentially\ngeneralized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.\n","authors":["Shuangquan Feng","Junhua Ma","Virginia R. de Sa"],"pdf_url":"https://arxiv.org/pdf/2312.03187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14790v1","updated":"2024-03-21T19:09:21Z","published":"2024-03-21T19:09:21Z","title":"Latent Diffusion Models for Attribute-Preserving Image Anonymization","summary":"  Generative techniques for image anonymization have great potential to\ngenerate datasets that protect the privacy of those depicted in the images,\nwhile achieving high data fidelity and utility. Existing methods have focused\nextensively on preserving facial attributes, but failed to embrace a more\ncomprehensive perspective that considers the scene and background into the\nanonymization process. This paper presents, to the best of our knowledge, the\nfirst approach to image anonymization based on Latent Diffusion Models (LDMs).\nEvery element of a scene is maintained to convey the same meaning, yet\nmanipulated in a way that makes re-identification difficult. We propose two\nLDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained\nControlNets, and a new controlling mechanism designed to increase the distance\nbetween the real and anonymized images. CAMOFULaGE-Light is based on the\nAdapter technique, coupled with an encoding designed to efficiently represent\nthe attributes of different persons in a scene. The former solution achieves\nsuperior performance on most metrics and benchmarks, while the latter cuts the\ninference time in half at the cost of fine-tuning a lightweight module. We show\nthrough extensive experimental comparison that the proposed method is\ncompetitive with the state-of-the-art concerning identity obfuscation whilst\nbetter preserving the original content of the image and tackling unresolved\nchallenges that current solutions fail to address.\n","authors":["Luca Piano","Pietro Basci","Fabrizio Lamberti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2403.14790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14789v1","updated":"2024-03-21T19:05:31Z","published":"2024-03-21T19:05:31Z","title":"On the exploitation of DCT statistics for cropping detectors","summary":"  {The study of frequency components derived from Discrete Cosine Transform\n(DCT) has been widely used in image analysis. In recent years it has been\nobserved that significant information can be extrapolated from them about the\nlifecycle of the image, but no study has focused on the analysis between them\nand the source resolution of the image. In this work, we investigated a novel\nimage resolution classifier that employs DCT statistics with the goal to detect\nthe original resolution of images; in particular the insight was exploited to\naddress the challenge of identifying cropped images. Training a Machine\nLearning (ML) classifier on entire images (not cropped), the generated model\ncan leverage this information to detect cropping. The results demonstrate the\nclassifier's reliability in distinguishing between cropped and not cropped\nimages, providing a dependable estimation of their original resolution. This\nadvancement has significant implications for image processing applications,\nincluding digital security, authenticity verification, and visual quality\nanalysis, by offering a new tool for detecting image manipulations and\nenhancing qualitative image assessment. This work opens new perspectives in the\nfield, with potential to transform image analysis and usage across multiple\ndomains.}\n","authors":["Claudio Vittorio Ragaglia","Francesco Guarnera","Sebastiano Battiato"],"pdf_url":"https://arxiv.org/pdf/2403.14789v1.pdf","comment":"8 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2401.10224v2","updated":"2024-03-21T18:59:50Z","published":"2024-01-18T18:59:09Z","title":"The Manga Whisperer: Automatically Generating Transcriptions for Comics","summary":"  In the past few decades, Japanese comics, commonly referred to as Manga, have\ntranscended both cultural and linguistic boundaries to become a true worldwide\nsensation. Yet, the inherent reliance on visual cues and illustration within\nmanga renders it largely inaccessible to individuals with visual impairments.\nIn this work, we seek to address this substantial barrier, with the aim of\nensuring that manga can be appreciated and actively engaged by everyone.\nSpecifically, we tackle the problem of diarisation i.e. generating a\ntranscription of who said what and when, in a fully automatic way.\n  To this end, we make the following contributions: (1) we present a unified\nmodel, Magi, that is able to (a) detect panels, text boxes and character boxes,\n(b) cluster characters by identity (without knowing the number of clusters\napriori), and (c) associate dialogues to their speakers; (2) we propose a novel\napproach that is able to sort the detected text boxes in their reading order\nand generate a dialogue transcript; (3) we annotate an evaluation benchmark for\nthis task using publicly available [English] manga pages. The code, evaluation\ndatasets and the pre-trained model can be found at:\nhttps://github.com/ragavsachdeva/magi.\n","authors":["Ragav Sachdeva","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2401.10224v2.pdf","comment":"Accepted at CVPR'24"},{"id":"http://arxiv.org/abs/2403.14783v1","updated":"2024-03-21T18:57:25Z","published":"2024-03-21T18:57:25Z","title":"Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot\n  Visual Question Answering","summary":"  This work explores the zero-shot capabilities of foundation models in Visual\nQuestion Answering (VQA) tasks. We propose an adaptive multi-agent system,\nnamed Multi-Agent VQA, to overcome the limitations of foundation models in\nobject detection and counting by using specialized agents as tools. Unlike\nexisting approaches, our study focuses on the system's performance without\nfine-tuning it on specific VQA datasets, making it more practical and robust in\nthe open world. We present preliminary experimental results under zero-shot\nscenarios and highlight some failure cases, offering new directions for future\nresearch.\n","authors":["Bowen Jiang","Zhijun Zhuang","Shreyas S. Shivakumar","Dan Roth","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2403.14783v1.pdf","comment":"A full version of the paper will be released soon. The codes are\n  available at https://github.com/bowen-upenn/Multi-Agent-VQA"},{"id":"http://arxiv.org/abs/2403.14781v1","updated":"2024-03-21T18:52:58Z","published":"2024-03-21T18:52:58Z","title":"Champ: Controllable and Consistent Human Image Animation with 3D\n  Parametric Guidance","summary":"  In this study, we introduce a methodology for human image animation by\nleveraging a 3D human parametric model within a latent diffusion framework to\nenhance shape alignment and motion guidance in curernt human generative\ntechniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)\nmodel as the 3D human parametric model to establish a unified representation of\nbody shape and pose. This facilitates the accurate capture of intricate human\ngeometry and motion characteristics from source videos. Specifically, we\nincorporate rendered depth images, normal maps, and semantic maps obtained from\nSMPL sequences, alongside skeleton-based motion guidance, to enrich the\nconditions to the latent diffusion model with comprehensive 3D shape and\ndetailed pose attributes. A multi-layer motion fusion module, integrating\nself-attention mechanisms, is employed to fuse the shape and motion latent\nrepresentations in the spatial domain. By representing the 3D human parametric\nmodel as the motion guidance, we can perform parametric shape alignment of the\nhuman body between the reference image and the source video motion.\nExperimental evaluations conducted on benchmark datasets demonstrate the\nmethodology's superior ability to generate high-quality human animations that\naccurately capture both pose and shape variations. Furthermore, our approach\nalso exhibits superior generalization capabilities on the proposed wild\ndataset. Project page: https://fudan-generative-vision.github.io/champ.\n","authors":["Shenhao Zhu","Junming Leo Chen","Zuozhuo Dai","Yinghui Xu","Xun Cao","Yao Yao","Hao Zhu","Siyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14778v1","updated":"2024-03-21T18:49:20Z","published":"2024-03-21T18:49:20Z","title":"Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image\n  Attacking","summary":"  In Virtual Reality (VR), adversarial attack remains a significant security\nthreat. Most deep learning-based methods for physical and digital adversarial\nattacks focus on enhancing attack performance by crafting adversarial examples\nthat contain large printable distortions that are easy for human observers to\nidentify. However, attackers rarely impose limitations on the naturalness and\ncomfort of the appearance of the generated attack image, resulting in a\nnoticeable and unnatural attack. To address this challenge, we propose a\nframework to incorporate style transfer to craft adversarial inputs of natural\nstyles that exhibit minimal detectability and maximum natural appearance, while\nmaintaining superior attack capabilities.\n","authors":["Qianyu Guo","Jiaming Fu","Yawen Lu","Dongming Gan"],"pdf_url":"https://arxiv.org/pdf/2403.14778v1.pdf","comment":"Accepted to IEEE VRW"},{"id":"http://arxiv.org/abs/2403.14774v1","updated":"2024-03-21T18:28:43Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v1.pdf","comment":"25 pages, 13 tables, 8 figures"},{"id":"http://arxiv.org/abs/2403.14773v1","updated":"2024-03-21T18:27:29Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Daniil Hayrapetyan","Hayk Poghosyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v1.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2403.14772v1","updated":"2024-03-21T18:26:23Z","published":"2024-03-21T18:26:23Z","title":"Improving Robustness to Model Inversion Attacks via Sparse Coding\n  Architectures","summary":"  Recent model inversion attack algorithms permit adversaries to reconstruct a\nneural network's private training data just by repeatedly querying the network\nand inspecting its outputs. In this work, we develop a novel network\narchitecture that leverages sparse-coding layers to obtain superior robustness\nto this class of attacks. Three decades of computer science research has\nstudied sparse coding in the context of image denoising, object recognition,\nand adversarial misclassification settings, but to the best of our knowledge,\nits connection to state-of-the-art privacy vulnerabilities remains unstudied.\nHowever, sparse coding architectures suggest an advantageous means to defend\nagainst model inversion attacks because they allow us to control the amount of\nirrelevant private information encoded in a network's intermediate\nrepresentations in a manner that can be computed efficiently during training\nand that is known to have little effect on classification accuracy.\nSpecifically, compared to networks trained with a variety of state-of-the-art\ndefenses, our sparse-coding architectures maintain comparable or higher\nclassification accuracy while degrading state-of-the-art training data\nreconstructions by factors of 1.1 to 18.3 across a variety of reconstruction\nquality metrics (PSNR, SSIM, FID). This performance advantage holds across 5\ndatasets ranging from CelebA faces to medical images and CIFAR-10, and across\nvarious state-of-the-art SGD-based and GAN-based inversion attacks, including\nPlug-&-Play attacks. We provide a cluster-ready PyTorch codebase to promote\nresearch and standardize defense evaluations.\n","authors":["Sayanton V. Dibbo","Adam Breuer","Juston Moore","Michael Teti"],"pdf_url":"https://arxiv.org/pdf/2403.14772v1.pdf","comment":"32 pages, 15 Tables, and 9 Figures"},{"id":"http://arxiv.org/abs/2403.14760v1","updated":"2024-03-21T18:02:20Z","published":"2024-03-21T18:02:20Z","title":"Can 3D Vision-Language Models Truly Understand Natural Language?","summary":"  Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new\navenues for human interaction with embodied agents or robots using natural\nlanguage. Despite this progress, we find a notable limitation: existing 3D-VL\nmodels exhibit sensitivity to the styles of language input, struggling to\nunderstand sentences with the same semantic meaning but written in different\nvariants. This observation raises a critical question: Can 3D vision-language\nmodels truly understand natural language? To test the language\nunderstandability of 3D-VL models, we first propose a language robustness task\nfor systematically assessing 3D-VL models across various tasks, benchmarking\ntheir performance when presented with different language style variants.\nImportantly, these variants are commonly encountered in applications requiring\ndirect interaction with humans, such as embodied robotics, given the diversity\nand unpredictability of human language. We propose a 3D Language Robustness\nDataset, designed based on the characteristics of human language, to facilitate\nthe systematic study of robustness. Our comprehensive evaluation uncovers a\nsignificant drop in the performance of all existing models across various 3D-VL\ntasks. Even the state-of-the-art 3D-LLM fails to understand some variants of\nthe same sentences. Further in-depth analysis suggests that the existing models\nhave a fragile and biased fusion module, which stems from the low diversity of\nthe existing dataset. Finally, we propose a training-free module driven by LLM,\nwhich improves language robustness. Datasets and code will be available at\ngithub.\n","authors":["Weipeng Deng","Runyu Ding","Jihan Yang","Jiahui Liu","Yijiang Li","Xiaojuan Qi","Edith Ngai"],"pdf_url":"https://arxiv.org/pdf/2403.14760v1.pdf","comment":"https://github.com/VincentDENGP/3D-LR"},{"id":"http://arxiv.org/abs/2403.14743v1","updated":"2024-03-21T18:00:00Z","published":"2024-03-21T18:00:00Z","title":"VURF: A General-purpose Reasoning and Self-refinement Framework for\n  Video Understanding","summary":"  Recent studies have demonstrated the effectiveness of Large Language Models\n(LLMs) as reasoning modules that can deconstruct complex tasks into more\nmanageable sub-tasks, particularly when applied to visual reasoning tasks for\nimages. In contrast, this paper introduces a Video Understanding and Reasoning\nFramework (VURF) based on the reasoning power of LLMs. Ours is a novel approach\nto extend the utility of LLMs in the context of video tasks, leveraging their\ncapacity to generalize from minimal input and output demonstrations within a\ncontextual framework. By presenting LLMs with pairs of instructions and their\ncorresponding high-level programs, we harness their contextual learning\ncapabilities to generate executable visual programs for video understanding. To\nenhance program's accuracy and robustness, we implement two important\nstrategies. Firstly, we employ a feedback-generation approach, powered by\nGPT-3.5, to rectify errors in programs utilizing unsupported functions.\nSecondly, taking motivation from recent works on self refinement of LLM\noutputs, we introduce an iterative procedure for improving the quality of the\nin-context examples by aligning the initial outputs to the outputs that would\nhave been generated had the LLM not been bound by the structure of the\nin-context examples. Our results on several video-specific tasks, including\nvisual QA, video anticipation, pose estimation and multi-video QA illustrate\nthe efficacy of these enhancements in improving the performance of visual\nprogramming approaches for video tasks. Our Codes and data will be publicly\nreleased.\n","authors":["Ahmad Mahmood","Ashmal Vayani","Muzammal Naseer","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14743v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2403.14602v1","updated":"2024-03-21T17:52:08Z","published":"2024-03-21T17:52:08Z","title":"ReNoise: Real Image Inversion Through Iterative Noising","summary":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n","authors":["Daniel Garibi","Or Patashnik","Andrey Voynov","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14602v1.pdf","comment":"project page at: https://garibida.github.io/ReNoise-Inversion/"},{"id":"http://arxiv.org/abs/2403.14554v1","updated":"2024-03-21T16:53:03Z","published":"2024-03-21T16:53:03Z","title":"Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering","summary":"  We propose Gaussian Frosting, a novel mesh-based representation for\nhigh-quality rendering and editing of complex 3D effects in real-time. Our\napproach builds on the recent 3D Gaussian Splatting framework, which optimizes\na set of 3D Gaussians to approximate a radiance field from images. We propose\nfirst extracting a base mesh from Gaussians during optimization, then building\nand refining an adaptive layer of Gaussians with a variable thickness around\nthe mesh to better capture the fine details and volumetric effects near the\nsurface, such as hair or grass. We call this layer Gaussian Frosting, as it\nresembles a coating of frosting on a cake. The fuzzier the material, the\nthicker the frosting. We also introduce a parameterization of the Gaussians to\nenforce them to stay inside the frosting layer and automatically adjust their\nparameters when deforming, rescaling, editing or animating the mesh. Our\nrepresentation allows for efficient rendering using Gaussian splatting, as well\nas editing and animation by modifying the base mesh. We demonstrate the\neffectiveness of our method on various synthetic and real scenes, and show that\nit outperforms existing surface-based approaches. We will release our code and\na web-based viewer as additional contributions. Our project page is the\nfollowing: https://anttwo.github.io/frosting/\n","authors":["Antoine Guédon","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2403.14554v1.pdf","comment":"Project Webpage: https://anttwo.github.io/frosting/"},{"id":"http://arxiv.org/abs/2403.01629v2","updated":"2024-03-21T15:16:34Z","published":"2024-03-03T22:29:37Z","title":"VR Research at Fraunhofer IGD, Darmstadt, Germany","summary":"  We present a historical outline of the research and developments of Virtual\nReality at the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt,\nGermany, from 1990 through 2000.\n","authors":["Wolfgang Felger","Martin Göbel","Dirk Reiners","Gabriel Zachmann"],"pdf_url":"https://arxiv.org/pdf/2403.01629v2.pdf","comment":"IEEE VR 2024 Workshop \"Archiving VR\""},{"id":"http://arxiv.org/abs/2312.12274v2","updated":"2024-03-21T12:51:31Z","published":"2023-12-19T15:56:19Z","title":"Intrinsic Image Diffusion for Indoor Single-view Material Estimation","summary":"  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Peter Kocsis","Vincent Sitzmann","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.12274v2.pdf","comment":"Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/\n  Video: https://youtu.be/lz0meJlj5cA"},{"id":"http://arxiv.org/abs/2403.14186v1","updated":"2024-03-21T07:21:51Z","published":"2024-03-21T07:21:51Z","title":"StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained\n  StyleGAN","summary":"  We propose a method that can generate cinemagraphs automatically from a still\nlandscape image using a pre-trained StyleGAN. Inspired by the success of recent\nunconditional video generation, we leverage a powerful pre-trained image\ngenerator to synthesize high-quality cinemagraphs. Unlike previous approaches\nthat mainly utilize the latent space of a pre-trained StyleGAN, our approach\nutilizes its deep feature space for both GAN inversion and cinemagraph\ngeneration. Specifically, we propose multi-scale deep feature warping (MSDFW),\nwhich warps the intermediate features of a pre-trained StyleGAN at different\nresolutions. By using MSDFW, the generated cinemagraphs are of high resolution\nand exhibit plausible looping animation. We demonstrate the superiority of our\nmethod through user studies and quantitative comparisons with state-of-the-art\ncinemagraph generation methods and a video generation method that uses a\npre-trained StyleGAN.\n","authors":["Jongwoo Choi","Kwanggyoon Seo","Amirsaman Ashtari","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.14186v1.pdf","comment":"Project website: https://jeolpyeoni.github.io/stylecinegan_project/"},{"id":"http://arxiv.org/abs/2403.14053v1","updated":"2024-03-21T00:35:31Z","published":"2024-03-21T00:35:31Z","title":"Leveraging Thermal Modality to Enhance Reconstruction in Low-Light\n  Conditions","summary":"  Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view\nsynthesis by learning the implicit volumetric representation of a scene from\nmulti-view images, which faithfully convey the colorimetric information.\nHowever, sensor noises will contaminate low-value pixel signals, and the lossy\ncamera image signal processor will further remove near-zero intensities in\nextremely dark situations, deteriorating the synthesis performance. Existing\napproaches reconstruct low-light scenes from raw images but struggle to recover\ntexture and boundary details in dark regions. Additionally, they are unsuitable\nfor high-speed models relying on explicit representations. To address these\nissues, we present Thermal-NeRF, which takes thermal and visible raw images as\ninputs, considering the thermal camera is robust to the illumination variation\nand raw images preserve any possible clues in the dark, to accomplish visible\nand thermal view synthesis simultaneously. Also, the first multi-view thermal\nand visible dataset (MVTV) is established to support the research on multimodal\nNeRF. Thermal-NeRF achieves the best trade-off between detail preservation and\nnoise smoothing and provides better synthesis performance than previous work.\nFinally, we demonstrate that both modalities are beneficial to each other in 3D\nreconstruction.\n","authors":["Jiacong Xu","Mingqian Liao","K Ram Prabhakar","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.14053v1.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.02362v2","updated":"2024-03-21T21:28:37Z","published":"2023-12-04T21:43:00Z","title":"PointNeRF++: A multi-scale, point-based Neural Radiance Field","summary":"  Point clouds offer an attractive source of information to complement images\nin neural scene representations, especially when few images are available.\nNeural rendering methods based on point clouds do exist, but they do not\nperform well when the point cloud quality is low -- e.g., sparse or incomplete,\nwhich is often the case with real-world data. We overcome these problems with a\nsimple representation that aggregates point clouds at multiple scale levels\nwith sparse voxel grids at different resolutions. To deal with point cloud\nsparsity, we average across multiple scale levels -- but only among those that\nare valid, i.e., that have enough neighboring points in proximity to the ray of\na pixel. To help model areas without points, we add a global voxel at the\ncoarsest scale, thus unifying ``classical'' and point-based NeRF formulations.\nWe validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,\noutperforming the state of the art, with a significant gap compared to other\nNeRF-based methods, especially on more challenging scenes.\n","authors":["Weiwei Sun","Eduard Trulls","Yang-Che Tseng","Sneha Sambandam","Gopal Sharma","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2312.02362v2.pdf","comment":"Project website: https://pointnerfpp.github.io/"}]},"2024-03-22T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2312.10070v2","updated":"2024-03-22T17:59:09Z","published":"2023-12-06T10:47:53Z","title":"Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting","summary":"  We present a dense simultaneous localization and mapping (SLAM) method that\nuses 3D Gaussians as a scene representation. Our approach enables\ninteractive-time reconstruction and photo-realistic rendering from real-world\nsingle-camera RGBD videos. To this end, we propose a novel effective strategy\nfor seeding new Gaussians for newly explored areas and their effective online\noptimization that is independent of the scene size and thus scalable to larger\nscenes. This is achieved by organizing the scene into sub-maps which are\nindependently optimized and do not need to be kept in memory. We further\naccomplish frame-to-model camera tracking by minimizing photometric and\ngeometric losses between the input and rendered frames. The Gaussian\nrepresentation allows for high-quality photo-realistic real-time rendering of\nreal-world scenes. Evaluation on synthetic and real-world datasets demonstrates\ncompetitive or superior performance in mapping, tracking, and rendering\ncompared to existing neural dense SLAM methods.\n","authors":["Vladimir Yugay","Yue Li","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.15369v1","updated":"2024-03-22T17:48:13Z","published":"2024-03-22T17:48:13Z","title":"OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV\n  Piloting in Large-scale Unexplored Ocean Environments","summary":"  We develop a hierarchical LLM-task-motion planning and replanning framework\nto efficiently ground an abstracted human command into tangible Autonomous\nUnderwater Vehicle (AUV) control through enhanced representations of the world.\nWe also incorporate a holistic replanner to provide real-world feedback with\nall planners for robust AUV operation. While there has been extensive research\nin bridging the gap between LLMs and robotic missions, they are unable to\nguarantee success of AUV applications in the vast and unknown ocean\nenvironment. To tackle specific challenges in marine robotics, we design a\nhierarchical planner to compose executable motion plans, which achieves\nplanning efficiency and solution quality by decomposing long-horizon missions\ninto sub-tasks. At the same time, real-time data stream is obtained by a\nreplanner to address environmental uncertainties during plan execution.\nExperiments validate that our proposed framework delivers successful AUV\nperformance of long-duration missions through natural language piloting.\n","authors":["Ruochu Yang","Fumin Zhang","Mengxue Hou"],"pdf_url":"https://arxiv.org/pdf/2403.15369v1.pdf","comment":"submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13783v2","updated":"2024-03-22T17:17:14Z","published":"2024-03-20T17:44:33Z","title":"A Convex Formulation of Frictional Contact for the Material Point Method\n  and Rigid Bodies","summary":"  In this paper, we introduce a novel convex formulation that seamlessly\nintegrates the Material Point Method (MPM) with articulated rigid body dynamics\nin frictional contact scenarios. We extend the linear corotational hyperelastic\nmodel into the realm of elastoplasticity and include an efficient return\nmapping algorithm. This approach is particularly effective for MPM simulations\ninvolving significant deformation and topology changes, while preserving the\nconvexity of the optimization problem. Our method ensures global convergence,\nenabling the use of large simulation time steps without compromising\nrobustness. We have validated our approach through rigorous testing and\nperformance evaluations, highlighting its superior capabilities in managing\ncomplex simulations relevant to robotics. Compared to previous MPM based\nrobotic simulators, our method significantly improves the stability of contact\nresolution -- a critical factor in robot manipulation tasks. We make our method\navailable in the open-source robotics toolkit, Drake.\n","authors":["Zeshun Zong","Chenfanfu Jiang","Xuchen Han"],"pdf_url":"https://arxiv.org/pdf/2403.13783v2.pdf","comment":"The supplemental video is available at https://youtu.be/5jrQtF5D0DA"},{"id":"http://arxiv.org/abs/2403.15335v1","updated":"2024-03-22T16:40:48Z","published":"2024-03-22T16:40:48Z","title":"Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared\n  Autonomy","summary":"  We present a novel approach that aims to address both safety and stability of\na haptic teleoperation system within a framework of Haptic Shared Autonomy\n(HSA). We use Control Barrier Functions (CBFs) to generate the control input\nthat follows the user's input as closely as possible while guaranteeing safety.\nIn the context of stability of the human-in-the-loop system, we limit the force\nfeedback perceived by the user via a small $L_2$-gain, which is achieved by\nlimiting the control and the force feedback via a differential constraint.\nSpecifically, with the property of HSA, we propose two pathways to design the\ncontrol and the force feedback: Sequential Control Force (SCF) and Joint\nControl Force (JCF). Both designs can achieve safety and stability but with\ndifferent responses to the user's commands. We conducted experimental\nsimulations to evaluate and investigate the properties of the designed methods.\nWe also tested the proposed method on a physical quadrotor UAV and a haptic\ninterface.\n","authors":["Dawei Zhang","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.15335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15333v1","updated":"2024-03-22T16:39:13Z","published":"2024-03-22T16:39:13Z","title":"Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications","summary":"  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended\nfor monitoring the safety of human workers, especially those working at\nheights. In the proposed dynamic formation scheme, one UAV acts as the leader\nof the formation and is equipped with sensors for human worker detection and\ngesture recognition. The follower UAVs maintain a predetermined formation\nrelative to the worker's position, thereby providing additional perspectives of\nthe monitored scene. Hand gestures allow the human worker to specify movements\nand action commands for the UAV team and initiate other mission-related\ncommands without the need for an additional communication channel or specific\nmarkers. Together with a novel unified human detection and tracking algorithm,\nhuman pose estimation approach and gesture detection pipeline, the proposed\napproach forms a first instance of an HSI system incorporating all these\nmodules onboard real-world UAVs. Simulations and field experiments with three\nUAVs and a human worker in a mock-up scenario showcase the effectiveness and\nresponsiveness of the proposed approach.\n","authors":["Vít Krátký","Giuseppe Silano","Matouš Vrba","Christos Papaioannidis","Ioannis Mademlis","Robert Pěnička","Ioannis Pitas","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2403.15333v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.00401v2","updated":"2024-03-22T16:32:24Z","published":"2023-09-30T14:54:31Z","title":"Learning High-level Semantic-Relational Concepts for SLAM","summary":"  Recent works on SLAM extend their pose graphs with higher-level semantic\nconcepts like Rooms exploiting relationships between them, to provide, not only\na richer representation of the situation/environment but also to improve the\naccuracy of its estimation. Concretely, our previous work, Situational Graphs\n(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the\nfactor optimization process, relies on semantic entities such as Planes and\nRooms, whose relationship is mathematically defined. Nevertheless, there is no\nunique approach to finding all the hidden patterns in lower-level factor-graphs\nthat correspond to high-level concepts of different natures. It is currently\ntackled with ad-hoc algorithms, which limits its graph expressiveness.\n  To overcome this limitation, in this work, we propose an algorithm based on\nGraph Neural Networks for learning high-level semantic-relational concepts that\ncan be inferred from the low-level factor graph. Given a set of mapped Planes\nour algorithm is capable of inferring Room entities relating to the Planes.\nAdditionally, to demonstrate the versatility of our method, our algorithm can\ninfer an additional semantic-relational concept, i.e. Wall, and its\nrelationship with its Planes. We validate our method in both simulated and real\ndatasets demonstrating improved performance over two baseline approaches.\nFurthermore, we integrate our method into the S-Graphs+ algorithm providing\nimproved pose and map accuracy compared to the baseline while further enhancing\nthe scene representation.\n","authors":["Jose Andres Millan-Romera","Hriday Bavle","Muhammad Shaheer","Martin R. Oswald","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2310.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15323v1","updated":"2024-03-22T16:19:56Z","published":"2024-03-22T16:19:56Z","title":"Introduction to Human-Robot Interaction: A Multi-Perspective\n  Introductory Course","summary":"  In this paper I describe the design of an introductory course in Human-Robot\nInteraction. This project-driven course is designed to introduce undergraduate\nand graduate engineering students, especially those enrolled in Computer\nScience, Mechanical Engineering, and Robotics degree programs, to key theories\nand methods used in the field of Human-Robot Interaction that they would\notherwise be unlikely to see in those degree programs. To achieve this aim, the\ncourse takes students all the way from stakeholder analysis to empirical\nevaluation, covering and integrating key Qualitative, Design, Computational,\nand Quantitative methods along the way. I detail the goals, audience, and\nformat of the course, and provide a detailed walkthrough of the course\nsyllabus.\n","authors":["Tom Williams"],"pdf_url":"https://arxiv.org/pdf/2403.15323v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.15306v1","updated":"2024-03-22T15:58:34Z","published":"2024-03-22T15:58:34Z","title":"HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet\n  Peppers","summary":"  Horticultural tasks such as pruning and selective harvesting are labor\nintensive and horticultural staff are hard to find. Automating these tasks is\nchallenging due to the semi-structured greenhouse workspaces, changing\nenvironmental conditions such as lighting, dense plant growth with many\nocclusions, and the need for gentle manipulation of non-rigid plant organs. In\nthis work, we present the three-armed system HortiBot, with two arms for\nmanipulation and a third arm as an articulated head for active perception using\nstereo cameras. Its perception system detects not only peppers, but also\npeduncles and stems in real time, and performs online data association to build\na world model of pepper plants. Collision-aware online trajectory generation\nallows all three arms to safely track their respective targets for observation,\ngrasping, and cutting. We integrated perception and manipulation to perform\nselective harvesting of peppers and evaluated the system in lab experiments.\nUsing active perception coupled with end-effector force torque sensing for\ncompliant manipulation, HortiBot achieves high success rates.\n","authors":["Christian Lenz","Rohit Menon","Michael Schreiber","Melvin Paul Jacob","Sven Behnke","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.15306v1.pdf","comment":"Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2024. C. Lenz and R. Menon contributed equally"},{"id":"http://arxiv.org/abs/2401.15174v3","updated":"2024-03-22T15:02:37Z","published":"2024-01-26T19:39:33Z","title":"LaMI: Large Language Models for Multi-Modal Human-Robot Interaction","summary":"  This paper presents an innovative large language model (LLM)-based robotic\nsystem for enhancing multi-modal human-robot interaction (HRI). Traditional HRI\nsystems relied on complex designs for intent estimation, reasoning, and\nbehavior generation, which were resource-intensive. In contrast, our system\nempowers researchers and practitioners to regulate robot behavior through three\nkey aspects: providing high-level linguistic guidance, creating \"atomic\nactions\" and expressions the robot can use, and offering a set of examples.\nImplemented on a physical robot, it demonstrates proficiency in adapting to\nmulti-modal inputs and determining the appropriate manner of action to assist\nhumans with its arms, following researchers' defined guidelines.\nSimultaneously, it coordinates the robot's lid, neck, and ear movements with\nspeech output to produce dynamic, multi-modal expressions. This showcases the\nsystem's potential to revolutionize HRI by shifting from conventional, manual\nstate-and-flow design methods to an intuitive, guidance-based, and\nexample-driven approach. Supplementary material can be found at\nhttps://hri-eu.github.io/Lami/\n","authors":["Chao Wang","Stephan Hasler","Daniel Tanneberg","Felix Ocker","Frank Joublin","Antonello Ceravola","Joerg Deigmoeller","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2401.15174v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15239v1","updated":"2024-03-22T14:32:27Z","published":"2024-03-22T14:32:27Z","title":"Guided Decoding for Robot Motion Generation and Adaption","summary":"  We address motion generation for high-DoF robot arms in complex settings with\nobstacles, via points, etc. A significant advancement in this domain is\nachieved by integrating Learning from Demonstration (LfD) into the motion\ngeneration process. This integration facilitates rapid adaptation to new tasks\nand optimizes the utilization of accumulated expertise by allowing robots to\nlearn and generalize from demonstrated trajectories.\n  We train a transformer architecture on a large dataset of simulated\ntrajectories. This architecture, based on a conditional variational autoencoder\ntransformer, learns essential motion generation skills and adapts these to meet\nauxiliary tasks and constraints. Our auto-regressive approach enables real-time\nintegration of feedback from the physical system, enhancing the adaptability\nand efficiency of motion generation. We show that our model can generate motion\nfrom initial and target points, but also that it can adapt trajectories in\nnavigating complex tasks, including obstacle avoidance, via points, and meeting\nvelocity and acceleration constraints, across platforms.\n","authors":["Nutan Chen","Elie Aljalbout","Botond Cseke","Patrick van der Smagt"],"pdf_url":"https://arxiv.org/pdf/2403.15239v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.15223v1","updated":"2024-03-22T14:15:27Z","published":"2024-03-22T14:15:27Z","title":"TriHelper: Zero-Shot Object Navigation with Dynamic Assistance","summary":"  Navigating toward specific objects in unknown environments without additional\ntraining, known as Zero-Shot object navigation, poses a significant challenge\nin the field of robotics, which demands high levels of auxiliary information\nand strategic planning. Traditional works have focused on holistic solutions,\noverlooking the specific challenges agents encounter during navigation such as\ncollision, low exploration efficiency, and misidentification of targets. To\naddress these challenges, our work proposes TriHelper, a novel framework\ndesigned to assist agents dynamically through three primary navigation\nchallenges: collision, exploration, and detection. Specifically, our framework\nconsists of three innovative components: (i) Collision Helper, (ii) Exploration\nHelper, and (iii) Detection Helper. These components work collaboratively to\nsolve these challenges throughout the navigation process. Experiments on the\nHabitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper\nsignificantly outperforms all existing baseline methods in Zero-Shot object\nnavigation, showcasing superior success rates and exploration efficiency. Our\nablation studies further underscore the effectiveness of each helper in\naddressing their respective challenges, notably enhancing the agent's\nnavigation capabilities. By proposing TriHelper, we offer a fresh perspective\non advancing the object navigation task, paving the way for future research in\nthe domain of Embodied AI and visual-based navigation.\n","authors":["Lingfeng Zhang","Qiang Zhang","Hao Wang","Erjia Xiao","Zixuan Jiang","Honglei Chen","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15223v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.15203v1","updated":"2024-03-22T13:46:51Z","published":"2024-03-22T13:46:51Z","title":"DITTO: Demonstration Imitation by Trajectory Transformation","summary":"  Teaching robots new skills quickly and conveniently is crucial for the\nbroader adoption of robotic systems. In this work, we address the problem of\none-shot imitation from a single human demonstration, given by an RGB-D video\nrecording through a two-stage process. In the first stage which is offline, we\nextract the trajectory of the demonstration. This entails segmenting\nmanipulated objects and determining their relative motion in relation to\nsecondary objects such as containers. Subsequently, in the live online\ntrajectory generation stage, we first \\mbox{re-detect} all objects, then we\nwarp the demonstration trajectory to the current scene, and finally, we trace\nthe trajectory with the robot. To complete these steps, our method makes\nleverages several ancillary models, including those for segmentation, relative\nobject pose estimation, and grasp prediction. We systematically evaluate\ndifferent combinations of correspondence and re-detection methods to validate\nour design decision across a diverse range of tasks. Specifically, we collect\ndemonstrations of ten different tasks including pick-and-place tasks as well as\narticulated object manipulation. Finally, we perform extensive evaluations on a\nreal robot system to demonstrate the effectiveness and utility of our approach\nin real-world scenarios. We make the code publicly available at\nhttp://ditto.cs.uni-freiburg.de.\n","authors":["Nick Heppert","Max Argus","Tim Welschehold","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15203v1.pdf","comment":"8 pages, 4 figures, 3 tables, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15183v1","updated":"2024-03-22T13:12:30Z","published":"2024-03-22T13:12:30Z","title":"CRPlace: Camera-Radar Fusion with BEV Representation for Place\n  Recognition","summary":"  The integration of complementary characteristics from camera and radar data\nhas emerged as an effective approach in 3D object detection. However, such\nfusion-based methods remain unexplored for place recognition, an equally\nimportant task for autonomous systems. Given that place recognition relies on\nthe similarity between a query scene and the corresponding candidate scene, the\nstationary background of a scene is expected to play a crucial role in the\ntask. As such, current well-designed camera-radar fusion methods for 3D object\ndetection can hardly take effect in place recognition because they mainly focus\non dynamic foreground objects. In this paper, a background-attentive\ncamera-radar fusion-based method, named CRPlace, is proposed to generate\nbackground-attentive global descriptors from multi-view images and radar point\nclouds for accurate place recognition. To extract stationary background\nfeatures effectively, we design an adaptive module that generates the\nbackground-attentive mask by utilizing the camera BEV feature and radar dynamic\npoints. With the guidance of a background mask, we devise a bidirectional\ncross-attention-based spatial fusion strategy to facilitate comprehensive\nspatial interaction between the background information of the camera BEV\nfeature and the radar BEV feature. As the first camera-radar fusion-based place\nrecognition network, CRPlace has been evaluated thoroughly on the nuScenes\ndataset. The results show that our algorithm outperforms a variety of baseline\nmethods across a comprehensive set of metrics (recall@1 reaches 91.2%).\n","authors":["Shaowei Fu","Yifan Duan","Yao Li","Chengzhen Meng","Yingjie Wang","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15171v1","updated":"2024-03-22T12:48:00Z","published":"2024-03-22T12:48:00Z","title":"AV-Occupant Perceived Risk Model for Cut-In Scenarios with Empirical\n  Evaluation","summary":"  Advancements in autonomous vehicle (AV) technologies necessitate precise\nestimation of perceived risk to enhance user comfort, acceptance and trust.\nThis paper introduces a novel AV-Occupant Risk (AVOR) model designed for\nperceived risk estimation during AV cut-in scenarios. An empirical study is\nconducted with 18 participants with realistic cut-in scenarios. Two factors\nwere investigated: scenario risk and scene population. 76% of subjective risk\nresponses indicate an increase in perceived risk at cut-in initiation. The\nexisting perceived risk model did not capture this critical phenomenon. Our\nAVOR model demonstrated a significant improvement in estimating perceived risk\nduring the early stages of cut-ins, especially for the high-risk scenario,\nenhancing modelling accuracy by up to 54%. The concept of the AVOR model can\nquantify perceived risk in other diverse driving contexts characterized by\ndynamic uncertainties, enhancing the reliability and human-centred focus of AV\nsystems.\n","authors":["Sarah Barendswaard","Tong Duy Son"],"pdf_url":"https://arxiv.org/pdf/2403.15171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03270v2","updated":"2024-03-22T12:40:23Z","published":"2024-03-05T19:11:17Z","title":"Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual\n  Manipulation Tasks","summary":"  Visual imitation learning has achieved impressive progress in learning\nunimanual manipulation tasks from a small set of visual observations, thanks to\nthe latest advances in computer vision. However, learning bimanual coordination\nstrategies and complex object relations from bimanual visual demonstrations, as\nwell as generalizing them to categorical objects in novel cluttered scenes\nremain unsolved challenges. In this paper, we extend our previous work on\nkeypoints-based visual imitation learning (\\mbox{K-VIL})~\\cite{gao_kvil_2023}\nto bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called\n\\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands,\nbimanual coordination strategies, and sub-symbolic task representations. Our\nbimanual task representation is object-centric, embodiment-independent, and\nviewpoint-invariant, thus generalizing well to categorical objects in novel\nscenes. We evaluate our approach in various real-world applications, showcasing\nits ability to learn fine-grained bimanual manipulation tasks from a small\nnumber of human demonstration videos. Videos and source code are available at\nhttps://sites.google.com/view/bi-kvil.\n","authors":["Jianfeng Gao","Xiaoshu Jin","Franziska Krebs","Noémie Jaquier","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.03270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15156v1","updated":"2024-03-22T12:11:06Z","published":"2024-03-22T12:11:06Z","title":"Infrastructure-Assisted Collaborative Perception in Automated Valet\n  Parking: A Safety Perspective","summary":"  Environmental perception in Automated Valet Parking (AVP) has been a\nchallenging task due to severe occlusions in parking garages. Although\nCollaborative Perception (CP) can be applied to broaden the field of view of\nconnected vehicles, the limited bandwidth of vehicular communications restricts\nits application. In this work, we propose a BEV feature-based CP network\narchitecture for infrastructure-assisted AVP systems. The model takes the\nroadside camera and LiDAR as optional inputs and adaptively fuses them with\nonboard sensors in a unified BEV representation. Autoencoder and downsampling\nare applied for channel-wise and spatial-wise dimension reduction, while\nsparsification and quantization further compress the feature map with little\nloss in data precision. Combining these techniques, the size of a BEV feature\nmap is effectively compressed to fit in the feasible data rate of the NR-V2X\nnetwork. With the synthetic AVP dataset, we observe that CP can effectively\nincrease perception performance, especially for pedestrians. Moreover, the\nadvantage of infrastructure-assisted CP is demonstrated in two typical\nsafety-critical scenarios in the AVP setting, increasing the maximum safe\ncruising speed by up to 3m/s in both scenarios.\n","authors":["Yukuan Jia","Jiawen Zhang","Shimeng Lu","Baokang Fan","Ruiqing Mao","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15156v1.pdf","comment":"7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring"},{"id":"http://arxiv.org/abs/2403.15151v1","updated":"2024-03-22T12:07:03Z","published":"2024-03-22T12:07:03Z","title":"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive\n  Museum Exhibit","summary":"  In 1997, the very first tour guide robot RHINO was deployed in a museum in\nGermany. With the ability to navigate autonomously through the environment, the\nrobot gave tours to over 2,000 visitors. Today, RHINO itself has become an\nexhibit and is no longer operational. In this paper, we present RHINO-VR, an\ninteractive museum exhibit using virtual reality (VR) that allows museum\nvisitors to experience the historical robot RHINO in operation in a virtual\nmuseum. RHINO-VR, unlike static exhibits, enables users to familiarize\nthemselves with basic mobile robotics concepts without the fear of damaging the\nexhibit. In the virtual environment, the user is able to interact with RHINO in\nVR by pointing to a location to which the robot should navigate and observing\nthe corresponding actions of the robot. To include other visitors who cannot\nuse the VR, we provide an external observation view to make RHINO visible to\nthem. We evaluated our system by measuring the frame rate of the VR simulation,\ncomparing the generated virtual 3D models with the originals, and conducting a\nuser study. The user-study showed that RHINO-VR improved the visitors'\nunderstanding of the robot's functionality and that they would recommend\nexperiencing the VR exhibit to others.\n","authors":["Erik Schlachhoff","Nils Dengler","Leif Van Holland","Patrick Stotko","Jorge de Heuvel","Reinhard Klein","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.15151v1.pdf","comment":"Submitted to IEEE International Symposium on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2403.15142v1","updated":"2024-03-22T11:52:31Z","published":"2024-03-22T11:52:31Z","title":"ALPINE: a climbing robot for operations in mountain environments","summary":"  Mountain slopes are perfect examples of harsh environments in which humans\nare required to perform difficult and dangerous operations such as removing\nunstable boulders, dangerous vegetation or deploying safety nets. A good\nreplacement for human intervention can be offered by climbing robots. The\ndifferent solutions existing in the literature are not up to the task for the\ndifficulty of the requirements (navigation, heavy payloads, flexibility in the\nexecution of the tasks). In this paper, we propose a robotic platform that can\nfill this gap. Our solution is based on a robot that hangs on ropes, and uses a\nretractable leg to jump away from the mountain walls. Our package of mechanical\nsolutions, along with the algorithms developed for motion planning and control,\ndelivers swift navigation on irregular and steep slopes, the possibility to\novercome or travel around significant natural barriers, and the ability to\ncarry heavy payloads and execute complex tasks. In the paper, we give a full\naccount of our main design and algorithmic choices and show the feasibility of\nthe solution through a large number of physically simulated scenarios.\n","authors":["Michele Focchi","Andrea Del Prete","Daniele Fontanelli","Marco Frego","Angelika Peer","Luigi Palopoli"],"pdf_url":"https://arxiv.org/pdf/2403.15142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15116v1","updated":"2024-03-22T11:20:30Z","published":"2024-03-22T11:20:30Z","title":"Collision Avoidance Safety Filter for an Autonomous E-Scooter using\n  Ultrasonic Sensors","summary":"  In this paper, we propose a collision avoidance safety filter for autonomous\nelectric scooters to enable safe operation of such vehicles in pedestrian\nareas. In particular, we employ multiple low-cost ultrasonic sensors to detect\na wide range of possible obstacles in front of the e-scooter. Based on possibly\nfaulty distance measurements, we design a filter to mitigate measurement noise\nand missing values as well as a gain-scheduled controller to limit the velocity\ncommanded to the e-scooter when required due to imminent collisions. The\nproposed controller structure is able to prevent collisions with unknown\nobstacles by deploying a reduced safe velocity ensuring a sufficiently large\nsafety distance. The collision avoidance approach is designed such that it may\nbe easily deployed in similar applications of general micromobility vehicles.\nThe effectiveness of our proposed safety filter is demonstrated in real-world\nexperiments.\n","authors":["Robin Strässer","Marc Seidel","Felix Brändle","David Meister","Raffaele Soloperto","David Hambach Ferrer","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2403.15116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15113v1","updated":"2024-03-22T11:08:56Z","published":"2024-03-22T11:08:56Z","title":"Set-membership target search and tracking within an unknown cluttered\n  area using cooperating UAVs equipped with vision systems","summary":"  This paper addresses the problem of target search and tracking using a fleet\nof cooperating UAVs evolving in some unknown region of interest containing an a\npriori unknown number of moving ground targets. Each drone is equipped with an\nembedded Computer Vision System (CVS), providing an image with labeled pixels\nand a depth map of the observed part of its environment. Moreover, a box\ncontaining the corresponding pixels in the image frame is available when a UAV\nidentifies a target. Hypotheses regarding information provided by the pixel\nclassification, depth map construction, and target identification algorithms\nare proposed to allow its exploitation by set-membership approaches. A\nset-membership target location estimator is developed using the information\nprovided by the CVS. Each UAV evaluates sets guaranteed to contain the location\nof the identified targets and a set possibly containing the locations of\ntargets still to be identified. Then, each UAV uses these sets to search and\ntrack targets cooperatively.\n","authors":["Maxime Zagar","Luc Meyer","Michel Kieffer","Hélène Piet-Lahanier"],"pdf_url":"https://arxiv.org/pdf/2403.15113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15107v1","updated":"2024-03-22T10:51:31Z","published":"2024-03-22T10:51:31Z","title":"PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic\n  Manipulation","summary":"  Humans seemingly incorporate potential touch signals in their perception. Our\ngoal is to equip robots with a similar capability, which we term \\ourmodel.\n\\ourmodel aims to predict the expected touch signal based on a visual patch\nrepresenting the touched area. We frame this problem as the task of learning a\nlow-dimensional visual-tactile embedding, wherein we encode a depth patch from\nwhich we decode the tactile signal. To accomplish this task, we employ ReSkin,\nan inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we\ncollect and train PseudoTouch on a dataset comprising aligned tactile and\nvisual data pairs obtained through random touching of eight basic geometric\nshapes. We demonstrate the efficacy of PseudoTouch through its application to\ntwo downstream tasks: object recognition and grasp stability prediction. In the\nobject recognition task, we evaluate the learned embedding's performance on a\nset of five basic geometric shapes and five household objects. Using\nPseudoTouch, we achieve an object recognition accuracy 84% after just ten\ntouches, surpassing a proprioception baseline. For the grasp stability task, we\nuse ACRONYM labels to train and evaluate a grasp success predictor using\nPseudoTouch's predictions derived from virtual depth information. Our approach\nyields an impressive 32% absolute improvement in accuracy compared to the\nbaseline relying on partial point cloud data. We make the data, code, and\ntrained models publicly available at http://pseudotouch.cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Nick Heppert","Abdallah Ayman","Eugenio Chisari","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15107v1.pdf","comment":"8 pages, 7 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.15102v1","updated":"2024-03-22T10:41:25Z","published":"2024-03-22T10:41:25Z","title":"Learning from Visual Demonstrations through Differentiable Nonlinear MPC\n  for Personalized Autonomous Driving","summary":"  Human-like autonomous driving controllers have the potential to enhance\npassenger perception of autonomous vehicles. This paper proposes DriViDOC: a\nmodel for Driving from Vision through Differentiable Optimal Control, and its\napplication to learn personalized autonomous driving controllers from human\ndemonstrations. DriViDOC combines the automatic inference of relevant features\nfrom camera frames with the properties of nonlinear model predictive control\n(NMPC), such as constraint satisfaction. Our approach leverages the\ndifferentiability of parametric NMPC, allowing for end-to-end learning of the\ndriving model from images to control. The model is trained on an offline\ndataset comprising various driving styles collected on a motion-base driving\nsimulator. During online testing, the model demonstrates successful imitation\nof different driving styles, and the interpreted NMPC parameters provide\ninsights into the achievement of specific driving behaviors. Our experimental\nresults show that DriViDOC outperforms other methods involving NMPC and neural\nnetworks, exhibiting an average improvement of 20% in imitation scores.\n","authors":["Flavia Sofia Acerbo","Jan Swevers","Tinne Tuytelaars","Tong Duy Son"],"pdf_url":"https://arxiv.org/pdf/2403.15102v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Accompanying video available at:\n  https://youtu.be/WxWPuAtJ08E"},{"id":"http://arxiv.org/abs/2403.15100v1","updated":"2024-03-22T10:39:22Z","published":"2024-03-22T10:39:22Z","title":"Subequivariant Reinforcement Learning Framework for Coordinated Motion\n  Control","summary":"  Effective coordination is crucial for motion control with reinforcement\nlearning, especially as the complexity of agents and their motions increases.\nHowever, many existing methods struggle to account for the intricate\ndependencies between joints. We introduce CoordiGraph, a novel architecture\nthat leverages subequivariant principles from physics to enhance coordination\nof motion control with reinforcement learning. This method embeds the\nprinciples of equivariance as inherent patterns in the learning process under\ngravity influence, which aids in modeling the nuanced relationships between\njoints vital for motion control. Through extensive experimentation with\nsophisticated agents in diverse environments, we highlight the merits of our\napproach. Compared to current leading methods, CoordiGraph notably enhances\ngeneralization and sample efficiency.\n","authors":["Haoyu Wang","Xiaoyu Tan","Xihe Qiu","Chao Qu"],"pdf_url":"https://arxiv.org/pdf/2403.15100v1.pdf","comment":"7 pages, 7 figures, 2024 IEEE International Conference on Robotics\n  and Automation"},{"id":"http://arxiv.org/abs/2304.09793v2","updated":"2024-03-22T10:36:32Z","published":"2023-04-19T16:21:14Z","title":"Event-based Simultaneous Localization and Mapping: A Comprehensive\n  Survey","summary":"  In recent decades, visual simultaneous localization and mapping (vSLAM) has\ngained significant interest in both academia and industry. It estimates camera\nmotion and reconstructs the environment concurrently using visual sensors on a\nmoving robot. However, conventional cameras are limited by hardware, including\nmotion blur and low dynamic range, which can negatively impact performance in\nchallenging scenarios like high-speed motion and high dynamic range\nillumination. Recent studies have demonstrated that event cameras, a new type\nof bio-inspired visual sensor, offer advantages such as high temporal\nresolution, dynamic range, low power consumption, and low latency. This paper\npresents a timely and comprehensive review of event-based vSLAM algorithms that\nexploit the benefits of asynchronous and irregular event streams for\nlocalization and mapping tasks. The review covers the working principle of\nevent cameras and various event representations for preprocessing event data.\nIt also categorizes event-based vSLAM methods into four main categories:\nfeature-based, direct, motion-compensation, and deep learning methods, with\ndetailed discussions and practical guidance for each approach. Furthermore, the\npaper evaluates the state-of-the-art methods on various benchmarks,\nhighlighting current challenges and future opportunities in this emerging\nresearch area. A public repository will be maintained to keep track of the\nrapid developments in this field at\n{\\url{https://github.com/kun150kun/ESLAM-survey}}.\n","authors":["Kunping Huang","Sen Zhang","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2304.09793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16973v2","updated":"2024-03-22T10:27:53Z","published":"2023-06-29T14:28:22Z","title":"Robust Direct Data-Driven Control for Probabilistic Systems","summary":"  We propose a data-driven control method for systems with aleatoric\nuncertainty, for example, robot fleets with variations between agents. Our\nmethod leverages shared trajectory data to increase the robustness of the\ndesigned controller and thus facilitate transfer to new variations without the\nneed for prior parameter and uncertainty estimations. In contrast to existing\nwork on experience transfer for performance, our approach focuses on robustness\nand uses data collected from multiple realizations to guarantee generalization\nto unseen ones. Our method is based on scenario optimization combined with\nrecent formulations for direct data-driven control. We derive lower bounds on\nthe amount of data required to achieve quadratic stability for probabilistic\nsystems with aleatoric uncertainty and demonstrate the benefits of our\ndata-driven method through a numerical example. We find that the learned\ncontrollers generalize well to high variations in the dynamics even when based\non only a few short open-loop trajectories. Robust experience transfer enables\nthe design of safe and robust controllers that work out of the box without any\nadditional learning during deployment.\n","authors":["Alexander von Rohr","Dmitrii Likhachev","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2306.16973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15079v1","updated":"2024-03-22T10:05:21Z","published":"2024-03-22T10:05:21Z","title":"Automated Feature Selection for Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning (IRL) is an imitation learning approach to\nlearning reward functions from expert demonstrations. Its use avoids the\ndifficult and tedious procedure of manual reward specification while retaining\nthe generalization power of reinforcement learning. In IRL, the reward is\nusually represented as a linear combination of features. In continuous state\nspaces, the state variables alone are not sufficiently rich to be used as\nfeatures, but which features are good is not known in general. To address this\nissue, we propose a method that employs polynomial basis functions to form a\ncandidate set of features, which are shown to allow the matching of statistical\nmoments of state distributions. Feature selection is then performed for the\ncandidates by leveraging the correlation between trajectory probabilities and\nfeature expectations. We demonstrate the approach's effectiveness by recovering\nreward functions that capture expert policies across non-linear control tasks\nof increasing complexity. Code, data, and videos are available at\nhttps://sites.google.com/view/feature4irl.\n","authors":["Daulet Baimukashev","Gokhan Alcan","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.15079v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15067v1","updated":"2024-03-22T09:48:40Z","published":"2024-03-22T09:48:40Z","title":"A Twin Delayed Deep Deterministic Policy Gradient Algorithm for\n  Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness","summary":"  Autonomous ground vehicle (UGV) navigation has the potential to revolutionize\nthe transportation system by increasing accessibility to disabled people,\nensure safety and convenience of use. However, UGV requires extensive and\nefficient testing and evaluation to ensure its acceptance for public use. This\ntesting are mostly done in a simulator which result to sim2real transfer gap.\nIn this paper, we propose a digital twin perception awareness approach for the\ncontrol of robot navigation without prior creation of the virtual environment\n(VT) environment state. To achieve this, we develop a twin delayed deep\ndeterministic policy gradient (TD3) algorithm that ensures collision avoidance\nand goal-based path planning. We demonstrate the performance of our approach on\ndifferent environment dynamics. We show that our approach is capable of\nefficiently avoiding collision with obstacles and navigating to its desired\ndestination, while at the same time safely avoids obstacles using the\ninformation received from the LIDAR sensor mounted on the robot. Our approach\nbridges the gap between sim-to-real transfer and contributes to the adoption of\nUGVs in real world. We validate our approach in simulation and a real-world\napplication in an office space.\n","authors":["Kabirat Olayemi","Mien Van","Sean McLoone","Yuzhu Sun","Jack Close","Nguyen Minh Nhat","Stephen McIlvanna"],"pdf_url":"https://arxiv.org/pdf/2403.15067v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.15054v1","updated":"2024-03-22T09:26:52Z","published":"2024-03-22T09:26:52Z","title":"Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality\n  Grasping","summary":"  Robotic grasping is a primitive skill for complex tasks and is fundamental to\nintelligence. For general 6-Dof grasping, most previous methods directly\nextract scene-level semantic or geometric information, while few of them\nconsider the suitability for various downstream applications, such as\ntarget-oriented grasping. Addressing this issue, we rethink 6-Dof grasp\ndetection from a grasp-centric view and propose a versatile grasp framework\ncapable of handling both scene-level and target-oriented grasping. Our\nframework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp\nModel. Specifically, the Flexible Guidance Module is compatible with both\nglobal (e.g., grasp heatmap) and local (e.g., visual grounding) guidance,\nenabling the generation of high-quality grasps across various tasks. The Local\nGrasp Model focuses on object-agnostic regional points and predicts grasps\nlocally and intently. Experiment results reveal that our framework achieves\nover 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset.\nFurthermore, real-world robotic tests in three distinct settings yield a 95%\nsuccess rate.\n","authors":["Wei Tang","Siang Chen","Pengwei Xie","Dingchang Hu","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15054v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.10519v2","updated":"2024-03-22T09:09:45Z","published":"2024-01-19T06:32:54Z","title":"A Wind-Aware Path Planning Method for UAV-Asisted Bridge Inspection","summary":"  In response to the gap in considering wind conditions in the bridge\ninspection using unmanned aerial vehicle (UAV) , this paper proposes a path\nplanning method for UAVs that takes into account the influence of wind, based\non the simulated annealing algorithm. The algorithm considers the wind factors,\nincluding the influence of different wind speeds and directions at the same\ntime on the path planning of the UAV. Firstly, An environment model is\nconstructed specifically for UAV bridge inspection, taking into account the\nvarious objective functions and constraint conditions of UAVs. A more\nsophisticated and precise mathematical model is then developed based on this\nenvironmental model to enable efficient and effective UAV path planning.\nSecondly, the bridge separation planning model is applied in a novel way, and a\nseries of parameters are simulated, including the adjustment of the initial\ntemperature value. The experimental results demonstrate that, compared with\ntraditional local search algorithms, the proposed method achieves a cost\nreduction of 30.05\\% and significantly improves effectiveness. Compared to path\nplanning methods that do not consider wind factors, the proposed approach\nyields more realistic and practical results for UAV applications, as\ndemonstrated by its improved effectiveness in simulations. These findings\nhighlight the value of our method in facilitating more accurate and efficient\nUAV path planning in wind-prone environments.\n","authors":["Jian Xu","Hua Dai"],"pdf_url":"https://arxiv.org/pdf/2401.10519v2.pdf","comment":"After carefully analysis, there is a bit design flaws in Algorithm 1.\n  The experimental work of the paper is not comprehensive,which lacks an\n  evaluation of the algorithm's running time"},{"id":"http://arxiv.org/abs/2402.17587v3","updated":"2024-03-22T07:23:51Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14997v1","updated":"2024-03-22T07:17:56Z","published":"2024-03-22T07:17:56Z","title":"Linear Quadratic Guidance Law for Joint Motion Planning of a\n  Pursuer-Turret Assembly","summary":"  This paper presents joint motion planning of a vehicle with an attached\nrotating turret. The turret has a limited range as well as the field of view.\nThe objective is capture a maneuvering target such that at the terminal time it\nis withing the field-of-view and range limits. Catering to it, we present a\nminimum effort guidance law that commensurate for the turn rate abilities of\nthe vehicle and the turret. The guidance law is obtained using linearization\nabout the collision triangle and admits an analytical solution. Simulation\nresults are presented to exemplify the cooperation between the turret and the\nvehicle.\n","authors":["Bhargav Jha","Shaunak Bopardikar","Alexander Von Moll","David Casbeer"],"pdf_url":"https://arxiv.org/pdf/2403.14997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07433v5","updated":"2024-03-22T06:42:03Z","published":"2023-02-15T02:32:26Z","title":"A Survey on Global LiDAR Localization: Challenges, Advances and Open\n  Problems","summary":"  Knowledge about the own pose is key for all mobile robot applications. Thus\npose estimation is part of the core functionalities of mobile robots. Over the\nlast two decades, LiDAR scanners have become the standard sensor for robot\nlocalization and mapping. This article aims to provide an overview of recent\nprogress and advancements in LiDAR-based global localization. We begin by\nformulating the problem and exploring the application scope. We then present a\nreview of the methodology, including recent advancements in several topics,\nsuch as maps, descriptor extraction, and cross-robot localization. The contents\nof the article are organized under three themes. The first theme concerns the\ncombination of global place retrieval and local pose estimation. The second\ntheme is upgrading single-shot measurements to sequential ones for sequential\nglobal localization. Finally, the third theme focuses on extending single-robot\nglobal localization to cross-robot localization in multi-robot systems. We\nconclude the survey with a discussion of open challenges and promising\ndirections in global LiDAR localization. To our best knowledge, this is the\nfirst comprehensive survey on global LiDAR localization for mobile robots.\n","authors":["Huan Yin","Xuecheng Xu","Sha Lu","Xieyuanli Chen","Rong Xiong","Shaojie Shen","Cyrill Stachniss","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2302.07433v5.pdf","comment":"Publishe on International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2403.14956v1","updated":"2024-03-22T05:21:05Z","published":"2024-03-22T05:21:05Z","title":"Boundary-Aware Value Function Generation for Safe Stochastic Motion\n  Planning","summary":"  Navigation safety is critical for many autonomous systems such as\nself-driving vehicles in an urban environment. It requires an explicit\nconsideration of boundary constraints that describe the borders of any\ninfeasible, non-navigable, or unsafe regions. We propose a principled\nboundary-aware safe stochastic planning framework with promising results. Our\nmethod generates a value function that can strictly distinguish the state\nvalues between free (safe) and non-navigable (boundary) spaces in the\ncontinuous state, naturally leading to a safe boundary-aware policy. At the\ncore of our solution lies a seamless integration of finite elements and\nkernel-based functions, where the finite elements allow us to characterize\nsafety-critical states' borders accurately, and the kernel-based function\nspeeds up computation for the non-safety-critical states. The proposed method\nwas evaluated through extensive simulations and demonstrated safe navigation\nbehaviors in mobile navigation tasks. Additionally, we demonstrate that our\napproach can maneuver safely and efficiently in cluttered real-world\nenvironments using a ground vehicle with strong external disturbances, such as\nnavigating on a slippery floor and against external human intervention.\n","authors":["Junhong Xu","Kai Yin","Jason M. Gregory","Kris Hauser","Lantao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14956v1.pdf","comment":"Accepted by International Journal of Robotics Research"},{"id":"http://arxiv.org/abs/2309.15271v2","updated":"2024-03-22T03:32:09Z","published":"2023-09-26T21:10:12Z","title":"Kinematic Modularity of Elementary Dynamic Actions","summary":"  In this paper, a kinematically modular approach to robot control is\npresented. The method involves structures called Elementary Dynamic Actions and\na network model combining these elements. With this control framework, a rich\nrepertoire of movements can be generated by combination of basic modules. The\nproblems of solving inverse kinematics, managing kinematic singularity and\nkinematic redundancy are avoided. The modular approach is robust against\ncontact and physical interaction, which makes it particularly effective for\ncontact-rich manipulation. Each kinematic module can be learned by Imitation\nLearning, thereby resulting in a modular learning strategy for robot control.\nThe theoretical foundations and their real robot implementation are presented.\nUsing a KUKA LBR iiwa14 robot, three tasks were considered: (1) generating a\nsequence of discrete movements, (2) generating a combination of discrete and\nrhythmic movements, and (3) a drawing and erasing task. The results obtained\nindicate that this modular approach has the potential to simplify the\ngeneration of a diverse range of robot actions.\n","authors":["Moses C. Nah","Johannes Lachner","Federico Tessari","Neville Hogan"],"pdf_url":"https://arxiv.org/pdf/2309.15271v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14545v2","updated":"2024-03-22T03:19:50Z","published":"2024-03-21T16:44:49Z","title":"Learning Hierarchical Control For Multi-Agent Capacity-Constrained\n  Systems","summary":"  This paper introduces a novel data-driven hierarchical control scheme for\nmanaging a fleet of nonlinear, capacity-constrained autonomous agents in an\niterative environment. We propose a control framework consisting of a\nhigh-level dynamic task assignment and routing layer and low-level motion\nplanning and tracking layer. Each layer of the control hierarchy uses a\ndata-driven Model Predictive Control (MPC) policy, maintaining bounded\ncomputational complexity at each calculation of a new task assignment or\nactuation input. We utilize collected data to iteratively refine estimates of\nagent capacity usage, and update MPC policy parameters accordingly. Our\napproach leverages tools from iterative learning control to integrate learning\nat both levels of the hierarchy, and coordinates learning between levels in\norder to maintain closed-loop feasibility and performance improvement of the\nconnected architecture.\n","authors":["Charlott Vallon","Alessandro Pinto","Bartolomeo Stellato","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2403.14545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07894v5","updated":"2024-03-22T02:10:49Z","published":"2023-06-13T16:39:39Z","title":"iSLAM: Imperative SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) stands as one of the critical\nchallenges in robot navigation. A SLAM system often consists of a front-end\ncomponent for motion estimation and a back-end system for eliminating\nestimation drifts. Recent advancements suggest that data-driven methods are\nhighly effective for front-end tasks, while geometry-based methods continue to\nbe essential in the back-end processes. However, such a decoupled paradigm\nbetween the data-driven front-end and geometry-based back-end can lead to\nsub-optimal performance, consequently reducing the system's capabilities and\ngeneralization potential. To solve this problem, we proposed a novel\nself-supervised imperative learning framework, named imperative SLAM (iSLAM),\nwhich fosters reciprocal correction between the front-end and back-end, thus\nenhancing performance without necessitating any external supervision.\nSpecifically, we formulate the SLAM problem as a bilevel optimization so that\nthe front-end and back-end are bidirectionally connected. As a result, the\nfront-end model can learn global geometric knowledge obtained through pose\ngraph optimization by back-propagating the residuals from the back-end\ncomponent. We showcase the effectiveness of this new framework through an\napplication of stereo-inertial SLAM. The experiments show that the iSLAM\ntraining strategy achieves an accuracy improvement of 22% on average over a\nbaseline model. To the best of our knowledge, iSLAM is the first SLAM system\nshowing that the front-end and back-end components can mutually correct each\nother in a self-supervised manner.\n","authors":["Taimeng Fu","Shaoshu Su","Yiren Lu","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2306.07894v5.pdf","comment":"The paper has been accepted by IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2306.06531v3","updated":"2024-03-22T00:21:04Z","published":"2023-06-10T21:58:29Z","title":"AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers","summary":"  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n","authors":["Yongchao Chen","Jacob Arkin","Charles Dawson","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2306.06531v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.15943v2","updated":"2024-03-22T00:11:21Z","published":"2023-09-27T18:40:36Z","title":"Scalable Multi-Robot Collaboration with Large Language Models:\n  Centralized or Decentralized Systems?","summary":"  A flurry of recent work has demonstrated that pre-trained large language\nmodels (LLMs) can be effective task planners for a variety of single-robot\ntasks. The planning performance of LLMs is significantly improved via prompting\ntechniques, such as in-context learning or re-prompting with state feedback,\nplacing new importance on the token budget for the context window. An\nunder-explored but natural next direction is to investigate LLMs as multi-robot\ntask planners. However, long-horizon, heterogeneous multi-robot planning\nintroduces new challenges of coordination while also pushing up against the\nlimits of context window length. It is therefore critical to find\ntoken-efficient LLM planning frameworks that are also able to reason about the\ncomplexities of multi-robot coordination. In this work, we compare the task\nsuccess rate and token efficiency of four multi-agent communication frameworks\n(centralized, decentralized, and two hybrid) as applied to four\ncoordination-dependent multi-agent 2D task scenarios for increasing numbers of\nagents. We find that a hybrid framework achieves better task success rates\nacross all four tasks and scales better to more agents. We further demonstrate\nthe hybrid frameworks in 3D simulations where the vision-to-text problem and\ndynamical errors are considered. See our project website\nhttps://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and\ncode.\n","authors":["Yongchao Chen","Jacob Arkin","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2309.15943v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.15648v1","updated":"2024-03-22T23:12:28Z","published":"2024-03-22T23:12:28Z","title":"SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning","summary":"  An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm\n","authors":["Weizheng Wang","Le Mao","Ruiqi Wang","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2403.15648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15637v1","updated":"2024-03-22T22:27:42Z","published":"2024-03-22T22:27:42Z","title":"CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor\n  and Indoor Environments","summary":"  We present ConVOI, a novel method for autonomous robot navigation in\nreal-world indoor and outdoor environments using Vision Language Models (VLMs).\nWe employ VLMs in two ways: first, we leverage their zero-shot image\nclassification capability to identify the context or scenario (e.g., indoor\ncorridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, and\nformulate context-based navigation behaviors as simple text prompts (e.g.\n``stay on the pavement\"). Second, we utilize their state-of-the-art semantic\nunderstanding and logical reasoning capabilities to compute a suitable\ntrajectory given the identified context. To this end, we propose a novel\nmulti-modal visual marking approach to annotate the obstacle-free regions in\nthe RGB image used as input to the VLM with numbers, by correlating it with a\nlocal occupancy map of the environment. The marked numbers ground image\nlocations in the real-world, direct the VLM's attention solely to navigable\nlocations, and elucidate the spatial relationships between them and terrains\ndepicted in the image to the VLM. Next, we query the VLM to select numbers on\nthe marked image that satisfy the context-based behavior text prompt, and\nconstruct a reference path using the selected numbers. Finally, we propose a\nmethod to extrapolate the reference trajectory when the robot's environmental\ncontext has not changed to prevent unnecessary VLM queries. We use the\nreference trajectory to guide a motion planner, and demonstrate that it leads\nto human-like behaviors (e.g. not cutting through a group of people, using\ncrosswalks, etc.) in various real-world indoor and outdoor scenarios.\n","authors":["Adarsh Jagan Sathyamoorthy","Kasun Weerakoon","Mohamed Elnoor","Anuj Zore","Brian Ichter","Fei Xia","Jie Tan","Wenhao Yu","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.15637v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.00186v3","updated":"2024-03-22T22:06:29Z","published":"2023-07-31T22:50:14Z","title":"Learning Complex Motion Plans using Neural ODEs with Safety and\n  Stability Guarantees","summary":"  We propose a Dynamical System (DS) approach to learn complex, possibly\nperiodic motion plans from kinesthetic demonstrations using Neural Ordinary\nDifferential Equations (NODE). To ensure reactivity and robustness to\ndisturbances, we propose a novel approach that selects a target point at each\ntime step for the robot to follow, by combining tools from control theory and\nthe target trajectory generated by the learned NODE. A correction term to the\nNODE model is computed online by solving a quadratic program that guarantees\nstability and safety using control Lyapunov functions and control barrier\nfunctions, respectively. Our approach outperforms baseline DS learning\ntechniques on the LASA handwriting dataset and complex periodic trajectories.\nIt is also validated on the Franka Emika robot arm to produce stable motions\nfor wiping and stirring tasks that do not have a single attractor, while being\nrobust to perturbations and safe around humans and obstacles.\n","authors":["Farhad Nawaz","Tianyu Li","Nikolai Matni","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2308.00186v3.pdf","comment":"accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.15621v1","updated":"2024-03-22T21:16:25Z","published":"2024-03-22T21:16:25Z","title":"Global Games with Negative Feedback for Autonomous Colony Maintenance\n  using Robot Teams","summary":"  In this article we address the colony maintenance problem, where a team of\nrobots are tasked with continuously maintaining the energy supply of an\nautonomous colony. We model this as a global game, where robots measure the\nenergy level of a central nest to determine whether or not to forage for energy\nsources. We design a mechanism that avoids the trivial equilibrium where all\nrobots always forage. Furthermore, we demonstrate that when the game is played\niteratively a negative feedback term stabilizes the number of foraging robots\nat a non-trivial Nash equilibrium. We compare our approach qualitatively to\nexisting global games, where a positive positive feedback term admits\nthreshold-based decision making, and encourages many robots to forage\nsimultaneously. We discuss how positive feedback can lead to a cascading\nfailure in the presence of a human who recruits robots for external tasks, and\nwe demonstrate the performance of our approach in simulation.\n","authors":["Logan E. Beaver"],"pdf_url":"https://arxiv.org/pdf/2403.15621v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.07139v2","updated":"2024-03-22T20:49:54Z","published":"2023-09-01T16:19:27Z","title":"A Traffic Management Framework for On-Demand Urban Air Mobility Systems","summary":"  Urban Air Mobility (UAM) offers a solution to current traffic congestion by\nproviding on-demand air mobility in urban areas. Effective traffic management\nis crucial for efficient operation of UAM systems, especially for high-demand\nscenarios. In this paper, we present a centralized traffic management framework\nfor on-demand UAM systems. Specifically, we provide a scheduling policy, called\nVertiSync, which schedules the aircraft for either servicing trip requests or\nrebalancing in the system subject to aircraft safety margins and energy\nrequirements. We characterize the system-level throughput of VertiSync, which\ndetermines the demand threshold at which passenger waiting times transition\nfrom being stabilized to being increasing over time. We show that the proposed\npolicy is able to maximize throughput for sufficiently large fleet sizes. We\ndemonstrate the performance of VertiSync through a case study for the city of\nLos Angeles, and show that it significantly reduces passenger waiting times\ncompared to a first-come first-serve scheduling policy.\n","authors":["Milad Pooladsanj","Ketan Savla","Petros A. Ioannou"],"pdf_url":"https://arxiv.org/pdf/2309.07139v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15577v1","updated":"2024-03-22T19:04:58Z","published":"2024-03-22T19:04:58Z","title":"Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based\n  Adaptive Cruise Control","summary":"  Autonomous driving depends on perception systems to understand the\nenvironment and to inform downstream decision-making. While advanced perception\nsystems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like\ncomprehension, their unpredictable behavior and lack of interpretability may\nhinder their deployment in safety critical scenarios. In this paper, we develop\nan Ensemble of DNN regressors (Deep Ensemble) that generates predictions with\nquantification of prediction uncertainties. In the scenario of Adaptive Cruise\nControl (ACC), we employ the Deep Ensemble to estimate distance headway to the\nlead vehicle from RGB images and enable the downstream controller to account\nfor the estimation uncertainty. We develop an adaptive cruise controller that\nutilizes Stochastic Model Predictive Control (MPC) with chance constraints to\nprovide a probabilistic safety guarantee. We evaluate our ACC algorithm using a\nhigh-fidelity traffic simulator and a real-world traffic dataset and\ndemonstrate the ability of the proposed approach to effect speed tracking and\ncar following while maintaining a safe distance headway. The\nout-of-distribution scenarios are also examined.\n","authors":["Xiao Li","H. Eric Tseng","Anouck Girard","Ilya Kolmanovsky"],"pdf_url":"https://arxiv.org/pdf/2403.15577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09317v2","updated":"2024-03-22T18:59:15Z","published":"2023-09-17T16:06:38Z","title":"Kinematics-aware Trajectory Generation and Prediction with Latent\n  Stochastic Differential Modeling","summary":"  Trajectory generation and trajectory prediction are two critical tasks in\nautonomous driving, which generate various trajectories for testing during\ndevelopment and predict the trajectories of surrounding vehicles during\noperation, respectively. In recent years, emerging data-driven deep\nlearning-based methods have shown great promise for these two tasks in learning\nvarious traffic scenarios and improving average performance without assuming\nphysical models. However, it remains a challenging problem for these methods to\nensure that the generated/predicted trajectories are physically realistic. This\nchallenge arises because learning-based approaches often function as opaque\nblack boxes and do not adhere to physical laws. Conversely, existing\nmodel-based methods provide physically feasible results but are constrained by\npredefined model structures, limiting their capabilities to address complex\nscenarios. To address the limitations of these two types of approaches, we\npropose a new method that integrates kinematic knowledge into neural stochastic\ndifferential equations (SDE) and designs a variational autoencoder based on\nthis latent kinematics-aware SDE (LK-SDE) to generate vehicle motions.\nExperimental results demonstrate that our method significantly outperforms both\nmodel-based and learning-based baselines in producing physically realistic and\nprecisely controllable vehicle trajectories. Additionally, it performs well in\npredicting unobservable physical variables in the latent space.\n","authors":["Ruochen Jiao","Yixuan Wang","Xiangguo Liu","Chao Huang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.09317v2.pdf","comment":"8 pages, conference paper in motion generation"},{"id":"http://arxiv.org/abs/2403.15569v1","updated":"2024-03-22T18:47:54Z","published":"2024-03-22T18:47:54Z","title":"Music to Dance as Language Translation using Sequence Models","summary":"  Synthesising appropriate choreographies from music remains an open problem.\nWe introduce MDLT, a novel approach that frames the choreography generation\nproblem as a translation task. Our method leverages an existing data set to\nlearn to translate sequences of audio into corresponding dance poses. We\npresent two variants of MDLT: one utilising the Transformer architecture and\nthe other employing the Mamba architecture. We train our method on AIST++ and\nPhantomDance data sets to teach a robotic arm to dance, but our method can be\napplied to a full humanoid robot. Evaluation metrics, including Average Joint\nError and Frechet Inception Distance, consistently demonstrate that, when given\na piece of music, MDLT excels at producing realistic and high-quality\nchoreography. The code can be found at github.com/meowatthemoon/MDLT.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2403.15569v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.15389v1","updated":"2024-03-22T17:59:58Z","published":"2024-03-22T17:59:58Z","title":"DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data","summary":"  Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.\n","authors":["Hanrong Ye","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15389v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15388v1","updated":"2024-03-22T17:59:52Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v1.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2312.10070v2","updated":"2024-03-22T17:59:09Z","published":"2023-12-06T10:47:53Z","title":"Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting","summary":"  We present a dense simultaneous localization and mapping (SLAM) method that\nuses 3D Gaussians as a scene representation. Our approach enables\ninteractive-time reconstruction and photo-realistic rendering from real-world\nsingle-camera RGBD videos. To this end, we propose a novel effective strategy\nfor seeding new Gaussians for newly explored areas and their effective online\noptimization that is independent of the scene size and thus scalable to larger\nscenes. This is achieved by organizing the scene into sub-maps which are\nindependently optimized and do not need to be kept in memory. We further\naccomplish frame-to-model camera tracking by minimizing photometric and\ngeometric losses between the input and rendered frames. The Gaussian\nrepresentation allows for high-quality photo-realistic real-time rendering of\nreal-world scenes. Evaluation on synthetic and real-world datasets demonstrates\ncompetitive or superior performance in mapping, tracking, and rendering\ncompared to existing neural dense SLAM methods.\n","authors":["Vladimir Yugay","Yue Li","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15383v1","updated":"2024-03-22T17:59:01Z","published":"2024-03-22T17:59:01Z","title":"ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars","summary":"  Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.\n","authors":["Zhenwei Wang","Tengfei Wang","Gerhard Hancke","Ziwei Liu","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2403.15383v1.pdf","comment":"Project page: https://3dthemestation.github.io/"},{"id":"http://arxiv.org/abs/2403.15382v1","updated":"2024-03-22T17:58:59Z","published":"2024-03-22T17:58:59Z","title":"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects","summary":"  We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.\n","authors":["Ruining Li","Chuanxia Zheng","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2403.15382v1.pdf","comment":"Project page: https://dragapart.github.io/"},{"id":"http://arxiv.org/abs/2403.15378v1","updated":"2024-03-22T17:58:16Z","published":"2024-03-22T17:58:16Z","title":"Long-CLIP: Unlocking the Long-Text Capability of CLIP","summary":"  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.\n","authors":["Beichen Zhang","Pan Zhang","Xiaoyi Dong","Yuhang Zang","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15378v1.pdf","comment":"All codes and models are publicly available at\n  https://github.com/beichenzbc/Long-CLIP"},{"id":"http://arxiv.org/abs/2403.15377v1","updated":"2024-03-22T17:57:42Z","published":"2024-03-22T17:57:42Z","title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding","summary":"  We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.\n","authors":["Yi Wang","Kunchang Li","Xinhao Li","Jiashuo Yu","Yinan He","Guo Chen","Baoqi Pei","Rongkun Zheng","Jilan Xu","Zun Wang","Yansong Shi","Tianxiang Jiang","Songze Li","Hongjie Zhang","Yifei Huang","Yu Qiao","Yali Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15377v1.pdf","comment":"a technical report about video understanding"},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.14617v2","updated":"2024-03-22T17:45:52Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v2.pdf","comment":"Project page at https://videoshop-editing.github.io/"},{"id":"http://arxiv.org/abs/2403.15361v1","updated":"2024-03-22T17:23:37Z","published":"2024-03-22T17:23:37Z","title":"Learning Topological Representations for Deep Image Understanding","summary":"  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n","authors":["Xiaoling Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15361v1.pdf","comment":"Ph.D. thesis from Stony Brook University. This thesis includes works\n  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,\n  arXiv:2206.01742"},{"id":"http://arxiv.org/abs/2403.15360v1","updated":"2024-03-22T17:22:56Z","published":"2024-03-22T17:22:56Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series","summary":"  Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.\n","authors":["Badri N. Patro","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.15360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15356v1","updated":"2024-03-22T17:11:47Z","published":"2024-03-22T17:11:47Z","title":"Neural Plasticity-Inspired Foundation Model for Observing the Earth\n  Crossing Modalities","summary":"  The development of foundation models has revolutionized our ability to\ninterpret the Earth's surface using satellite observational data. Traditional\nmodels have been siloed, tailored to specific sensors or data types like\noptical, radar, and hyperspectral, each with its own unique characteristics.\nThis specialization hinders the potential for a holistic analysis that could\nbenefit from the combined strengths of these diverse data sources. Our novel\napproach introduces the Dynamic One-For-All (DOFA) model, leveraging the\nconcept of neural plasticity in brain science to integrate various data\nmodalities into a single framework adaptively. This dynamic hypernetwork,\nadjusting to different wavelengths, enables a single versatile Transformer\njointly trained on data from five sensors to excel across 12 distinct Earth\nobservation tasks, including sensors never seen during pretraining. DOFA's\ninnovative design offers a promising leap towards more accurate, efficient, and\nunified Earth observation analysis, showcasing remarkable adaptability and\nperformance in harnessing the potential of multimodal Earth observation data.\n","authors":["Zhitong Xiong","Yi Wang","Fahong Zhang","Adam J. Stewart","Joëlle Hanna","Damian Borth","Ioannis Papoutsis","Bertrand Le Saux","Gustau Camps-Valls","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.15356v1.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.15353v1","updated":"2024-03-22T17:08:03Z","published":"2024-03-22T17:08:03Z","title":"Fully automated workflow for the design of patient-specific orthopaedic\n  implants: application to total knee arthroplasty","summary":"  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing\npain and improving mobility. While arthroplasty has known several technical\nimprovements, a significant share of patients are still unsatisfied with their\nsurgery. Personalised arthroplasty improves surgical outcomes however current\nsolutions require delays, making it difficult to integrate in clinical routine.\nWe propose a fully automated workflow to design patient-specific implants,\npresented for total knee arthroplasty, the most widely performed arthroplasty\nin the world nowadays.\n  The proposed pipeline first uses artificial neural networks to segment the\nproximal and distal extremities of the femur and tibia. Then the full bones are\nreconstructed using augmented statistical shape models, combining shape and\nlandmarks information. Finally, 77 morphological parameters are computed to\ndesign patient-specific implants. The developed workflow has been trained using\n91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in\nterms of accuracy and execution time.\n  The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$\nfor the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 5 minutes.\n  The proposed workflow allows for a fast and reliable personalisation of knee\nimplants, directly from the patient CT image without requiring any manual\nintervention. It establishes a patient-specific pre-operative planning for TKA\nin a very short time making it easily available for all patients. Combined with\nefficient implant manufacturing techniques, this solution could help answer the\ngrowing number of arthroplasties while reducing complications and improving the\npatients' satisfaction.\n","authors":["Aziliz Guezou-Philippe","Arnaud Clavé","Ehouarn Maguet","Ludivine Maintier","Charles Garraud","Jean-Rassaire Fouefack","Valérie Burdin","Eric Stindel","Guillaume Dardenne"],"pdf_url":"https://arxiv.org/pdf/2403.15353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14125v3","updated":"2024-03-22T17:06:53Z","published":"2023-12-21T18:46:41Z","title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation","summary":"  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n","authors":["Dan Kondratyuk","Lijun Yu","Xiuye Gu","José Lezama","Jonathan Huang","Grant Schindler","Rachel Hornung","Vighnesh Birodkar","Jimmy Yan","Ming-Chang Chiu","Krishna Somandepalli","Hassan Akbari","Yair Alon","Yong Cheng","Josh Dillon","Agrim Gupta","Meera Hahn","Anja Hauth","David Hendon","Alonso Martinez","David Minnen","Mikhail Sirotenko","Kihyuk Sohn","Xuan Yang","Hartwig Adam","Ming-Hsuan Yang","Irfan Essa","Huisheng Wang","David A. Ross","Bryan Seybold","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.14125v3.pdf","comment":"Project page: http://sites.research.google/videopoet/"},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10115v2","updated":"2024-03-22T16:46:36Z","published":"2023-12-15T09:57:21Z","title":"SkySense: A Multi-Modal Remote Sensing Foundation Model Towards\n  Universal Interpretation for Earth Observation Imagery","summary":"  Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense\npotential towards a generic model for Earth Observation. Nevertheless, these\nworks primarily focus on a single modality without temporal and geo-context\nmodeling, hampering their capabilities for diverse tasks. In this study, we\npresent SkySense, a generic billion-scale model, pre-trained on a curated\nmulti-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal\nsequences. SkySense incorporates a factorized multi-modal spatiotemporal\nencoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)\ndata as input. This encoder is pre-trained by our proposed Multi-Granularity\nContrastive Learning to learn representations across different modal and\nspatial granularities. To further enhance the RSI representations by the\ngeo-context clue, we introduce Geo-Context Prototype Learning to learn\nregion-aware prototypes upon RSI's multi-modal spatiotemporal features. To our\nbest knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules\ncan be flexibly combined or used individually to accommodate various tasks. It\ndemonstrates remarkable generalization capabilities on a thorough evaluation\nencompassing 16 datasets over 7 tasks, from single- to multi-modal, static to\ntemporal, and classification to localization. SkySense surpasses 18 recent\nRSFMs in all test scenarios. Specifically, it outperforms the latest models\nsuch as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and\n3.61% on average respectively. We will release the pre-trained weights to\nfacilitate future research and Earth Observation applications.\n","authors":["Xin Guo","Jiangwei Lao","Bo Dang","Yingying Zhang","Lei Yu","Lixiang Ru","Liheng Zhong","Ziyuan Huang","Kang Wu","Dingxiang Hu","Huimei He","Jian Wang","Jingdong Chen","Ming Yang","Yongjun Zhang","Yansheng Li"],"pdf_url":"https://arxiv.org/pdf/2312.10115v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2312.00094v2","updated":"2024-03-22T16:38:34Z","published":"2023-11-30T13:07:19Z","title":"Fast ODE-based Sampling for Diffusion Models in Around 5 Steps","summary":"  Sampling from diffusion models can be treated as solving the corresponding\nordinary differential equations (ODEs), with the aim of obtaining an accurate\nsolution with as few number of function evaluations (NFE) as possible.\nRecently, various fast samplers utilizing higher-order ODE solvers have emerged\nand achieved better performance than the initial first-order one. However,\nthese numerical methods inherently result in certain approximation errors,\nwhich significantly degrades sample quality with extremely small NFE (e.g.,\naround 5). In contrast, based on the geometric observation that each sampling\ntrajectory almost lies in a two-dimensional subspace embedded in the ambient\nspace, we propose Approximate MEan-Direction Solver (AMED-Solver) that\neliminates truncation errors by directly learning the mean direction for fast\ndiffusion sampling. Besides, our method can be easily used as a plugin to\nfurther improve existing ODE-based samplers. Extensive experiments on image\nsynthesis with the resolution ranging from 32 to 512 demonstrate the\neffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,\n10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is\navailable at https://github.com/zju-pi/diff-sampler.\n","authors":["Zhenyu Zhou","Defang Chen","Can Wang","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00094v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14520v2","updated":"2024-03-22T16:35:49Z","published":"2024-03-21T16:17:57Z","title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference","summary":"  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n","authors":["Han Zhao","Min Zhang","Wei Zhao","Pengxiang Ding","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15330v1","updated":"2024-03-22T16:35:38Z","published":"2024-03-22T16:35:38Z","title":"Selectively Informative Description can Reduce Undesired Embedding\n  Entanglements in Text-to-Image Personalization","summary":"  In text-to-image personalization, a timely and crucial challenge is the\ntendency of generated images overfitting to the biases present in the reference\nimages. We initiate our study with a comprehensive categorization of the biases\ninto background, nearby-object, tied-object, substance (in style\nre-contextualization), and pose biases. These biases manifest in the generated\nimages due to their entanglement into the subject embedding. This undesired\nembedding entanglement not only results in the reflection of biases from the\nreference images into the generated images but also notably diminishes the\nalignment of the generated images with the given generation prompt. To address\nthis challenge, we propose SID~(Selectively Informative Description), a text\ndescription strategy that deviates from the prevalent approach of only\ncharacterizing the subject's class identification. SID is generated utilizing\nmultimodal GPT-4 and can be seamlessly integrated into optimization-based\nmodels. We present comprehensive experimental results along with analyses of\ncross-attention maps, subject-alignment, non-subject-disentanglement, and\ntext-alignment.\n","authors":["Jimyeong Kim","Jungwon Park","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.15330v1.pdf","comment":"Published at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.04690v2","updated":"2024-03-22T16:26:40Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v2.pdf","comment":"Project page: https://github.com/SHI-Labs/NATTEN"},{"id":"http://arxiv.org/abs/2403.15317v1","updated":"2024-03-22T16:11:29Z","published":"2024-03-22T16:11:29Z","title":"Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for\n  Weakly Semi-supervised 3D Object Detection","summary":"  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization.In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n","authors":["Hongzhi Gao","Zheng Chen","Zehui Chen","Lin Chen","Jiaming Liu","Shanghang Zhang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15317v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.15316v1","updated":"2024-03-22T16:10:38Z","published":"2024-03-22T16:10:38Z","title":"Ultrasound Imaging based on the Variance of a Diffusion Restoration\n  Model","summary":"  Despite today's prevalence of ultrasound imaging in medicine, ultrasound\nsignal-to-noise ratio is still affected by several sources of noise and\nartefacts. Moreover, enhancing ultrasound image quality involves balancing\nconcurrent factors like contrast, resolution, and speckle preservation.\nRecently, there has been progress in both model-based and learning-based\napproaches addressing the problem of ultrasound image reconstruction. Bringing\nthe best from both worlds, we propose a hybrid reconstruction method combining\nan ultrasound linear direct model with a learning-based prior coming from a\ngenerative Denoising Diffusion model. More specifically, we rely on the\nunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model\n(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this\npaper proposes an empirical model to characterize the stochasticity of\ndiffusion reconstruction of ultrasound images, and shows the interest of its\nvariance as an echogenicity map estimator. We conduct experiments on synthetic,\nin-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging\napproach in achieving high-quality image reconstructions from single plane-wave\nacquisitions and in comparison to state-of-the-art methods.\n","authors":["Yuxin Zhang","Clément Huneau","Jérôme Idier","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2403.15316v1.pdf","comment":"5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap\n  with arXiv:2310.20618"},{"id":"http://arxiv.org/abs/2403.15314v1","updated":"2024-03-22T16:06:43Z","published":"2024-03-22T16:06:43Z","title":"Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel\n  Segmentation","summary":"  Personalized 3D vascular models can aid in a range of diagnostic, prognostic,\nand treatment-planning tasks relevant to cardiovascular disease management.\nDeep learning provides a means to automatically obtain such models. Ideally, a\nuser should have control over the exact region of interest (ROI) to be included\nin a vascular model, and the model should be watertight and highly accurate. To\nthis end, we propose a combination of a global controller leveraging voxel mask\nsegmentations to provide boundary conditions for vessels of interest to a\nlocal, iterative vessel segmentation model. We introduce the preservation of\nscale- and rotational symmetries in the local segmentation model, leading to\ngeneralisation to vessels of unseen sizes and orientations. Combined with the\nglobal controller, this enables flexible 3D vascular model building, without\nadditional retraining. We demonstrate the potential of our method on a dataset\ncontaining abdominal aortic aneurysms (AAAs). Our method performs on par with a\nstate-of-the-art segmentation model in the segmentation of AAAs, iliac arteries\nand renal arteries, while providing a watertight, smooth surface segmentation.\nMoreover, we demonstrate that by adapting the global controller, we can easily\nextend vessel sections in the 3D model.\n","authors":["Patryk Rygiel","Dieuwertje Alblas","Christoph Brune","Kak Khee Yeung","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.15314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15313v1","updated":"2024-03-22T16:06:05Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  Accurate detection and tracking of surrounding objects is essential to enable\nself-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have\nset the benchmark for high performance, the appeal of camera-only solutions\nlies in their cost-effectiveness. Notably, despite the prevalent use of Radio\nDetection and Ranging (RADAR) sensors in automotive systems, their potential in\n3D detection and tracking has been largely disregarded due to data sparsity and\nmeasurement noise. As a recent development, the combination of RADARs and\ncameras is emerging as a promising solution. This paper presents Camera-RADAR\n3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object\ndetection, and Multi-Object Tracking (MOT). Building upon the foundations of\nthe State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates\nsubstantial improvements in both detection and tracking capabilities, by\nincorporating the spatial and velocity information of the RADAR sensor.\nExperimental results demonstrate an absolute improvement in detection\nperformance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in\nAverage Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when\nleveraging both modalities. CR3DT bridges the gap between high-performance and\ncost-effective perception systems in autonomous driving, by capitalizing on the\nubiquitous presence of RADAR in automotive applications.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Kühne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.12505v2","updated":"2024-03-22T15:41:20Z","published":"2024-03-19T07:11:53Z","title":"Semantics, Distortion, and Style Matter: Towards Source-free UDA for\n  Panoramic Segmentation","summary":"  This paper addresses an interesting yet challenging problem -- source-free\nunsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic\nsegmentation -- given only a pinhole image-trained model (i.e., source) and\nunlabeled panoramic images (i.e., target). Tackling this problem is nontrivial\ndue to the semantic mismatches, style discrepancies, and inevitable distortion\nof panoramic images. To this end, we propose a novel method that utilizes\nTangent Projection (TP) as it has less distortion and meanwhile slits the\nequirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.\nBoth projections are shown effective in extracting knowledge from the source\nmodel. However, the distinct projection discrepancies between source and target\ndomains impede the direct knowledge transfer; thus, we propose a panoramic\nprototype adaptation module (PPAM) to integrate panoramic prototypes from the\nextracted knowledge for adaptation. We then impose the loss constraints on both\npredictions and prototypes and propose a cross-dual attention module (CDAM) at\nthe feature level to better align the spatial and channel characteristics\nacross the domains and projections. Both knowledge extraction and transfer\nprocesses are synchronously updated to reach the best performance. Extensive\nexperiments on the synthetic and real-world benchmarks, including outdoor and\nindoor scenarios, demonstrate that our method achieves significantly better\nperformance than prior SFUDA methods for pinhole-to-panoramic adaptation.\n","authors":["Xu Zheng","Pengyuan Zhou","Athanasios V. Vasilakos","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12505v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2310.00632v2","updated":"2024-03-22T15:38:53Z","published":"2023-10-01T10:06:01Z","title":"Win-Win: Training High-Resolution Vision Transformers from Two Windows","summary":"  Transformers have become the standard in state-of-the-art vision\narchitectures, achieving impressive performance on both image-level and dense\npixelwise tasks. However, training vision transformers for high-resolution\npixelwise tasks has a prohibitive cost. Typical solutions boil down to\nhierarchical architectures, fast and approximate attention, or training on\nlow-resolution crops. This latter solution does not constrain architectural\nchoices, but it leads to a clear performance drop when testing at resolutions\nsignificantly higher than that used for training, thus requiring ad-hoc and\nslow post-processing schemes. In this paper, we propose a novel strategy for\nefficient training and inference of high-resolution vision transformers. The\nkey principle is to mask out most of the high-resolution inputs during\ntraining, keeping only N random windows. This allows the model to learn local\ninteractions between tokens inside each window, and global interactions between\ntokens from different windows. As a result, the model can directly process the\nhigh-resolution input at test time without any special trick. We show that this\nstrategy is effective when using relative positional embedding such as rotary\nembeddings. It is 4 times faster to train than a full-resolution network, and\nit is straightforward to use at test time compared to existing approaches. We\napply this strategy to three dense prediction tasks with high-resolution data.\nFirst, we show on the task of semantic segmentation that a simple setting with\n2 windows performs best, hence the name of our method: Win-Win. Second, we\nconfirm this result on the task of monocular depth prediction. Third, we\nfurther extend it to the binocular task of optical flow, reaching\nstate-of-the-art performance on the Spring benchmark that contains Full-HD\nimages with an order of magnitude faster inference than the best competitor.\n","authors":["Vincent Leroy","Jerome Revaud","Thomas Lucas","Philippe Weinzaepfel"],"pdf_url":"https://arxiv.org/pdf/2310.00632v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.11170v2","updated":"2024-03-22T15:31:39Z","published":"2024-01-20T08:46:06Z","title":"Inducing High Energy-Latency of Large Vision-Language Models with\n  Verbose Images","summary":"  Large vision-language models (VLMs) such as GPT-4 have achieved exceptional\nperformance across various multi-modal tasks. However, the deployment of VLMs\nnecessitates substantial energy consumption and computational resources. Once\nattackers maliciously induce high energy consumption and latency time\n(energy-latency cost) during inference of VLMs, it will exhaust computational\nresources. In this paper, we explore this attack surface about availability of\nVLMs and aim to induce high energy-latency cost during inference of VLMs. We\nfind that high energy-latency cost during inference of VLMs can be manipulated\nby maximizing the length of generated sequences. To this end, we propose\nverbose images, with the goal of crafting an imperceptible perturbation to\ninduce VLMs to generate long sentences during inference. Concretely, we design\nthree loss objectives. First, a loss is proposed to delay the occurrence of\nend-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop\ngenerating further tokens. Moreover, an uncertainty loss and a token diversity\nloss are proposed to increase the uncertainty over each generated token and the\ndiversity among all tokens of the whole generated sequence, respectively, which\ncan break output dependency at token-level and sequence-level. Furthermore, a\ntemporal weight adjustment algorithm is proposed, which can effectively balance\nthese losses. Extensive experiments demonstrate that our verbose images can\nincrease the length of generated sequences by 7.87 times and 8.56 times\ncompared to original images on MS-COCO and ImageNet datasets, which presents\npotential challenges for various applications. Our code is available at\nhttps://github.com/KuofengGao/Verbose_Images.\n","authors":["Kuofeng Gao","Yang Bai","Jindong Gu","Shu-Tao Xia","Philip Torr","Zhifeng Li","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2401.11170v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2308.13712v3","updated":"2024-03-22T15:30:57Z","published":"2023-08-25T23:54:15Z","title":"Residual Denoising Diffusion Models","summary":"  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n","authors":["Jiawei Liu","Qiang Wang","Huijie Fan","Yinong Wang","Yandong Tang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2308.13712v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15272v1","updated":"2024-03-22T15:15:44Z","published":"2024-03-22T15:15:44Z","title":"WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization","summary":"  Despite the advancements in deep learning for camera relocalization tasks,\nobtaining ground truth pose labels required for the training process remains a\ncostly endeavor. While current weakly supervised methods excel in lightweight\nlabel generation, their performance notably declines in scenarios with sparse\nviews. In response to this challenge, we introduce WSCLoc, a system capable of\nbeing customized to various deep learning-based relocalization models to\nenhance their performance under weakly-supervised and sparse view conditions.\nThis is realized with two stages. In the initial stage, WSCLoc employs a\nmultilayer perceptron-based structure called WFT-NeRF to co-optimize image\nreconstruction quality and initial pose information. To ensure a stable\nlearning process, we incorporate temporal information as input. Furthermore,\ninstead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to\nexplicitly enforce a scale constraint. In the second stage, we co-optimize the\npre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by\nTime-Encoding based Random View Synthesis and supervised by inter-frame\ngeometric constraints that consider pose, depth, and RGB information. We\nvalidate our approaches on two publicly available datasets, one outdoor and one\nindoor. Our experimental results demonstrate that our weakly-supervised\nrelocalization solutions achieve superior pose estimation accuracy in\nsparse-view scenarios, comparable to state-of-the-art camera relocalization\nmethods. We will make our code publicly available.\n","authors":["Jialu Wang","Kaichen Zhou","Andrew Markham","Niki Trigoni"],"pdf_url":"https://arxiv.org/pdf/2403.15272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15260v1","updated":"2024-03-22T15:00:29Z","published":"2024-03-22T15:00:29Z","title":"Hyperbolic Metric Learning for Visual Outlier Detection","summary":"  Out-Of-Distribution (OOD) detection is critical to deploy deep learning\nmodels in safety-critical applications. However, the inherent hierarchical\nconcept structure of visual data, which is instrumental to OOD detection, is\noften poorly captured by conventional methods based on Euclidean geometry. This\nwork proposes a metric framework that leverages the strengths of Hyperbolic\ngeometry for OOD detection. Inspired by previous works that refine the decision\nboundary for OOD data with synthetic outliers, we extend this method to\nHyperbolic space. Interestingly, we find that synthetic outliers do not benefit\nOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we\nexplore the relationship between OOD detection performance and Hyperbolic\nembedding dimension, addressing practical concerns in resource-constrained\nenvironments. Extensive experiments show that our framework improves the FPR95\nfor OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and\nCIFAR-100 respectively compared to Euclidean methods.\n","authors":["Alvaro Gonzalez-Jimenez","Simone Lionetti","Dena Bazazian","Philippe Gottfrois","Fabian Gröger","Marc Pouly","Alexander Navarini"],"pdf_url":"https://arxiv.org/pdf/2403.15260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15249v1","updated":"2024-03-22T14:47:18Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v1.pdf","comment":"Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2403.15248v1","updated":"2024-03-22T14:46:51Z","published":"2024-03-22T14:46:51Z","title":"Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks","summary":"  Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.\n","authors":["Sudhir Sornapudi","Rajhans Singh"],"pdf_url":"https://arxiv.org/pdf/2403.15248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v2","updated":"2024-03-22T14:46:05Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.15245v1","updated":"2024-03-22T14:41:55Z","published":"2024-03-22T14:41:55Z","title":"Reasoning-Enhanced Object-Centric Learning for Videos","summary":"  Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperiment results on various datasets show that STATM can significantly\nenhance object-centric learning capabilities of slot-based video models.\n","authors":["Jian Li","Pu Ren","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15241v1","updated":"2024-03-22T14:34:17Z","published":"2024-03-22T14:34:17Z","title":"IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object\n  Detection","summary":"  Bird's eye view (BEV) representation has emerged as a dominant solution for\ndescribing 3D space in autonomous driving scenarios. However, objects in the\nBEV representation typically exhibit small sizes, and the associated point\ncloud context is inherently sparse, which leads to great challenges for\nreliable 3D perception. In this paper, we propose IS-Fusion, an innovative\nmultimodal fusion framework that jointly captures the Instance- and Scene-level\ncontextual information. IS-Fusion essentially differs from existing approaches\nthat only focus on the BEV scene-level fusion by explicitly incorporating\ninstance-level multimodal information, thus facilitating the instance-centric\ntasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF)\nmodule and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid\nand Grid-to-Region transformers to capture the multimodal scene context at\ndifferent granularities. IGF mines instance candidates, explores their\nrelationships, and aggregates the local multimodal context for each instance.\nThese instances then serve as guidance to enhance the scene feature and yield\nan instance-aware BEV representation. On the challenging nuScenes benchmark,\nIS-Fusion outperforms all the published multimodal works to date. Code is\navailable at: https://github.com/yinjunbo/IS-Fusion.\n","authors":["Junbo Yin","Jianbing Shen","Runnan Chen","Wei Li","Ruigang Yang","Pascal Frossard","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15241v1.pdf","comment":"Accepted to CVPR 2024; Code: https://github.com/yinjunbo/IS-Fusion"},{"id":"http://arxiv.org/abs/2403.15238v1","updated":"2024-03-22T14:32:02Z","published":"2024-03-22T14:32:02Z","title":"WEEP: A method for spatial interpretation of weakly supervised CNN\n  models in computational pathology","summary":"  Deep learning enables the modelling of high-resolution histopathology\nwhole-slide images (WSI). Weakly supervised learning of tile-level data is\ntypically applied for tasks where labels only exist on the patient or WSI level\n(e.g. patient outcomes or histological grading). In this context, there is a\nneed for improved spatial interpretability of predictions from such models. We\npropose a novel method, Wsi rEgion sElection aPproach (WEEP), for model\ninterpretation. It provides a principled yet straightforward way to establish\nthe spatial area of WSI required for assigning a particular prediction label.\nWe demonstrate WEEP on a binary classification task in the area of breast\ncancer computational pathology. WEEP is easy to implement, is directly\nconnected to the model-based decision process, and offers information relevant\nto both research and diagnostic applications.\n","authors":["Abhinav Sharma","Bojing Liu","Mattias Rantalainen"],"pdf_url":"https://arxiv.org/pdf/2403.15238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15234v1","updated":"2024-03-22T14:27:58Z","published":"2024-03-22T14:27:58Z","title":"Shadow Generation for Composite Image Using Diffusion model","summary":"  In the realm of image composition, generating realistic shadow for the\ninserted foreground remains a formidable challenge. Previous works have\ndeveloped image-to-image translation models which are trained on paired\ntraining data. However, they are struggling to generate shadows with accurate\nshapes and intensities, hindered by data scarcity and inherent task complexity.\nIn this paper, we resort to foundation model with rich prior knowledge of\nnatural shadow images. Specifically, we first adapt ControlNet to our task and\nthen propose intensity modulation modules to improve the shadow intensity.\nMoreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel\ndata acquisition pipeline. Experimental results on both DESOBA and DESOBAv2\ndatasets as well as real composite images demonstrate the superior capability\nof our model for shadow generation task. The dataset, code, and model are\nreleased at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.\n","authors":["Qingyang Liu","Junqi You","Jianting Wang","Xinhao Tao","Bo Zhang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15234v1.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.11376v2","updated":"2024-03-22T14:25:14Z","published":"2024-03-18T00:03:48Z","title":"ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation","summary":"  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: https://github.com/UARK-AICV/ShapeFormer\n","authors":["Minh Tran","Winston Bounsavy","Khoa Vo","Anh Nguyen","Tri Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2403.11376v2.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.15227v1","updated":"2024-03-22T14:20:54Z","published":"2024-03-22T14:20:54Z","title":"LeGO: Leveraging a Surface Deformation Network for Animatable Stylized\n  Face Generation with One Example","summary":"  Recent advances in 3D face stylization have made significant strides in few\nto zero-shot settings. However, the degree of stylization achieved by existing\nmethods is often not sufficient for practical applications because they are\nmostly based on statistical 3D Morphable Models (3DMM) with limited variations.\nTo this end, we propose a method that can produce a highly stylized 3D face\nmodel with desired topology. Our methods train a surface deformation network\nwith 3DMM and translate its domain to the target style using a paired exemplar.\nThe network achieves stylization of the 3D face mesh by mimicking the style of\nthe target using a differentiable renderer and directional CLIP losses.\nAdditionally, during the inference process, we utilize a Mesh Agnostic Encoder\n(MAGE) that takes deformation target, a mesh of diverse topologies as input to\nthe stylization process and encodes its shape into our latent space. The\nresulting stylized face model can be animated by commonly used 3DMM blend\nshapes. A set of quantitative and qualitative evaluations demonstrate that our\nmethod can produce highly stylized face meshes according to a given style and\noutput them in a desired topology. We also demonstrate example applications of\nour method including image-based stylized avatar generation, linear\ninterpolation of geometric styles, and facial animation of stylized avatars.\n","authors":["Soyeon Yoon","Kwan Yun","Kwanggyoon Seo","Sihun Cha","Jung Eun Yoo","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.15227v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.15218v1","updated":"2024-03-22T14:07:07Z","published":"2024-03-22T14:07:07Z","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations","summary":"  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n","authors":["Pranav Kulkarni","Adway Kanhere","Dharmam Savani","Andrew Chan","Devina Chatterjee","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2403.15218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06205v3","updated":"2024-03-22T14:05:33Z","published":"2024-03-10T13:04:01Z","title":"S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes","summary":"  Current 3D stylization methods often assume static scenes, which violates the\ndynamic nature of our real world. To address this limitation, we present\nS-DyRF, a reference-based spatio-temporal stylization method for dynamic neural\nradiance fields. However, stylizing dynamic 3D scenes is inherently challenging\ndue to the limited availability of stylized reference images along the temporal\naxis. Our key insight lies in introducing additional temporal cues besides the\nprovided reference. To this end, we generate temporal pseudo-references from\nthe given stylized reference. These pseudo-references facilitate the\npropagation of style information from the reference to the entire dynamic 3D\nscene. For coarse style transfer, we enforce novel views and times to mimic the\nstyle details present in pseudo-references at the feature level. To preserve\nhigh-frequency details, we create a collection of stylized temporal pseudo-rays\nfrom temporal pseudo-references. These pseudo-rays serve as detailed and\nexplicit stylization guidance for achieving fine style transfer. Experiments on\nboth synthetic and real-world datasets demonstrate that our method yields\nplausible stylized results of space-time view synthesis on dynamic 3D scenes.\n","authors":["Xingyi Li","Zhiguo Cao","Yizheng Wu","Kewei Wang","Ke Xian","Zhe Wang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.06205v3.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://xingyi-li.github.io/s-dyrf/"},{"id":"http://arxiv.org/abs/2402.00631v2","updated":"2024-03-22T14:00:37Z","published":"2024-01-31T11:52:33Z","title":"Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity\n  Personalized Diffusion Generation","summary":"  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable\nDiffusion Model, have made significant progress in generating diverse and\nhigh-quality images using text prompts alone. However, when non-famous users\nrequire personalized image generation for their identities (IDs), the T2I\nmodels fail to accurately generate their ID-related images. The main problem is\nthat pre-trained T2I models do not learn the mapping between the new ID prompts\nand their corresponding visual content. The previous methods either failed to\naccurately fit the face region or lost the interactive generative ability with\nother existing concepts in T2I models. In other words, they are unable to\ngenerate T2I-aligned and semantic-fidelity images for the given prompts with\nother concepts such as scenes (``Eiffel Tower''), actions (``holding a\nbasketball''), and facial attributes (``eyes closed''). In this paper, we focus\non inserting accurate and interactive ID embedding into the Stable Diffusion\nModel for semantic-fidelity personalized generation. We address this challenge\nfrom two perspectives: face-wise region fitting and semantic-fidelity token\noptimization. Specifically, we first visualize the attention overfit problem\nand propose a face-wise attention loss to fit the face region instead of\nentangling ID-unrelated information, such as face layout and background. This\nkey trick significantly enhances the ID accuracy and interactive generative\nability with other existing concepts. Then, we optimize one ID representation\nas multiple per-stage tokens where each token contains two disentangled\nfeatures. This expansion of the textual conditioning space improves\nsemantic-fidelity control. Extensive experiments validate that our results\nexhibit superior ID accuracy, text-based manipulation ability, and\ngeneralization compared to previous methods.\n","authors":["Yang Li","Songlin Yang","Wei Wang","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2402.00631v2.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.15212v1","updated":"2024-03-22T13:55:52Z","published":"2024-03-22T13:55:52Z","title":"GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition","summary":"  Skeleton-based action recognition (SAR) in videos is an important but\nchallenging task in computer vision. The recent state-of-the-art models for SAR\nare primarily based on graph convolutional neural networks (GCNs), which are\npowerful in extracting the spatial information of skeleton data. However, it is\nyet clear that such GCN-based models can effectively capture the temporal\ndynamics of human action sequences. To this end, we propose the DevLSTM module,\nwhich exploits the path development -- a principled and parsimonious\nrepresentation for sequential data by leveraging the Lie group structure. The\npath development, originated from Rough path theory, can effectively capture\nthe order of events in high-dimensional stream data with massive dimension\nreduction and consequently enhance the LSTM module substantially. Our proposed\nG-DevLSTM module can be conveniently plugged into the temporal graph,\ncomplementing existing advanced GCN-based models. Our empirical studies on the\nNTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid\nmodel significantly outperforms the current best-performing methods in SAR\ntasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.\n","authors":["Lei Jiang","Weixin Yang","Xin Zhang","Hao Ni"],"pdf_url":"https://arxiv.org/pdf/2403.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06020v2","updated":"2024-03-22T13:51:55Z","published":"2024-03-09T21:45:31Z","title":"Multi-conditioned Graph Diffusion for Neural Architecture Search","summary":"  Neural architecture search automates the design of neural network\narchitectures usually by exploring a large and thus complex architecture search\nspace. To advance the architecture search, we present a graph diffusion-based\nNAS approach that uses discrete conditional graph diffusion processes to\ngenerate high-performing neural network architectures. We then propose a\nmulti-conditioned classifier-free guidance approach applied to graph diffusion\nnetworks to jointly impose constraints such as high accuracy and low hardware\nlatency. Unlike the related work, our method is completely differentiable and\nrequires only a single model training. In our evaluations, we show promising\nresults on six standard benchmarks, yielding novel and unique architectures at\na fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we\ndemonstrate the generalisability and efficiency of our method through\nexperiments on ImageNet dataset.\n","authors":["Rohan Asthana","Joschua Conrad","Youssef Dawoud","Maurits Ortmanns","Vasileios Belagiannis"],"pdf_url":"https://arxiv.org/pdf/2403.06020v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2403.15209v1","updated":"2024-03-22T13:50:27Z","published":"2024-03-22T13:50:27Z","title":"MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection","summary":"  Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in obvious\ncases, especially due to the modality bias learned from statistically biased\ndatasets. From these problems, we anticipate that maybe understanding the\ncomplementary information itself is difficult to achieve from vision-only\nmodels. Accordingly, we propose a novel Multispectral Chain-of-Thought\nDetection (MSCoTDet) framework, which incorporates Large Language Models (LLMs)\nto understand the complementary information at the semantic level and further\nenhance the fusion process. Specifically, we generate text descriptions of the\npedestrian in each RGB and thermal modality and design a Multispectral\nChain-of-Thought (MSCoT) prompting, which models a step-by-step process to\nfacilitate cross-modal reasoning at the semantic level and perform accurate\ndetection. Moreover, we design a Language-driven Multi-modal Fusion (LMF)\nstrategy that enables fusing vision-driven and language-driven detections.\nExtensive experiments validate that MSCoTDet improves multispectral pedestrian\ndetection.\n","authors":["Taeheon Kim","Sangyun Chung","Damin Yeom","Youngjoon Yu","Hak Gu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.15209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15203v1","updated":"2024-03-22T13:46:51Z","published":"2024-03-22T13:46:51Z","title":"DITTO: Demonstration Imitation by Trajectory Transformation","summary":"  Teaching robots new skills quickly and conveniently is crucial for the\nbroader adoption of robotic systems. In this work, we address the problem of\none-shot imitation from a single human demonstration, given by an RGB-D video\nrecording through a two-stage process. In the first stage which is offline, we\nextract the trajectory of the demonstration. This entails segmenting\nmanipulated objects and determining their relative motion in relation to\nsecondary objects such as containers. Subsequently, in the live online\ntrajectory generation stage, we first \\mbox{re-detect} all objects, then we\nwarp the demonstration trajectory to the current scene, and finally, we trace\nthe trajectory with the robot. To complete these steps, our method makes\nleverages several ancillary models, including those for segmentation, relative\nobject pose estimation, and grasp prediction. We systematically evaluate\ndifferent combinations of correspondence and re-detection methods to validate\nour design decision across a diverse range of tasks. Specifically, we collect\ndemonstrations of ten different tasks including pick-and-place tasks as well as\narticulated object manipulation. Finally, we perform extensive evaluations on a\nreal robot system to demonstrate the effectiveness and utility of our approach\nin real-world scenarios. We make the code publicly available at\nhttp://ditto.cs.uni-freiburg.de.\n","authors":["Nick Heppert","Max Argus","Tim Welschehold","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15203v1.pdf","comment":"8 pages, 4 figures, 3 tables, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15194v1","updated":"2024-03-22T13:27:57Z","published":"2024-03-22T13:27:57Z","title":"Your Image is My Video: Reshaping the Receptive Field via Image-To-Video\n  Differentiable AutoAugmentation and Fusion","summary":"  The landscape of deep learning research is moving towards innovative\nstrategies to harness the true potential of data. Traditionally, emphasis has\nbeen on scaling model architectures, resulting in large and complex neural\nnetworks, which can be difficult to train with limited computational resources.\nHowever, independently of the model size, data quality (i.e. amount and\nvariability) is still a major factor that affects model generalization. In this\nwork, we propose a novel technique to exploit available data through the use of\nautomatic data augmentation for the tasks of image classification and semantic\nsegmentation. We introduce the first Differentiable Augmentation Search method\n(DAS) to generate variations of images that can be processed as videos.\nCompared to previous approaches, DAS is extremely fast and flexible, allowing\nthe search on very large search spaces in less than a GPU day. Our intuition is\nthat the increased receptive field in the temporal dimension provided by DAS\ncould lead to benefits also to the spatial receptive field. More specifically,\nwe leverage DAS to guide the reshaping of the spatial receptive field by\nselecting task-dependant transformations. As a result, compared to standard\naugmentation alternatives, we improve in terms of accuracy on ImageNet,\nCifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when\nplugging-in our DAS over different light-weight video backbones.\n","authors":["Sofia Casarin","Cynthia I. Ugwu","Sergio Escalera","Oswald Lanz"],"pdf_url":"https://arxiv.org/pdf/2403.15194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v2","updated":"2024-03-22T13:25:53Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v2.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.15192v1","updated":"2024-03-22T13:24:50Z","published":"2024-03-22T13:24:50Z","title":"SFOD: Spiking Fusion Object Detector","summary":"  Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.\n","authors":["Yimeng Fan","Wei Zhang","Changsong Liu","Mingyang Li","Wenrui Lu"],"pdf_url":"https://arxiv.org/pdf/2403.15192v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2403.15182v1","updated":"2024-03-22T13:11:26Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability. In this article we focus on Euclidean equivariant\nPDE-G-CNNs where the feature maps are two dimensional throughout. We call this\nvariant of the framework a PDE-CNN. We list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals. Furthermore, we experimentally confirm for small\nnetworks that PDE-CNNs offer fewer parameters, better performance, and data\nefficiency in comparison to CNNs. We also investigate what effect the use of\ndifferent semifields has on the performance of the models.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08709v2","updated":"2024-03-22T12:55:14Z","published":"2023-04-18T02:45:18Z","title":"You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object\n  Tracking","summary":"  In the classical tracking-by-detection (TBD) paradigm, detection and tracking\nare separately and sequentially conducted, and data association must be\nproperly performed to achieve satisfactory tracking performance. In this paper,\na new end-to-end multi-object tracking framework is proposed, which integrates\nobject detection and multi-object tracking into a single model. The proposed\ntracking framework eliminates the complex data association process in the\nclassical TBD paradigm, and requires no additional training. Secondly, the\nregression confidence of historical trajectories is investigated, and the\npossible states of a trajectory (weak object or strong object) in the current\nframe are predicted. Then, a confidence fusion module is designed to guide\nnon-maximum suppression for trajectories and detections to achieve ordered and\nrobust tracking. Thirdly, by integrating historical trajectory features, the\nregression performance of the detector is enhanced, which better reflects the\nocclusion and disappearance patterns of objects in real world. Lastly,\nextensive experiments are conducted on the commonly used KITTI and Waymo\ndatasets. The results show that the proposed framework can achieve robust\ntracking by using only a 2D detector and a 3D detector, and it is proven more\naccurate than many of the state-of-the-art TBD-based multi-modal tracking\nmethods. The source codes of the proposed method are available at\nhttps://github.com/wangxiyang2022/YONTD-MOT.\n","authors":["Xiyang Wang","Chunyun Fu","Jiawei He","Mingguang Huang","Ting Meng","Siyu Zhang","Hangning Zhou","Ziyao Xu","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.08709v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.15173v1","updated":"2024-03-22T12:54:33Z","published":"2024-03-22T12:54:33Z","title":"LSK3DNet: Towards Effective and Efficient 3D Perception with Large\n  Sparse Kernels","summary":"  Autonomous systems need to process large-scale, sparse, and irregular point\nclouds with limited compute resources. Consequently, it is essential to develop\nLiDAR perception methods that are both efficient and effective. Although\nnaively enlarging 3D kernel size can enhance performance, it will also lead to\na cubically-increasing overhead. Therefore, it is crucial to develop\nstreamlined 3D large kernel designs that eliminate redundant weights and work\neffectively with larger kernels. In this paper, we propose an efficient and\neffective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages\ndynamic pruning to amplify the 3D kernel size. Our method comprises two core\ncomponents: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight\nSelection (CWS). SDS dynamically prunes and regrows volumetric weights from the\nbeginning to learn a large sparse 3D kernel. It not only boosts performance but\nalso significantly reduces model size and computational cost. Moreover, CWS\nselects the most important channels for 3D convolution during training and\nsubsequently prunes the redundant channels to accelerate inference for 3D\nvision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark\ndatasets and five tracks compared with classical models and large kernel\ndesigns. Notably, LSK3DNet achieves the state-of-the-art performance on\nSemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with\nroughly 40% model size reduction and 60% computing operations reduction\ncompared to the naive large 3D kernel model.\n","authors":["Tuo Feng","Wenguan Wang","Fan Ma","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15173v1.pdf","comment":"Accepted at CVPR 2024; Project page:\n  https://github.com/FengZicai/LSK3DNet"},{"id":"http://arxiv.org/abs/2403.13248v2","updated":"2024-03-22T12:43:56Z","published":"2024-03-20T02:19:21Z","title":"Mora: Enabling Generalist Video Generation via A Multi-Agent Framework","summary":"  Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents.\n","authors":["Zhengqing Yuan","Ruoxi Chen","Zhaoxu Li","Haolong Jia","Lifang He","Chi Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.13248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07846v3","updated":"2024-03-22T12:41:50Z","published":"2023-09-14T16:40:44Z","title":"MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image\n  Acquisition Systems","summary":"  Neural Radiance Fields (NeRF) use multi-view images for 3D scene\nrepresentation, demonstrating remarkable performance. As one of the primary\nsources of multi-view images, multi-camera systems encounter challenges such as\nvarying intrinsic parameters and frequent pose changes. Most previous\nNeRF-based methods assume a unique camera and rarely consider multi-camera\nscenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic\nparameters still remain susceptible to suboptimal solutions when these\nparameters are poor initialized. In this paper, we propose MC-NeRF, a method\nthat enables joint optimization of both intrinsic and extrinsic parameters\nalongside NeRF. The method also supports each image corresponding to\nindependent camera parameters. First, we tackle coupling issue and the\ndegenerate case that arise from the joint optimization between intrinsic and\nextrinsic parameters. Second, based on the proposed solutions, we introduce an\nefficient calibration image acquisition scheme for multi-camera systems,\nincluding the design of calibration object. Finally, we present an end-to-end\nnetwork with training sequence that enables the estimation of intrinsic and\nextrinsic parameters, along with the rendering network. Furthermore,\nrecognizing that most existing datasets are designed for a unique camera, we\nconstruct a real multi-camera image acquisition system and create a\ncorresponding new dataset, which includes both simulated data and real-world\ncaptured images. Experiments confirm the effectiveness of our method when each\nimage corresponds to different camera parameters. Specifically, we use\nmulti-cameras, each with different intrinsic and extrinsic parameters in\nreal-world system, to achieve 3D scene representation without providing initial\nposes.\n","authors":["Yu Gao","Lutong Su","Hao Liang","Yufeng Yue","Yi Yang","Mengyin Fu"],"pdf_url":"https://arxiv.org/pdf/2309.07846v3.pdf","comment":"This manuscript is currently under review"},{"id":"http://arxiv.org/abs/2312.04964v2","updated":"2024-03-22T12:34:13Z","published":"2023-12-07T12:09:56Z","title":"ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and\n  Self-Prompting","summary":"  The long-tailed distribution problem in medical image analysis reflects a\nhigh prevalence of common conditions and a low prevalence of rare ones, which\nposes a significant challenge in developing a unified model capable of\nidentifying rare or novel tumor categories not encountered during training. In\nthis paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)\nbased on query-disentangling and self-prompting to segment unseen tumor\ncategories beyond the training set. ZePT disentangles the object queries into\ntwo subsets and trains them in two stages. Initially, it learns a set of\nfundamental queries for organ segmentation through an object-aware feature\ngrouping strategy, which gathers organ-level visual features. Subsequently, it\nrefines the other set of advanced queries that focus on the auto-generated\nvisual prompts for unseen tumor segmentation. Moreover, we introduce\nquery-knowledge alignment at the feature level to enhance each query's\ndiscriminative representation and generalizability. Extensive experiments on\nvarious tumor segmentation tasks demonstrate the performance superiority of\nZePT, which surpasses the previous counterparts and evidence the promising\nability for zero-shot tumor segmentation in real-world settings.\n","authors":["Yankai Jiang","Zhongzhen Huang","Rongzhao Zhang","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.04964v2.pdf","comment":"This paper has been accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07359v3","updated":"2024-03-22T12:33:51Z","published":"2024-03-12T06:45:34Z","title":"FSC: Few-point Shape Completion","summary":"  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n","authors":["Xianzu Wu","Xianfeng Wu","Tianyu Luan","Yajing Bai","Zhongyuan Lai","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.07359v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15161v1","updated":"2024-03-22T12:20:23Z","published":"2024-03-22T12:20:23Z","title":"FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos","summary":"  Digitising the 3D world into a clean, CAD model-based representation has\nimportant applications for augmented reality and robotics. Current\nstate-of-the-art methods are computationally intensive as they individually\nencode each detected object and optimise CAD alignments in a second stage. In\nthis work, we propose FastCAD, a real-time method that simultaneously retrieves\nand aligns CAD models for all objects in a given scene. In contrast to previous\nworks, we directly predict alignment parameters and shape embeddings. We\nachieve high-quality shape retrievals by learning CAD embeddings in a\ncontrastive learning framework and distilling those into FastCAD. Our\nsingle-stage method accelerates the inference time by a factor of 50 compared\nto other methods operating on RGB-D scans while outperforming them on the\nchallenging Scan2CAD alignment benchmark. Further, our approach collaborates\nseamlessly with online 3D reconstruction techniques. This enables the real-time\ngeneration of precise CAD model-based reconstructions from videos at 10 FPS.\nDoing so, we significantly improve the Scan2CAD alignment accuracy in the video\nsetting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to\n29.6%.\n","authors":["Florian Langer","Jihong Ju","Georgi Dikov","Gerhard Reitmayr","Mohsen Ghafoorian"],"pdf_url":"https://arxiv.org/pdf/2403.15161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15156v1","updated":"2024-03-22T12:11:06Z","published":"2024-03-22T12:11:06Z","title":"Infrastructure-Assisted Collaborative Perception in Automated Valet\n  Parking: A Safety Perspective","summary":"  Environmental perception in Automated Valet Parking (AVP) has been a\nchallenging task due to severe occlusions in parking garages. Although\nCollaborative Perception (CP) can be applied to broaden the field of view of\nconnected vehicles, the limited bandwidth of vehicular communications restricts\nits application. In this work, we propose a BEV feature-based CP network\narchitecture for infrastructure-assisted AVP systems. The model takes the\nroadside camera and LiDAR as optional inputs and adaptively fuses them with\nonboard sensors in a unified BEV representation. Autoencoder and downsampling\nare applied for channel-wise and spatial-wise dimension reduction, while\nsparsification and quantization further compress the feature map with little\nloss in data precision. Combining these techniques, the size of a BEV feature\nmap is effectively compressed to fit in the feasible data rate of the NR-V2X\nnetwork. With the synthetic AVP dataset, we observe that CP can effectively\nincrease perception performance, especially for pedestrians. Moreover, the\nadvantage of infrastructure-assisted CP is demonstrated in two typical\nsafety-critical scenarios in the AVP setting, increasing the maximum safe\ncruising speed by up to 3m/s in both scenarios.\n","authors":["Yukuan Jia","Jiawen Zhang","Shimeng Lu","Baokang Fan","Ruiqing Mao","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15156v1.pdf","comment":"7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring"},{"id":"http://arxiv.org/abs/2403.15152v1","updated":"2024-03-22T12:08:16Z","published":"2024-03-22T12:08:16Z","title":"A Multimodal Approach for Cross-Domain Image Retrieval","summary":"  Image generators are gaining vast amount of popularity and have rapidly\nchanged how digital content is created. With the latest AI technology, millions\nof high quality images are being generated by the public, which are constantly\nmotivating the research community to push the limits of generative models to\ncreate more complex and realistic images. This paper focuses on Cross-Domain\nImage Retrieval (CDIR) which can be used as an additional tool to inspect\ncollections of generated images by determining the level of similarity between\nimages in a dataset. An ideal retrieval system would be able to generalize to\nunseen complex images from multiple domains (e.g., photos, drawings and\npaintings). To address this goal, we propose a novel caption-matching approach\nthat leverages multimodal language-vision architectures pre-trained on large\ndatasets. The method is tested on DomainNet and Office-Home datasets and\nconsistently achieves state-of-the-art performance over the latest approaches\nin the literature for cross-domain image retrieval. In order to verify the\neffectiveness with AI-generated images, the method was also put to test with a\ndatabase composed by samples collected from Midjourney, which is a widely used\ngenerative platform for content creation.\n","authors":["Lucas Iijima","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2403.15152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15150v1","updated":"2024-03-22T12:06:40Z","published":"2024-03-22T12:06:40Z","title":"An In-Depth Analysis of Data Reduction Methods for Sustainable Deep\n  Learning","summary":"  In recent years, Deep Learning has gained popularity for its ability to solve\ncomplex classification tasks, increasingly delivering better results thanks to\nthe development of more accurate models, the availability of huge volumes of\ndata and the improved computational capabilities of modern computers. However,\nthese improvements in performance also bring efficiency problems, related to\nthe storage of datasets and models, and to the waste of energy and time\ninvolved in both the training and inference processes. In this context, data\nreduction can help reduce energy consumption when training a deep learning\nmodel. In this paper, we present up to eight different methods to reduce the\nsize of a tabular training dataset, and we develop a Python package to apply\nthem. We also introduce a representativeness metric based on topology to\nmeasure how similar are the reduced datasets and the full training dataset.\nAdditionally, we develop a methodology to apply these data reduction methods to\nimage datasets for object detection tasks. Finally, we experimentally compare\nhow these data reduction methods affect the representativeness of the reduced\ndataset, the energy consumption and the predictive performance of the model.\n","authors":["Víctor Toscano-Durán","Javier Perera-Lago","Eduardo Paluzo-Hidalgo","Rocío Gonzalez-Diaz","Miguel Ángel Gutierrez-Naranjo","Matteo Rucco"],"pdf_url":"https://arxiv.org/pdf/2403.15150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12550v2","updated":"2024-03-22T12:05:53Z","published":"2024-03-19T08:49:48Z","title":"RGBD GS-ICP SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) with dense representation plays\na key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)\napplications. Recent advancements in dense representation SLAM have highlighted\nthe potential of leveraging neural scene representation and 3D Gaussian\nrepresentation for high-fidelity spatial representation. In this paper, we\npropose a novel dense representation SLAM approach with a fusion of Generalized\nIterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast\nto existing methods, we utilize a single Gaussian map for both tracking and\nmapping, resulting in mutual benefits. Through the exchange of covariances\nbetween tracking and mapping processes with scale alignment techniques, we\nminimize redundant computations and achieve an efficient system. Additionally,\nwe enhance tracking accuracy and mapping quality through our keyframe selection\nmethods. Experimental results demonstrate the effectiveness of our approach,\nshowing an incredibly fast speed up to 107 FPS (for the entire system) and\nsuperior quality of the reconstructed map.\n","authors":["Seongbo Ha","Jiung Yeon","Hyeonwoo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15143v1","updated":"2024-03-22T11:53:03Z","published":"2024-03-22T11:53:03Z","title":"Modular Deep Active Learning Framework for Image Annotation: A Technical\n  Report for the Ophthalmo-AI Project","summary":"  Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.\n","authors":["Md Abdul Kadir","Hasan Md Tusfiqur Alam","Pascale Maul","Hans-Jürgen Profitlich","Moritz Wolf","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.15143v1.pdf","comment":"DFKI Technical Report"},{"id":"http://arxiv.org/abs/2403.15139v1","updated":"2024-03-22T11:48:09Z","published":"2024-03-22T11:48:09Z","title":"Deep Generative Model based Rate-Distortion for Image Downscaling\n  Assessment","summary":"  In this paper, we propose Image Downscaling Assessment by Rate-Distortion\n(IDA-RD), a novel measure to quantitatively evaluate image downscaling\nalgorithms. In contrast to image-based methods that measure the quality of\ndownscaled images, ours is process-based that draws ideas from rate-distortion\ntheory to measure the distortion incurred during downscaling. Our main idea is\nthat downscaling and super-resolution (SR) can be viewed as the encoding and\ndecoding processes in the rate-distortion model, respectively, and that a\ndownscaling algorithm that preserves more details in the resulting\nlow-resolution (LR) images should lead to less distorted high-resolution (HR)\nimages in SR. In other words, the distortion should increase as the downscaling\nalgorithm deteriorates. However, it is non-trivial to measure this distortion\nas it requires the SR algorithm to be blind and stochastic. Our key insight is\nthat such requirements can be met by recent SR algorithms based on deep\ngenerative models that can find all matching HR images for a given LR image on\ntheir learned image manifolds. Extensive experimental results show the\neffectiveness of our IDA-RD measure.\n","authors":["Yuanbang Liang","Bhavesh Garg","Paul L Rosin","Yipeng Qin"],"pdf_url":"https://arxiv.org/pdf/2403.15139v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11220v3","updated":"2024-03-22T11:42:40Z","published":"2024-03-17T13:43:10Z","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations","summary":"  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n","authors":["Yuwei Zhang","Yan Wu","Yanming Liu","Xinyue Peng"],"pdf_url":"https://arxiv.org/pdf/2403.11220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13408v2","updated":"2024-03-22T11:41:38Z","published":"2024-03-20T08:50:15Z","title":"S2DM: Sector-Shaped Diffusion Models for Video Generation","summary":"  Diffusion models have achieved great success in image generation. However,\nwhen leveraging this idea for video generation, we face significant challenges\nin maintaining the consistency and continuity across video frames. This is\nmainly caused by the lack of an effective framework to align frames of videos\nwith desired temporal features while preserving consistent semantic and\nstochastic features. In this work, we propose a novel Sector-Shaped Diffusion\nModel (S2DM) whose sector-shaped diffusion region is formed by a set of\nray-shaped reverse diffusion processes starting at the same noise point. S2DM\ncan generate a group of intrinsically related data sharing the same semantic\nand stochastic features while varying on temporal features with appropriate\nguided conditions. We apply S2DM to video generation tasks, and explore the use\nof optical flow as temporal conditions. Our experimental results show that S2DM\noutperforms many existing methods in the task of video generation without any\ntemporal-feature modelling modules. For text-to-video generation tasks where\ntemporal conditions are not explicitly given, we propose a two-stage generation\nstrategy which can decouple the generation of temporal features from\nsemantic-content features. We show that, without additional training, our model\nintegrated with another temporal conditions generative model can still achieve\ncomparable performance with existing works. Our results can be viewd at\nhttps://s2dm.github.io/S2DM/.\n","authors":["Haoran Lang","Yuxuan Ge","Zheng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.13408v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15132v1","updated":"2024-03-22T11:33:04Z","published":"2024-03-22T11:33:04Z","title":"Transfer CLIP for Generalizable Image Denoising","summary":"  Image denoising is a fundamental task in computer vision. While prevailing\ndeep learning-based supervised and self-supervised methods have excelled in\neliminating in-distribution noise, their susceptibility to out-of-distribution\n(OOD) noise remains a significant challenge. The recent emergence of\ncontrastive language-image pre-training (CLIP) model has showcased exceptional\ncapabilities in open-world image recognition and segmentation. Yet, the\npotential for leveraging CLIP to enhance the robustness of low-level tasks\nremains largely unexplored. This paper uncovers that certain dense features\nextracted from the frozen ResNet image encoder of CLIP exhibit\ndistortion-invariant and content-related properties, which are highly desirable\nfor generalizable denoising. Leveraging these properties, we devise an\nasymmetrical encoder-decoder denoising network, which incorporates dense\nfeatures including the noisy image and its multi-scale features from the frozen\nResNet encoder of CLIP into a learnable image decoder to achieve generalizable\ndenoising. The progressive feature augmentation strategy is further proposed to\nmitigate feature overfitting and improve the robustness of the learnable\ndecoder. Extensive experiments and comparisons conducted across diverse OOD\nnoises, including synthetic noise, real-world sRGB noise, and low-dose CT image\nnoise, demonstrate the superior generalization ability of our method.\n","authors":["Jun Cheng","Dong Liang","Shan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.15132v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.15127v1","updated":"2024-03-22T11:30:10Z","published":"2024-03-22T11:30:10Z","title":"Gradient-based Sampling for Class Imbalanced Semi-supervised Object\n  Detection","summary":"  Current semi-supervised object detection (SSOD) algorithms typically assume\nclass balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets\n(MS-COCO, etc). This assumption can be easily violated since real world\ndatasets can be extremely class imbalanced in nature, thus making the\nperformance of semi-supervised object detectors far from satisfactory. Besides,\nthe research for this problem in SSOD is severely under-explored. To bridge\nthis research gap, we comprehensively study the class imbalance problem for\nSSOD under more challenging scenarios, thus forming the first experimental\nsetting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet\neffective gradient-based sampling framework that tackles the class imbalance\nproblem from the perspective of two types of confirmation biases. To tackle\nconfirmation bias towards majority classes, the gradient-based reweighting and\ngradient-based thresholding modules leverage the gradients from each class to\nfully balance the influence of the majority and minority classes. To tackle the\nconfirmation bias from incorrect pseudo labels of minority classes, the\nclass-rebalancing sampling module resamples unlabeled data following the\nguidance of the gradient-based reweighting module. Experiments on three\nproposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that\nour method outperforms current class imbalanced object detectors by clear\nmargins, serving as a baseline for future research in CI-SSOD. Code will be\navailable at https://github.com/nightkeepers/CI-SSOD.\n","authors":["Jiaming Li","Xiangru Lin","Wei Zhang","Xiao Tan","Yingying Li","Junyu Han","Errui Ding","Jingdong Wang","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.15127v1.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2403.15124v1","updated":"2024-03-22T11:27:43Z","published":"2024-03-22T11:27:43Z","title":"EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic\n  Surgeries using Gaussian Splatting","summary":"  Precise camera tracking, high-fidelity 3D tissue reconstruction, and\nreal-time online visualization are critical for intrabody medical imaging\ndevices such as endoscopes and capsule robots. However, existing SLAM\n(Simultaneous Localization and Mapping) methods often struggle to achieve both\ncomplete high-quality surgical field reconstruction and efficient computation,\nrestricting their intraoperative applications among endoscopic surgeries. In\nthis paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic\nsurgeries, which integrates streamlined Gaussian representation and\ndifferentiable rasterization to facilitate over 100 fps rendering speed during\nonline camera tracking and tissue reconstructing. Extensive experiments show\nthat EndoGSLAM achieves a better trade-off between intraoperative availability\nand reconstruction quality than traditional or neural SLAM approaches, showing\ntremendous potential for endoscopic surgeries. The project page is at\nhttps://EndoGSLAM.loping151.com\n","authors":["Kailing Wang","Chen Yang","Yuehao Wang","Sikuang Li","Yan Wang","Qi Dou","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2403.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15121v1","updated":"2024-03-22T11:24:31Z","published":"2024-03-22T11:24:31Z","title":"SYNCS: Synthetic Data and Contrastive Self-Supervised Training for\n  Central Sulcus Segmentation","summary":"  Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with\nprofound societal impact. Identifying risk markers early is crucial for\nunderstanding disease progression and enabling preventive measures. The Danish\nHigh Risk and Resilience Study (VIA) focuses on understanding early disease\nprocesses, particularly in children with familial high risk (FHR).\nUnderstanding structural brain changes associated with these diseases during\nearly stages is essential for effective interventions. The central sulcus (CS)\nis a prominent brain landmark related to brain regions involved in motor and\nsensory processing. Analyzing CS morphology can provide valuable insights into\nneurodevelopmental abnormalities in the FHR group. However, segmenting the\ncentral sulcus (CS) presents challenges due to its variability, especially in\nadolescents. This study introduces two novel approaches to improve CS\nsegmentation: synthetic data generation to model CS variability and\nself-supervised pre-training with multi-task learning to adapt models to new\ncohorts. These methods aim to enhance segmentation performance across diverse\npopulations, eliminating the need for extensive preprocessing.\n","authors":["Vladyslav Zalevskyi","Kristoffer Hougaard Madsen"],"pdf_url":"https://arxiv.org/pdf/2403.15121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15119v1","updated":"2024-03-22T11:21:51Z","published":"2024-03-22T11:21:51Z","title":"An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic\n  Wild Person Re-Identification","summary":"  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n","authors":["Lei Zhang","Xiaowei Fu","Fuxiang Huang","Yi Yang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.15119v1.pdf","comment":"Accepted by IJCV in 2024"},{"id":"http://arxiv.org/abs/2403.15107v1","updated":"2024-03-22T10:51:31Z","published":"2024-03-22T10:51:31Z","title":"PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic\n  Manipulation","summary":"  Humans seemingly incorporate potential touch signals in their perception. Our\ngoal is to equip robots with a similar capability, which we term \\ourmodel.\n\\ourmodel aims to predict the expected touch signal based on a visual patch\nrepresenting the touched area. We frame this problem as the task of learning a\nlow-dimensional visual-tactile embedding, wherein we encode a depth patch from\nwhich we decode the tactile signal. To accomplish this task, we employ ReSkin,\nan inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we\ncollect and train PseudoTouch on a dataset comprising aligned tactile and\nvisual data pairs obtained through random touching of eight basic geometric\nshapes. We demonstrate the efficacy of PseudoTouch through its application to\ntwo downstream tasks: object recognition and grasp stability prediction. In the\nobject recognition task, we evaluate the learned embedding's performance on a\nset of five basic geometric shapes and five household objects. Using\nPseudoTouch, we achieve an object recognition accuracy 84% after just ten\ntouches, surpassing a proprioception baseline. For the grasp stability task, we\nuse ACRONYM labels to train and evaluate a grasp success predictor using\nPseudoTouch's predictions derived from virtual depth information. Our approach\nyields an impressive 32% absolute improvement in accuracy compared to the\nbaseline relying on partial point cloud data. We make the data, code, and\ntrained models publicly available at http://pseudotouch.cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Nick Heppert","Abdallah Ayman","Eugenio Chisari","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15107v1.pdf","comment":"8 pages, 7 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.15103v1","updated":"2024-03-22T10:42:25Z","published":"2024-03-22T10:42:25Z","title":"Improving cross-domain brain tissue segmentation in fetal MRI with\n  synthetic data","summary":"  Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)\nplays a crucial role in the study of in utero neurodevelopment. However,\nautomated tools face substantial domain shift challenges as they must be robust\nto highly heterogeneous clinical data, often limited in numbers and lacking\nannotations. Indeed, high variability of the fetal brain morphology, MRI\nacquisition parameters, and superresolution reconstruction (SR) algorithms\nadversely affect the model's performance when evaluated out-of-domain. In this\nwork, we introduce FetalSynthSeg, a domain randomization method to segment\nfetal brain MRI, inspired by SynthSeg. Our results show that models trained\nsolely on synthetic data outperform models trained on real data in out-ofdomain\nsettings, validated on a 120-subject cross-domain dataset. Furthermore, we\nextend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and\nreconstructed with novel SR models, showcasing robustness across different\nmagnetic field strengths and SR algorithms. Leveraging a generative synthetic\napproach, we tackle the domain shift problem in fetal brain MRI and offer\ncompelling prospects for applications in fields with limited and highly\nheterogeneous data.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Margaux Roulet","Jordina Aviles Verddera","Jana Hutter","Hamza Kebiri","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2403.15103v1.pdf","comment":"10 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15098v1","updated":"2024-03-22T10:36:50Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, \\textit{e.g.,} in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\n\\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}.\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00354v3","updated":"2024-03-22T10:36:47Z","published":"2023-09-30T12:17:36Z","title":"AI-Dentify: Deep learning for proximal caries detection on bitewing\n  x-ray -- HUNT4 Oral Health Study","summary":"  Background: Dental caries diagnosis requires the manual inspection of\ndiagnostic bitewing images of the patient, followed by a visual inspection and\nprobing of the identified dental pieces with potential lesions. Yet the use of\nartificial intelligence, and in particular deep-learning, has the potential to\naid in the diagnosis by providing a quick and informative analysis of the\nbitewing images.\n  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were\nannotated individually by six different experts, and used to train three\ndifferent object detection deep-learning architectures: RetinaNet (ResNet50),\nYOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197\nimages, annotated jointly by the same six dentist, was used for evaluation. A\nfive-fold cross validation scheme was used to evaluate the performance of the\nAI models.\n  Results: he trained models show an increase in average precision and\nF1-score, and decrease of false negative rate, with respect to the dental\nclinicians. When compared against the dental clinicians, the YOLOv5 model shows\nthe largest improvement, reporting 0.647 mean average precision, 0.548 mean\nF1-score, and 0.149 mean false negative rate. Whereas the best annotators on\neach of these metrics reported 0.299, 0.495, and 0.164 respectively.\n  Conclusion: Deep-learning models have shown the potential to assist dental\nprofessionals in the diagnosis of caries. Yet, the task remains challenging due\nto the artifacts natural to the bitewing images.\n","authors":["Javier Pérez de Frutos","Ragnhild Holden Helland","Shreya Desai","Line Cathrine Nymoen","Thomas Langø","Theodor Remman","Abhijit Sen"],"pdf_url":"https://arxiv.org/pdf/2310.00354v3.pdf","comment":"24 pages, 5 figure, 7 tables"},{"id":"http://arxiv.org/abs/2304.09793v2","updated":"2024-03-22T10:36:32Z","published":"2023-04-19T16:21:14Z","title":"Event-based Simultaneous Localization and Mapping: A Comprehensive\n  Survey","summary":"  In recent decades, visual simultaneous localization and mapping (vSLAM) has\ngained significant interest in both academia and industry. It estimates camera\nmotion and reconstructs the environment concurrently using visual sensors on a\nmoving robot. However, conventional cameras are limited by hardware, including\nmotion blur and low dynamic range, which can negatively impact performance in\nchallenging scenarios like high-speed motion and high dynamic range\nillumination. Recent studies have demonstrated that event cameras, a new type\nof bio-inspired visual sensor, offer advantages such as high temporal\nresolution, dynamic range, low power consumption, and low latency. This paper\npresents a timely and comprehensive review of event-based vSLAM algorithms that\nexploit the benefits of asynchronous and irregular event streams for\nlocalization and mapping tasks. The review covers the working principle of\nevent cameras and various event representations for preprocessing event data.\nIt also categorizes event-based vSLAM methods into four main categories:\nfeature-based, direct, motion-compensation, and deep learning methods, with\ndetailed discussions and practical guidance for each approach. Furthermore, the\npaper evaluates the state-of-the-art methods on various benchmarks,\nhighlighting current challenges and future opportunities in this emerging\nresearch area. A public repository will be maintained to keep track of the\nrapid developments in this field at\n{\\url{https://github.com/kun150kun/ESLAM-survey}}.\n","authors":["Kunping Huang","Sen Zhang","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2304.09793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14370v2","updated":"2024-03-22T10:26:33Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v2.pdf","comment":"Project page: https://synctweedies.github.io/"},{"id":"http://arxiv.org/abs/2402.15756v2","updated":"2024-03-22T10:19:06Z","published":"2024-02-24T08:07:48Z","title":"Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models\n  Revisited","summary":"  Conventional tracking paradigm takes in instantaneous measurements such as\nrange and bearing, and produces object tracks across time. In applications such\nas autonomous driving, lidar measurements in the form of point clouds are\nusually passed through a \"virtual sensor\" realized by a deep learning model, to\nproduce \"measurements\" such as bounding boxes, which are in turn ingested by a\ntracking module to produce object tracks. Very often multiple lidar sweeps are\naccumulated in a buffer to merge and become the input to the virtual sensor. We\nargue in this paper that such an input already contains temporal information,\nand therefore the virtual sensor output should also contain temporal\ninformation, not just instantaneous values for the time corresponding to the\nend of the buffer. In particular, we present the deep learning model called\nMULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,\na pair of bounding boxes at both the end time and the beginning time of the\ninput buffer. This is achieved with fairly straightforward changes in commonly\nused lidar detection models, and with only marginal extra processing, but the\nresulting symmetry is satisfying. Such paired detections make it possible not\nonly to construct rudimentary trackers fairly easily, but also to construct\nmore sophisticated trackers that can exploit the extra information conveyed by\nthe pair and be robust to choices of motion models and object birth/death\nmodels. We have conducted preliminary training and experimentation using Waymo\nOpen Dataset, which shows the efficacy of our proposed method.\n","authors":["Lingji Chen"],"pdf_url":"https://arxiv.org/pdf/2402.15756v2.pdf","comment":"My previous employer Motional is requiring a review and approval\n  process before I can publish this paper"},{"id":"http://arxiv.org/abs/2403.15089v1","updated":"2024-03-22T10:15:53Z","published":"2024-03-22T10:15:53Z","title":"IFSENet : Harnessing Sparse Iterations for Interactive Few-shot\n  Segmentation Excellence","summary":"  Training a computer vision system to segment a novel class typically requires\ncollecting and painstakingly annotating lots of images with objects from that\nclass. Few-shot segmentation techniques reduce the required number of images to\nlearn to segment a new class, but careful annotations of object boundaries are\nstill required. On the other hand, interactive segmentation techniques only\nfocus on incrementally improving the segmentation of one object at a time\n(typically, using clicks given by an expert) in a class-agnostic manner. We\ncombine the two concepts to drastically reduce the effort required to train\nsegmentation models for novel classes. Instead of trivially feeding interactive\nsegmentation masks as ground truth to a few-shot segmentation model, we propose\nIFSENet, which can accept sparse supervision on a single or few support images\nin the form of clicks to generate masks on support (training, at least clicked\nupon once) as well as query (test, never clicked upon) images. To trade-off\neffort for accuracy flexibly, the number of images and clicks can be\nincrementally added to the support set to further improve the segmentation of\nsupport as well as query images. The proposed model approaches the accuracy of\nprevious state-of-the-art few-shot segmentation models with considerably lower\nannotation effort (clicks instead of maps), when tested on Pascal and SBD\ndatasets on query images. It also works well as an interactive segmentation\nmethod on support images.\n","authors":["Shreyas Chandgothia","Ardhendu Sekhar","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2403.15089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15082v1","updated":"2024-03-22T10:06:31Z","published":"2024-03-22T10:06:31Z","title":"Cell Variational Information Bottleneck Network","summary":"  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n","authors":["Zhonghua Zhai","Chen Ju","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.15082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15068v1","updated":"2024-03-22T09:48:50Z","published":"2024-03-22T09:48:50Z","title":"Integrating multiscale topology in digital pathology with pyramidal\n  graph convolutional networks","summary":"  Graph convolutional networks (GCNs) have emerged as a powerful alternative to\nmultiple instance learning with convolutional neural networks in digital\npathology, offering superior handling of structural information across various\nspatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole\nslide images (WSI). However, graph message-passing algorithms often suffer from\noversmoothing when aggregating a large neighborhood. Hence, effective modeling\nof multi-range interactions relies on the careful construction of the graph.\nOur proposed multi-scale GCN (MS-GCN) tackles this issue by leveraging\ninformation across multiple magnification levels in WSIs. MS-GCN enables the\nsimultaneous modeling of long-range structural dependencies at lower\nmagnifications and high-resolution cellular details at higher magnifications,\nakin to analysis pipelines usually conducted by pathologists. The\narchitecture's unique configuration allows for the concurrent modeling of\nstructural patterns at lower magnifications and detailed cellular features at\nhigher ones, while also quantifying the contribution of each magnification\nlevel to the prediction. Through testing on different datasets, MS-GCN\ndemonstrates superior performance over existing single-magnification GCN\nmethods. The enhancement in performance and interpretability afforded by our\nmethod holds promise for advancing computational pathology models, especially\nin tasks requiring extensive spatial context.\n","authors":["Victor Ibañez","Przemyslaw Szostak","Quincy Wong","Konstanty Korski","Samaneh Abbasi-Sureshjani","Alvaro Gomariz"],"pdf_url":"https://arxiv.org/pdf/2403.15068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15064v1","updated":"2024-03-22T09:46:11Z","published":"2024-03-22T09:46:11Z","title":"Recent Trends in 3D Reconstruction of General Non-Rigid Scenes","summary":"  Reconstructing models of the real world, including 3D geometry, appearance,\nand motion of real scenes, is essential for computer graphics and computer\nvision. It enables the synthesizing of photorealistic novel views, useful for\nthe movie industry and AR/VR applications. It also facilitates the content\ncreation necessary in computer games and AR/VR by avoiding laborious manual\ndesign processes. Further, such models are fundamental for intelligent\ncomputing systems that need to interpret real-world scenes and actions to act\nand interact safely with the human world. Notably, the world surrounding us is\ndynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a\nseverely underconstrained and challenging problem. This state-of-the-art report\n(STAR) offers the reader a comprehensive summary of state-of-the-art techniques\nwith monocular and multi-view inputs such as data from RGB and RGB-D sensors,\namong others, conveying an understanding of different approaches, their\npotential applications, and promising further research directions. The report\ncovers 3D reconstruction of general non-rigid scenes and further addresses the\ntechniques for scene decomposition, editing and controlling, and generalizable\nand generative modeling. More specifically, we first review the common and\nfundamental concepts necessary to understand and navigate the field and then\ndiscuss the state-of-the-art techniques by reviewing recent approaches that use\ntraditional and machine-learning-based neural representations, including a\ndiscussion on the newly enabled applications. The STAR is concluded with a\ndiscussion of the remaining limitations and open challenges.\n","authors":["Raza Yunus","Jan Eric Lenssen","Michael Niemeyer","Yiyi Liao","Christian Rupprecht","Christian Theobalt","Gerard Pons-Moll","Jia-Bin Huang","Vladislav Golyanik","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2403.15064v1.pdf","comment":"42 pages, 18 figures, 5 tables; State-of-the-Art Report at\n  EUROGRAPHICS 2024"},{"id":"http://arxiv.org/abs/2403.15063v1","updated":"2024-03-22T09:40:52Z","published":"2024-03-22T09:40:52Z","title":"Towards a Comprehensive, Efficient and Promptable Anatomic Structure\n  Segmentation Model using 3D Whole-body CT Scans","summary":"  Segment anything model (SAM) demonstrates strong generalization ability on\nnatural image segmentation. However, its direct adaption in medical image\nsegmentation tasks shows significant performance drops with inferior accuracy\nand unstable results. It may also requires an excessive number of prompt points\nto obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI\nscans, a 2D SAM model has to separately handle hundreds of 2D slices. Although\nquite a few studies explore adapting SAM into medical image volumes, the\nefficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods\nonly capable of segmenting specific organs/tumors. In this work, we propose a\ncomprehensive and scalable 3D SAM model for whole-body CT segmentation, named\nCT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation\nmodel using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively,\nensuring the model's accurate responses to higher-dimensional spatial prompts\nis crucial, and 3D patch-wise training is required due to GPU memory\nconstraints. For this purpose, we propose two key technical developments: 1) a\nprogressively and spatially aligned prompt encoding method to effectively\nencode click prompts in local 3D space; and 2) a cross-patch prompt learning\nscheme to capture more 3D spatial context, which is beneficial for reducing the\nediting workloads when interactively prompting on large organs. CT-SAM3D is\ntrained and validated using a curated dataset of 1204 CT scans containing 107\nwhole-body anatomies, reporting significantly better quantitative performance\nagainst all previous SAM-derived models by a large margin with much fewer click\nprompts. Our model can handle segmenting unseen organ as well. Code, data, and\nour 3D interactive segmentation tool with quasi-real-time responses will be\nmade publicly available.\n","authors":["Heng Guo","Jianfeng Zhang","Jiaxing Huang","Tony C. W. Mok","Dazhou Guo","Ke Yan","Le Lu","Dakai Jin","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15061v1","updated":"2024-03-22T09:38:16Z","published":"2024-03-22T09:38:16Z","title":"Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic\n  Range Videos","summary":"  High Dynamic Range (HDR) videos are able to represent wider ranges of\ncontrasts and colors than Standard Dynamic Range (SDR) videos, giving more\nvivid experiences. Due to this, HDR videos are expected to grow into the\ndominant video modality of the future. However, HDR videos are incompatible\nwith existing SDR displays, which form the majority of affordable consumer\ndisplays on the market. Because of this, HDR videos must be processed by\ntone-mapping them to reduced bit-depths to service a broad swath of SDR-limited\nvideo consumers. Here, we analyze the impact of tone-mapping operators on the\nvisual quality of streaming HDR videos. To this end, we built the first\nlarge-scale subjectively annotated open-source database of compressed\ntone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40\nunique HDR source contents. The videos in the database were labeled with more\nthan 750,000 subjective quality annotations, collected from more than 1,600\nunique human observers. We demonstrate the usefulness of the new subjective\ndatabase by benchmarking objective models of visual quality on it. We envision\nthat the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant\nprogress on HDR video tone mapping and quality assessment in the future. To\nthis end, we make the database freely available to the community at\nhttps://live.ece.utexas.edu/research/LIVE_TMHDR/index.html\n","authors":["Abhinau K. Venkataramanan","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2403.15061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12379v4","updated":"2024-03-22T09:36:53Z","published":"2023-12-19T18:11:19Z","title":"Mixture of Cluster-conditional LoRA Experts for Vision-language\n  Instruction Tuning","summary":"  Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized\nthe development of versatile models with zero-shot generalization across a wide\nrange of downstream vision-language tasks. However, the diversity of training\ntasks of different sources and formats would lead to inevitable task conflicts,\nwhere different tasks conflict for the same set of model parameters, resulting\nin sub-optimal instructionfollowing abilities. To address that, we propose the\nMixture of Clusterconditional LoRA Experts (MoCLE), a novel Mixture of Experts\n(MoE) architecture designed to activate the task-customized model parameters\nbased on the instruction clusters. A separate universal expert is further\nincorporated to improve generalization capabilities of MoCLE for novel\ninstructions. Extensive experiments on 11 zero-shot tasks demonstrate the\neffectiveness of MoCLE.\n","authors":["Yunhao Gou","Zhili Liu","Kai Chen","Lanqing Hong","Hang Xu","Aoxue Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12379v4.pdf","comment":"Project website: https://gyhdog99.github.io/projects/mocle/"},{"id":"http://arxiv.org/abs/2403.15059v1","updated":"2024-03-22T09:32:31Z","published":"2024-03-22T09:32:31Z","title":"MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition\n  Integration","summary":"  Recent advances in tuning-free personalized image generation based on\ndiffusion models are impressive. However, to improve subject fidelity, existing\nmethods either retrain the diffusion model or infuse it with dense visual\nembeddings, both of which suffer from poor generalization and efficiency. Also,\nthese methods falter in multi-subject image generation due to the unconstrained\ncross-attention mechanism. In this paper, we propose MM-Diff, a unified and\ntuning-free image personalization framework capable of generating high-fidelity\nimages of both single and multiple subjects in seconds. Specifically, to\nsimultaneously enhance text consistency and subject fidelity, MM-Diff employs a\nvision encoder to transform the input image into CLS and patch embeddings. CLS\nembeddings are used on the one hand to augment the text embeddings, and on the\nother hand together with patch embeddings to derive a small number of\ndetail-rich subject embeddings, both of which are efficiently integrated into\nthe diffusion model through the well-designed multimodal cross-attention\nmechanism. Additionally, MM-Diff introduces cross-attention map constraints\nduring the training phase, ensuring flexible multi-subject image sampling\nduring inference without any predefined inputs (e.g., layout). Extensive\nexperiments demonstrate the superior performance of MM-Diff over other leading\nmethods.\n","authors":["Zhichao Wei","Qingkun Su","Long Qin","Weizhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15049v1","updated":"2024-03-22T09:15:36Z","published":"2024-03-22T09:15:36Z","title":"Continual Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation (VLN) agents navigate to a destination using\nnatural language instructions and the visual information they observe. Existing\nmethods for training VLN agents presuppose fixed datasets, leading to a\nsignificant limitation: the introduction of new environments necessitates\nretraining with previously encountered environments to preserve their\nknowledge. This makes it difficult to train VLN agents that operate in the\never-changing real world. To address this limitation, we present the Continual\nVision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents\ntrained through a continual learning process. For the training and evaluation\nof CVLN agents, we re-arrange existing VLN datasets to propose two datasets:\nCVLN-I, focused on navigation via initial-instruction interpretation, and\nCVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we\npropose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR)\nand Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging\nepisodes based on action perplexity, while ESR replays previously predicted\naction logits to preserve learned behaviors. We demonstrate the effectiveness\nof the proposed methods on CVLN through extensive experiments.\n","authors":["Seongjun Jeong","Gi-Cheon Kang","Seongho Choi","Joochan Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v1","updated":"2024-03-22T09:13:09Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v1.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.09572v2","updated":"2024-03-22T09:07:06Z","published":"2024-03-14T17:03:04Z","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text\n  Transformation","summary":"  Multimodal large language models (MLLMs) have shown impressive reasoning\nabilities, which, however, are also more vulnerable to jailbreak attacks than\ntheir LLM predecessors. Although still capable of detecting unsafe responses,\nwe observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be\neasily bypassed due to the introduction of image features. To construct robust\nMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free\nprotecting approach that exploits the inherent safety awareness of MLLMs, and\ngenerates safer responses via adaptively transforming unsafe images into texts\nto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.\nExperiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO\nenhances model safety significantly (e.g., a 37.6% improvement on the\nMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while\nconsistently maintaining utility results on common MLLM benchmarks.\nFurthermore, we show that ECSO can be used as a data engine to generate\nsupervised-finetuning (SFT) data for MLLM alignment without extra human\nintervention.\n","authors":["Yunhao Gou","Kai Chen","Zhili Liu","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09572v2.pdf","comment":"Project Page: https://gyhdog99.github.io/projects/ecso/"},{"id":"http://arxiv.org/abs/2403.15044v1","updated":"2024-03-22T09:00:24Z","published":"2024-03-22T09:00:24Z","title":"Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour\n  Analysis In-the-wild","summary":"  Multimodal fusion is a significant method for most multimodal tasks. With the\nrecent surge in the number of large pre-trained models, combining both\nmultimodal fusion methods and pre-trained model features can achieve\noutstanding performance in many multimodal tasks. In this paper, we present our\napproach, which leverages both advantages for addressing the task of Expression\n(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the\nAff-Wild2 database using pre-trained models, then extract the final hidden\nlayers of the models as features. Following preprocessing and interpolation or\nconvolution to align the extracted features, different models are employed for\nmodal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.\n","authors":["Zhuofan Wen","Fengyu Zhang","Siyuan Zhang","Haiyang Sun","Mingyu Xu","Licai Sun","Zheng Lian","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2403.15044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08856v3","updated":"2024-03-22T08:51:55Z","published":"2023-08-17T08:29:54Z","title":"MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose\n  and Size Estimation","summary":"  Recently there has been a growing interest in category-level object pose and\nsize estimation, and prevailing methods commonly rely on single view RGB-D\nimages. However, one disadvantage of such methods is that they require accurate\ndepth maps which cannot be produced by consumer-grade sensors. Furthermore,\nmany practical real-world situations involve a moving camera that continuously\nobserves its surroundings, and the temporal information of the input video\nstreams is simply overlooked by single-view methods. We propose a novel\nsolution that makes use of RGB video streams. Our framework consists of three\nmodules: a scale-aware monocular dense SLAM solution, a lightweight object pose\npredictor, and an object-level pose graph optimizer. The SLAM module utilizes a\nvideo stream and additional scale-sensitive readings to estimate camera poses\nand metric depth. The object pose predictor then generates canonical object\nrepresentations from RGB images. The object pose is estimated through geometric\nregistration of these canonical object representations with estimated object\ndepth points. All per-view estimates finally undergo optimization within a pose\ngraph, culminating in the output of robust and accurate canonical object poses.\nOur experimental results demonstrate that when utilizing public dataset\nsequences with high-quality depth information, the proposed method exhibits\ncomparable performance to state-of-the-art RGB-D methods. We also collect and\nevaluate on new datasets containing depth maps of varying quality to further\nquantitatively benchmark the proposed method alongside previous RGB-D based\nmethods. We demonstrate a significant advantage in scenarios where depth input\nis absent or the quality of depth sensing is limited.\n","authors":["Jiaqi Yang","Yucong Chen","Xiangting Meng","Chenxin Yan","Min Li","Ran Cheng","Lige Liu","Tao Sun","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2308.08856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14189v3","updated":"2024-03-22T08:45:52Z","published":"2023-11-23T20:14:50Z","title":"D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object\n  Reconstruction","summary":"  Reconstructing hand-held objects from a single RGB image is a challenging\ntask in computer vision. In contrast to prior works that utilize deterministic\nmodeling paradigms, we employ a point cloud denoising diffusion model to\naccount for the probabilistic nature of this problem. In the core, we introduce\ncentroid-fixed dual-stream conditional diffusion for monocular hand-held object\nreconstruction (D-SCo), tackling two predominant challenges. First, to avoid\nthe object centroid from deviating, we utilize a novel hand-constrained\ncentroid fixing paradigm, enhancing the stability of diffusion and reverse\nprocesses and the precision of feature projection. Second, we introduce a\ndual-stream denoiser to semantically and geometrically model hand-object\ninteractions with a novel unified hand-object semantic embedding, enhancing the\nreconstruction performance of the hand-occluded region of the object.\nExperiments on the synthetic ObMan dataset and three real-world datasets HO3D,\nMOW and DexYCB demonstrate that our approach can surpass all other\nstate-of-the-art methods. Codes will be released.\n","authors":["Bowen Fu","Gu Wang","Chenyangguang Zhang","Yan Di","Ziqin Huang","Zhiying Leng","Fabian Manhardt","Xiangyang Ji","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2311.14189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15033v1","updated":"2024-03-22T08:32:30Z","published":"2024-03-22T08:32:30Z","title":"Toward Tiny and High-quality Facial Makeup with Data Amplify Learning","summary":"  Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.\n","authors":["Qiaoqiao Jin","Xuanhong Chen","Meiguang Jin","Ying Cheng","Rui Shi","Yucheng Zheng","Yupeng Zhu","Bingbing Ni"],"pdf_url":"https://arxiv.org/pdf/2403.15033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15032v1","updated":"2024-03-22T08:27:25Z","published":"2024-03-22T08:27:25Z","title":"An Integrated Neighborhood and Scale Information Network for Open-Pit\n  Mine Change Detection in High-Resolution Remote Sensing Images","summary":"  Open-pit mine change detection (CD) in high-resolution (HR) remote sensing\nimages plays a crucial role in mineral development and environmental\nprotection. Significant progress has been made in this field in recent years,\nlargely due to the advancement of deep learning techniques. However, existing\ndeep-learning-based CD methods encounter challenges in effectively integrating\nneighborhood and scale information, resulting in suboptimal performance.\nTherefore, by exploring the influence patterns of neighborhood and scale\ninformation, this paper proposes an Integrated Neighborhood and Scale\nInformation Network (INSINet) for open-pit mine CD in HR remote sensing images.\nSpecifically, INSINet introduces 8-neighborhood-image information to acquire a\nlarger receptive field, improving the recognition of center image boundary\nregions. Drawing on techniques of skip connection, deep supervision, and\nattention mechanism, the multi-path deep supervised attention (MDSA) module is\ndesigned to enhance multi-scale information fusion and change feature\nextraction. Experimental analysis reveals that incorporating neighborhood and\nscale information enhances the F1 score of INSINet by 6.40%, with improvements\nof 3.08% and 3.32% respectively. INSINet outperforms existing methods with an\nOverall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of\n83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing\nimages.\n","authors":["Zilin Xie","Kangning Li","Jinbao Jiang","Jinzhong Yang","Xiaojun Qiao","Deshuai Yuan","Cheng Nie"],"pdf_url":"https://arxiv.org/pdf/2403.15032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15031v1","updated":"2024-03-22T08:26:31Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.15026v1","updated":"2024-03-22T08:16:59Z","published":"2024-03-22T08:16:59Z","title":"VRSO: Visual-Centric Reconstruction for Static Object Annotation","summary":"  As a part of the perception results of intelligent driving systems, static\nobject detection (SOD) in 3D space provides crucial cues for driving\nenvironment understanding. With the rapid deployment of deep neural networks\nfor SOD tasks, the demand for high-quality training samples soars. The\ntraditional, also reliable, way is manual labeling over the dense LiDAR point\nclouds and reference images. Though most public driving datasets adopt this\nstrategy to provide SOD ground truth (GT), it is still expensive (requires\nLiDAR scanners) and low-efficient (time-consuming and unscalable) in practice.\nThis paper introduces VRSO, a visual-centric approach for static object\nannotation. VRSO is distinguished in low cost, high efficiency, and high\nquality: (1) It recovers static objects in 3D space with only camera images as\ninput, and (2) manual labeling is barely involved since GT for SOD tasks is\ngenerated based on an automatic reconstruction and annotation pipeline. (3)\nExperiments on the Waymo Open Dataset show that the mean reprojection error\nfrom VRSO annotation is only 2.6 pixels, around four times lower than the Waymo\nlabeling (10.6 pixels). Source code is available at:\nhttps://github.com/CaiYingFeng/VRSO.\n","authors":["Chenyao Yu","Yingfeng Cai","Jiaxin Zhang","Hui Kong","Wei Sui","Cong Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15026v1.pdf","comment":"submitted to iros 2024"},{"id":"http://arxiv.org/abs/2305.03907v3","updated":"2024-03-22T08:10:07Z","published":"2023-05-06T02:53:13Z","title":"Listen to Look into the Future: Audio-Visual Egocentric Gaze\n  Anticipation","summary":"  Egocentric gaze anticipation serves as a key building block for the emerging\ncapability of Augmented Reality. Notably, gaze behavior is driven by both\nvisual cues and audio signals during daily activities. Motivated by this\nobservation, we introduce the first model that leverages both the video and\naudio modalities for egocentric gaze anticipation. Specifically, we propose a\nContrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two\nmodules to separately capture audio-visual correlations in spatial and temporal\ndimensions, and applies a contrastive loss on the re-weighted audio-visual\nfeatures from fusion modules for representation learning. We conduct extensive\nablation studies and thorough analysis using two egocentric video datasets:\nEgo4D and Aria, to validate our model design. We demonstrate the audio improves\nthe performance by +2.5% and +2.4% on the two datasets. Our model also\noutperforms the prior state-of-the-art methods by at least +1.9% and +1.6%.\nMoreover, we provide visualizations to show the gaze anticipation results and\nprovide additional insights into audio-visual representation learning. The code\nand data split are available on our website\n(https://bolinlai.github.io/CSTS-EgoGazeAnticipation/).\n","authors":["Bolin Lai","Fiona Ryan","Wenqi Jia","Miao Liu","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2305.03907v3.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2403.15019v1","updated":"2024-03-22T08:05:30Z","published":"2024-03-22T08:05:30Z","title":"BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance\n  Segmentation","summary":"  3D instance segmentation (3DIS) is a crucial task, but point-level\nannotations are tedious in fully supervised settings. Thus, using bounding\nboxes (bboxes) as annotations has shown great potential. The current mainstream\napproach is a two-step process, involving the generation of pseudo-labels from\nbox annotations and the training of a 3DIS network with the pseudo-labels.\nHowever, due to the presence of intersections among bboxes, not every point has\na determined instance label, especially in overlapping areas. To generate\nhigher quality pseudo-labels and achieve more precise weakly supervised 3DIS\nresults, we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D\nInstance Segmentation (BSNet), which devises a novel pseudo-labeler called\nSimulation-assisted Transformer. The labeler consists of two main components.\nThe first is Simulation-assisted Mean Teacher, which introduces Mean Teacher\nfor the first time in this task and constructs simulated samples to assist the\nlabeler in acquiring prior knowledge about overlapping areas. To better model\nlocal-global structure, we also propose Local-Global Aware Attention as the\ndecoder for teacher and student labelers. Extensive experiments conducted on\nthe ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is\navailable at\n\\href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.\n","authors":["Jiahao Lu","Jiacheng Deng","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15017v1","updated":"2024-03-22T08:03:10Z","published":"2024-03-22T08:03:10Z","title":"Vehicle Detection Performance in Nordic Region","summary":"  This paper addresses the critical challenge of vehicle detection in the harsh\nwinter conditions in the Nordic regions, characterized by heavy snowfall,\nreduced visibility, and low lighting. Due to their susceptibility to\nenvironmental distortions and occlusions, traditional vehicle detection methods\nhave struggled in these adverse conditions. The advanced proposed deep learning\narchitectures brought promise, yet the unique difficulties of detecting\nvehicles in Nordic winters remain inadequately addressed. This study uses the\nNordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to\nevaluate the performance of state-of-the-art vehicle detection algorithms under\nchallenging weather conditions. Our methodology includes a comprehensive\nevaluation of single-stage, two-stage, and transformer-based detectors against\nthe NVD. We propose a series of enhancements tailored to each detection\nframework, including data augmentation, hyperparameter tuning, transfer\nlearning, and novel strategies designed explicitly for the DETR model. Our\nfindings not only highlight the limitations of current detection systems in the\nNordic environment but also offer promising directions for enhancing these\nalgorithms for improved robustness and accuracy in vehicle detection amidst the\ncomplexities of winter landscapes. The code and the dataset are available at\nhttps://nvd.ltu-ai.dev\n","authors":["Hamam Mokayed","Rajkumar Saini","Oluwatosin Adewumi","Lama Alkhaled","Bjorn Backe","Palaiahnakote Shivakumara","Olle Hagner","Yan Chai Hum"],"pdf_url":"https://arxiv.org/pdf/2403.15017v1.pdf","comment":"submitted to ICPR2024"},{"id":"http://arxiv.org/abs/2403.15013v1","updated":"2024-03-22T07:57:27Z","published":"2024-03-22T07:57:27Z","title":"Extracting Human Attention through Crowdsourced Patch Labeling","summary":"  In image classification, a significant problem arises from bias in the\ndatasets. When it contains only specific types of images, the classifier begins\nto rely on shortcuts - simplistic and erroneous rules for decision-making. This\nleads to high performance on the training dataset but inferior results on new,\nvaried images, as the classifier's generalization capability is reduced. For\nexample, if the images labeled as mustache consist solely of male figures, the\nmodel may inadvertently learn to classify images by gender rather than the\npresence of a mustache. One approach to mitigate such biases is to direct the\nmodel's attention toward the target object's location, usually marked using\nbounding boxes or polygons for annotation. However, collecting such annotations\nrequires substantial time and human effort. Therefore, we propose a novel\npatch-labeling method that integrates AI assistance with crowdsourcing to\ncapture human attention from images, which can be a viable solution for\nmitigating bias. Our method consists of two steps. First, we extract the\napproximate location of a target using a pre-trained saliency detection model\nsupplemented by human verification for accuracy. Then, we determine the\nhuman-attentive area in the image by iteratively dividing the image into\nsmaller patches and employing crowdsourcing to ascertain whether each patch can\nbe classified as the target object. We demonstrated the effectiveness of our\nmethod in mitigating bias through improved classification accuracy and the\nrefined focus of the model. Also, crowdsourced experiments validate that our\nmethod collects human annotation up to 3.4 times faster than annotating object\nlocations with polygons, significantly reducing the need for human resources.\nWe conclude the paper by discussing the advantages of our method in a\ncrowdsourcing context, mainly focusing on aspects of human errors and\naccessibility.\n","authors":["Minsuk Chang","Seokhyeon Park","Hyeon Jeon","Aeri Cho","Soohyun Lee","Jinwook Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15013v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.00029v3","updated":"2024-03-22T07:52:28Z","published":"2023-12-29T05:28:35Z","title":"6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation","summary":"  Estimating the 6D object pose from a single RGB image often involves noise\nand indeterminacy due to challenges such as occlusions and cluttered\nbackgrounds. Meanwhile, diffusion models have shown appealing performance in\ngenerating high-quality images from random noise with high indeterminacy\nthrough step-by-step denoising. Inspired by their denoising capability, we\npropose a novel diffusion-based framework (6D-Diff) to handle the noise and\nindeterminacy in object pose estimation for better performance. In our\nframework, to establish accurate 2D-3D correspondence, we formulate 2D\nkeypoints detection as a reverse diffusion (denoising) process. To facilitate\nsuch a denoising process, we design a Mixture-of-Cauchy-based forward diffusion\nprocess and condition the reverse process on the object features. Extensive\nexperiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our\nframework.\n","authors":["Li Xu","Haoxuan Qu","Yujun Cai","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2401.00029v3.pdf","comment":"CVPR 2024 CAMERA-READY"},{"id":"http://arxiv.org/abs/2403.15011v1","updated":"2024-03-22T07:49:55Z","published":"2024-03-22T07:49:55Z","title":"Cell Tracking according to Biological Needs -- Strong Mitosis-aware\n  Random-finite Sets Tracker with Aleatoric Uncertainty","summary":"  Cell tracking and segmentation assist biologists in extracting insights from\nlarge-scale microscopy time-lapse data. Driven by local accuracy metrics,\ncurrent tracking approaches often suffer from a lack of long-term consistency.\nTo address this issue, we introduce an uncertainty estimation technique for\nneural tracking-by-regression frameworks and incorporate it into our novel\nextended Poisson multi-Bernoulli mixture tracker. Our uncertainty estimation\nidentifies uncertain associations within high-performing tracking-by-regression\nmethods using problem-specific test-time augmentations. Leveraging this\nuncertainty, along with a novel mitosis-aware assignment problem formulation,\nour tracker resolves false associations and mitosis detections stemming from\nlong-term conflicts. We evaluate our approach on nine competitive datasets and\ndemonstrate that it outperforms the current state-of-the-art on biologically\nrelevant metrics substantially, achieving improvements by a factor of\napproximately $5.75$. Furthermore, we uncover new insights into the behavior of\ntracking-by-regression uncertainty.\n","authors":["Timo Kaiser","Maximilian Schier","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2403.15011v1.pdf","comment":"23 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.15010v1","updated":"2024-03-22T07:47:13Z","published":"2024-03-22T07:47:13Z","title":"Clean-image Backdoor Attacks","summary":"  To gather a significant quantity of annotated training data for\nhigh-performance image classification models, numerous companies opt to enlist\nthird-party providers to label their unlabeled data. This practice is widely\nregarded as secure, even in cases where some annotated errors occur, as the\nimpact of these minor inaccuracies on the final performance of the models is\nnegligible and existing backdoor attacks require attacker's ability to poison\nthe training images. Nevertheless, in this paper, we propose clean-image\nbackdoor attacks which uncover that backdoors can still be injected via a\nfraction of incorrect labels without modifying the training images.\nSpecifically, in our attacks, the attacker first seeks a trigger feature to\ndivide the training images into two parts: those with the feature and those\nwithout it. Subsequently, the attacker falsifies the labels of the former part\nto a backdoor class. The backdoor will be finally implanted into the target\nmodel after it is trained on the poisoned data. During the inference phase, the\nattacker can activate the backdoor in two ways: slightly modifying the input\nimage to obtain the trigger feature, or taking an image that naturally has the\ntrigger feature as input. We conduct extensive experiments to demonstrate the\neffectiveness and practicality of our attacks. According to the experimental\nresults, we conclude that our attacks seriously jeopardize the fairness and\nrobustness of image classification models, and it is necessary to be vigilant\nabout the incorrect labels in outsourced labeling.\n","authors":["Dazhong Rong","Shuheng Shen","Xinyi Fu","Peng Qian","Jianhai Chen","Qinming He","Xing Fu","Weiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15009v1","updated":"2024-03-22T07:45:51Z","published":"2024-03-22T07:45:51Z","title":"TexRO: Generating Delicate Textures of 3D Models by Recursive\n  Optimization","summary":"  This paper presents TexRO, a novel method for generating delicate textures of\na known 3D mesh by optimizing its UV texture. The key contributions are\ntwo-fold. We propose an optimal viewpoint selection strategy, that finds the\nmost miniature set of viewpoints covering all the faces of a mesh. Our\nviewpoint selection strategy guarantees the completeness of a generated result.\nWe propose a recursive optimization pipeline that optimizes a UV texture at\nincreasing resolutions, with an adaptive denoising method that re-uses existing\ntextures for new texture generation. Through extensive experimentation, we\ndemonstrate the superior performance of TexRO in terms of texture quality,\ndetail preservation, visual consistency, and, notably runtime speed,\noutperforming other current methods. The broad applicability of TexRO is\nfurther confirmed through its successful use on diverse 3D models.\n","authors":["Jinbo Wu","Xing Liu","Chenming Wu","Xiaobo Gao","Jialun Liu","Xinqi Liu","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15009v1.pdf","comment":"Technical report. Project page:\n  \\href{https://3d-aigc.github.io/TexRO}{https://3d-aigc.github.io/TexRO}"},{"id":"http://arxiv.org/abs/2403.15008v1","updated":"2024-03-22T07:45:50Z","published":"2024-03-22T07:45:50Z","title":"Tri-Perspective View Decomposition for Geometry-Aware Depth Completion","summary":"  Depth completion is a vital task for autonomous driving, as it involves\nreconstructing the precise 3D geometry of a scene from sparse and noisy depth\nmeasurements. However, most existing methods either rely only on 2D depth\nrepresentations or directly incorporate raw 3D point clouds for compensation,\nwhich are still insufficient to capture the fine-grained 3D geometry of the\nscene. To address this challenge, we introduce Tri-Perspective view\nDecomposition (TPVD), a novel framework that can explicitly model 3D geometry.\nIn particular, (1) TPVD ingeniously decomposes the original point cloud into\nthree 2D views, one of which corresponds to the sparse depth input. (2) We\ndesign TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D\naggregation, where a Distance-Aware Spherical Convolution (DASC) is applied.\n(3) By adaptively choosing TPV affinitive neighbors, the newly proposed\nGeometric Spatial Propagation Network (GSPN) further improves the geometric\nconsistency. As a result, our TPVD outperforms existing methods on KITTI,\nNYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset\nnamed TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color\ncamera on smartphones. Project page:\nhttps://yanzq95.github.io/projectpage/TOFDC/index.html\n","authors":["Zhiqiang Yan","Yuankai Lin","Kun Wang","Yupeng Zheng","Yufei Wang","Zhenyu Zhang","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15008v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15004v1","updated":"2024-03-22T07:32:21Z","published":"2024-03-22T07:32:21Z","title":"ParFormer: Vision Transformer Baseline with Parallel Local Global Token\n  Mixer and Convolution Attention Patch Embedding","summary":"  This work presents ParFormer as an enhanced transformer architecture that\nallows the incorporation of different token mixers into a single stage, hence\nimproving feature extraction capabilities. Integrating both local and global\ndata allows for precise representation of short- and long-range spatial\nrelationships without the need for computationally intensive methods such as\nshifting windows. Along with the parallel token mixer encoder, We offer the\nConvolutional Attention Patch Embedding (CAPE) as an enhancement of standard\npatch embedding to improve token mixer extraction with a convolutional\nattention module. Our comprehensive evaluation demonstrates that our ParFormer\noutperforms CNN-based and state-of-the-art transformer-based architectures in\nimage classification and several complex tasks such as object recognition. The\nproposed CAPE has been demonstrated to benefit the overall MetaFormer\narchitecture, even while utilizing the Identity Mapping Token Mixer, resulting\nin a 0.5\\% increase in accuracy. The ParFormer models outperformed ConvNeXt and\nSwin Transformer for the pure convolution and transformer model in accuracy.\nFurthermore, our model surpasses the current leading hybrid transformer by\nreaching competitive Top-1 scores in the ImageNet-1K classification test.\nSpecifically, our model variants with 11M, 23M, and 34M parameters achieve\nscores of 80.4\\%, 82.1\\%, and 83.1\\%, respectively. Code:\nhttps://github.com/novendrastywn/ParFormer-CAPE-2024\n","authors":["Novendra Setyawan","Ghufron Wahyu Kurniawan","Chi-Chia Sun","Jun-Wei Hsieh","Hui-Kai Su","Wen-Kai Kuo"],"pdf_url":"https://arxiv.org/pdf/2403.15004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00248v2","updated":"2024-03-22T07:25:03Z","published":"2023-12-30T14:24:33Z","title":"Promoting Segment Anything Model towards Highly Accurate Dichotomous\n  Image Segmentation","summary":"  The Segment Anything Model (SAM) represents a significant breakthrough into\nfoundation models for computer vision, providing a large-scale image\nsegmentation model. However, despite SAM's zero-shot performance, its\nsegmentation masks lack fine-grained details, particularly in accurately\ndelineating object boundaries. We have high expectations regarding whether SAM,\nas a foundation model, can be improved towards highly accurate object\nsegmentation, which is known as dichotomous image segmentation (DIS). To\naddress this issue, we propose DIS-SAM, which advances SAM towards DIS with\nextremely accurate details. DIS-SAM is a framework specifically tailored for\nhighly accurate segmentation, maintaining SAM's promptable design. DIS-SAM\nemploys a two-stage approach, integrating SAM with a modified IS-Net dedicated\nto DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced\nsegmentation accuracy compared to SAM and HQ-SAM.\n","authors":["Xianjie Liu","Keren Fu","Qijun Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.00248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17587v3","updated":"2024-03-22T07:23:51Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14178v2","updated":"2024-03-22T07:23:22Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v2.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2403.14999v1","updated":"2024-03-22T07:21:09Z","published":"2024-03-22T07:21:09Z","title":"Magic for the Age of Quantized DNNs","summary":"  Recently, the number of parameters in DNNs has explosively increased, as\nexemplified by LLMs (Large Language Models), making inference on small-scale\ncomputers more difficult. Model compression technology is, therefore, essential\nfor integration into products. In this paper, we propose a method of\nquantization-aware training. We introduce a novel normalization (Layer-Batch\nNormalization) that is independent of the mini-batch size and does not require\nany additional computation cost during inference. Then, we quantize the weights\nby the scaled round-clip function with the weight standardization. We also\nquantize activation functions using the same function and apply surrogate\ngradients to train the model with both quantized weights and the quantized\nactivation functions. We call this method Magic for the age of Quantised DNNs\n(MaQD). Experimental results show that our quantization method can be achieved\nwith minimal accuracy degradation.\n","authors":["Yoshihide Sawada","Ryuji Saiin","Kazuma Suetake"],"pdf_url":"https://arxiv.org/pdf/2403.14999v1.pdf","comment":"14 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.14995v1","updated":"2024-03-22T07:12:48Z","published":"2024-03-22T07:12:48Z","title":"Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive\n  Segmentation","summary":"  Unsupervised Domain Adaptation (UDA) endeavors to adjust models trained on a\nsource domain to perform well on a target domain without requiring additional\nannotations. In the context of domain adaptive semantic segmentation, which\ntackles UDA for dense prediction, the goal is to circumvent the need for costly\npixel-level annotations. Typically, various prevailing methods baseline rely on\nconstructing intermediate domains via cross-domain mixed sampling techniques to\nmitigate the performance decline caused by domain gaps. However, such\napproaches generate synthetic data that diverge from real-world distributions,\npotentially leading the model astray from the true target distribution. To\naddress this challenge, we propose a novel auxiliary task called Guidance\nTraining. This task facilitates the effective utilization of cross-domain mixed\nsampling techniques while mitigating distribution shifts from the real world.\nSpecifically, Guidance Training guides the model to extract and reconstruct the\ntarget-domain feature distribution from mixed data, followed by decoding the\nreconstructed target-domain features to make pseudo-label predictions.\nImportantly, integrating Guidance Training incurs minimal training overhead and\nimposes no additional inference burden. We demonstrate the efficacy of our\napproach by integrating it with existing methods, consistently improving\nperformance. The implementation will be available at\nhttps://github.com/Wenlve-Zhou/Guidance-Training.\n","authors":["Wenlve Zhou","Zhiheng Zhou","Tianlei Wang","Delu Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.14995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15243v3","updated":"2024-03-22T07:05:58Z","published":"2023-11-26T09:06:40Z","title":"ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection methods often exploit auxiliary outliers\nto train model identifying OOD samples, especially discovering challenging\noutliers from auxiliary outliers dataset to improve OOD detection. However,\nthey may still face limitations in effectively distinguishing between the most\nchallenging OOD samples that are much like in-distribution (ID) data, i.e.,\n\\idlike samples. To this end, we propose a novel OOD detection framework that\ndiscovers \\idlike outliers using CLIP \\cite{DBLP:conf/icml/RadfordKHRGASAM21}\nfrom the vicinity space of the ID samples, thus helping to identify these most\nchallenging OOD samples. Then a prompt learning framework is proposed that\nutilizes the identified \\idlike outliers to further leverage the capabilities\nof CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a\nsmall number of ID samples to learn the prompts of the model without exposing\nother auxiliary outlier datasets. By focusing on the most challenging \\idlike\nOOD samples and elegantly exploiting the capabilities of CLIP, our method\nachieves superior few-shot learning performance on various real-world image\ndatasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method\nreduces the average FPR95 by 12.16\\% and improves the average AUROC by 2.76\\%,\ncompared to state-of-the-art methods). Code is available at\nhttps://github.com/ycfate/ID-like.\n","authors":["Yichen Bai","Zongbo Han","Changqing Zhang","Bing Cao","Xiaoheng Jiang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2311.15243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19122v2","updated":"2024-03-22T07:03:54Z","published":"2024-02-29T13:00:22Z","title":"BigGait: Learning Gait Representation You Want by Large Vision Models","summary":"  Gait recognition stands as one of the most pivotal remote identification\ntechnologies and progressively expands across research and industry\ncommunities. However, existing gait recognition methods heavily rely on\ntask-specific upstream driven by supervised learning to provide explicit gait\nrepresentations like silhouette sequences, which inevitably introduce expensive\nannotation costs and potential error accumulation. Escaping from this trend,\nthis work explores effective gait representations based on the all-purpose\nknowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a\nsimple yet efficient gait framework, termed BigGait. Specifically, the Gait\nRepresentation Extractor (GRE) within BigGait draws upon design principles from\nestablished gait representations, effectively transforming all-purpose\nknowledge into implicit gait representations without requiring third-party\nsupervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that\nBigGait significantly outperforms the previous methods in both within-domain\nand cross-domain tasks in most cases, and provides a more practical paradigm\nfor learning the next-generation gait representation. Finally, we delve into\nprospective challenges and promising directions in LVMs-based gait recognition,\naiming to inspire future work in this emerging topic. The source code is\navailable at https://github.com/ShiqiYu/OpenGait.\n","authors":["Dingqiang Ye","Chao Fan","Jingzhe Ma","Xiaoming Liu","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2402.19122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10413v2","updated":"2024-03-22T07:03:50Z","published":"2023-10-16T13:56:56Z","title":"Image super-resolution via dynamic network","summary":"  Convolutional neural networks (CNNs) depend on deep network architectures to\nextract accurate information for image super-resolution. However, obtained\ninformation of these CNNs cannot completely express predicted high-quality\nimages for complex scenes. In this paper, we present a dynamic network for\nimage super-resolution (DSRNet), which contains a residual enhancement block,\nwide enhancement block, feature refinement block and construction block. The\nresidual enhancement block is composed of a residual enhanced architecture to\nfacilitate hierarchical features for image super-resolution. To enhance\nrobustness of obtained super-resolution model for complex scenes, a wide\nenhancement block achieves a dynamic architecture to learn more robust\ninformation to enhance applicability of an obtained super-resolution model for\nvarying scenes. To prevent interference of components in a wide enhancement\nblock, a refinement block utilizes a stacked architecture to accurately learn\nobtained features. Also, a residual learning operation is embedded in the\nrefinement block to prevent long-term dependency problem. Finally, a\nconstruction block is responsible for reconstructing high-quality images.\nDesigned heterogeneous architecture can not only facilitate richer structural\ninformation, but also be lightweight, which is suitable for mobile digital\ndevices. Experimental results shows that our method is more competitive in\nterms of performance and recovering time of image super-resolution and\ncomplexity. The code of DSRNet can be obtained at\nhttps://github.com/hellloxiaotian/DSRNet.\n","authors":["Chunwei Tian","Xuanyu Zhang","Qi Zhang","Mingming Yang","Zhaojie Ju"],"pdf_url":"https://arxiv.org/pdf/2310.10413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14987v1","updated":"2024-03-22T06:45:45Z","published":"2024-03-22T06:45:45Z","title":"Generative Active Learning for Image Synthesis Personalization","summary":"  This paper presents a pilot study that explores the application of active\nlearning, traditionally studied in the context of discriminative models, to\ngenerative models. We specifically focus on image synthesis personalization\ntasks. The primary challenge in conducting active learning on generative models\nlies in the open-ended nature of querying, which differs from the closed form\nof querying in discriminative models that typically target a single concept. We\nintroduce the concept of anchor directions to transform the querying process\ninto a semi-open problem. We propose a direction-based uncertainty sampling\nstrategy to enable generative active learning and tackle the\nexploitation-exploration dilemma. Extensive experiments are conducted to\nvalidate the effectiveness of our approach, demonstrating that an open-source\nmodel can achieve superior performance compared to closed-source models\ndeveloped by large companies, such as Google's StyleDrop. The source code is\navailable at https://github.com/zhangxulu1996/GAL4Personalization.\n","authors":["Xulu Zhang","Wengyu Zhang","Xiao-Yong Wei","Jinlin Wu","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03408v4","updated":"2024-03-22T06:45:41Z","published":"2023-12-06T10:46:53Z","title":"Open-sourced Data Ecosystem in Autonomous Driving: the Present and\n  Future","summary":"  With the continuous maturation and application of autonomous driving\ntechnology, a systematic examination of open-source autonomous driving datasets\nbecomes instrumental in fostering the robust evolution of the industry\necosystem. Current autonomous driving datasets can broadly be categorized into\ntwo generations. The first-generation autonomous driving datasets are\ncharacterized by relatively simpler sensor modalities, smaller data scale, and\nis limited to perception-level tasks. KITTI, introduced in 2012, serves as a\nprominent representative of this initial wave. In contrast, the\nsecond-generation datasets exhibit heightened complexity in sensor modalities,\ngreater data scale and diversity, and an expansion of tasks from perception to\nencompass prediction and control. Leading examples of the second generation\ninclude nuScenes and Waymo, introduced around 2019. This comprehensive review,\nconducted in collaboration with esteemed colleagues from both academia and\nindustry, systematically assesses over seventy open-source autonomous driving\ndatasets from domestic and international sources. It offers insights into\nvarious aspects, such as the principles underlying the creation of high-quality\ndatasets, the pivotal role of data engine systems, and the utilization of\ngenerative foundation models to facilitate scalable data generation.\nFurthermore, this review undertakes an exhaustive analysis and discourse\nregarding the characteristics and data scales that future third-generation\nautonomous driving datasets should possess. It also delves into the scientific\nand technical challenges that warrant resolution. These endeavors are pivotal\nin advancing autonomous innovation and fostering technological enhancement in\ncritical domains. For further details, please refer to\nhttps://github.com/OpenDriveLab/DriveAGI.\n","authors":["Hongyang Li","Yang Li","Huijie Wang","Jia Zeng","Huilin Xu","Pinlong Cai","Li Chen","Junchi Yan","Feng Xu","Lu Xiong","Jingdong Wang","Futang Zhu","Chunjing Xu","Tiancai Wang","Fei Xia","Beipeng Mu","Zhihui Peng","Dahua Lin","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2312.03408v4.pdf","comment":"This article is a simplified English translation of corresponding\n  Chinese article. Please refer to Chinese version for the complete content"},{"id":"http://arxiv.org/abs/2403.11735v2","updated":"2024-03-22T06:39:20Z","published":"2024-03-18T12:43:38Z","title":"LSKNet: A Foundation Lightweight Backbone for Remote Sensing","summary":"  Remote sensing images pose distinct challenges for downstream tasks due to\ntheir inherent complexity. While a considerable amount of research has been\ndedicated to remote sensing classification, object detection and semantic\nsegmentation, most of these studies have overlooked the valuable prior\nknowledge embedded within remote sensing scenarios. Such prior knowledge can be\nuseful because remote sensing objects may be mistakenly recognized without\nreferencing a sufficiently long-range context, which can vary for different\nobjects. This paper considers these priors and proposes a lightweight Large\nSelective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its\nlarge spatial receptive field to better model the ranging context of various\nobjects in remote sensing scenarios. To our knowledge, large and selective\nkernel mechanisms have not been previously explored in remote sensing images.\nWithout bells and whistles, our lightweight LSKNet sets new state-of-the-art\nscores on standard remote sensing classification, object detection and semantic\nsegmentation benchmarks. Our comprehensive analysis further validated the\nsignificance of the identified priors and the effectiveness of LSKNet. The code\nis available at https://github.com/zcablii/LSKNet.\n","authors":["Yuxuan Li","Xiang Li","Yimain Dai","Qibin Hou","Li Liu","Yongxiang Liu","Ming-Ming Cheng","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11735v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.09030"},{"id":"http://arxiv.org/abs/2403.14977v1","updated":"2024-03-22T06:22:20Z","published":"2024-03-22T06:22:20Z","title":"Piecewise-Linear Manifolds for Deep Metric Learning","summary":"  Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.\n","authors":["Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2403.14977v1.pdf","comment":"Accepted at CPAL 2024 (Oral)"},{"id":"http://arxiv.org/abs/2403.14974v1","updated":"2024-03-22T06:04:37Z","published":"2024-03-22T06:04:37Z","title":"AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and\n  Dynamic Weighting Strategies","summary":"  With the continuous improvements of deepfake methods, forgery messages have\ntransitioned from single-modality to multi-modal fusion, posing new challenges\nfor existing forgery detection algorithms. In this paper, we propose AVT2-DWF,\nthe Audio-Visual dual Transformers grounded in Dynamic Weight Fusion, which\naims to amplify both intra- and cross-modal forgery cues, thereby enhancing\ndetection capabilities. AVT2-DWF adopts a dual-stage approach to capture both\nspatial characteristics and temporal dynamics of facial expressions. This is\nachieved through a face transformer with an n-frame-wise tokenization strategy\nencoder and an audio transformer encoder. Subsequently, it uses multi-modal\nconversion with dynamic weight fusion to address the challenge of heterogeneous\ninformation fusion between audio and visual modalities. Experiments on\nDeepfakeTIMIT, FakeAVCeleb, and DFDC datasets indicate that AVT2-DWF achieves\nstate-of-the-art performance intra- and cross-dataset Deepfake detection. Code\nis available at https://github.com/raining-dev/AVT2-DWF.\n","authors":["Rui Wang","Dengpan Ye","Long Tang","Yunming Zhang","Jiacheng Deng"],"pdf_url":"https://arxiv.org/pdf/2403.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14973v1","updated":"2024-03-22T06:04:11Z","published":"2024-03-22T06:04:11Z","title":"Trajectory Regularization Enhances Self-Supervised Geometric\n  Representation","summary":"  Self-supervised learning (SSL) has proven effective in learning high-quality\nrepresentations for various downstream tasks, with a primary focus on semantic\ntasks. However, its application in geometric tasks remains underexplored,\npartially due to the absence of a standardized evaluation method for geometric\nrepresentations. To address this gap, we introduce a new pose-estimation\nbenchmark for assessing SSL geometric representations, which demands training\nwithout semantic or pose labels and achieving proficiency in both semantic and\ngeometric downstream tasks. On this benchmark, we study enhancing SSL geometric\nrepresentations without sacrificing semantic classification accuracy. We find\nthat leveraging mid-layer representations improves pose-estimation performance\nby 10-20%. Further, we introduce an unsupervised trajectory-regularization\nloss, which improves performance by an additional 4% and improves\ngeneralization ability on out-of-distribution data. We hope the proposed\nbenchmark and methods offer new insights and improvements in self-supervised\ngeometric representation learning.\n","authors":["Jiayun Wang","Stella X. Yu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04669v3","updated":"2024-03-22T05:41:55Z","published":"2023-09-09T03:01:38Z","title":"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual\n  Tokenization","summary":"  Recently, the remarkable advance of the Large Language Model (LLM) has\ninspired researchers to transfer its extraordinary reasoning capability to both\nvision and language data. However, the prevailing approaches primarily regard\nthe visual input as a prompt and focus exclusively on optimizing the text\ngeneration process conditioned upon vision content by a frozen LLM. Such an\ninequitable treatment of vision and language heavily constrains the model's\npotential. In this paper, we break through this limitation by representing both\nvision and language in a unified form. Specifically, we introduce a\nwell-designed visual tokenizer to translate the non-linguistic image into a\nsequence of discrete tokens like a foreign language that LLM can read. The\nresulting visual tokens encompass high-level semantics worthy of a word and\nalso support dynamic sequence length varying from the image. Coped with this\ntokenizer, the presented foundation model called LaVIT can handle both image\nand text indiscriminately under the same generative learning paradigm. This\nunification empowers LaVIT to serve as an impressive generalist interface to\nunderstand and generate multi-modal content simultaneously. Extensive\nexperiments further showcase that it outperforms the existing models by a large\nmargin on massive vision-language tasks. Our code and models are available at\nhttps://github.com/jy0205/LaVIT.\n","authors":["Yang Jin","Kun Xu","Kun Xu","Liwei Chen","Chao Liao","Jianchao Tan","Quzhe Huang","Bin Chen","Chenyi Lei","An Liu","Chengru Song","Xiaoqiang Lei","Di Zhang","Wenwu Ou","Kun Gai","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2309.04669v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.14966v1","updated":"2024-03-22T05:38:15Z","published":"2024-03-22T05:38:15Z","title":"DreamFlow: High-Quality Text-to-3D Generation by Approximating\n  Probability Flow","summary":"  Recent progress in text-to-3D generation has been achieved through the\nutilization of score distillation methods: they make use of the pre-trained\ntext-to-image (T2I) diffusion models by distilling via the diffusion model\ntraining objective. However, such an approach inevitably results in the use of\nrandom timesteps at each update, which increases the variance of the gradient\nand ultimately prolongs the optimization process. In this paper, we propose to\nenhance the text-to-3D optimization by leveraging the T2I diffusion prior in\nthe generative sampling process with a predetermined timestep schedule. To this\nend, we interpret text-to3D optimization as a multi-view image-to-image\ntranslation problem, and propose a solution by approximating the probability\nflow. By leveraging the proposed novel optimization algorithm, we design\nDreamFlow, a practical three-stage coarseto-fine text-to-3D optimization\nframework that enables fast generation of highquality and high-resolution\n(i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5\ntimes faster than the existing state-of-the-art text-to-3D method, while\nproducing more photorealistic 3D contents. Visit our project page\n(https://kyungmnlee.github.io/dreamflow.github.io/) for visualizations.\n","authors":["Kyungmin Lee","Kihyuk Sohn","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2403.14966v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.03849v2","updated":"2024-03-22T05:03:34Z","published":"2023-12-06T19:02:40Z","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction\n  Tuning","summary":"  Generating instructional images of human daily actions from an egocentric\nviewpoint serves as a key step towards efficient skill transfer. In this paper,\nwe introduce a novel problem -- egocentric action frame generation. The goal is\nto synthesize an image depicting an action in the user's context (i.e., action\nframe) by conditioning on a user prompt and an input egocentric image. Notably,\nexisting egocentric action datasets lack the detailed annotations that describe\nthe execution of actions. Additionally, existing diffusion-based image\nmanipulation models are sub-optimal in controlling the state transition of an\naction in egocentric image pixel space because of the domain gap. To this end,\nwe propose to Learn EGOcentric (LEGO) action frame generation via visual\ninstruction tuning. First, we introduce a prompt enhancement scheme to generate\nenriched action descriptions from a visual large language model (VLLM) by\nvisual instruction tuning. Then we propose a novel method to leverage image and\ntext embeddings from the VLLM as additional conditioning to improve the\nperformance of a diffusion model. We validate our model on two egocentric\ndatasets -- Ego4D and Epic-Kitchens. Our experiments show substantial\nimprovement over prior image manipulation models in both quantitative and\nqualitative evaluation. We also conduct detailed ablation studies and analysis\nto provide insights in our method. More details of the dataset and code are\navailable on the website (https://bolinlai.github.io/Lego_EgoActGen/).\n","authors":["Bolin Lai","Xiaoliang Dai","Lawrence Chen","Guan Pang","James M. Rehg","Miao Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03849v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2403.09920v3","updated":"2024-03-22T04:53:29Z","published":"2024-03-14T23:41:00Z","title":"Predicting Generalization of AI Colonoscopy Models to Unseen Data","summary":"  $\\textbf{Background}$: Generalizability of AI colonoscopy algorithms is\nimportant for wider adoption in clinical practice. However, current techniques\nfor evaluating performance on unseen data require expensive and time-intensive\nlabels.\n  $\\textbf{Methods}$: We use a \"Masked Siamese Network\" (MSN) to identify novel\nphenomena in unseen data and predict polyp detector performance. MSN is trained\nto predict masked out regions of polyp images, without any labels. We test\nMSN's ability to be trained on data only from Israel and detect unseen\ntechniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes\nfrom Japan (354 videos, 128 hours). We also test MSN's ability to predict\nperformance of Computer Aided Detection (CADe) of polyps on colonoscopies from\nboth countries, even though MSN is not trained on data from Japan.\n  $\\textbf{Results}$: MSN correctly identifies NBI and CE as less similar to\nIsrael whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p <\n10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99%\naccuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite\nbeing trained only on whitelight, and is the only method that is robust to\nnoisy labels. MSN predicts CADe polyp detector performance on in-domain Israel\nand out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few\nexamples of Japan detector performance to train on, MSN prediction of Japan\nperformance improves (r=0.56).\n  $\\textbf{Conclusion}$: Our technique can identify distribution shifts in\nclinical data and can predict CADe detector performance on unseen data, without\nlabels. Our self-supervised approach can aid in detecting when data in practice\nis different from training, such as between hospitals or data has meaningfully\nshifted from training. MSN has potential for application to medical image\ndomains beyond colonoscopy.\n","authors":["Joel Shor","Carson McNeil","Yotam Intrator","Joseph R Ledsam","Hiro-o Yamano","Daisuke Tsurumaru","Hiroki Kayama","Atsushi Hamabe","Koji Ando","Mitsuhiko Ota","Haruei Ogino","Hiroshi Nakase","Kaho Kobayashi","Masaaki Miyo","Eiji Oki","Ichiro Takemasa","Ehud Rivlin","Roman Goldenberg"],"pdf_url":"https://arxiv.org/pdf/2403.09920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13602v3","updated":"2024-03-22T04:46:55Z","published":"2023-11-22T18:59:53Z","title":"Retrieval-Augmented Layout Transformer for Content-Aware Layout\n  Generation","summary":"  Content-aware graphic layout generation aims to automatically arrange visual\nelements along with a given content, such as an e-commerce product image. In\nthis paper, we argue that the current layout generation approaches suffer from\nthe limited training data for the high-dimensional layout structure. We show\nthat a simple retrieval augmentation can significantly improve the generation\nquality. Our model, which is named Retrieval-Augmented Layout Transformer\n(RALF), retrieves nearest neighbor layout examples based on an input image and\nfeeds these results into an autoregressive generator. Our model can apply\nretrieval augmentation to various controllable generation tasks and yield\nhigh-quality layouts within a unified architecture. Our extensive experiments\nshow that RALF successfully generates content-aware layouts in both constrained\nand unconstrained settings and significantly outperforms the baselines.\n","authors":["Daichi Horita","Naoto Inoue","Kotaro Kikuchi","Kota Yamaguchi","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2311.13602v3.pdf","comment":"Accepted to CVPR 2024, Project website:\n  https://udonda.github.io/RALF/"},{"id":"http://arxiv.org/abs/2403.14947v1","updated":"2024-03-22T04:39:15Z","published":"2024-03-22T04:39:15Z","title":"GPT-Connect: Interaction between Text-Driven Human Motion Generator and\n  3D Scenes in a Training-free Manner","summary":"  Recently, while text-driven human motion generation has received massive\nresearch attention, most existing text-driven motion generators are generally\nonly designed to generate motion sequences in a blank background. While this is\nthe case, in practice, human beings naturally perform their motions in 3D\nscenes, rather than in a blank background. Considering this, we here aim to\nperform scene-aware text-drive motion generation instead. Yet, intuitively\ntraining a separate scene-aware motion generator in a supervised way can\nrequire a large amount of motion samples to be troublesomely collected and\nannotated in a large scale of different 3D scenes. To handle this task rather\nin a relatively convenient manner, in this paper, we propose a novel\nGPT-connect framework. In GPT-connect, we enable scene-aware motion sequences\nto be generated directly utilizing the existing blank-background human motion\ngenerator, via leveraging ChatGPT to connect the existing motion generator with\nthe 3D scene in a totally training-free manner. Extensive experiments\ndemonstrate the efficacy and generalizability of our proposed framework.\n","authors":["Haoxuan Qu","Ziyan Guo","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14944v1","updated":"2024-03-22T04:34:59Z","published":"2024-03-22T04:34:59Z","title":"CLIP-VQDiffusion : Langauge Free Training of Text To Image generation\n  using CLIP and vector quantized diffusion model","summary":"  There has been a significant progress in text conditional image generation\nmodels. Recent advancements in this field depend not only on improvements in\nmodel structures, but also vast quantities of text-image paired datasets.\nHowever, creating these kinds of datasets is very costly and requires a\nsubstantial amount of labor. Famous face datasets don't have corresponding text\ncaptions, making it difficult to develop text conditional image generation\nmodels on these datasets. Some research has focused on developing text to image\ngeneration models using only images without text captions. Here, we propose\nCLIP-VQDiffusion, which leverage the pretrained CLIP model to provide\nmultimodal text-image representations and strong image generation capabilities.\nOn the FFHQ dataset, our model outperformed previous state-of-the-art methods\nby 4.4% in clipscore and generated very realistic images even when the text was\nboth in and out of distribution. The pretrained models and codes will soon be\navailable at https://github.com/INFINIQ-AI1/CLIPVQDiffusion\n","authors":["Seungdae Han","Joohee Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14944v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.14939v1","updated":"2024-03-22T04:16:33Z","published":"2024-03-22T04:16:33Z","title":"STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians","summary":"  Recent progress in pre-trained diffusion models and 3D generation have\nspurred interest in 4D content creation. However, achieving high-fidelity 4D\ngeneration with spatial-temporal consistency remains a challenge. In this work,\nwe propose STAG4D, a novel framework that combines pre-trained diffusion models\nwith dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing\ninspiration from 3D generation techniques, we utilize a multi-view diffusion\nmodel to initialize multi-view images anchoring on the input video frames,\nwhere the video can be either real-world captured or generated by a video\ndiffusion model. To ensure the temporal consistency of the multi-view sequence\ninitialization, we introduce a simple yet effective fusion strategy to leverage\nthe first frame as a temporal anchor in the self-attention computation. With\nthe almost consistent multi-view sequences, we then apply the score\ndistillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian\nspatting is specially crafted for the generation task, where an adaptive\ndensification strategy is proposed to mitigate the unstable Gaussian gradient\nfor robust optimization. Notably, the proposed pipeline does not require any\npre-training or fine-tuning of diffusion networks, offering a more accessible\nand practical solution for the 4D generation task. Extensive experiments\ndemonstrate that our method outperforms prior 4D generation works in rendering\nquality, spatial-temporal consistency, and generation robustness, setting a new\nstate-of-the-art for 4D generation from diverse inputs, including text, image,\nand video.\n","authors":["Yifei Zeng","Yanqin Jiang","Siyu Zhu","Yuanxun Lu","Youtian Lin","Hao Zhu","Weiming Hu","Xun Cao","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2403.14939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08262v3","updated":"2024-03-22T04:06:31Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14937v1","updated":"2024-03-22T03:47:02Z","published":"2024-03-22T03:47:02Z","title":"Survey on Modeling of Articulated Objects","summary":"  3D modeling of articulated objects is a research problem within computer\nvision, graphics, and robotics. Its objective is to understand the shape and\nmotion of the articulated components, represent the geometry and mobility of\nobject parts, and create realistic models that reflect articulated objects in\nthe real world. This survey provides a comprehensive overview of the current\nstate-of-the-art in 3D modeling of articulated objects, with a specific focus\non the task of articulated part perception and articulated object creation\n(reconstruction and generation). We systematically review and discuss the\nrelevant literature from two perspectives: geometry processing and articulation\nmodeling. Through this survey, we highlight the substantial progress made in\nthese areas, outline the ongoing challenges, and identify gaps for future\nresearch. Our survey aims to serve as a foundational reference for researchers\nand practitioners in computer vision and graphics, offering insights into the\ncomplexities of articulated object modeling.\n","authors":["Jiayi Liu","Manolis Savva","Ali Mahdavi-Amiri"],"pdf_url":"https://arxiv.org/pdf/2403.14937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08699v3","updated":"2024-03-22T03:31:22Z","published":"2024-01-14T12:38:49Z","title":"On Image Search in Histopathology","summary":"  Pathology images of histopathology can be acquired from camera-mounted\nmicroscopes or whole slide scanners. Utilizing similarity calculations to match\npatients based on these images holds significant potential in research and\nclinical contexts. Recent advancements in search technologies allow for\nimplicit quantification of tissue morphology across diverse primary sites,\nfacilitating comparisons and enabling inferences about diagnosis, and\npotentially prognosis, and predictions for new patients when compared against a\ncurated database of diagnosed and treated cases. In this paper, we\ncomprehensively review the latest developments in image search technologies for\nhistopathology, offering a concise overview tailored for computational\npathology researchers seeking effective, fast and efficient image search\nmethods in their work.\n","authors":["H. R. Tizhoosh","Liron Pantanowitz"],"pdf_url":"https://arxiv.org/pdf/2401.08699v3.pdf","comment":"A chapter in the Book \"Artificial INtelligence in Digital Pathology\"\n  by Cohen and Chauhan, 2024"},{"id":"http://arxiv.org/abs/2401.06312v3","updated":"2024-03-22T03:14:53Z","published":"2024-01-12T00:49:49Z","title":"Video Super-Resolution Transformer with Masked Inter&Intra-Frame\n  Attention","summary":"  Recently, Vision Transformer has achieved great success in recovering missing\ndetails in low-resolution sequences, i.e., the video super-resolution (VSR)\ntask. Despite its superiority in VSR accuracy, the heavy computational burden\nas well as the large memory footprint hinder the deployment of\nTransformer-based VSR models on constrained devices. In this paper, we address\nthe above issue by proposing a novel feature-level masked processing framework:\nVSR with Masked Intra and inter frame Attention (MIA-VSR). The core of MIA-VSR\nis leveraging feature-level temporal continuity between adjacent frames to\nreduce redundant computations and make more rational use of previously enhanced\nSR features. Concretely, we propose an intra-frame and inter-frame attention\nblock which takes the respective roles of past features and input features into\nconsideration and only exploits previously enhanced features to provide\nsupplementary information. In addition, an adaptive block-wise mask prediction\nmodule is developed to skip unimportant computations according to feature\nsimilarity between adjacent frames. We conduct detailed ablation studies to\nvalidate our contributions and compare the proposed method with recent\nstate-of-the-art VSR approaches. The experimental results demonstrate that\nMIA-VSR improves the memory and computation efficiency over state-of-the-art\nmethods, without trading off PSNR accuracy. The code is available at\nhttps://github.com/LabShuHangGU/MIA-VSR.\n","authors":["Xingyu Zhou","Leheng Zhang","Xiaorui Zhao","Keze Wang","Leida Li","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06312v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2312.05239v2","updated":"2024-03-22T03:13:23Z","published":"2023-12-08T18:44:09Z","title":"SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational\n  Score Distillation","summary":"  Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques.\n","authors":["Thuan Hoang Nguyen","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2312.05239v2.pdf","comment":"Accepted to CVPR 2024; Project Page:\n  https://thuanz123.github.io/swiftbrush/"},{"id":"http://arxiv.org/abs/2311.10278v2","updated":"2024-03-22T03:09:25Z","published":"2023-11-17T01:55:15Z","title":"Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint","summary":"  Human fingerprints serve as one unique and powerful characteristic for each\nperson, from which policemen can recognize the identity. Similar to humans,\nmany natural bodies and intrinsic mechanical qualities can also be uniquely\nidentified from surface characteristics. To measure the elasto-plastic\nproperties of one material, one formally sharp indenter is pushed into the\nmeasured body under constant force and retracted, leaving a unique residual\nimprint of the minute size from several micrometers to nanometers. However, one\ngreat challenge is how to map the optical image of this residual imprint into\nthe real wanted mechanical properties, \\ie, the tensile force curve. In this\npaper, we propose a novel method to use multi-fidelity neural networks (MFNN)\nto solve this inverse problem. We first build up the NN model via pure\nsimulation data, and then bridge the sim-to-real gap via transfer learning.\nConsidering the difficulty of collecting real experimental data, we use NN to\ndig out the unknown physics and also implant the known physics into the\ntransfer learning framework, thus highly improving the model stability and\ndecreasing the data requirement. The final constructed model only needs\nthree-shot calibration of real materials. We tested the final model across 20\nreal materials and achieved satisfying accuracy. This work serves as one great\nexample of applying machine learning into scientific research, especially under\nthe constraints of data limitation and fidelity variance.\n","authors":["Yongchao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.10278v2.pdf","comment":"15 pages, 11 figure"},{"id":"http://arxiv.org/abs/2305.10061v2","updated":"2024-03-22T03:07:25Z","published":"2023-05-17T09:04:22Z","title":"Rethinking Boundary Discontinuity Problem for Oriented Object Detection","summary":"  Oriented object detection has been developed rapidly in the past few years,\nwhere rotation equivariance is crucial for detectors to predict rotated boxes.\nIt is expected that the prediction can maintain the corresponding rotation when\nobjects rotate, but severe mutation in angular prediction is sometimes observed\nwhen objects rotate near the boundary angle, which is well-known boundary\ndiscontinuity problem. The problem has been long believed to be caused by the\nsharp loss increase at the angular boundary, and widely used joint-optim\nIoU-like methods deal with this problem by loss-smoothing. However, we\nexperimentally find that even state-of-the-art IoU-like methods actually fail\nto solve the problem. On further analysis, we find that the key to solution\nlies in encoding mode of the smoothing function rather than in joint or\nindependent optimization. In existing IoU-like methods, the model essentially\nattempts to fit the angular relationship between box and object, where the\nbreak point at angular boundary makes the predictions highly unstable.To deal\nwith this issue, we propose a dual-optimization paradigm for angles. We\ndecouple reversibility and joint-optim from single smoothing function into two\ndistinct entities, which for the first time achieves the objectives of both\ncorrecting angular boundary and blending angle with other parameters.Extensive\nexperiments on multiple datasets show that boundary discontinuity problem is\nwell-addressed. Moreover, typical IoU-like methods are improved to the same\nlevel without obvious performance gap. The code is available at\nhttps://github.com/hangxu-cv/cvpr24acm.\n","authors":["Hang Xu","Xinyuan Liu","Haonan Xu","Yike Ma","Zunjie Zhu","Chenggang Yan","Feng Dai"],"pdf_url":"https://arxiv.org/pdf/2305.10061v2.pdf","comment":"cvpr 2024"},{"id":"http://arxiv.org/abs/2311.16194v2","updated":"2024-03-22T02:53:59Z","published":"2023-11-26T14:24:13Z","title":"BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP","summary":"  Contrastive Vision-Language Pre-training, known as CLIP, has shown promising\neffectiveness in addressing downstream image recognition tasks. However, recent\nworks revealed that the CLIP model can be implanted with a downstream-oriented\nbackdoor. On downstream tasks, one victim model performs well on clean samples\nbut predicts a specific target class whenever a specific trigger is present.\nFor injecting a backdoor, existing attacks depend on a large amount of\nadditional data to maliciously fine-tune the entire pre-trained CLIP model,\nwhich makes them inapplicable to data-limited scenarios. In this work,\nmotivated by the recent success of learnable prompts, we address this problem\nby injecting a backdoor into the CLIP model in the prompt learning stage. Our\nmethod named BadCLIP is built on a novel and effective mechanism in backdoor\nattacks on CLIP, i.e., influencing both the image and text encoders with the\ntrigger. It consists of a learnable trigger applied to images and a\ntrigger-aware context generator, such that the trigger can change text features\nvia trigger-aware prompts, resulting in a powerful and generalizable attack.\nExtensive experiments conducted on 11 datasets verify that the clean accuracy\nof BadCLIP is similar to those of advanced prompt learning methods and the\nattack success rate is higher than 99% in most cases. BadCLIP is also\ngeneralizable to unseen classes, and shows a strong generalization capability\nunder cross-dataset and cross-domain settings.\n","authors":["Jiawang Bai","Kuofeng Gao","Shaobo Min","Shu-Tao Xia","Zhifeng Li","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16194v2.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.01697v4","updated":"2024-03-22T02:47:00Z","published":"2023-12-04T07:36:04Z","title":"Hulk: A Universal Knowledge Translator for Human-Centric Tasks","summary":"  Human-centric perception tasks, e.g., pedestrian detection, skeleton-based\naction recognition, and pose estimation, have wide industrial applications,\nsuch as metaverse and sports analysis. There is a recent surge to develop\nhuman-centric foundation models that can benefit a broad range of human-centric\nperception tasks. While many human-centric foundation models have achieved\nsuccess, they did not explore 3D and vision-language tasks for human-centric\nand required task-specific finetuning. These limitations restrict their\napplication to more downstream tasks and situations. To tackle these problems,\nwe present Hulk, the first multimodal human-centric generalist model, capable\nof addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks\nwithout task-specific finetuning. The key to achieving this is condensing\nvarious task-specific heads into two general heads, one for discrete\nrepresentations, e.g., languages, and the other for continuous representations,\ne.g., location coordinates. The outputs of two heads can be further stacked\ninto four distinct input and output modalities. This uniform representation\nenables Hulk to treat diverse human-centric tasks as modality translation,\nintegrating knowledge across a wide range of tasks. Comprehensive evaluations\nof Hulk on 12 benchmarks covering 8 human-centric tasks demonstrate the\nsuperiority of our proposed method, achieving state-of-the-art performance in\n11 benchmarks. The code is available on https://github.com/OpenGVLab/Hulk.\n","authors":["Yizhou Wang","Yixuan Wu","Shixiang Tang","Weizhen He","Xun Guo","Feng Zhu","Lei Bai","Rui Zhao","Jian Wu","Tong He","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2312.01697v4.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.02340v4","updated":"2024-03-22T02:33:39Z","published":"2022-12-05T15:15:27Z","title":"CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text\n  Detection","summary":"  Recently, segmentation-based methods are quite popular in scene text\ndetection, which mainly contain two steps: text kernel segmentation and\nexpansion. However, the segmentation process only considers each pixel\nindependently, and the expansion process is difficult to achieve a favorable\naccuracy-speed trade-off. In this paper, we propose a Context-aware and\nBoundary-guided Network (CBN) to tackle these problems. In CBN, a basic text\ndetector is firstly used to predict initial segmentation results. Then, we\npropose a context-aware module to enhance text kernel feature representations,\nwhich considers both global and local contexts. Finally, we introduce a\nboundary-guided module to expand enhanced text kernels adaptively with only the\npixels on the contours, which not only obtains accurate text boundaries but\nalso keeps high speed, especially on high-resolution output maps. In\nparticular, with a lightweight backbone, the basic detector equipped with our\nproposed CBN achieves state-of-the-art results on several popular benchmarks,\nand our proposed CBN can be plugged into several segmentation-based methods.\nCode is available at https://github.com/XiiZhao/cbn.pytorch.\n","authors":["Xi Zhao","Wei Feng","Zheng Zhang","Jingjing Lv","Xin Zhu","Zhangang Lin","Jinghe Hu","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2212.02340v4.pdf","comment":"Accepted by IJCV 2024. Code is available at\n  https://github.com/XiiZhao/cbn.pytorch"},{"id":"http://arxiv.org/abs/2308.12038v3","updated":"2024-03-22T02:24:57Z","published":"2023-08-23T09:55:41Z","title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across\n  Languages","summary":"  Recently there has been a significant surge in multimodal learning in terms\nof both image-to-text and text-to-image generation. However, the success is\ntypically limited to English, leaving other languages largely behind. Building\na competitive counterpart in other languages is highly challenging due to the\nlow-resource nature of non-English multimodal data (i.e., lack of large-scale,\nhigh-quality image-text data). In this work, we propose MPM, an effective\ntraining paradigm for training large multimodal models in non-English\nlanguages. MPM demonstrates that Multilingual language models can Pivot\nzero-shot Multimodal learning across languages. Specifically, based on a strong\nmultilingual large language model, multimodal models pretrained on English-only\nimage-text data can well generalize to other languages in a (quasi)-zero-shot\nmanner, even surpassing models trained on image-text data in native languages.\nTaking Chinese as a practice of MPM, we build large multimodal models VisCPM in\nimage-to-text and text-to-image generation, which achieve state-of-the-art\n(open-source) performance in Chinese. To facilitate future research, we\nopen-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.\n","authors":["Jinyi Hu","Yuan Yao","Chongyi Wang","Shan Wang","Yinxu Pan","Qianyu Chen","Tianyu Yu","Hanghao Wu","Yue Zhao","Haoye Zhang","Xu Han","Yankai Lin","Jiao Xue","Dahai Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12038v3.pdf","comment":"https://github.com/OpenBMB/VisCPM.git"},{"id":"http://arxiv.org/abs/2307.00574v5","updated":"2024-03-22T02:18:11Z","published":"2023-07-02T13:57:45Z","title":"Bidirectional Temporal Diffusion Model for Temporally Consistent Human\n  Animation","summary":"  We introduce a method to generate temporally coherent human animation from a\nsingle image, a video, or a random noise. This problem has been formulated as\nmodeling of an auto-regressive generation, i.e., to regress past frames to\ndecode future frames. However, such unidirectional generation is highly prone\nto motion drifting over time, generating unrealistic human animation with\nsignificant artifacts such as appearance distortion. We claim that\nbidirectional temporal modeling enforces temporal coherence on a generative\nnetwork by largely suppressing the motion ambiguity of human appearance. To\nprove our claim, we design a novel human animation framework using a denoising\ndiffusion model: a neural network learns to generate the image of a person by\ndenoising temporal Gaussian noises whose intermediate results are\ncross-conditioned bidirectionally between consecutive frames. In the\nexperiments, our method demonstrates strong performance compared to existing\nunidirectional approaches with realistic temporal coherence.\n","authors":["Tserendorj Adiya","Jae Shin Yoon","Jungeun Lee","Sanghun Kim","Hwasup Lim"],"pdf_url":"https://arxiv.org/pdf/2307.00574v5.pdf","comment":"Project page: see https://typest.github.io/btdm"},{"id":"http://arxiv.org/abs/2403.14468v2","updated":"2024-03-22T02:16:40Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2306.07894v5","updated":"2024-03-22T02:10:49Z","published":"2023-06-13T16:39:39Z","title":"iSLAM: Imperative SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) stands as one of the critical\nchallenges in robot navigation. A SLAM system often consists of a front-end\ncomponent for motion estimation and a back-end system for eliminating\nestimation drifts. Recent advancements suggest that data-driven methods are\nhighly effective for front-end tasks, while geometry-based methods continue to\nbe essential in the back-end processes. However, such a decoupled paradigm\nbetween the data-driven front-end and geometry-based back-end can lead to\nsub-optimal performance, consequently reducing the system's capabilities and\ngeneralization potential. To solve this problem, we proposed a novel\nself-supervised imperative learning framework, named imperative SLAM (iSLAM),\nwhich fosters reciprocal correction between the front-end and back-end, thus\nenhancing performance without necessitating any external supervision.\nSpecifically, we formulate the SLAM problem as a bilevel optimization so that\nthe front-end and back-end are bidirectionally connected. As a result, the\nfront-end model can learn global geometric knowledge obtained through pose\ngraph optimization by back-propagating the residuals from the back-end\ncomponent. We showcase the effectiveness of this new framework through an\napplication of stereo-inertial SLAM. The experiments show that the iSLAM\ntraining strategy achieves an accuracy improvement of 22% on average over a\nbaseline model. To the best of our knowledge, iSLAM is the first SLAM system\nshowing that the front-end and back-end components can mutually correct each\nother in a self-supervised manner.\n","authors":["Taimeng Fu","Shaoshu Su","Yiren Lu","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2306.07894v5.pdf","comment":"The paper has been accepted by IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2403.14910v1","updated":"2024-03-22T02:06:44Z","published":"2024-03-22T02:06:44Z","title":"Defying Imbalanced Forgetting in Class Incremental Learning","summary":"  We observe a high level of imbalance in the accuracy of different classes in\nthe same old task for the first time. This intriguing phenomenon, discovered in\nreplay-based Class Incremental Learning (CIL), highlights the imbalanced\nforgetting of learned classes, as their accuracy is similar before the\noccurrence of catastrophic forgetting. This discovery remains previously\nunidentified due to the reliance on average incremental accuracy as the\nmeasurement for CIL, which assumes that the accuracy of classes within the same\ntask is similar. However, this assumption is invalid in the face of\ncatastrophic forgetting. Further empirical studies indicate that this\nimbalanced forgetting is caused by conflicts in representation between\nsemantically similar old and new classes. These conflicts are rooted in the\ndata imbalance present in replay-based CIL methods. Building on these insights,\nwe propose CLass-Aware Disentanglement (CLAD) to predict the old classes that\nare more likely to be forgotten and enhance their accuracy. Importantly, CLAD\ncan be seamlessly integrated into existing CIL methods. Extensive experiments\ndemonstrate that CLAD consistently improves current replay-based methods,\nresulting in performance gains of up to 2.56%.\n","authors":["Shixiong Xu","Gaofeng Meng","Xing Nie","Bolin Ni","Bin Fan","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.14910v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2310.00258v2","updated":"2024-03-22T01:46:44Z","published":"2023-09-30T05:19:10Z","title":"NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free\n  Knowledge Distillation","summary":"  Data-Free Knowledge Distillation (DFKD) has made significant recent strides\nby transferring knowledge from a teacher neural network to a student neural\nnetwork without accessing the original data. Nonetheless, existing approaches\nencounter a significant challenge when attempting to generate samples from\nrandom noise inputs, which inherently lack meaningful information.\nConsequently, these models struggle to effectively map this noise to the\nground-truth sample distribution, resulting in prolonging training times and\nlow-quality outputs. In this paper, we propose a novel Noisy Layer Generation\nmethod (NAYER) which relocates the random source from the input to a noisy\nlayer and utilizes the meaningful constant label-text embedding (LTE) as the\ninput. LTE is generated by using the language model once, and then it is stored\nin memory for all subsequent training processes. The significance of LTE lies\nin its ability to contain substantial meaningful inter-class information,\nenabling the generation of high-quality samples with only a few training steps.\nSimultaneously, the noisy layer plays a key role in addressing the issue of\ndiversity in sample generation by preventing the model from overemphasizing the\nconstrained label information. By reinitializing the noisy layer in each\niteration, we aim to facilitate the generation of diverse samples while still\nretaining the method's efficiency, thanks to the ease of learning provided by\nLTE. Experiments carried out on multiple datasets demonstrate that our NAYER\nnot only outperforms the state-of-the-art methods but also achieves speeds 5 to\n15 times faster than previous approaches. The code is available at\nhttps://github.com/tmtuan1307/nayer.\n","authors":["Minh-Tuan Tran","Trung Le","Xuan-May Le","Mehrtash Harandi","Quan Hung Tran","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2310.00258v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.11038v2","updated":"2024-03-22T01:33:14Z","published":"2023-12-18T09:16:48Z","title":"UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray\n  Classification","summary":"  Vision-Language Pre-training (VLP) that utilizes the multi-modal information\nto promote the training efficiency and effectiveness, has achieved great\nsuccess in vision recognition of natural domains and shown promise in medical\nimaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly\npay attention to the exploration on single dataset of CXRs, which locks the\npotential of this powerful paradigm on larger hybrid of multi-source CXRs\ndatasets. We identify that although blending samples from the diverse sources\noffers the advantages to improve the model generalization, it is still\nchallenging to maintain the consistent superiority for the task of each source\ndue to the existing heterogeneity among sources. To handle this dilemma, we\ndesign a Conquer-and-Divide pre-training framework, termed as UniChest, aiming\nto make full use of the collaboration benefit of multiple sources of CXRs while\nreducing the negative influence of the source heterogeneity. Specially, the\n``Conquer\" stage in UniChest encourages the model to sufficiently capture\nmulti-source common patterns, and the ``Divide\" stage helps squeeze\npersonalized patterns into different small experts (query networks). We conduct\nthorough experiments on many benchmarks, e.g., ChestX-ray14, CheXpert,\nVindr-CXR, Shenzhen, Open-I and SIIM-ACR Pneumothorax, verifying the\neffectiveness of UniChest over a range of baselines, and release our codes and\npre-training models at https://github.com/Elfenreigen/UniChest.\n","authors":["Tianjie Dai","Ruipeng Zhang","Feng Hong","Jiangchao Yao","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11038v2.pdf","comment":"Accepted at IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2311.18773v2","updated":"2024-03-22T01:21:14Z","published":"2023-11-30T18:19:23Z","title":"Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video\n  Understanding","summary":"  Learning from videos is an emerging research area that enables robots to\nacquire skills from human demonstrations, such as procedural videos. To do\nthis, video-language models must be able to obtain structured understandings,\nsuch as the temporal segmentation of a demonstration into sequences of actions\nand skills, and to generalize the understandings to novel domains. In pursuit\nof this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)\nstep recognition and (2) intra-video retrieval over a dataset of temporally\nsegmented and labeled tasks in International Space Station spacewalk\nrecordings. In tandem, the two tasks quantify a model's ability to make use of:\n(1) out-of-domain visual information; (2) a high temporal context window; and\n(3) multimodal (e.g. visual and speech) domains. This departs from existing\nbenchmarks for procedural video understanding, which typically deal with short\ncontext lengths and can be solved with a single modality. Spacewalk-18, with\nits inherent multimodal and long-form complexity, exposes the high difficulty\nof task recognition and segmentation. We find that state-of-the-art methods\nperform poorly on our benchmark, but improvements can be obtained by\nincorporating information from longer-range temporal context across different\nmodalities. Our experiments underscore the need to develop new approaches to\nthese tasks. Data, model, and code will be released at\nhttps://brown-palm.github.io/Spacewalk-18/.\n","authors":["Rohan Myer Krishnan","Zitian Tang","Zhiqiu Yu","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2311.18773v2.pdf","comment":"Under submission. Code and models will be released at\n  https://brown-palm.github.io/Spacewalk-18/"},{"id":"http://arxiv.org/abs/2305.14521v2","updated":"2024-03-22T01:20:41Z","published":"2023-05-23T20:49:45Z","title":"Few-shot Adaption to Distribution Shifts By Mixing Source and Target\n  Embeddings","summary":"  Pretrained machine learning models need to be adapted to distribution shifts\nwhen deployed in new target environments. When obtaining labeled data from the\ntarget distribution is expensive, few-shot adaptation with only a few examples\nfrom the target distribution becomes essential. In this work, we propose\nMixPro, a lightweight and highly data-efficient approach for few-shot\nadaptation. MixPro first generates a relatively large dataset by mixing\n(linearly combining) pre-trained embeddings of large source data with those of\nthe few target examples. This process preserves important features of both\nsource and target distributions, while mitigating the specific noise in the\nsmall target data. Then, it trains a linear classifier on the mixed embeddings\nto effectively adapts the model to the target distribution without overfitting\nthe small target data. Theoretically, we demonstrate the advantages of MixPro\nover previous methods. Our experiments, conducted across various model\narchitectures on 8 datasets featuring different types of distribution shifts,\nreveal that MixPro can outperform baselines by up to 7\\%, with only 2-4 target\nexamples.\n","authors":["Yihao Xue","Ali Payani","Yu Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2305.14521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13802v3","updated":"2024-03-22T01:17:25Z","published":"2023-05-23T08:15:02Z","title":"Online Open-set Semi-supervised Object Detection with Dual Competing\n  Head","summary":"  Open-set semi-supervised object detection (OSSOD) task leverages practical\nopen-set unlabeled datasets that comprise both in-distribution (ID) and\nout-of-distribution (OOD) instances for conducting semi-supervised object\ndetection (SSOD). The main challenge in OSSOD is distinguishing and filtering\nthe OOD instances (i.e., outliers) during pseudo-labeling since OODs will\naffect the performance. The only OSSOD work employs an additional offline OOD\ndetection network trained solely with labeled data to solve this problem.\nHowever, the limited labeled data restricts the potential for improvement.\nMeanwhile, the offline strategy results in low efficiency. To alleviate these\nissues, this paper proposes an end-to-end online OSSOD framework that improves\nperformance and efficiency: 1) We propose a semi-supervised outlier filtering\nmethod that more effectively filters the OOD instances using both labeled and\nunlabeled data. 2) We propose a threshold-free Dual Competing OOD head that\nfurther improves the performance by suppressing the error accumulation during\nsemi-supervised outlier filtering. 3) Our proposed method is an online\nend-to-end trainable OSSOD framework. Experimental results show that our method\nachieves state-of-the-art performance on several OSSOD benchmarks compared to\nexisting methods. Moreover, additional experiments show that our method is more\nefficient and can be easily applied to different SSOD frameworks to boost their\nperformance.\n","authors":["Zerun Wang","Ling Xiao","Liuyu Xiang","Zhaotian Weng","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2305.13802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14898v1","updated":"2024-03-22T01:04:51Z","published":"2024-03-22T01:04:51Z","title":"Web-based Melanoma Detection","summary":"  Melanoma is the most aggressive form of skin cancer, and early detection can\nsignificantly increase survival rates and prevent cancer spread. However,\ndeveloping reliable automated detection techniques is difficult due to the lack\nof standardized datasets and evaluation methods. This study introduces a\nunified melanoma classification approach that supports 54 combinations of 11\ndatasets and 24 state-of-the-art deep learning architectures. It enables a fair\ncomparison of 1,296 experiments and results in a lightweight model deployable\nto the web-based MeshNet architecture named Mela-D. This approach can run up to\n33x faster by reducing parameters 24x to yield an analogous 88.8\\% accuracy\ncomparable with ResNet50 on previously unseen images. This allows efficient and\naccurate melanoma detection in real-world settings that can run on\nconsumer-level hardware.\n","authors":["SangHyuk Kim","Edward Gaibor","Daniel Haehn"],"pdf_url":"https://arxiv.org/pdf/2403.14898v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.14897v1","updated":"2024-03-22T01:02:09Z","published":"2024-03-22T01:02:09Z","title":"Geometric Generative Models based on Morphological Equivariant PDEs and\n  GANs","summary":"  Content and image generation consist in creating or generating data from\nnoisy information by extracting specific features such as texture, edges, and\nother thin image structures. We are interested here in generative models, and\ntwo main problems are addressed. Firstly, the improvements of specific feature\nextraction while accounting at multiscale levels intrinsic geometric features;\nand secondly, the equivariance of the network to reduce its complexity and\nprovide a geometric interpretability. To proceed, we propose a geometric\ngenerative model based on an equivariant partial differential equation (PDE)\nfor group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on\nmorphology operators and generative adversarial networks (GANs). Equivariant\nmorphological PDE layers are composed of multiscale dilations and erosions\nformulated in Riemannian manifolds, while group symmetries are defined on a Lie\ngroup. We take advantage of the Lie group structure to properly integrate the\nequivariance in layers, and are able to use the Riemannian metric to solve the\nmultiscale morphological operations. Each point of the Lie group is associated\nwith a unique point in the manifold, which helps us derive a metric on the\nRiemannian manifold from a tensor field invariant under the Lie group so that\nthe induced metric has the same symmetries. The proposed geometric\nmorphological GAN (GM-GAN) is obtained by using the proposed morphological\nequivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.\nGM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results\nshow that GM-GAN model outperforms classical GAN.\n","authors":["El Hadji S. Diop","Thierno Fall","Alioune Mbengue","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2403.14897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11125v3","updated":"2024-03-22T00:36:02Z","published":"2023-11-18T17:14:07Z","title":"SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for\n  Category-Level Pose Estimation","summary":"  Category-level object pose estimation, aiming to predict the 6D pose and 3D\nsize of objects from known categories, typically struggles with large\nintra-class shape variation. Existing works utilizing mean shapes often fall\nshort of capturing this variation. To address this issue, we present\nSecondPose, a novel approach integrating object-specific geometric features\nwith semantic category priors from DINOv2. Leveraging the advantage of DINOv2\nin providing SE(3)-consistent semantic features, we hierarchically extract two\ntypes of SE(3)-invariant geometric features to further encapsulate\nlocal-to-global object-specific information. These geometric features are then\npoint-aligned with DINOv2 features to establish a consistent object\nrepresentation under SE(3) transformations, facilitating the mapping from\ncamera space to the pre-defined canonical space, thus further enhancing pose\nestimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose\nachieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more\ncomplex dataset HouseCat6D which provides photometrically challenging objects,\nSecondPose still surpasses other competitors by a large margin.\n","authors":["Yamei Chen","Yan Di","Guangyao Zhai","Fabian Manhardt","Chenyangguang Zhang","Ruida Zhang","Federico Tombari","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2311.11125v3.pdf","comment":"CVPR 2024 accepted. Code is available at:\n  https://github.com/NOrangeeroli/SecondPose"},{"id":"http://arxiv.org/abs/2403.12211v2","updated":"2024-03-22T00:17:11Z","published":"2024-03-18T19:51:55Z","title":"A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with\n  Missingness","summary":"  Medical records often consist of different modalities, such as images, text,\nand tabular information. Integrating all modalities offers a holistic view of a\npatient's condition, while analyzing them longitudinally provides a better\nunderstanding of disease progression. However, real-world longitudinal medical\nrecords present challenges: 1) patients may lack some or all of the data for a\nspecific timepoint, and 2) certain modalities or views might be absent for all\npatients during a particular period. In this work, we introduce a unified model\nfor longitudinal multi-modal multi-view prediction with missingness. Our method\nallows as many timepoints as desired for input, and aims to leverage all\navailable data, regardless of their availability. We conduct extensive\nexperiments on the knee osteoarthritis dataset from the Osteoarthritis\nInitiative for pain and Kellgren-Lawrence grade prediction at a future\ntimepoint. We demonstrate the effectiveness of our method by comparing results\nfrom our unified model to specific models that use the same modality and view\ncombinations during training and evaluation. We also show the benefit of having\nextended temporal data and provide post-hoc analysis for a deeper understanding\nof each modality/view's importance for different tasks.\n","authors":["Boqi Chen","Junier Oliva","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2403.12211v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15227v1","updated":"2024-03-22T14:20:54Z","published":"2024-03-22T14:20:54Z","title":"LeGO: Leveraging a Surface Deformation Network for Animatable Stylized\n  Face Generation with One Example","summary":"  Recent advances in 3D face stylization have made significant strides in few\nto zero-shot settings. However, the degree of stylization achieved by existing\nmethods is often not sufficient for practical applications because they are\nmostly based on statistical 3D Morphable Models (3DMM) with limited variations.\nTo this end, we propose a method that can produce a highly stylized 3D face\nmodel with desired topology. Our methods train a surface deformation network\nwith 3DMM and translate its domain to the target style using a paired exemplar.\nThe network achieves stylization of the 3D face mesh by mimicking the style of\nthe target using a differentiable renderer and directional CLIP losses.\nAdditionally, during the inference process, we utilize a Mesh Agnostic Encoder\n(MAGE) that takes deformation target, a mesh of diverse topologies as input to\nthe stylization process and encodes its shape into our latent space. The\nresulting stylized face model can be animated by commonly used 3DMM blend\nshapes. A set of quantitative and qualitative evaluations demonstrate that our\nmethod can produce highly stylized face meshes according to a given style and\noutput them in a desired topology. We also demonstrate example applications of\nour method including image-based stylized avatar generation, linear\ninterpolation of geometric styles, and facial animation of stylized avatars.\n","authors":["Soyeon Yoon","Kwan Yun","Kwanggyoon Seo","Sihun Cha","Jung Eun Yoo","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.15227v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.15064v1","updated":"2024-03-22T09:46:11Z","published":"2024-03-22T09:46:11Z","title":"Recent Trends in 3D Reconstruction of General Non-Rigid Scenes","summary":"  Reconstructing models of the real world, including 3D geometry, appearance,\nand motion of real scenes, is essential for computer graphics and computer\nvision. It enables the synthesizing of photorealistic novel views, useful for\nthe movie industry and AR/VR applications. It also facilitates the content\ncreation necessary in computer games and AR/VR by avoiding laborious manual\ndesign processes. Further, such models are fundamental for intelligent\ncomputing systems that need to interpret real-world scenes and actions to act\nand interact safely with the human world. Notably, the world surrounding us is\ndynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a\nseverely underconstrained and challenging problem. This state-of-the-art report\n(STAR) offers the reader a comprehensive summary of state-of-the-art techniques\nwith monocular and multi-view inputs such as data from RGB and RGB-D sensors,\namong others, conveying an understanding of different approaches, their\npotential applications, and promising further research directions. The report\ncovers 3D reconstruction of general non-rigid scenes and further addresses the\ntechniques for scene decomposition, editing and controlling, and generalizable\nand generative modeling. More specifically, we first review the common and\nfundamental concepts necessary to understand and navigate the field and then\ndiscuss the state-of-the-art techniques by reviewing recent approaches that use\ntraditional and machine-learning-based neural representations, including a\ndiscussion on the newly enabled applications. The STAR is concluded with a\ndiscussion of the remaining limitations and open challenges.\n","authors":["Raza Yunus","Jan Eric Lenssen","Michael Niemeyer","Yiyi Liao","Christian Rupprecht","Christian Theobalt","Gerard Pons-Moll","Jia-Bin Huang","Vladislav Golyanik","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2403.15064v1.pdf","comment":"42 pages, 18 figures, 5 tables; State-of-the-Art Report at\n  EUROGRAPHICS 2024"},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/1808.07840v2","updated":"2024-03-22T07:27:29Z","published":"2018-08-23T16:55:53Z","title":"Learning to Importance Sample in Primary Sample Space","summary":"  Importance sampling is one of the most widely used variance reduction\nstrategies in Monte Carlo rendering. In this paper, we propose a novel\nimportance sampling technique that uses a neural network to learn how to sample\nfrom a desired density represented by a set of samples. Our approach considers\nan existing Monte Carlo rendering algorithm as a black box. During a\nscene-dependent training phase, we learn to generate samples with a desired\ndensity in the primary sample space of the rendering algorithm using maximum\nlikelihood estimation. We leverage a recent neural network architecture that\nwas designed to represent real-valued non-volume preserving ('Real NVP')\ntransformations in high dimensional spaces. We use Real NVP to non-linearly\nwarp primary sample space and obtain desired densities. In addition, Real NVP\nefficiently computes the determinant of the Jacobian of the warp, which is\nrequired to implement the change of integration variables implied by the warp.\nA main advantage of our approach is that it is agnostic of underlying light\ntransport effects, and can be combined with many existing rendering techniques\nby treating them as a black box. We show that our approach leads to effective\nvariance reduction in several practical scenarios.\n","authors":["Quan Zheng","Matthias Zwicker"],"pdf_url":"https://arxiv.org/pdf/1808.07840v2.pdf","comment":"11 pages, 14 figure; authors' version, the definitive version of\n  record is available at https://onlinelibrary.wiley.com/doi/10.1111/cgf.13628"}]},"2024-03-25T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.16696v1","updated":"2024-03-25T12:27:24Z","published":"2024-03-25T12:27:24Z","title":"BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based\n  Obstacle Avoidance","summary":"  Nano-drones, distinguished by their agility, minimal weight, and\ncost-effectiveness, are particularly well-suited for exploration in confined,\ncluttered and narrow spaces. Recognizing transparent, highly reflective or\nabsorbing materials, such as glass and metallic surfaces is challenging, as\nclassical sensors, such as cameras or laser rangers, often do not detect them.\nInspired by bats, which can fly at high speeds in complete darkness with the\nhelp of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering\nsensor-deck employing a lightweight and low-power ultrasonic sensor for\nnano-drone autonomous navigation. This paper first provides insights about\nsensor characteristics, highlighting the influence of motor noise on the\nultrasound readings, then it introduces the results of extensive experimental\ntests for obstacle avoidance (OA) in a diverse environment. Results show that\n\\textit{BatDeck} allows exploration for a flight time of 8 minutes while\ncovering 136m on average before crash in a challenging environment with\ntransparent and reflective obstacles, proving the effectiveness of ultrasonic\nsensors for OA on nano-drones.\n","authors":["Hanna Müller","Victor Kartsch","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.16696v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16669v1","updated":"2024-03-25T12:07:24Z","published":"2024-03-25T12:07:24Z","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network","summary":"  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n","authors":["Yin Zhang","Jinhong Deng","Peidong Liu","Wen Li","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16669v1.pdf","comment":"17 pages, 11 figures. Accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2403.16664v1","updated":"2024-03-25T11:57:30Z","published":"2024-03-25T11:57:30Z","title":"Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation\n  in Unknown Environments","summary":"  This paper focuses on the acquisition of mapless navigation skills within\nunknown environments. We introduce the Skill Q-Network (SQN), a novel\nreinforcement learning method featuring an adaptive skill ensemble mechanism.\nUnlike existing methods, our model concurrently learns a high-level skill\ndecision process alongside multiple low-level navigation skills, all without\nthe need for prior knowledge. Leveraging a tailored reward function for mapless\nnavigation, the SQN is capable of learning adaptive maneuvers that incorporate\nboth exploration and goal-directed skills, enabling effective navigation in new\nenvironments. Our experiments demonstrate that our SQN can effectively navigate\ncomplex environments, exhibiting a 40% higher performance compared to baseline\nmodels. Without explicit guidance, SQN discovers how to combine low-level skill\npolicies, showcasing both goal-directed navigations to reach destinations and\nexploration maneuvers to escape from local minimum regions in challenging\nscenarios. Remarkably, our adaptive skill ensemble method enables zero-shot\ntransfer to out-of-distribution domains, characterized by unseen observations\nfrom non-convex obstacles or uneven, subterranean-like environments.\n","authors":["Hyunki Seong","David Hyunchul Shim"],"pdf_url":"https://arxiv.org/pdf/2403.16664v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16652v1","updated":"2024-03-25T11:40:32Z","published":"2024-03-25T11:40:32Z","title":"Trajectory Planning of Robotic Manipulator in Dynamic Environment\n  Exploiting DRL","summary":"  This study is about the implementation of a reinforcement learning algorithm\nin the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick\nand place the randomly placed block at a random target point in an unknown\nenvironment. The obstacle is randomly moving which creates a hurdle in picking\nthe object. The objective of the robot is to avoid the obstacle and pick the\nblock with constraints to a fixed timestamp. In this literature, we have\napplied a deep deterministic policy gradient (DDPG) algorithm and compared the\nmodel's efficiency with dense and sparse rewards.\n","authors":["Osama Ahmad","Zawar Hussain","Hammad Naeem"],"pdf_url":"https://arxiv.org/pdf/2403.16652v1.pdf","comment":"Accepted in ICIESTR-2024"},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16634v1","updated":"2024-03-25T11:22:38Z","published":"2024-03-25T11:22:38Z","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for\n  Computations in Matlab","summary":"  Geometric algebra (GA) is a mathematical tool for geometric computing,\nproviding a framework that allows a unified and compact approach to geometric\nrelations which in other mathematical systems are typically described using\ndifferent more complicated elements. This fact has led to an increasing\nadoption of GA in applied mathematics and engineering problems. However, the\nscarcity of symbolic implementations of GA and its inherent complexity,\nrequiring a specific mathematical background, make it challenging and less\nintuitive for engineers to work with. This prevents wider adoption among more\napplied professionals. To address this challenge, this paper introduces SUGAR\n(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox\ndesigned for Matlab and licensed under the MIT License. SUGAR facilitates the\ntranslation of GA concepts into Matlab and provides a collection of\nuser-friendly functions tailored for GA computations, including support for\nsymbolic operations. It supports both numeric and symbolic computations in\nhigh-dimensional GAs. Specifically tailored for applied mathematics and\nengineering applications, SUGAR has been meticulously engineered to represent\ngeometric elements and transformations within two and three-dimensional\nprojective and conformal geometric algebras, aligning with established\ncomputational methodologies in the literature. Furthermore, SUGAR efficiently\nhandles functions of multivectors, such as exponential, logarithmic,\nsinusoidal, and cosine functions, enhancing its applicability across various\nengineering domains, including robotics, control systems, and power\nelectronics. Finally, this work includes four distinct validation examples,\ndemonstrating SUGAR's capabilities across the above-mentioned fields and its\npractical utility in addressing real-world applied mathematics and engineering\nproblems.\n","authors":["Manel Velasco","Isiah Zaplana","Arnau Dória-Cerezo","Pau Martí"],"pdf_url":"https://arxiv.org/pdf/2403.16634v1.pdf","comment":"33 pages, 6 figures, journal paper submitted to ACM TOMS"},{"id":"http://arxiv.org/abs/2310.00262v2","updated":"2024-03-25T10:45:50Z","published":"2023-09-30T05:26:42Z","title":"Robust Integral Consensus Control of Multi-Agent Networks Perturbed by\n  Matched and Unmatched Disturbances: The Case of Directed Graphs","summary":"  This work presents a new method to design consensus controllers for perturbed\ndouble integrator systems whose interconnection is described by a directed\ngraph containing a rooted spanning tree. We propose new robust controllers to\nsolve the consensus and synchronization problems when the systems are under the\neffects of matched and unmatched disturbances. In both problems, we present\nsimple continuous controllers, whose integral actions allow us to handle the\ndisturbances. A rigorous stability analysis based on Lyapunov's direct method\nfor unperturbed networked systems is presented. To assess the performance of\nour result, a representative simulation study is presented.\n","authors":["Jose Guadalupe Romero","David Navarro-Alarcon"],"pdf_url":"https://arxiv.org/pdf/2310.00262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16613v1","updated":"2024-03-25T10:43:47Z","published":"2024-03-25T10:43:47Z","title":"Technical Development of a Semi-Autonomous Robotic Partition","summary":"  This technical description details the design and engineering process of a\nsemi-autonomous robotic partition. This robotic partition prototype was\nsubsequently employed in a longer-term evaluation in-the-wild study conducted\nby the authors in a real-world office setting.\n","authors":["Binh Vinh Duc Nguyen","Andrew Vande Moere"],"pdf_url":"https://arxiv.org/pdf/2403.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16606v1","updated":"2024-03-25T10:33:20Z","published":"2024-03-25T10:33:20Z","title":"ROXIE: Defining a Robotic eXplanation and Interpretability Engine","summary":"  In an era where autonomous robots increasingly inhabit public spaces, the\nimperative for transparency and interpretability in their decision-making\nprocesses becomes paramount. This paper presents the overview of a Robotic\neXplanation and Interpretability Engine (ROXIE), which addresses this critical\nneed, aiming to demystify the opaque nature of complex robotic behaviors. This\npaper elucidates the key features and requirements needed for providing\ninformation and explanations about robot decision-making processes. It also\noverviews the suite of software components and libraries available for\ndeployment with ROS 2, empowering users to provide comprehensive explanations\nand interpretations of robot processes and behaviors, thereby fostering trust\nand collaboration in human-robot interactions.\n","authors":["Francisco J. Rodríguez-Lera","Miguel A. González-Santamarta","Alejandro González-Cantón","Laura Fernández-Becerra","David Sobrín-Hidalgo","Angel Manuel Guerrero-Higueras"],"pdf_url":"https://arxiv.org/pdf/2403.16606v1.pdf","comment":"7 pages, 3 figures, 1 tables, Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.16600v1","updated":"2024-03-25T10:20:50Z","published":"2024-03-25T10:20:50Z","title":"Research Challenges for Adaptive Architecture: Empowering Occupants of\n  Multi-Occupancy Buildings","summary":"  This positional paper outlines our vision of 'adaptive architecture', which\ninvolves the integration of robotic technology to physically change an\narchitectural space in supporting the changing needs of its occupants, in\nresponse to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data &\nTechnology\" call on \"How do new technologies enable and empower the inhabitants\nof multi-occupancy buildings?\". Specifically, while adaptive architecture holds\npromise for enhancing occupant satisfaction, comfort, and overall health and\nwell-being, there remains a range of research challenges of (1) how it can\neffectively support individual occupants, while (2) mediating the conflicting\nneeds of collocated others, and (3) integrating meaningfully into the\nsociocultural characteristics of their building community.\n","authors":["Binh Vinh Duc Nguyen","Andrew Vande Moere"],"pdf_url":"https://arxiv.org/pdf/2403.16600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16595v1","updated":"2024-03-25T10:16:51Z","published":"2024-03-25T10:16:51Z","title":"The Adaptive Workplace: Orchestrating Architectural Services around the\n  Wellbeing of Individual Occupants","summary":"  As the academic consortia members of the EU Horizon project SONATA\n(\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the\nworkshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\"\nby proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to\nadapt a workplace to the ever-changing needs of individual occupants, instead\nof that occupants are expected to adapt to their workplace.\n","authors":["Andrew Vande Moere","Sara Arko","Alena Safrova Drasilova","Tomáš Ondráček","Ilaria Pigliautile","Benedetta Pioppi","Anna Laura Pisello","Jakub Prochazka","Paula Acuna Roncancio","Davide Schaumann","Marcel Schweiker","Binh Vinh Duc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16593v1","updated":"2024-03-25T10:09:42Z","published":"2024-03-25T10:09:42Z","title":"Counter-example guided Imitation Learning of Feedback Controllers from\n  Temporal Logic Specifications","summary":"  We present a novel method for imitation learning for control requirements\nexpressed using Signal Temporal Logic (STL). More concretely we focus on the\nproblem of training a neural network to imitate a complex controller. The\nlearning process is guided by efficient data aggregation based on\ncounter-examples and a coverage measure. Moreover, we introduce a method to\nevaluate the performance of the learned controller via parameterization and\nparameter estimation of the STL requirements. We demonstrate our approach with\na flying robot case study.\n","authors":["Thao Dang","Alexandre Donzé","Inzemamul Haque","Nikolaos Kekatos","Indranil Saha"],"pdf_url":"https://arxiv.org/pdf/2403.16593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16560v1","updated":"2024-03-25T09:18:48Z","published":"2024-03-25T09:18:48Z","title":"Active Admittance Control with Iterative Learning for General-Purpose\n  Contact-Rich Manipulation","summary":"  Force interaction is inevitable when robots face multiple operation\nscenarios. How to make the robot competent in force control for generalized\noperations such as multi-tasks still remains a challenging problem. Aiming at\nthe reproducibility of interaction tasks and the lack of a generalized force\ncontrol framework for multi-task scenarios, this paper proposes a novel hybrid\ncontrol framework based on active admittance control with iterative learning\nparameters-tunning mechanism. The method adopts admittance control as the\nunderlying algorithm to ensure flexibility, and iterative learning as the\nhigh-level algorithm to regulate the parameters of the admittance model. The\nwhole algorithm has flexibility and learning ability, which is capable of\nachieving the goal of excellent versatility. Four representative interactive\nrobot manipulation tasks are chosen to investigate the consistency and\ngeneralisability of the proposed method. Experiments are designed to verify the\neffectiveness of the whole framework, and an average of 98.21% and 91.52%\nimprovement of RMSE is obtained relative to the traditional admittance control\nas well as the model-free adaptive control, respectively.\n","authors":["Bo Zhou","Yuyao Sun","Wenbo Liu","Ruixuan Jiao","Fang Fang","Shihua Li"],"pdf_url":"https://arxiv.org/pdf/2403.16560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16535v1","updated":"2024-03-25T08:26:20Z","published":"2024-03-25T08:26:20Z","title":"Arm-Constrained Curriculum Learning for Loco-Manipulation of the\n  Wheel-Legged Robot","summary":"  Incorporating a robotic manipulator into a wheel-legged robot enhances its\nagility and expands its potential for practical applications. However, the\npresence of potential instability and uncertainties presents additional\nchallenges for control objectives. In this paper, we introduce an\narm-constrained curriculum learning architecture to tackle the issues\nintroduced by adding the manipulator. Firstly, we develop an arm-constrained\nreinforcement learning algorithm to ensure safety and stability in control\nperformance. Additionally, to address discrepancies in reward settings between\nthe arm and the base, we propose a reward-aware curriculum learning method. The\npolicy is first trained in Isaac gym and transferred to the physical robot to\ndo dynamic grasping tasks, including the door-opening task, fan-twitching task\nand the relay-baton-picking and following task. The results demonstrate that\nour proposed approach effectively controls the arm-equipped wheel-legged robot\nto master dynamic grasping skills, allowing it to chase and catch a moving\nobject while in motion. The code can be found at\nhttps://github.com/aCodeDog/legged-robots-manipulation. To view the\nsupplemental video, please visit https://youtu.be/sNXT-rwPNMM.\n","authors":["Zifan Wang","Yufei Jia","Lu Shi","Haoyu Wang","Haizhou Zhao","Xueyang Li","Jinni Zhou","Jun Ma","Guyue Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16489v1","updated":"2024-03-25T07:17:44Z","published":"2024-03-25T07:17:44Z","title":"Spatially temporally distributed informative path planning for\n  multi-robot systems","summary":"  This paper investigates the problem of informative path planning for a mobile\nrobotic sensor network in spatially temporally distributed mapping. The robots\nare able to gather noisy measurements from an area of interest during their\nmovements to build a Gaussian Process (GP) model of a spatio-temporal field.\nThe model is then utilized to predict the spatio-temporal phenomenon at\ndifferent points of interest. To spatially and temporally navigate the group of\nrobots so that they can optimally acquire maximal information gains while their\nconnectivity is preserved, we propose a novel multistep prediction informative\npath planning optimization strategy employing our newly defined local cost\nfunctions. By using the dual decomposition method, it is feasible and practical\nto effectively solve the optimization problem in a distributed manner. The\nproposed method was validated through synthetic experiments utilizing\nreal-world data sets.\n","authors":["Binh Nguyen","Linh Nguyen","Truong X. Nghiem","Hung La","Jose Baca","Pablo Rangel","Miguel Cid Montoya","Thang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16485v1","updated":"2024-03-25T07:12:51Z","published":"2024-03-25T07:12:51Z","title":"Real-time Model Predictive Control with Zonotope-Based Neural Networks\n  for Bipedal Social Navigation","summary":"  This study addresses the challenge of bipedal navigation in a dynamic\nhuman-crowded environment, a research area that remains largely underexplored\nin the field of legged navigation. We propose two cascaded zonotope-based\nneural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future\ntrajectory prediction and an Ego-agent Social Network (ESN) for ego-agent\nsocial path planning. Representing future paths as zonotopes allows for\nefficient reachability-based planning and collision checking. The ESN is then\nintegrated with a Model Predictive Controller (ESN-MPC) for footstep planning\nfor our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a\ncollision-free optimal trajectory by optimizing through the gradients of ESN.\nESN-MPC optimal trajectory is sent to the low-level controller for full-order\nsimulation of Digit. The overall proposed framework is validated with extensive\nsimulations on randomly generated initial settings with varying human crowd\ndensities.\n","authors":["Abdulaziz Shamsah","Krishanu Agarwal","Shreyas Kousik","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16485v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.16478v1","updated":"2024-03-25T07:04:24Z","published":"2024-03-25T07:04:24Z","title":"Towards Cooperative Maneuver Planning in Mixed Traffic at Urban\n  Intersections","summary":"  Connected automated driving promises a significant improvement of traffic\nefficiency and safety on highways and in urban areas. Apart from sharing of\nawareness and perception information over wireless communication links,\ncooperative maneuver planning may facilitate active guidance of connected\nautomated vehicles at urban intersections. Research in automatic intersection\nmanagement put forth a large body of works that mostly employ rule-based or\noptimization-based approaches primarily in fully automated simulated\nenvironments. In this work, we present two cooperative planning approaches that\nare capable of handling mixed traffic, i.e., the road being shared by automated\nvehicles and regular vehicles driven by humans. Firstly, we propose an\noptimization-based planner trained on real driving data that cyclically selects\nthe most efficient out of multiple predicted coordinated maneuvers.\nAdditionally, we present a cooperative planning approach based on graph-based\nreinforcement learning, which conquers the lack of ground truth data for\ncooperative maneuvers. We present evaluation results of both cooperative\nplanners in high-fidelity simulation and real-world traffic. Simulative\nexperiments in fully automated traffic and mixed traffic show that cooperative\nmaneuver planning leads to less delay due to interaction and a reduced number\nof stops. In real-world experiments with three prototype connected automated\nvehicles in public traffic, both planners demonstrate their ability to perform\nefficient cooperative maneuvers.\n","authors":["Marvin Klimke","Max Bastian Mertens","Benjamin Völz","Michael Buchholz"],"pdf_url":"https://arxiv.org/pdf/2403.16478v1.pdf","comment":"M. Klimke and M. Mertens are both first authors with equal\n  contribution. 11 pages, 10 figures, 2 tables, submitted to IEEE Transactions\n  on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16430v1","updated":"2024-03-25T05:21:19Z","published":"2024-03-25T05:21:19Z","title":"AeroBridge: Autonomous Drone Handoff System for Emergency Battery\n  Service","summary":"  This paper proposes an Emergency Battery Service (EBS) for drones in which an\nEBS drone flies to a drone in the field with a depleted battery and transfers a\nfresh battery to the exhausted drone. The authors present a unique battery\ntransfer mechanism and drone localization that uses the Cross Marker Position\n(CMP) method. The main challenges include a stable and balanced transfer that\nprecisely localizes the receiver drone. The proposed EBS drone mitigates the\neffects of downwash due to the vertical proximity between the drones by\nimplementing diagonal alignment with the receiver, reducing the distance to 0.5\nm between the two drones. CFD analysis shows that diagonal instead of\nperpendicular alignment minimizes turbulence, and the authors verify the actual\nsystem for change in output airflow and thrust measurements. The CMP\nmarker-based localization method enables position lock for the EBS drone with\nup to 0.9 cm accuracy. The performance of the transfer mechanism is validated\nexperimentally by successful mid-air transfer in 5 seconds, where the EBS drone\nis within 0.5 m vertical distance from the receiver drone, wherein 4m/s\nturbulence does not affect the transfer process.\n","authors":["Avishkar Seth","Alice James","Endrowednes Kuantama","Richard Han","Subhas Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2403.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17463v2","updated":"2024-03-25T05:16:38Z","published":"2024-01-30T21:51:57Z","title":"A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev\n  Interpolation","summary":"  We propose a new metric for robot state estimation based on the recently\nintroduced $\\text{SE}_2(3)$ Lie group definition. Our metric is related to\nprior metrics for SLAM but explicitly takes into account the linear velocity of\nthe state estimate, improving over current pose-based trajectory analysis. This\nhas the benefit of providing a single, quantitative metric to evaluate state\nestimation algorithms against, while being compatible with existing tools and\nlibraries. Since ground truth data generally consists of pose data from motion\ncapture systems, we also propose an approach to compute the ground truth linear\nvelocity based on polynomial interpolation. Using Chebyshev interpolation and a\npseudospectral parameterization, we can accurately estimate the ground truth\nlinear velocity of the trajectory in an optimal fashion with best approximation\nerror. We demonstrate how this approach performs on multiple robotic platforms\nwhere accurate state estimation is vital, and compare it to alternative\napproaches such as finite differences. The pseudospectral parameterization also\nprovides a means of trajectory data compression as an additional benefit.\nExperimental results show our method provides a valid and accurate means of\ncomparing state estimation systems, which is also easy to interpret and report.\n","authors":["Varun Agrawal","Frank Dellaert"],"pdf_url":"https://arxiv.org/pdf/2401.17463v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.16425v1","updated":"2024-03-25T05:10:34Z","published":"2024-03-25T05:10:34Z","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in\n  Event Cameras","summary":"  Event cameras are increasingly popular in robotics due to their beneficial\nfeatures, such as low latency, energy efficiency, and high dynamic range.\nNevertheless, their downstream task performance is greatly influenced by the\noptimization of bias parameters. These parameters, for instance, regulate the\nnecessary change in light intensity to trigger an event, which in turn depends\non factors such as the environment lighting and camera motion. This paper\nintroduces feedback control algorithms that automatically tune the bias\nparameters through two interacting methods: 1) An immediate, on-the-fly fast\nadaptation of the refractory period, which sets the minimum interval between\nconsecutive events, and 2) if the event rate exceeds the specified bounds even\nafter changing the refractory period repeatedly, the controller adapts the\npixel bandwidth and event thresholds, which stabilizes after a short period of\nnoise events across all pixels (slow adaptation). Our evaluation focuses on the\nvisual place recognition task, where incoming query images are compared to a\ngiven reference database. We conducted comprehensive evaluations of our\nalgorithms' adaptive feedback control in real-time. To do so, we collected the\nQCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366\nrepeated traversals of a Scout Mini robot navigating through a 100 meter long\nindoor lab setting (totaling over 35km distance traveled) in varying brightness\nconditions with ground truth location information. Our proposed feedback\ncontrollers result in superior performance when compared to the standard bias\nsettings and prior feedback control methods. Our findings also detail the\nimpact of bias adjustments on task performance and feature ablation studies on\nthe fast and slow adaptation mechanisms.\n","authors":["Gokul B. Nair","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2403.16425v1.pdf","comment":"8 pages, 9 figures, paper under review"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16419v1","updated":"2024-03-25T04:45:16Z","published":"2024-03-25T04:45:16Z","title":"Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on\n  Vertically Challenging Terrain","summary":"  Wheeled robots have recently demonstrated superior mechanical capability to\ntraverse vertically challenging terrain (e.g., extremely rugged boulders\ncomparable in size to the vehicles themselves). Negotiating such terrain\nintroduces significant variations of vehicle pose in all six Degrees-of-Freedom\n(DoFs), leading to imbalanced contact forces, varying momentum, and chassis\ndeformation due to non-rigid tires and suspensions. To autonomously navigate on\nvertically challenging terrain, all these factors need to be efficiently\nreasoned within limited onboard computation and strict real-time constraints.\nIn this paper, we propose a 6-DoF kinodynamics learning approach that is\nattentive only to the specific underlying terrain critical to the current\nvehicle-terrain interaction, so that it can be efficiently queried in real-time\nmotion planners onboard small robots. Physical experiment results show our\nTerrain-Attentive Learning demonstrates on average 51.1% reduction in model\nprediction error among all 6 DoFs compared to a state-of-the-art model for\nvertically challenging terrain.\n","authors":["Aniket Datar","Chenhui Pan","Mohammad Nazeri","Anuj Pokhrel","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16400v1","updated":"2024-03-25T03:30:37Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10670v2","updated":"2024-03-25T02:52:43Z","published":"2024-02-16T13:21:33Z","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models","summary":"  Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.\n","authors":["Yuxuan Kuang","Hai Lin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.10670v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.11384v2","updated":"2024-03-25T02:10:23Z","published":"2024-03-18T00:22:30Z","title":"Towards Massive Interaction with Generalist Robotics: A Systematic\n  Review of XR-enabled Remote Human-Robot Interaction Systems","summary":"  This survey provides an exhaustive review of the applications of extended\nreality (XR) technologies in the field of remote human-computer interaction\n(HRI). We developed a systematic search strategy based on the PRISMA\nmethodology. From the initial 2,561 articles selected, 100 research papers that\nmet our inclusion criteria were included. We categorized and summarized the\ndomain in detail, delving into XR technologies, including augmented reality\n(AR), virtual reality (VR), and mixed reality (MR), and their applications in\nfacilitating intuitive and effective remote control and interaction with\nrobotic systems.The survey highlights existing articles on the application of\nXR technologies, user experience enhancement, and various interaction designs\nfor XR in remote HRI, providing insights into current trends and future\ndirections. We also identified potential gaps and opportunities for future\nresearch to improve remote HRI systems through XR technology to guide and\ninform future XR and robotics research.\n","authors":["Xian Wang","Luyao Shen","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16366v1","updated":"2024-03-25T02:04:06Z","published":"2024-03-25T02:04:06Z","title":"SE(3) Linear Parameter Varying Dynamical Systems for Globally\n  Asymptotically Stable End-Effector Control","summary":"  Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into\nan autonomous first-order DS that enables reactive responses to perturbations,\nwhile ensuring globally asymptotic stability at the target. However, the\ncurrent LPV-DS framework is established on Euclidean data only and has not been\napplicable to broader robotic applications requiring pose control. In this\npaper we present an extension to the current LPV-DS framework, named\nQuaternion-DS, which efficiently learns a DS-based motion policy for\norientation. Leveraging techniques from differential geometry and Riemannian\nstatistics, our approach properly handles the non-Euclidean orientation data in\nquaternion space, enabling the integration with positional control, namely\nSE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is\npreserved. Through simulation and real robot experiments, we validate our\nmethod, demonstrating its ability to efficiently and accurately reproduce the\noriginal SE(3) trajectory while exhibiting strong robustness to perturbations\nin task space.\n","authors":["Sunan Sun","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2403.16366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02609v3","updated":"2024-03-25T01:50:40Z","published":"2023-09-05T22:53:37Z","title":"Directionality-Aware Mixture Model Parallel Sampling for Efficient\n  Linear Parameter Varying Dynamical System Learning","summary":"  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective\napproach that learns stable, time-invariant motion policies using statistical\nmodeling and semi-definite optimization to encode complex motions for reactive\nrobot control. Despite its strengths, the LPV-DS learning approach faces\nchallenges in achieving a high model accuracy without compromising the\ncomputational efficiency. To address this, we introduce the\nDirectionality-Aware Mixture Model (DAMM), a novel statistical model that\napplies the Riemannian metric on the n-sphere $\\mathbb{S}^n$ to efficiently\nblend non-Euclidean directional data with $\\mathbb{R}^m$ Euclidean states.\nAdditionally, we develop a hybrid Markov chain Monte Carlo technique that\ncombines Gibbs Sampling with Split/Merge Proposal, allowing for parallel\ncomputation to drastically speed up inference. Our extensive empirical tests\ndemonstrate that LPV-DS integrated with DAMM achieves higher reproduction\naccuracy, better model efficiency, and near real-time/online learning compared\nto standard estimation methods on various datasets. Lastly, we demonstrate its\nsuitability for incrementally learning multi-behavior policies in real-world\nrobot experiments.\n","authors":["Sunan Sun","Haihui Gao","Tianyu Li","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2309.02609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16356v1","updated":"2024-03-25T01:33:03Z","published":"2024-03-25T01:33:03Z","title":"Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain\n  Mapping and Locomotion Stability","summary":"  We study the problem of bipedal robot navigation in complex environments with\nuncertain and rough terrain. In particular, we consider a scenario in which the\nrobot is expected to reach a desired goal location by traversing an environment\nwith uncertain terrain elevation. Such terrain uncertainties induce not only\nuntraversable regions but also robot motion perturbations. Thus, the problems\nof terrain mapping and locomotion stability are intertwined. We evaluate three\ndifferent kernels for Gaussian process (GP) regression to learn the terrain\nelevation. We also learn the motion deviation resulting from both the terrain\nas well as the discrepancy between the reduced-order Prismatic Inverted\nPendulum Model used for planning and the full-order locomotion dynamics. We\npropose a hierarchical locomotion-dynamics-aware sampling-based navigation\nplanner. The global navigation planner plans a series of local waypoints to\nreach the desired goal locations while respecting locomotion stability\nconstraints. Then, a local navigation planner is used to generate a sequence of\ndynamically feasible footsteps to reach local waypoints. We develop a novel\ntrajectory evaluation metric to minimize motion deviation and maximize\ninformation gain of the terrain elevation map. We evaluate the efficacy of our\nplanning framework on Digit bipedal robot simulation in MuJoCo.\n","authors":["Kasidit Muenprasitivej","Jesse Jiang","Abdulaziz Shamsah","Samuel Coogan","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16356v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09900v2","updated":"2024-03-25T01:29:27Z","published":"2024-03-14T22:22:22Z","title":"DTG : Diffusion-based Trajectory Generation for Mapless Global\n  Navigation","summary":"  We present a novel end-to-end diffusion-based trajectory generation method,\nDTG, for mapless global navigation in challenging outdoor scenarios with\nocclusions and unstructured off-road features like grass, buildings, bushes,\netc. Given a distant goal, our approach computes a trajectory that satisfies\nthe following goals: (1) minimize the travel distance to the goal; (2) maximize\nthe traversability by choosing paths that do not lie in undesirable areas.\nSpecifically, we present a novel Conditional RNN(CRNN) for diffusion models to\nefficiently generate trajectories. Furthermore, we propose an adaptive training\nmethod that ensures that the diffusion model generates more traversable\ntrajectories. We evaluate our methods in various outdoor scenes and compare the\nperformance with other global navigation algorithms on a Husky robot. In\npractice, we observe at least a 15% improvement in traveling distance and\naround a 7% improvement in traversability.\n","authors":["Jing Liang","Amirreza Payandeh","Daeun Song","Xuesu Xiao","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.09900v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.17270v1","updated":"2024-03-25T23:28:57Z","published":"2024-03-25T23:28:57Z","title":"Human Stress Response and Perceived Safety during Encounters with\n  Quadruped Robots","summary":"  Despite the rise of mobile robot deployments in home and work settings,\nperceived safety of users and bystanders is understudied in the human-robot\ninteraction (HRI) literature. To address this, we present a study designed to\nidentify elements of a human-robot encounter that correlate with observed\nstress response. Stress is a key component of perceived safety and is strongly\nassociated with human physiological response. In this study a Boston Dynamics\nSpot and a Unitree Go1 navigate autonomously through a shared environment\noccupied by human participants wearing multimodal physiological sensors to\ntrack their electrocardiography (ECG) and electrodermal activity (EDA). The\nencounters are varied through several trials and participants self-rate their\nstress levels after each encounter. The study resulted in a multidimensional\ndataset archiving various objective and subjective aspects of a human-robot\nencounter, containing insights for understanding perceived safety in such\nencounters. To this end, acute stress responses were decoded from the human\nparticipants' ECG and EDA and compared across different human-robot encounter\nconditions. Statistical analysis of data indicate that on average (1)\nparticipants feel more stress during encounters compared to baselines, (2)\nparticipants feel more stress encountering multiple robots compared to a single\nrobot and (3) participants stress increases during navigation behavior compared\nwith search behavior.\n","authors":["Ryan Gupta","Hyonyoung Shin","Emily Norman","Keri K. Stephens","Nanshu Lu","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2403.17270v1.pdf","comment":"7 pages, 7 figs, 5 tables"},{"id":"http://arxiv.org/abs/2309.14150v8","updated":"2024-03-25T23:25:17Z","published":"2023-09-25T14:04:31Z","title":"Fast LiDAR Informed Visual Search in Unseen Indoor Environments","summary":"  This paper explores the problem of planning for visual search without prior\nmap information. We leverage the pixel-wise environment perception problem\nwhere one is given wide Field of View 2D scan data and must perform LiDAR\nsegmentation to contextually label points in the surroundings. These pixel\nclassifications provide an informed prior on which to plan next best viewpoints\nduring visual search tasks. We present LIVES: LiDAR Informed Visual Search, a\nmethod aimed at finding objects of interest in unknown indoor environments. A\nrobust map-free classifier is trained from expert data collected using a simple\ncart platform equipped with a map-based classifier. An autonomous exploration\nplanner takes the contextual data from scans and uses that prior to plan\nviewpoints more likely to yield detection of the search target. We propose a\nutility function that accounts for traditional metrics like information gain\nand path cost and for the contextual information. LIVES is baselined against\nseveral existing exploration methods in simulation to verify its performance.\nIt is validated in real-world experiments with single and multiple search\nobjects with a Spot robot in two unseen environments. Videos of experiments,\nimplementation details and open source code can be found at\nhttps://sites.google.com/view/lives-2024/home.\n","authors":["Ryan Gupta","Kyle Morgenstein","Steven Ortega","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2309.14150v8.pdf","comment":"6 pages + references. 6 figures. 1 algorithm. 1 table"},{"id":"http://arxiv.org/abs/2403.17266v1","updated":"2024-03-25T23:19:19Z","published":"2024-03-25T23:19:19Z","title":"Exploring CausalWorld: Enhancing robotic manipulation via knowledge\n  transfer and curriculum learning","summary":"  This study explores a learning-based tri-finger robotic arm manipulating\ntask, which requires complex movements and coordination among the fingers. By\nemploying reinforcement learning, we train an agent to acquire the necessary\nskills for proficient manipulation. To enhance the efficiency and effectiveness\nof the learning process, two knowledge transfer strategies, fine-tuning and\ncurriculum learning, were utilized within the soft actor-critic architecture.\nFine-tuning allows the agent to leverage pre-trained knowledge and adapt it to\nnew tasks. Several variations like model transfer, policy transfer, and\nacross-task transfer were implemented and evaluated. To eliminate the need for\npretraining, curriculum learning decomposes the advanced task into simpler,\nprogressive stages, mirroring how humans learn. The number of learning stages,\nthe context of the sub-tasks, and the transition timing were found to be the\ncritical design parameters. The key factors of two learning strategies and\ncorresponding effects were explored in context-aware and context-unaware\nscenarios, enabling us to identify the scenarios where the methods demonstrate\noptimal performance, derive conclusive insights, and contribute to a broader\nrange of learning-based engineering applications.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2403.17266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12465v2","updated":"2024-03-25T22:57:28Z","published":"2024-03-19T05:46:20Z","title":"Diagrammatic Instructions to Specify Spatial Objectives and Constraints\n  with Applications to Mobile Base Placement","summary":"  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach\nfor human operators to specify objectives and constraints that are related to\nspatial regions in the working environment. Human operators are enabled to\nsketch out regions directly on camera images that correspond to the objectives\nand constraints. These sketches are projected to 3D spatial coordinates, and\ncontinuous Spatial Instruction Maps (SIMs) are learned upon them. These maps\ncan then be integrated into optimization problems for tasks of robots. In\nparticular, we demonstrate how Spatial Diagrammatic Instructions can be applied\nto solve the Base Placement Problem of mobile manipulators, which concerns the\nbest place to put the manipulator to facilitate a certain task. Human operators\ncan specify, via sketch, spatial regions of interest for a manipulation task\nand permissible regions for the mobile manipulator to be at. Then, an\noptimization problem that maximizes the manipulator's reachability, or\ncoverage, over the designated regions of interest while remaining in the\npermissible regions is solved. We provide extensive empirical evaluations, and\nshow that our formulation of Spatial Instruction Maps provides accurate\nrepresentations of user-specified diagrammatic instructions. Furthermore, we\ndemonstrate that our diagrammatic approach to the Mobile Base Placement Problem\nenables higher quality solutions and faster run-time.\n","authors":["Qilin Sun","Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2403.12465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17249v1","updated":"2024-03-25T22:51:27Z","published":"2024-03-25T22:51:27Z","title":"Impact-Aware Bimanual Catching of Large-Momentum Objects","summary":"  This paper investigates one of the most challenging tasks in dynamic\nmanipulation -- catching large-momentum moving objects. Beyond the realm of\nquasi-static manipulation, dealing with highly dynamic objects can\nsignificantly improve the robot's capability of interacting with its\nsurrounding environment. Yet, the inevitable motion mismatch between the fast\nmoving object and the approaching robot will result in large impulsive forces,\nwhich lead to the unstable contacts and irreversible damage to both the object\nand the robot. To address the above problems, we propose an online optimization\nframework to: 1) estimate and predict the linear and angular motion of the\nobject; 2) search and select the optimal contact locations across every surface\nof the object to mitigate impact through sequential quadratic programming\n(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and\ncontact force for both robots using multi-mode trajectory optimization (MMTO);\nand 4) realise the impact-aware catching motion on the compliant robotic system\nbased on indirect force controller. We validate the impulse distribution,\ncontact selection, and impact-aware MMTO algorithms in simulation and\ndemonstrate the benefits of the proposed framework in real-world experiments\nincluding catching large-momentum moving objects with well-defined motion,\nconstrained motion and free-flying motion.\n","authors":["Lei Yan","Theodoros Stouraitis","João Moura","Wenfu Xu","Michael Gienger","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2403.17249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17247v1","updated":"2024-03-25T22:49:56Z","published":"2024-03-25T22:49:56Z","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","summary":"  We consider a setting in which $N$ agents aim to speedup a common Stochastic\nApproximation (SA) problem by acting in parallel and communicating with a\ncentral server. We assume that the up-link transmissions to the server are\nsubject to asynchronous and potentially unbounded time-varying delays. To\nmitigate the effect of delays and stragglers while reaping the benefits of\ndistributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm\nfor multi-agent Stochastic Approximation. We provide a finite-time analysis of\n\\texttt{DASA} assuming that the agents' stochastic observation processes are\nindependent Markov chains. Significantly advancing existing results,\n\\texttt{DASA} is the first algorithm whose convergence rate depends only on the\nmixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly\nachieving an $N$-fold convergence speedup under Markovian sampling. Our work is\nrelevant for various SA applications, including multi-agent and distributed\ntemporal difference (TD) learning, Q-learning and stochastic optimization with\ncorrelated data.\n","authors":["Nicolo Dal Fabbro","Arman Adibi","H. Vincent Poor","Sanjeev R. Kulkarni","Aritra Mitra","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.17247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17246v1","updated":"2024-03-25T22:47:13Z","published":"2024-03-25T22:47:13Z","title":"TwoStep: Multi-agent Task Planning using Classical Planners and Large\n  Language Models","summary":"  Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, for example that two agents\nin the domain can execute an action simultaneously if postconditions of each do\nnot interfere with preconditions of the other. A human expert can decompose a\ngoal into largely independent constituent parts and assign each agent to one of\nthese subgoals to take advantage of simultaneous actions for faster execution\nof plan steps, each using only single agent planning. By contrast, large\nlanguage models (LLMs) used for directly inferring plan steps do not guarantee\nexecution success, but do leverage commonsense reasoning to assemble action\nsequences. We combine the strengths of classical planning and LLMs by\napproximating human intuitions for two-agent planning goal decomposition. We\ndemonstrate that LLM-based goal decomposition leads to faster planning times\nthan solving multi-agent PDDL problems directly while simultaneously achieving\nfewer plan execution steps than a single agent plan alone and preserving\nexecution success. Additionally, we find that LLM-based approximations of\nsubgoals can achieve similar multi-agent execution steps than those specified\nby human experts. Website and resources at https://glamor-usc.github.io/twostep\n","authors":["Ishika Singh","David Traum","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2403.17246v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.17238v1","updated":"2024-03-25T22:39:20Z","published":"2024-03-25T22:39:20Z","title":"Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks","summary":"  Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP.\n","authors":["Jonathan Salfity","Selma Wanna","Minkyu Choi","Mitch Pryor"],"pdf_url":"https://arxiv.org/pdf/2403.17238v1.pdf","comment":"8 pages, 3 figures. IROS 2024 Submission"},{"id":"http://arxiv.org/abs/2403.17234v1","updated":"2024-03-25T22:21:23Z","published":"2024-03-25T22:21:23Z","title":"Speeding Up Path Planning via Reinforcement Learning in MCTS for\n  Automated Parking","summary":"  In this paper, we address a method that integrates reinforcement learning\ninto the Monte Carlo tree search to boost online path planning under fully\nobservable environments for automated parking tasks. Sampling-based planning\nmethods under high-dimensional space can be computationally expensive and\ntime-consuming. State evaluation methods are useful by leveraging the prior\nknowledge into the search steps, making the process faster in a real-time\nsystem. Given the fact that automated parking tasks are often executed under\ncomplex environments, a solid but lightweight heuristic guidance is challenging\nto compose in a traditional analytical way. To overcome this limitation, we\npropose a reinforcement learning pipeline with a Monte Carlo tree search under\nthe path planning framework. By iteratively learning the value of a state and\nthe best action among samples from its previous cycle's outcomes, we are able\nto model a value estimator and a policy generator for given states. By doing\nthat, we build up a balancing mechanism between exploration and exploitation,\nspeeding up the path planning process while maintaining its quality without\nusing human expert driver data.\n","authors":["Xinlong Zheng","Xiaozhou Zhang","Donghao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17232v1","updated":"2024-03-25T22:19:58Z","published":"2024-03-25T22:19:58Z","title":"PROSPECT: Precision Robot Spectroscopy Exploration and Characterization\n  Tool","summary":"  Near Infrared (NIR) spectroscopy is widely used in industrial quality control\nand automation to test the purity and material quality of items. In this\nresearch, we propose a novel sensorized end effector and acquisition strategy\nto capture spectral signatures from objects and register them with a 3D point\ncloud. Our methodology first takes a 3D scan of an object generated by a\ntime-of-flight depth camera and decomposes the object into a series of planned\nviewpoints covering the surface. We generate motion plans for a robot\nmanipulator and end-effector to visit these viewpoints while maintaining a\nfixed distance and surface normal to ensure maximal spectral signal quality\nenabled by the spherical motion of the end-effector. By continuously acquiring\nsurface reflectance values as the end-effector scans the target object, the\nautonomous system develops a four-dimensional model of the target object:\nposition in an R^3 coordinate frame, and a wavelength vector denoting the\nassociated spectral signature. We demonstrate this system in building\nspectral-spatial object profiles of increasingly complex geometries. As a point\nof comparison, we show our proposed system and spectral acquisition planning\nyields more consistent signal signals than naive point scanning strategies for\ncapturing spectral information over complex surface geometries. Our work\nrepresents a significant step towards high-resolution spectral-spatial sensor\nfusion for automated quality assessment.\n","authors":["Nathaniel Hanson","Gary Lvov","Vedant Rautela","Samuel Hibbard","Ethan Holand","Charles DiMarzio","Taşkın Padır"],"pdf_url":"https://arxiv.org/pdf/2403.17232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17231v1","updated":"2024-03-25T22:17:51Z","published":"2024-03-25T22:17:51Z","title":"Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from\n  Learned Hallucination","summary":"  This paper presents a self-supervised learning method to safely learn a\nmotion planner for ground robots to navigate environments with dense and\ndynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict\nobstacles, classical motion planners may not be able to keep up with limited\nonboard computation. For learning-based planners, high-quality demonstrations\nare difficult to acquire for imitation learning while reinforcement learning\nbecomes inefficient due to the high probability of collision during\nexploration. To safely and efficiently provide training data, the Learning from\nHallucination (LfH) approaches synthesize difficult navigation environments\nbased on past successful navigation experiences in relatively easy or\ncompletely open ones, but unfortunately cannot address dynamic obstacles. In\nour new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and\nlearn a novel latent distribution and sample dynamic obstacles from it, so the\ngenerated training data can be used to learn a motion planner to navigate in\ndynamic environments. Dyna-LfLH is evaluated on a ground robot in both\nsimulated and physical environments and achieves up to 25% better success rate\ncompared to baselines.\n","authors":["Saad Abdul Ghani","Zizhao Wang","Peter Stone","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17231v1.pdf","comment":"Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2310.10863v2","updated":"2024-03-25T20:46:28Z","published":"2023-10-16T22:23:18Z","title":"Greedy Perspectives: Multi-Drone View Planning for Collaborative\n  Perception in Cluttered Environments","summary":"  Deployment of teams of aerial robots could enable large-scale filming of\ndynamic groups of people (actors) in complex environments for applications in\nareas such as team sports and cinematography. Toward this end, methods for\nsubmodular maximization via sequential greedy planning can be used for scalable\noptimization of camera views across teams of robots but face challenges with\nefficient coordination in cluttered environments. Obstacles can produce\nocclusions and increase chances of inter-robot collision which can violate\nrequirements for near-optimality guarantees. To coordinate teams of aerial\nrobots in filming groups of people in dense environments, a more general\nview-planning approach is required. We explore how collision and occlusion\nimpact performance in filming applications through the development of a\nmulti-robot multi-actor view planner with an occlusion-aware objective for\nfilming groups of people and compare with a formation planner and a greedy\nplanner that ignores inter-robot collisions. We evaluate our approach based on\nfive test environments and complex multi-actor behaviors. Compared with a\nformation planner, our sequential planner generates 14% greater view reward\nover the actors for three scenarios and comparable performance to formation\nplanning on two others. We also observe near identical view rewards for\nsequential planning both with and without inter-robot collision constraints\nwhich indicates that robots are able to avoid collisions without impairing\nperformance in the perception task. Overall, we demonstrate effective\ncoordination of teams of aerial robots for filming groups that may split,\nmerge, or spread apart and in environments cluttered with obstacles that may\ncause collisions or occlusions.\n","authors":["Krishna Suresh","Aditya Rauniyar","Micah Corah","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2310.10863v2.pdf","comment":"Submitted to IROS'24; 8 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2309.10275v2","updated":"2024-03-25T20:28:22Z","published":"2023-09-19T03:02:43Z","title":"Optimizing Crowd-Aware Multi-Agent Path Finding through Local\n  Broadcasting with Graph Neural Networks","summary":"  Multi-Agent Path Finding (MAPF) in crowded environments presents a\nchallenging problem in motion planning, aiming to find collision-free paths for\nall agents in the system. MAPF finds a wide range of applications in various\ndomains, including aerial swarms, autonomous warehouse robotics, and\nself-driving vehicles. Current approaches to MAPF generally fall into two main\ncategories: centralized and decentralized planning. Centralized planning\nsuffers from the curse of dimensionality when the number of agents or states\nincreases and thus does not scale well in large and complex environments. On\nthe other hand, decentralized planning enables agents to engage in real-time\npath planning within a partially observable environment, demonstrating implicit\ncoordination. However, they suffer from slow convergence and performance\ndegradation in dense environments. In this paper, we introduce CRAMP, a novel\ncrowd-aware decentralized reinforcement learning approach to address this\nproblem by enabling efficient local communication among agents via Graph Neural\nNetworks (GNNs), facilitating situational awareness and decision-making\ncapabilities in congested environments. We test CRAMP on simulated environments\nand demonstrate that our method outperforms the state-of-the-art decentralized\nmethods for MAPF on various metrics. CRAMP improves the solution quality up to\n59% measured in makespan and collision count, and up to 35% improvement in\nsuccess rate in comparison to previous methods.\n","authors":["Phu Pham","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.10275v2.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2305.18983v3","updated":"2024-03-25T20:21:25Z","published":"2023-05-30T12:27:47Z","title":"SO(2)-Equivariant Downwash Models for Close Proximity Flight","summary":"  Multirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%).\n","authors":["H. Smith","A. Shankar","J. Gielis","J. Blumenkamp","A. Prorok"],"pdf_url":"https://arxiv.org/pdf/2305.18983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17161v1","updated":"2024-03-25T20:18:12Z","published":"2024-03-25T20:18:12Z","title":"Multi-Contact Inertial Estimation and Localization in Legged Robots","summary":"  Optimal estimation is a promising tool for multi-contact inertial estimation\nand localization. To harness its advantages in robotics, it is crucial to solve\nthese large and challenging optimization problems efficiently. To tackle this,\nwe (i) develop a multiple-shooting solver that exploits both temporal and\nparametric structures through a parametrized Riccati recursion. Additionally,\nwe (ii) propose an inertial local manifold that ensures its full physical\nconsistency. It also enhances convergence compared to the singularity-free\nlog-Cholesky approach. To handle its singularities, we (iii) introduce a\nnullspace approach in our optimal estimation solver. We (iv) finally develop\nthe analytical derivatives of contact dynamics for both inertial\nparametrizations. Our framework can successfully solve estimation problems for\ncomplex maneuvers such as brachiation in humanoids. We demonstrate its\nnumerical capabilities across various robotics tasks and its benefits in\nexperimental trials with the Go1 robot.\n","authors":["Sergi Martinez","Robert Griffin","Carlos Mastalli"],"pdf_url":"https://arxiv.org/pdf/2403.17161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01519v2","updated":"2024-03-25T20:14:46Z","published":"2023-10-02T18:11:01Z","title":"Decision-Oriented Learning Using Differentiable Submodular Maximization\n  for Multi-Robot Coordination","summary":"  We present a differentiable, decision-oriented learning framework for cost\nprediction in a class of multi-robot decision-making problems, in which the\nrobots need to trade off the task performance with the costs of taking actions\nwhen they select actions to take. Specifically, we consider the cases where the\ntask performance is measured by a known monotone submodular function (e.g.,\ncoverage, mutual information), and the cost of actions depends on the context\n(e.g., wind and terrain conditions). We need to learn a function that maps the\ncontext to the costs. Classically, we treat such a learning problem and the\ndownstream decision-making problem as two decoupled problems, i.e., we first\nlearn to predict the cost function without considering the downstream\ndecision-making problem, and then use the learned function for predicting the\ncost and using it in the decision-making problem. However, the loss function\nused in learning a prediction function may not be aligned with the downstream\ndecision-making. We propose a decision-oriented learning framework that\nincorporates the downstream task performance in the prediction phase via a\ndifferentiable optimization layer. The main computational challenge in such a\nframework is to make the combinatorial optimization, i.e., non-monotone\nsubmodular maximization, differentiable. This function is not naturally\ndifferentiable. We propose the Differentiable Cost Scaled Greedy algorithm\n(D-CSG), which is a continuous and differentiable relaxation of CSG. We\ndemonstrate the efficacy of the proposed framework through numerical\nsimulations. The results show that the proposed framework can result in better\nperformance than the traditional two-stage approach.\n","authors":["Guangyao Shi","Chak Lam Shek","Nare Karapetyan","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.01519v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01543"},{"id":"http://arxiv.org/abs/2403.17147v1","updated":"2024-03-25T19:50:07Z","published":"2024-03-25T19:50:07Z","title":"Hearing the shape of an arena with spectral swarm robotics","summary":"  Swarm robotics promises adaptability to unknown situations and robustness\nagainst failures. However, it still struggles with global tasks that require\nunderstanding the broader context in which the robots operate, such as\nidentifying the shape of the arena in which the robots are embedded. Biological\nswarms, such as shoals of fish, flocks of birds, and colonies of insects,\nroutinely solve global geometrical problems through the diffusion of local\ncues. This paradigm can be explicitly described by mathematical models that\ncould be directly computed and exploited by a robotic swarm. Diffusion over a\ndomain is mathematically encapsulated by the Laplacian, a linear operator that\nmeasures the local curvature of a function. Crucially the geometry of a domain\ncan generally be reconstructed from the eigenspectrum of its Laplacian. Here we\nintroduce spectral swarm robotics where robots diffuse information to their\nneighbors to emulate the Laplacian operator - enabling them to \"hear\" the\nspectrum of their arena. We reveal a universal scaling that links the optimal\nnumber of robots (a global parameter) with their optimal radius of interaction\n(a local parameter). We validate experimentally spectral swarm robotics under\nchallenging conditions with the one-shot classification of arena shapes using a\nsparse swarm of Kilobots. Spectral methods can assist with challenging tasks\nwhere robots need to build an emergent consensus on their environment, such as\nadaptation to unknown terrains, division of labor, or quorum sensing. Spectral\nmethods may extend beyond robotics to analyze and coordinate swarms of agents\nof various natures, such as traffic or crowds, and to better understand the\nlong-range dynamics of natural systems emerging from short-range interactions.\n","authors":["Leo Cazenille","Nicolas Lobato-Dauzier","Alessia Loi","Mika Ito","Olivier Marchal","Nathanael Aubert-Kato","Nicolas Bredeche","Anthony J. Genot"],"pdf_url":"https://arxiv.org/pdf/2403.17147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07021v2","updated":"2024-03-25T19:46:25Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v2.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.14887v2","updated":"2024-03-25T19:25:21Z","published":"2024-03-21T23:44:42Z","title":"GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile\n  Sensing and Proprioception","summary":"  Compared to fully-actuated robotic end-effectors, underactuated ones are\ngenerally more adaptive, robust, and cost-effective. However, state estimation\nfor underactuated hands is usually more challenging. Vision-based tactile\nsensors, like Gelsight, can mitigate this issue by providing high-resolution\ntactile sensing and accurate proprioceptive sensing. As such, we present\nGelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,\nhigh-resolution vision-based tactile sensing and proprioceptive sensing\ncapabilities. In order to reduce the amount of embedded hardware, i.e. the\ncameras and motors, we optimize the linkage transmission with a planar linkage\nmechanism simulator and develop a planar reflection simulator to simplify the\ntactile sensing hardware. As a result, GelLink only requires one motor to\nactuate the three phalanges, and one camera to capture tactile signals along\nthe entire finger. Overall, GelLink is a compact robotic finger that shows\nadaptability and robustness when performing grasping tasks. The integration of\nvision-based tactile sensors can significantly enhance the capabilities of\nunderactuated fingers and potentially broaden their future usage.\n","authors":["Yuxiang Ma","Jialiang Zhao","Edward Adelson"],"pdf_url":"https://arxiv.org/pdf/2403.14887v2.pdf","comment":"Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7\n  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and\n  Automation)"},{"id":"http://arxiv.org/abs/2403.17136v1","updated":"2024-03-25T19:18:25Z","published":"2024-03-25T19:18:25Z","title":"Adaptive Step Duration for Precise Foot Placement: Achieving Robust\n  Bipedal Locomotion on Terrains with Restricted Footholds","summary":"  This paper introduces a novel multi-step preview foot placement planning\nalgorithm designed to enhance the robustness of bipedal robotic walking across\nchallenging terrains with restricted footholds. Traditional one-step preview\nplanning struggles to maintain stability when stepping areas are severely\nlimited, such as with random stepping stones. In this work, we developed a\ndiscrete-time Model Predictive Control (MPC) based on the step-to-step discrete\nevolution of the Divergent Component of Motion (DCM) of bipedal locomotion.\nThis approach adaptively changes the step duration for optimal foot placement\nunder constraints, thereby ensuring the robot's operational viability over\nmultiple future steps and significantly improving its ability to navigate\nthrough environments with tight constraints on possible footholds. The\neffectiveness of this planning algorithm is demonstrated through simulations\nthat include a variety of complex stepping-stone configurations and external\nperturbations. These tests underscore the algorithm's improved performance for\nnavigating foothold-restricted environments, even with the presence of external\ndisturbances.\n","authors":["Zhaoyang Xiang","Victor Paredes","Ayonga Hereid"],"pdf_url":"https://arxiv.org/pdf/2403.17136v1.pdf","comment":"8 pages, 8 figures, submitted to CDC 2024, for associated simulation\n  video, see https://youtu.be/2jhikPlZmbE"},{"id":"http://arxiv.org/abs/2403.05972v3","updated":"2024-03-25T19:15:44Z","published":"2024-03-09T17:37:05Z","title":"C3D: Cascade Control with Change Point Detection and Deep Koopman\n  Learning for Autonomous Surface Vehicles","summary":"  In this paper, we discuss the development and deployment of a robust\nautonomous system capable of performing various tasks in the maritime domain\nunder unknown dynamic conditions. We investigate a data-driven approach based\non modular design for ease of transfer of autonomy across different maritime\nsurface vessel platforms. The data-driven approach alleviates issues related to\na priori identification of system models that may become deficient under\nevolving system behaviors or shifting, unanticipated, environmental influences.\nOur proposed learning-based platform comprises a deep Koopman system model and\na change point detector that provides guidance on domain shifts prompting\nrelearning under severe exogenous and endogenous perturbations. Motion control\nof the autonomous system is achieved via an optimal controller design. The\nKoopman linearized model naturally lends itself to a linear-quadratic regulator\n(LQR) control design. We propose the C3D control architecture Cascade Control\nwith Change Point Detection and Deep Koopman Learning. The framework is\nverified in station keeping task on an ASV in both simulation and real\nexperiments. The approach achieved at least 13.9 percent improvement in mean\ndistance error in all test cases compared to the methods that do not consider\nsystem changes.\n","authors":["Jianwen Li","Hyunsang Park","Wenjian Hao","Lei Xin","Jalil Chavez-Galaviz","Ajinkya Chaudhary","Meredith Bloss","Kyle Pattison","Christopher Vo","Devesh Upadhyay","Shreyas Sundaram","Shaoshuai Mou","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2403.05972v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.17124v1","updated":"2024-03-25T19:04:59Z","published":"2024-03-25T19:04:59Z","title":"Grounding Language Plans in Demonstrations Through Counterfactual\n  Perturbations","summary":"  Grounding the common-sense reasoning of Large Language Models in physical\ndomains remains a pivotal yet unsolved problem for embodied AI. Whereas prior\nworks have focused on leveraging LLMs directly for planning in symbolic spaces,\nthis work uses LLMs to guide the search of task structures and constraints\nimplicit in multi-step demonstrations. Specifically, we borrow from\nmanipulation planning literature the concept of mode families, which group\nrobot configurations by specific motion constraints, to serve as an abstraction\nlayer between the high-level language representations of an LLM and the\nlow-level physical trajectories of a robot. By replaying a few human\ndemonstrations with synthetic perturbations, we generate coverage over the\ndemonstrations' state space with additional successful executions as well as\ncounterfactuals that fail the task. Our explanation-based learning framework\ntrains an end-to-end differentiable neural network to predict successful\ntrajectories from failures and as a by-product learns classifiers that ground\nlow-level states and images in mode families without dense labeling. The\nlearned grounding classifiers can further be used to translate language plans\ninto reactive policies in the physical domain in an interpretable manner. We\nshow our approach improves the interpretability and reactivity of imitation\nlearning through 2D navigation and simulated and real robot manipulation tasks.\nWebsite: https://sites.google.com/view/grounding-plans\n","authors":["Yanwei Wang","Tsun-Hsuan Wang","Jiayuan Mao","Michael Hagenow","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2403.17124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17111v1","updated":"2024-03-25T18:49:12Z","published":"2024-03-25T18:49:12Z","title":"Vision-Based Dexterous Motion Planning by Dynamic Movement Primitives\n  with Human Hand Demonstration","summary":"  This paper proposes a vision-based framework for a 7-degree-of-freedom\nrobotic manipulator, with the primary objective of facilitating its capacity to\nacquire information from human hand demonstrations for the execution of\ndexterous pick-and-place tasks. Most existing works only focus on the position\ndemonstration without considering the orientations. In this paper, by employing\na single depth camera, MediaPipe is applied to generate the three-dimensional\ncoordinates of a human hand, thereby comprehensively recording the hand's\nmotion, encompassing the trajectory of the wrist, orientation of the hand, and\nthe grasp motion. A mean filter is applied during data pre-processing to smooth\nthe raw data. The demonstration is designed to pick up an object at a specific\nangle, navigate around obstacles in its path and subsequently, deposit it\nwithin a sloped container. The robotic system demonstrates its learning\ncapabilities, facilitated by the implementation of Dynamic Movement Primitives,\nenabling the assimilation of user actions into its trajectories with different\nstart and end poi\n","authors":["Nuo Chen","Ya-Jun Pan"],"pdf_url":"https://arxiv.org/pdf/2403.17111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17099v1","updated":"2024-03-25T18:37:01Z","published":"2024-03-25T18:37:01Z","title":"Berry Twist: a Twisting-Tube Soft Robotic Gripper for Blackberry\n  Harvesting","summary":"  As global demand for fruits and vegetables continues to rise, the\nagricultural industry faces challenges in securing adequate labor. Robotic\nharvesting devices offer a promising solution to solve this issue. However,\nharvesting delicate fruits, notably blackberries, poses unique challenges due\nto their fragility. This study introduces and evaluates a prototype robotic\ngripper specifically designed for blackberry harvesting. The gripper features\nan innovative fabric tube mechanism employing motorized twisting action to\ngently envelop the fruit, ensuring uniform pressure application and minimizing\ndamage. Three types of tubes were developed, varying in elasticity and\ncompressibility using foam padding, spandex, and food-safe cotton cheesecloth.\nPerformance testing focused on assessing each gripper's ability to detach and\nrelease blackberries, with emphasis on quantifying damage rates. Results\nindicate the proposed gripper achieved an 82% success rate in detaching\nblackberries and a 95% success rate in releasing them, showcasing the promised\npotential for robotic harvesting applications.\n","authors":["Johannes F. Elfferich","Ebrahim Shahabi","Cosimo Della Santina","Dimitra Dodou"],"pdf_url":"https://arxiv.org/pdf/2403.17099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.01041v8","updated":"2024-03-25T18:21:39Z","published":"2020-04-01T06:37:54Z","title":"On the Feedback Law in Stochastic Optimal Nonlinear Control","summary":"  We consider the problem of nonlinear stochastic optimal control. This problem\nis thought to be fundamentally intractable owing to Bellman's ``curse of\ndimensionality\". We present a result that shows that repeatedly solving an\nopen-loop deterministic problem from the current state with progressively\nshorter horizons, similar to Model Predictive Control (MPC), results in a\nfeedback policy that is $O(\\epsilon^4)$ near to the true global stochastic\noptimal policy, \\nxx{where $\\epsilon$ is a perturbation parameter modulating\nthe noise.} We show that the optimal deterministic feedback problem has a\nperturbation structure in that higher-order terms of the feedback law do not\naffect lower-order terms, and that this structure is lost in the optimal\nstochastic feedback problem. Consequently, solving the Stochastic Dynamic\nProgramming problem is highly susceptible to noise, even when tractable, and in\npractice, the MPC-type feedback law offers superior performance even for\nstochastic systems.\n","authors":["Mohamed Naveed Gul Mohamed","Suman Chakravorty","Raman Goyal","Ran Wang"],"pdf_url":"https://arxiv.org/pdf/2004.01041v8.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2002.10505,\n  arXiv:2002.09478"},{"id":"http://arxiv.org/abs/2403.17084v1","updated":"2024-03-25T18:18:12Z","published":"2024-03-25T18:18:12Z","title":"A Comparative Analysis of Visual Odometry in Virtual and Real-World\n  Railways Environments","summary":"  Perception tasks play a crucial role in the development of automated\noperations and systems across multiple application fields. In the railway\ntransportation domain, these tasks can improve the safety, reliability, and\nefficiency of various perations, including train localization, signal\nrecognition, and track discrimination. However, collecting considerable and\nprecisely labeled datasets for testing such novel algorithms poses extreme\nchallenges in the railway environment due to the severe restrictions in\naccessing the infrastructures and the practical difficulties associated with\nproperly equipping trains with the required sensors, such as cameras and\nLiDARs. The remarkable innovations of graphic engine tools offer new solutions\nto craft realistic synthetic datasets. To illustrate the advantages of\nemploying graphic simulation for early-stage testing of perception tasks in the\nrailway domain, this paper presents a comparative analysis of the performance\nof a SLAM algorithm applied both in a virtual synthetic environment and a\nreal-world scenario. The analysis leverages virtual railway environments\ncreated with the latest version of Unreal Engine, facilitating data collection\nand allowing the examination of challenging scenarios, including\nlow-visibility, dangerous operational modes, and complex environments. The\nresults highlight the feasibility and potentiality of graphic simulation to\nadvance perception tasks in the railway domain.\n","authors":["Gianluca D'Amico","Mauro Marinoni","Giorgio Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2403.17084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17067v1","updated":"2024-03-25T18:03:50Z","published":"2024-03-25T18:03:50Z","title":"Trajectory Optimization with Global Yaw Parameterization for\n  Field-of-View Constrained Autonomous Flight","summary":"  Trajectory generation for quadrotors with limited field-of-view sensors has\nnumerous applications such as aerial exploration, coverage, inspection,\nvideography, and target tracking. Most previous works simplify the task of\noptimizing yaw trajectories by either aligning the heading of the robot with\nits velocity, or potentially restricting the feasible space of candidate\ntrajectories by using a limited yaw domain to circumvent angular singularities.\nIn this paper, we propose a novel \\textit{global} yaw parameterization method\nfor trajectory optimization that allows a 360-degree yaw variation as demanded\nby the underlying algorithm. This approach effectively bypasses inherent\nsingularities by including supplementary quadratic constraints and transforming\nthe final decision variables into the desired state representation. This method\nsignificantly reduces the needed control effort, and improves optimization\nfeasibility. Furthermore, we apply the method to several examples of different\napplications that require jointly optimizing over both the yaw and position\ntrajectories. Ultimately, we present a comprehensive numerical analysis and\nevaluation of our proposed method in both simulation and real-world\nexperiments.\n","authors":["Yuwei Wu","Yuezhan Tao","Igor Spasojevic","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.17067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17010v1","updated":"2024-03-25T17:59:59Z","published":"2024-03-25T17:59:59Z","title":"Calib3D: Calibrating Model Preferences for Reliable 3D Scene\n  Understanding","summary":"  Safety-critical 3D scene understanding tasks necessitate not only accurate\nbut also confident predictions from 3D perception models. This study introduces\nCalib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D\nscene understanding models from an uncertainty estimation viewpoint. We\ncomprehensively evaluate 28 state-of-the-art models across 10 diverse 3D\ndatasets, uncovering insightful phenomena that cope with both the aleatoric and\nepistemic uncertainties in 3D scene understanding. We discover that despite\nachieving impressive levels of accuracy, existing models frequently fail to\nprovide reliable uncertainty estimates -- a pitfall that critically undermines\ntheir applicability in safety-sensitive contexts. Through extensive analysis of\nkey factors such as network capacity, LiDAR representations, rasterization\nresolutions, and 3D data augmentation techniques, we correlate these aspects\ndirectly with the model calibration efficacy. Furthermore, we introduce DeptS,\na novel depth-aware scaling approach aimed at enhancing 3D model calibration.\nExtensive experiments across a wide range of configurations validate the\nsuperiority of our method. We hope this work could serve as a cornerstone for\nfostering reliable 3D scene understanding. Code and benchmark toolkits are\npublicly available.\n","authors":["Lingdong Kong","Xiang Xu","Jun Cen","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17010v1.pdf","comment":"Preprint; 37 pages, 8 figures, 11 tables; Code at\n  https://github.com/ldkong1205/Calib3D"},{"id":"http://arxiv.org/abs/2403.17009v1","updated":"2024-03-25T17:59:58Z","published":"2024-03-25T17:59:58Z","title":"Optimizing LiDAR Placements for Robust Driving Perception in Adverse\n  Conditions","summary":"  The robustness of driving perception systems under unprecedented conditions\nis crucial for safety-critical usages. Latest advancements have prompted\nincreasing interests towards multi-LiDAR perception. However, prevailing\ndriving datasets predominantly utilize single-LiDAR systems and collect data\ndevoid of adverse conditions, failing to capture the complexities of real-world\nenvironments accurately. Addressing these gaps, we proposed Place3D, a\nfull-cycle pipeline that encompasses LiDAR placement optimization, data\ngeneration, and downstream evaluations. Our framework makes three appealing\ncontributions. 1) To identify the most effective configurations for multi-LiDAR\nsystems, we introduce a Surrogate Metric of the Semantic Occupancy Grids\n(M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we\npropose a novel optimization strategy to refine multi-LiDAR placements. 3)\nCentered around the theme of multi-condition multi-LiDAR perception, we collect\na 364,000-frame dataset from both clean and adverse conditions. Extensive\nexperiments demonstrate that LiDAR placements optimized using our approach\noutperform various baselines. We showcase exceptional robustness in both 3D\nobject detection and LiDAR semantic segmentation tasks, under diverse adverse\nweather and sensor failure conditions. Code and benchmark toolkit are publicly\navailable.\n","authors":["Ye Li","Lingdong Kong","Hanjiang Hu","Xiaohao Xu","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17009v1.pdf","comment":"Preprint; 40 pages, 11 figures, 15 tables; Code at\n  https://github.com/ywyeli/Place3D"},{"id":"http://arxiv.org/abs/2403.16996v1","updated":"2024-03-25T17:59:01Z","published":"2024-03-25T17:59:01Z","title":"DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving","summary":"  End-to-end driving has made significant progress in recent years,\ndemonstrating benefits such as system simplicity and competitive driving\nperformance under both open-loop and closed-loop settings. Nevertheless, the\nlack of interpretability and controllability in its driving decisions hinders\nreal-world deployment for end-to-end driving systems. In this paper, we collect\na comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA\nsimulator. It contains sensor data, control decisions, and chain-of-thought\nlabels to indicate the reasoning process. We utilize the challenging driving\nscenarios from the CARLA leaderboard 2.0, which involve high-speed driving and\nlane-changing, and propose a rule-based expert policy to control the vehicle\nand generate ground truth labels for its reasoning process across different\ndriving aspects and the final decisions. This dataset can serve as an open-loop\nend-to-end driving benchmark, enabling the evaluation of accuracy in various\nchain-of-thought aspects and the final decision. In addition, we propose a\nbaseline model called DriveCoT-Agent, trained on our dataset, to generate\nchain-of-thought predictions and final decisions. The trained model exhibits\nstrong performance in both open-loop and closed-loop evaluations, demonstrating\nthe effectiveness of our proposed dataset.\n","authors":["Tianqi Wang","Enze Xie","Ruihang Chu","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2403.16996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16967v1","updated":"2024-03-25T17:26:08Z","published":"2024-03-25T17:26:08Z","title":"Visual Whole-Body Control for Legged Loco-Manipulation","summary":"  We study the problem of mobile manipulation using legged robots equipped with\nan arm, namely legged loco-manipulation. The robot legs, while usually utilized\nfor mobility, offer an opportunity to amplify the manipulation capabilities by\nconducting whole-body control. That is, the robot can control the legs and the\narm at the same time to extend its workspace. We propose a framework that can\nconduct the whole-body control autonomously with visual observations. Our\napproach, namely \\ourFull~(\\our), is composed of a low-level policy using all\ndegrees of freedom to track the end-effector manipulator position and a\nhigh-level policy proposing the end-effector position based on visual inputs.\nWe train both levels of policies in simulation and perform Sim2Real transfer\nfor real robot deployment. We perform extensive experiments and show\nsignificant improvements over baselines in picking up diverse objects in\ndifferent configurations (heights, locations, orientations) and environments.\nProject page: https://wholebody-b1.github.io\n","authors":["Minghuan Liu","Zixuan Chen","Xuxin Cheng","Yandong Ji","Ruihan Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16967v1.pdf","comment":"The first two authors contribute equally. Project page:\n  https://wholebody-b1.github.io"},{"id":"http://arxiv.org/abs/2403.16956v1","updated":"2024-03-25T17:17:35Z","published":"2024-03-25T17:17:35Z","title":"Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy","summary":"  Multi-agent, collaborative sensor fusion is a vital component of a\nmulti-national intelligence toolkit. In safety-critical and/or contested\nenvironments, adversaries may infiltrate and compromise a number of agents. We\nanalyze state of the art multi-target tracking algorithms under this\ncompromised agent threat model. We prove that the track existence probability\ntest (\"track score\") is significantly vulnerable to even small numbers of\nadversaries. To add security awareness, we design a trust estimation framework\nusing hierarchical Bayesian updating. Our framework builds beliefs of trust on\ntracks and agents by mapping sensor measurements to trust pseudomeasurements\n(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case\nstudies, our trust estimation algorithm accurately estimates the\ntrustworthiness of tracks/agents, subject to observability limitations.\n","authors":["R. Spencer Hallyburton","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2403.16956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16953v1","updated":"2024-03-25T17:15:13Z","published":"2024-03-25T17:15:13Z","title":"Learning Symbolic and Subsymbolic Temporal Task Constraints from\n  Bimanual Human Demonstrations","summary":"  Learning task models of bimanual manipulation from human demonstration and\ntheir execution on a robot should take temporal constraints between actions\ninto account. This includes constraints on (i) the symbolic level such as\nprecedence relations or temporal overlap in the execution, and (ii) the\nsubsymbolic level such as the duration of different actions, or their starting\nand end points in time. Such temporal constraints are crucial for temporal\nplanning, reasoning, and the exact timing for the execution of bimanual actions\non a bimanual robot. In our previous work, we addressed the learning of\ntemporal task constraints on the symbolic level and demonstrated how a robot\ncan leverage this knowledge to respond to failures during execution. In this\nwork, we propose a novel model-driven approach for the combined learning of\nsymbolic and subsymbolic temporal task constraints from multiple bimanual human\ndemonstrations. Our main contributions are a subsymbolic foundation of a\ntemporal task model that describes temporal nexuses of actions in the task\nbased on distributions of temporal differences between semantic action\nkeypoints, as well as a method based on fuzzy logic to derive symbolic temporal\ntask constraints from this representation. This complements our previous work\non learning comprehensive temporal task models by integrating symbolic and\nsubsymbolic information based on a subsymbolic foundation, while still\nmaintaining the symbolic expressiveness of our previous approach. We compare\nour proposed approach with our previous pure-symbolic approach and show that we\ncan reproduce and even outperform it. Additionally, we show how the subsymbolic\ntemporal task constraints can synchronize otherwise unimanual movement\nprimitives for bimanual behavior on a humanoid robot.\n","authors":["Christian Dreher","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.16953v1.pdf","comment":"8 pages, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2310.07822v2","updated":"2024-03-25T17:06:57Z","published":"2023-10-11T19:03:55Z","title":"Body-mounted MR-conditional Robot for Minimally Invasive Liver\n  Intervention","summary":"  MR-guided microwave ablation (MWA) has proven effective in treating\nhepatocellular carcinoma (HCC) with small-sized tumors, but the\nstate-of-the-art technique suffers from sub-optimal workflow due to speed and\naccuracy of needle placement. This paper presents a compact body-mounted\nMR-conditional robot that can operate in closed-bore MR scanners for accurate\nneedle guidance. The robotic platform consists of two stacked Cartesian XY\nstages, each with two degrees of freedom, that facilitate needle guidance. The\nrobot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel\ngear transmission systems. Pneumatic valves and control mechatronics are\nlocated inside the MRI control room and are connected to the robot with\npneumatic transmission lines and optical fibers. Free space experiments\nindicated robot-assisted needle insertion error of 2.6$\\pm$1.3 mm at an\ninsertion depth of 80 mm. The MR-guided phantom studies were conducted to\nverify the MR-conditionality and targeting performance of the robot. Future\nwork will focus on the system optimization and validations in animal trials.\n","authors":["Zhefeng Huang","Anthony L. Gunderman","Samuel E. Wilcox","Saikat Sengupta","Jay Shah","Aiming Lu","David Woodrum","Yue Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07822v2.pdf","comment":"10 figures"},{"id":"http://arxiv.org/abs/2403.06186v3","updated":"2024-03-25T16:18:38Z","published":"2024-03-10T12:06:45Z","title":"Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems","summary":"  Brain-robot interaction (BRI) empowers individuals to control\n(semi-)automated machines through their brain activity, either passively or\nactively. In the past decade, BRI systems have achieved remarkable success,\npredominantly harnessing electroencephalogram (EEG) signals as the central\ncomponent. This paper offers an up-to-date and exhaustive examination of 87\ncurated studies published during the last five years (2018-2023), focusing on\nidentifying the research landscape of EEG-based BRI systems. This review aims\nto consolidate and underscore methodologies, interaction modes, application\ncontexts, system evaluation, existing challenges, and potential avenues for\nfuture investigations in this domain. Based on our analysis, we present a BRI\nsystem model with three entities: Brain, Robot, and Interaction, depicting the\ninternal relationships of a BRI system. We especially investigate the essence\nand principles on interaction modes between human brains and robots, a domain\nthat has not yet been identified anywhere. We then discuss these entities with\ndifferent dimensions encompassed. Within this model, we scrutinize and classify\ncurrent research, reveal insights, specify challenges, and provide\nrecommendations for future research trajectories in this field. Meanwhile, we\nenvision our findings offer a design space for future human-robot interaction\n(HRI) research, informing the creation of efficient BRI frameworks.\n","authors":["Yuchong Zhang","Nona Rajabi","Farzaneh Taleb","Andrii Matviienko","Yong Ma","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.06186v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10855v2","updated":"2024-03-25T16:07:24Z","published":"2024-03-16T08:30:55Z","title":"Reinforcement Learning with Options and State Representation","summary":"  The current thesis aims to explore the reinforcement learning field and build\non existing methods to produce improved ones to tackle the problem of learning\nin high-dimensional and complex environments. It addresses such goals by\ndecomposing learning tasks in a hierarchical fashion known as Hierarchical\nReinforcement Learning.\n  We start in the first chapter by getting familiar with the Markov Decision\nProcess framework and presenting some of its recent techniques that the\nfollowing chapters use. We then proceed to build our Hierarchical Policy\nlearning as an answer to the limitations of a single primitive policy. The\nhierarchy is composed of a manager agent at the top and employee agents at the\nlower level.\n  In the last chapter, which is the core of this thesis, we attempt to learn\nlower-level elements of the hierarchy independently of the manager level in\nwhat is known as the \"Eigenoption\". Based on the graph structure of the\nenvironment, Eigenoptions allow us to build agents that are aware of the\ngeometric and dynamic properties of the environment. Their decision-making has\na special property: it is invariant to symmetric transformations of the\nenvironment, allowing as a consequence to greatly reduce the complexity of the\nlearning task.\n","authors":["Ayoub Ghriss","Masashi Sugiyama","Alessandro Lazaric"],"pdf_url":"https://arxiv.org/pdf/2403.10855v2.pdf","comment":"Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP"},{"id":"http://arxiv.org/abs/2403.16880v1","updated":"2024-03-25T15:47:06Z","published":"2024-03-25T15:47:06Z","title":"DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World\n  Representation and Label Optimization Techniques","summary":"  Maps provide robots with crucial environmental knowledge, thereby enabling\nthem to perform interactive tasks effectively. Easily accessing accurate\nabstract-to-detailed geometric and semantic concepts from maps is crucial for\nrobots to make informed and efficient decisions. To comprehensively model the\nenvironment and effectively manage the map data structure, we propose\nDHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed\nDistance Field (TSDF) submaps and panoptic labels to hierarchically model the\nenvironment. The output map is able to maintain both voxel- and submap-level\nmetric and semantic information. Two modules are presented to enhance the\nmapping efficiency and label consistency: (1) an inter-submaps label fusion\nstrategy to eliminate duplicate points across submaps and (2) a conditional\nrandom field (CRF) based approach to enhance panoptic labels through object\nlabel comprehension and contextual information. We conducted experiments with\ntwo public datasets including indoor and outdoor scenarios. Our system performs\ncomparably to state-of-the-art (SOTA) methods across geometry and label\naccuracy evaluation metrics. The experiment results highlight the effectiveness\nand scalability of our system, as it is capable of constructing precise\ngeometry and maintaining consistent panoptic labels. Our code is publicly\navailable at https://github.com/hutslib/DHP-Mapping.\n","authors":["Tianshuai Hu","Jianhao Jiao","Yucheng Xu","Hongji Liu","Sheng Wang","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16880v1.pdf","comment":"Submit to IROS 2024. Project website\n  https://github.com/hutslib/DHP-Mapping"},{"id":"http://arxiv.org/abs/2403.16877v1","updated":"2024-03-25T15:42:09Z","published":"2024-03-25T15:42:09Z","title":"Proprioception Is All You Need: Terrain Classification for Boreal\n  Forests","summary":"  Recent works in field robotics highlighted the importance of resiliency\nagainst different types of terrains. Boreal forests, in particular, are home to\nmany mobility-impeding terrains that should be considered for off-road\nautonomous navigation. Also, being one of the largest land biomes on Earth,\nboreal forests are an area where autonomous vehicles are expected to become\nincreasingly common. In this paper, we address this issue by introducing\nBorealTC, a publicly available dataset for proprioceptive-based terrain\nclassification (TC). Recorded with a Husky A200, our dataset contains 116 min\nof Inertial Measurement Unit (IMU), motor current, and wheel odometry data,\nfocusing on typical boreal forest terrains, notably snow, ice, and silty loam.\nCombining our dataset with another dataset from the state-of-the-art, we\nevaluate both a Convolutional Neural Network (CNN) and the novel state space\nmodel (SSM)-based Mamba architecture on a TC task. Interestingly, we show that\nwhile CNN outperforms Mamba on each separate dataset, Mamba achieves greater\naccuracy when trained on a combination of both. In addition, we demonstrate\nthat Mamba's learning capacity is greater than a CNN for increasing amounts of\ndata. We show that the combination of two TC datasets yields a latent space\nthat can be interpreted with the properties of the terrains. We also discuss\nthe implications of merging datasets on classification. Our source code and\ndataset are publicly available online:\nhttps://github.com/norlab-ulaval/BorealTC.\n","authors":["Damien LaRocque","William Guimont-Martin","David-Alexandre Duclos","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2403.16877v1.pdf","comment":"Submitted to the 2024 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.16875v1","updated":"2024-03-25T15:39:46Z","published":"2024-03-25T15:39:46Z","title":"TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in\n  Deformable Granular Environments","summary":"  Terrain-aware perception holds the potential to improve the robustness and\naccuracy of autonomous robot navigation in the wilds, thereby facilitating\neffective off-road traversals. However, the lack of multi-modal perception\nacross various motion patterns hinders the solutions of Simultaneous\nLocalization And Mapping (SLAM), especially when confronting non-geometric\nhazards in demanding landscapes. In this paper, we first propose a\nTerrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy\nterrains. It incorporates various types of robotic proprioception and distinct\nground interactions for the unique challenges and benchmark of multi-sensor\nfusion SLAM. The versatile sensor suite comprises stereo frame cameras,\nmultiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK\ndevice. This ensemble is hardware-synchronized, well-calibrated, and\nself-contained. Utilizing both wheeled and quadrupedal locomotion, we\nefficiently collect comprehensive sequences to capture rich unstructured\nscenarios. It spans the spectrum of scope, terrain interactions, scene changes,\nground-level properties, and dynamic robot characteristics. We benchmark\nseveral state-of-the-art SLAM methods against ground truth and provide\nperformance validations. Corresponding challenges and limitations are also\nreported. All associated resources are accessible upon request at\n\\url{https://tailrobot.github.io/}.\n","authors":["Chen Yao","Yangtao Ge","Guowei Shi","Zirui Wang","Ningbo Yang","Zheng Zhu","Hexiang Wei","Yuntian Zhao","Jing Wu","Zhenzhong Jia"],"pdf_url":"https://arxiv.org/pdf/2403.16875v1.pdf","comment":"Submitted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2403.16859v1","updated":"2024-03-25T15:23:14Z","published":"2024-03-25T15:23:14Z","title":"A Semi-Lagrangian Approach for Time and Energy Path Planning\n  Optimization in Static Flow Fields","summary":"  Efficient path planning for autonomous mobile robots is a critical problem\nacross numerous domains, where optimizing both time and energy consumption is\nparamount. This paper introduces a novel methodology that considers the dynamic\ninfluence of an environmental flow field and considers geometric constraints,\nincluding obstacles and forbidden zones, enriching the complexity of the\nplanning problem. We formulate it as a multi-objective optimal control problem,\npropose a novel transformation called Harmonic Transformation, and apply a\nsemi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is\nobtained considering two distinct approaches: a deterministic method and an\nevolutionary-based one, both of which are designed to make use of the proposed\nHarmonic Transformation. Through an extensive analysis of these approaches, we\ndemonstrate their efficacy in finding optimized paths.\n","authors":["Víctor C. da S. Campos","Armando A. Neto","Douglas G. Macharet"],"pdf_url":"https://arxiv.org/pdf/2403.16859v1.pdf","comment":"12 pages, initial paper submission; Preprint submitted to the IEEE\n  Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2402.13848v2","updated":"2024-03-25T14:45:53Z","published":"2024-02-21T14:50:24Z","title":"Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps","summary":"  Bird's-eye view (BEV) maps are an important geometrically structured\nrepresentation widely used in robotics, in particular self-driving vehicles and\nterrestrial robots. Existing algorithms either require depth information for\nthe geometric projection, which is not always reliably available, or are\ntrained end-to-end in a fully supervised way to map visual first-person\nobservations to BEV representation, and are therefore restricted to the output\nmodality they have been trained for. In contrast, we propose a new model\ncapable of performing zero-shot projections of any modality available in a\nfirst person view to the corresponding BEV map. This is achieved by\ndisentangling the geometric inverse perspective projection from the modality\ntransformation, eg. RGB to occupancy. The method is general and we showcase\nexperiments projecting to BEV three different modalities: semantic\nsegmentation, motion vectors and object bounding boxes detected in first\nperson. We experimentally show that the model outperforms competing methods, in\nparticular the widely used baseline resorting to monocular depth estimation.\n","authors":["Gianluca Monaci","Leonid Antsfeld","Boris Chidlovskii","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2402.13848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16803v1","updated":"2024-03-25T14:21:49Z","published":"2024-03-25T14:21:49Z","title":"Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View\n  Planning","summary":"  Object reconstruction is relevant for many autonomous robotic tasks that\nrequire interaction with the environment. A key challenge in such scenarios is\nplanning view configurations to collect informative measurements for\nreconstructing an initially unknown object. One-shot view planning enables\nefficient data collection by predicting view configurations and planning the\nglobally shortest path connecting all views at once. However, geometric priors\nabout the object are required to conduct one-shot view planning. In this work,\nwe propose a novel one-shot view planning approach that utilizes the powerful\n3D generation capabilities of diffusion models as priors. By incorporating such\ngeometric priors into our pipeline, we achieve effective one-shot view planning\nstarting with only a single RGB image of the object to be reconstructed. Our\nplanning experiments in simulation and real-world setups indicate that our\napproach balances well between object reconstruction quality and movement cost.\n","authors":["Sicong Pan","Liren Jin","Xuying Huang","Cyrill Stachniss","Marija Popović","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.16803v1.pdf","comment":"Sicong Pan and Liren Jin have equal contribution. Submitted to IROS\n  2024"},{"id":"http://arxiv.org/abs/2403.16794v1","updated":"2024-03-25T14:13:09Z","published":"2024-03-25T14:13:09Z","title":"CurbNet: Curb Detection Framework Based on LiDAR Point Cloud\n  Segmentation","summary":"  Curb detection is an important function in intelligent driving and can be\nused to determine drivable areas of the road. However, curbs are difficult to\ndetect due to the complex road environment. This paper introduces CurbNet, a\nnovel framework for curb detection, leveraging point cloud segmentation.\nAddressing the dearth of comprehensive curb datasets and the absence of 3D\nannotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,\nwhich represents the largest and most categorically diverse collection of curb\npoint clouds currently available. Recognizing that curbs are primarily\ncharacterized by height variations, our approach harnesses spatially-rich 3D\npoint clouds for training. To tackle the challenges presented by the uneven\ndistribution of curb features on the xy-plane and their reliance on z-axis\nhigh-frequency features, we introduce the multi-scale and channel attention\n(MSCA) module, a bespoke solution designed to optimize detection performance.\nMoreover, we propose an adaptive weighted loss function group, specifically\nformulated to counteract the imbalance in the distribution of curb point clouds\nrelative to other categories. Our extensive experimentation on 2 major datasets\nhas yielded results that surpass existing benchmarks set by leading curb\ndetection and point cloud segmentation models. By integrating multi-clustering\nand curve fitting techniques in our post-processing stage, we have\nsubstantially reduced noise in curb detection, thereby enhancing precision to\n0.8744. Notably, CurbNet has achieved an exceptional average metrics of over\n0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.\nFurthermore, corroborative real-world experiments and dataset analyzes mutually\nvalidate each other, solidifying CurbNet's superior detection proficiency and\nits robust generalizability.\n","authors":["Guoyang Zhao","Fulong Ma","Yuxuan Liu","Weiqing Qi","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16786v1","updated":"2024-03-25T14:01:58Z","published":"2024-03-25T14:01:58Z","title":"DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking","summary":"  Efficiency and reliability are critical in robotic bin-picking as they\ndirectly impact the productivity of automated industrial processes. However,\ntraditional approaches, demanding static objects and fixed collisions, lead to\ndeployment limitations, operational inefficiencies, and process unreliability.\nThis paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges\ntraditional static assumptions. The DBPF endows the robot with the reactivity\nto pick multiple moving arbitrary objects while avoiding dynamic obstacles,\nsuch as the moving bin. Combined with scene-level pose generation, the proposed\npose selection metric leverages the Tendency-Aware Manipulability Network\noptimizing suction pose determination. Heuristic task-specific designs like\nvelocity-matching, dynamic obstacle avoidance, and the resight policy, enhance\nthe picking success rate and reliability. Empirical experiments demonstrate the\nimportance of these components. Our method achieves an average 84% success\nrate, surpassing the 60% of the most comparable baseline, crucially, with zero\ncollisions. Further evaluations under diverse dynamic scenarios showcase DBPF's\nrobust performance in dynamic bin-picking. Results suggest that our framework\noffers a promising solution for efficient and reliable robotic bin-picking\nunder dynamics.\n","authors":["Yichuan Li","Junkai Zhao","Yixiao Li","Zheng Wu","Rui Cao","Masayoshi Tomizuka","Yunhui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16786v1.pdf","comment":"8 pages, 5 figures. This paper has been accepted by IEEE RA-L on\n  2024-03-24. See the supplementary video at youtube:\n  https://youtu.be/n5af2VsKhkg"},{"id":"http://arxiv.org/abs/2403.16781v1","updated":"2024-03-25T13:55:49Z","published":"2024-03-25T13:55:49Z","title":"Visual Action Planning with Multiple Heterogeneous Agents","summary":"  Visual planning methods are promising to handle complex settings where\nextracting the system state is challenging. However, none of the existing works\ntackles the case of multiple heterogeneous agents which are characterized by\ndifferent capabilities and/or embodiment. In this work, we propose a method to\nrealize visual action planning in multi-agent settings by exploiting a roadmap\nbuilt in a low-dimensional structured latent space and used for planning. To\nenable multi-agent settings, we infer possible parallel actions from a dataset\ncomposed of tuples associated with individual actions. Next, we evaluate\nfeasibility and cost of them based on the capabilities of the multi-agent\nsystem and endow the roadmap with this information, building a capability\nlatent space roadmap (C-LSR). Additionally, a capability suggestion strategy is\ndesigned to inform the human operator about possible missing capabilities when\nno paths are found. The approach is validated in a simulated burger cooking\ntask and a real-world box packing task.\n","authors":["Martina Lippi","Michael C. Welle","Marco Moletta","Alessandro Marino","Andrea Gasparri","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11742v2","updated":"2024-03-25T13:47:06Z","published":"2024-03-18T12:54:33Z","title":"Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization","summary":"  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own Optimal Control\nProblem, with ADMM facilitating consistency between their optimizations. This\napproach not only decreases the computational time but also allows for\neffective scaling with more complex robot configurations, facilitating the\nintegration of additional subsystems such as articulated arms on a quadruped\nrobot. We demonstrate, through numerical evaluations, the convergence of our\napproach on two systems with increasing complexity. In addition, we showcase\nthat our approach converges towards the same solution when compared to a\nstate-of-the-art centralized whole-body MPC implementation. Moreover, we\nquantitatively compare the computational efficiency of our method to the\ncentralized approach, revealing up to a 75\\% reduction in computational time.\nOverall, our approach offers a promising avenue for accelerating MPC solutions\nfor legged robots, paving the way for more effective utilization of the\ncomputational performance of modern hardware.\n","authors":["Lorenzo Amatucci","Giulio Turrisi","Angelo Bratta","Victor Barasuol","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16764v1","updated":"2024-03-25T13:41:57Z","published":"2024-03-25T13:41:57Z","title":"Low-Cost Teleoperation with Haptic Feedback through Vision-based Tactile\n  Sensors for Rigid and Soft Object Manipulation","summary":"  Haptic feedback is essential for humans to successfully perform complex and\ndelicate manipulation tasks. A recent rise in tactile sensors has enabled\nrobots to leverage the sense of touch and expand their capability drastically.\nHowever, many tasks still need human intervention/guidance. For this reason, we\npresent a teleoperation framework designed to provide haptic feedback to human\noperators based on the data from camera-based tactile sensors mounted on the\nrobot gripper. Partial autonomy is introduced to prevent slippage of grasped\nobjects during task execution. Notably, we rely exclusively on low-cost\noff-the-shelf hardware to realize an affordable solution. We demonstrate the\nversatility of the framework on nine different objects ranging from rigid to\nsoft and fragile ones, using three different operators on real hardware.\n","authors":["Martina Lippi","Michael C. Welle","Maciej K. Wozniak","Andrea Gasparri","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16764v1.pdf","comment":"https://vision-tactile-manip.github.io/teleop/"},{"id":"http://arxiv.org/abs/2401.16700v2","updated":"2024-03-25T13:33:51Z","published":"2024-01-30T03:00:25Z","title":"Towards Precise 3D Human Pose Estimation with Multi-Perspective\n  Spatial-Temporal Relational Transformers","summary":"  3D human pose estimation captures the human joint points in three-dimensional\nspace while keeping the depth information and physical structure. That is\nessential for applications that require precise pose information, such as\nhuman-computer interaction, scene understanding, and rehabilitation training.\nDue to the challenges in data collection, mainstream datasets of 3D human pose\nestimation are primarily composed of multi-view video data collected in\nlaboratory environments, which contains rich spatial-temporal correlation\ninformation besides the image frame content. Given the remarkable\nself-attention mechanism of transformers, capable of capturing the\nspatial-temporal correlation from multi-view video datasets, we propose a\nmulti-stage framework for 3D sequence-to-sequence (seq2seq) human pose\ndetection. Firstly, the spatial module represents the human pose feature by\nintra-image content, while the frame-image relation module extracts temporal\nrelationships and 3D spatial positional relationship features between the\nmulti-perspective images. Secondly, the self-attention mechanism is adopted to\neliminate the interference from non-human body parts and reduce computing\nresources. Our method is evaluated on Human3.6M, a popular 3D human pose\ndetection dataset. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on this dataset. The source code will be available\nat https://github.com/WUJINHUAN/3D-human-pose.\n","authors":["Jianbin Jiao","Xina Cheng","Weijie Chen","Xiaoting Yin","Hao Shi","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16700v2.pdf","comment":"Accepted to IJCNN 2024. The source code will be available at\n  https://github.com/WUJINHUAN/3D-human-pose"},{"id":"http://arxiv.org/abs/2402.02423v2","updated":"2024-03-25T13:20:46Z","published":"2024-02-04T09:40:22Z","title":"Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement\n  Learning with Diverse Human Feedback","summary":"  Reinforcement Learning with Human Feedback (RLHF) has received significant\nattention for performing tasks without the need for costly manual reward design\nby aligning human preferences. It is crucial to consider diverse human feedback\ntypes and various learning methods in different environments. However,\nquantifying progress in RLHF with diverse feedback is challenging due to the\nlack of standardized annotation platforms and widely used unified benchmarks.\nTo bridge this gap, we introduce Uni-RLHF, a comprehensive system\nimplementation tailored for RLHF. It aims to provide a complete workflow from\nreal human feedback, fostering progress in the development of practical\nproblems. Uni-RLHF contains three packages: 1) a universal multi-feedback\nannotation platform, 2) large-scale crowdsourced feedback datasets, and 3)\nmodular offline RLHF baseline implementations. Uni-RLHF develops a\nuser-friendly annotation interface tailored to various feedback types,\ncompatible with a wide range of mainstream RL environments. We then establish a\nsystematic pipeline of crowdsourced annotations, resulting in large-scale\nannotated datasets comprising more than 15 million steps across 30+ popular\ntasks. Through extensive experiments, the results in the collected datasets\ndemonstrate competitive performance compared to those from well-designed manual\nrewards. We evaluate various design choices and offer insights into their\nstrengths and potential areas of improvement. We wish to build valuable\nopen-source platforms, datasets, and baselines to facilitate the development of\nmore robust and reliable RLHF solutions based on realistic human feedback. The\nwebsite is available at https://uni-rlhf.github.io/.\n","authors":["Yifu Yuan","Jianye Hao","Yi Ma","Zibin Dong","Hebin Liang","Jinyi Liu","Zhixin Feng","Kai Zhao","Yan Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.02423v2.pdf","comment":"Published as a conference paper at ICLR 2024. The website is\n  available at https://uni-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2403.16730v1","updated":"2024-03-25T13:04:20Z","published":"2024-03-25T13:04:20Z","title":"A Robotic Skill Learning System Built Upon Diffusion Policies and\n  Foundation Models","summary":"  In this paper, we build upon two major recent developments in the field,\nDiffusion Policies for visuomotor manipulation and large pre-trained multimodal\nfoundational models to obtain a robotic skill learning system. The system can\nobtain new skills via the behavioral cloning approach of visuomotor diffusion\npolicies given teleoperated demonstrations. Foundational models are being used\nto perform skill selection given the user's prompt in natural language. Before\nexecuting a skill the foundational model performs a precondition check given an\nobservation of the workspace. We compare the performance of different\nfoundational models to this end as well as give a detailed experimental\nevaluation of the skills taught by the user in simulation and the real world.\nFinally, we showcase the combined system on a challenging food serving scenario\nin the real world. Videos of all experimental executions, as well as the\nprocess of teaching new skills in simulation and the real world, are available\non the project's website.\n","authors":["Nils Ingelhag","Jesper Munkeby","Jonne van Haastregt","Anastasia Varava","Michael C. Welle","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16730v1.pdf","comment":"https://roboskillframework.github.io"},{"id":"http://arxiv.org/abs/2311.00390v3","updated":"2024-03-25T12:02:27Z","published":"2023-11-01T09:33:11Z","title":"A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing","summary":"  Aerial robots have garnered significant attention due to their potential\napplications in various industries, such as inspection, search and rescue, and\ndrone delivery. Successful missions often depend on the ability of these robots\nto grasp and land effectively. This paper presents a novel modular soft gripper\ndesign tailored explicitly for aerial grasping and landing operations. The\nproposed modular pneumatic soft gripper incorporates a feed-forward\nproportional controller to regulate pressure, enabling compliant gripping\ncapabilities. The modular connectors of the soft fingers offer two\nconfigurations for the 4-tip soft gripper, H-base (cylindrical) and X-base\n(spherical), allowing adaptability to different target objects. Additionally,\nthe gripper can serve as a soft landing gear when deflated, eliminating the\nneed for an extra landing gear. This design reduces weight, simplifies aerial\nmanipulation control, and enhances flight efficiency. We demonstrate the\nefficacy of indoor aerial grasping and achieve a maximum payload of 217 g using\nthe proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).\n","authors":["Hiu Ching Cheung","Ching-Wei Chang","Bailun Jiang","Chih-Yung Wen","Henry K. Chu"],"pdf_url":"https://arxiv.org/pdf/2311.00390v3.pdf","comment":"7 pages, 13 figures, accepted by IEEE RoboSoft 2024"},{"id":"http://arxiv.org/abs/2209.09020v3","updated":"2024-03-25T09:38:32Z","published":"2022-09-09T09:48:35Z","title":"Vehicle Trajectory Tracking Through Magnetic Sensors: A Case Study of\n  Two-lane Road","summary":"  Intelligent Transportation Systems (ITS) have a pressing need for efficient\nand reliable traffic surveillance solutions. This paper for the first time\nproposes a surveillance system that utilizes low-cost magnetic sensors for\ndetecting and tracking vehicles continuously along the road. The system uses\nmultiple sensors mounted along the roadside and lane boundaries to capture the\nmovement of vehicles. Real-time measurement data is collected by base stations\nand processed to produce vehicle trajectories that include position, timestamp,\nand speed. To address the challenge of tracking vehicles continuously on a road\nnetwork using a large amount of unlabeled magnetic sensor measurements, we\nfirst define a vehicle trajectory tracking problem. We then propose a\ngraph-based data association algorithm to track each detected vehicle, and\ndesign a related online algorithm framework respectively. We finally validate\nthe performance via both experimental simulation and real-world road\ndeployment. The experimental results demonstrate that the proposed solution\nprovides a cost-effective solution to capture the driving status of vehicles\nand on that basis form various traffic safety and efficiency applications.\n","authors":["Xiaojiang Ren","Yan Wang","Yingfan Geng"],"pdf_url":"https://arxiv.org/pdf/2209.09020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16374v1","updated":"2024-03-25T02:38:34Z","published":"2024-03-25T02:38:34Z","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions\n  for Autonomous Driving","summary":"  Accurate motion prediction of pedestrians, cyclists, and other surrounding\nvehicles (all called agents) is very important for autonomous driving. Most\nexisting works capture map information through an one-stage interaction with\nmap by vector-based attention, to provide map constraints for social\ninteraction and multi-modal differentiation. However, these methods have to\nencode all required map rules into the focal agent's feature, so as to retain\nall possible intentions' paths while at the meantime to adapt to potential\nsocial interaction. In this work, a progressive interaction network is proposed\nto enable the agent's feature to progressively focus on relevant maps, in order\nto better learn agents' feature representation capturing the relevant map\nconstraints. The network progressively encode the complex influence of map\nconstraints into the agent's feature through graph convolutions at the\nfollowing three stages: after historical trajectory encoder, after social\ninteraction, and after multi-modal differentiation. In addition, a weight\nallocation mechanism is proposed for multi-modal training, so that each mode\ncan obtain learning opportunities from a single-mode ground truth. Experiments\nhave validated the superiority of progressive interactions to the existing\none-stage interaction, and demonstrate the effectiveness of each component.\nEncouraging results were obtained in the challenging benchmarks.\n","authors":["Yinke Dong","Haifeng Yuan","Hongkun Liu","Wei Jing","Fangzhen Li","Hongmin Liu","Bin Fan"],"pdf_url":"https://arxiv.org/pdf/2403.16374v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.02969v2","updated":"2024-03-25T12:45:03Z","published":"2024-03-05T13:45:46Z","title":"Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception","summary":"  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a\ncognitive framework for diverse visual-language tasks. Recent efforts have been\nmade to equip MLLMs with visual perceiving and grounding capabilities. However,\nthere still remains a gap in providing fine-grained pixel-level perceptions and\nextending interactions beyond text-specific inputs. In this work, we propose\n{\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object\nperceptions and natural language descriptions from multi-modality references,\nsuch as texts, boxes, images, or audio. This innovation empowers users with\ngreater flexibility to engage with the model beyond textual and regional\nprompts, without modality-specific designs. Through our proposed refocusing\nmechanism, the generated grounding output is guided to better focus on the\nreferenced object, implicitly incorporating additional pixel-level supervision.\nThis simple modification utilizes attention scores generated during the\ninference of LLM, eliminating the need for extra computations while exhibiting\nperformance enhancements in both grounding masks and referring expressions.\nWith only publicly available training data, our model achieves state-of-the-art\nresults across multiple benchmarks, including diverse modality referring\nsegmentation and region-level referring expression generation.\n","authors":["Junwen He","Yifan Wang","Lijun Wang","Huchuan Lu","Jun-Yan He","Jin-Peng Lan","Bin Luo","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.02969v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16697v1","updated":"2024-03-25T12:31:01Z","published":"2024-03-25T12:31:01Z","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","summary":"  Source-Free Domain Generalization (SFDG) aims to develop a model that works\nfor unseen target domains without relying on any source domain. Recent work,\nPromptStyler, employs text prompts to simulate different distribution shifts in\nthe joint vision-language space, allowing the model to generalize effectively\nto unseen domains without using any images. However, 1) PromptStyler's style\ngeneration strategy has limitations, as all style patterns are fixed after the\nfirst training phase. This leads to the training set in the second training\nphase being restricted to a limited set of styles. Additionally, 2) the frozen\ntext encoder in PromptStyler result in the encoder's output varying with the\nstyle of the input text prompts, making it difficult for the model to learn\ndomain-invariant features. In this paper, we introduce Dynamic PromptStyler\n(DPStyler), comprising Style Generation and Style Removal modules to address\nthese issues. The Style Generation module refreshes all styles at every\ntraining epoch, while the Style Removal module eliminates variations in the\nencoder's output features caused by input styles. Moreover, since the Style\nGeneration module, responsible for generating style word vectors using random\nsampling or style mixing, makes the model sensitive to input text prompts, we\nintroduce a model ensemble method to mitigate this sensitivity. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art methods\non benchmark datasets.\n","authors":["Yunlong Tang","Yuxuan Wan","Lei Qi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2403.16697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16669v1","updated":"2024-03-25T12:07:24Z","published":"2024-03-25T12:07:24Z","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network","summary":"  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n","authors":["Yin Zhang","Jinhong Deng","Peidong Liu","Wen Li","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16669v1.pdf","comment":"17 pages, 11 figures. Accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05305v2","updated":"2024-03-25T11:48:27Z","published":"2024-02-07T22:50:47Z","title":"Knowledge Distillation for Road Detection based on cross-model\n  Semi-Supervised Learning","summary":"  The advancement of knowledge distillation has played a crucial role in\nenabling the transfer of knowledge from larger teacher models to smaller and\nmore efficient student models, and is particularly beneficial for online and\nresource-constrained applications. The effectiveness of the student model\nheavily relies on the quality of the distilled knowledge received from the\nteacher. Given the accessibility of unlabelled remote sensing data,\nsemi-supervised learning has become a prevalent strategy for enhancing model\nperformance. However, relying solely on semi-supervised learning with smaller\nmodels may be insufficient due to their limited capacity for feature\nextraction. This limitation restricts their ability to exploit training data.\nTo address this issue, we propose an integrated approach that combines\nknowledge distillation and semi-supervised learning methods. This hybrid\napproach leverages the robust capabilities of large models to effectively\nutilise large unlabelled data whilst subsequently providing the small student\nmodel with rich and informative features for enhancement. The proposed\nsemi-supervised learning-based knowledge distillation (SSLKD) approach\ndemonstrates a notable improvement in the performance of the student model, in\nthe application of road segmentation, surpassing the effectiveness of\ntraditional semi-supervised learning methods.\n","authors":["Wanli Ma","Oktay Karakus","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2402.05305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06744v2","updated":"2024-03-25T11:35:55Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in diffusion models have enabled 3D generation from a single\nimage. However, current methods often produce suboptimal results for novel\nviews, with blurred textures and deviations from the reference image, limiting\ntheir practical applications. In this paper, we introduce HiFi-123, a method\ndesigned for high-fidelity and multi-view consistent 3D generation. Our\ncontributions are twofold: First, we propose a Reference-Guided Novel View\nEnhancement (RGNV) technique that significantly improves the fidelity of\ndiffusion-based zero-shot novel view synthesis methods. Second, capitalizing on\nthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.\nWhen incorporated into the optimization-based image-to-3D pipeline, our method\nsignificantly improves 3D generation quality, achieving state-of-the-art\nperformance. Comprehensive evaluations demonstrate the effectiveness of our\napproach over existing methods, both qualitatively and quantitatively. Video\nresults are available on the project page.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Wenbo Hu","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v2.pdf","comment":"Project Page: https://drexubery.github.io/HiFi-123/"},{"id":"http://arxiv.org/abs/2403.16646v1","updated":"2024-03-25T11:32:05Z","published":"2024-03-25T11:32:05Z","title":"Clustering Propagation for Universal Medical Image Segmentation","summary":"  Prominent solutions for medical image segmentation are typically tailored for\nautomatic or interactive setups, posing challenges in facilitating progress\nachieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$\nnecessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both\ntraining time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$\nissues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$\nuniversal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$\nSlice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive\nsegmentation within a single model and one training session. Inspired by\nclustering-based segmentation techniques, S2VNet makes full use of the\nslice-wise structure of volumetric data by initializing cluster centers from\nthe cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This\nenables knowledge acquired from prior slices to assist in the segmentation of\nthe current slice, further efficiently bridging the communication between\nremote slices using mere 2D networks. Moreover, such a framework readily\naccommodates interactive segmentation with no architectural change, simply by\ninitializing centroids from user inputs. S2VNet distinguishes itself by swift\ninference speeds and reduced memory consumption compared to prevailing 3D\nsolutions. It can also handle multi-class interactions with each of them\nserving to initialize different centroids. Experiments on three benchmarks\ndemonstrate S2VNet surpasses task-specified solutions on both\nautomatic/interactive setups.\n","authors":["Yuhang Ding","Liulei Li","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16646v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16643v1","updated":"2024-03-25T11:29:19Z","published":"2024-03-25T11:29:19Z","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free\n  Super-Resolution","summary":"  Artifact-free super-resolution (SR) aims to translate low-resolution images\ninto their high-resolution counterparts with a strict integrity of the original\ncontent, eliminating any distortions or synthetic details. While traditional\ndiffusion-based SR techniques have demonstrated remarkable abilities to enhance\nimage detail, they are prone to artifact introduction during iterative\nprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,\ndeviate from the true structure of the source image, thus challenging the\nintegrity of the super-resolution process. In this work, we propose\nSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that\ndelves into the latent space to effectively identify and mitigate the\npropagation of artifacts. Our SARGD begins by using an artifact detector to\nidentify implausible pixels, creating a binary mask that highlights artifacts.\nFollowing this, the Reality Guidance Refinement (RGR) process refines artifacts\nby integrating this mask with realistic latent representations, improving\nalignment with the original image. Nonetheless, initial realistic-latent\nrepresentations from lower-quality images result in over-smoothing in the final\noutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.\nIt dynamically computes a reality score, enhancing the sharpness of the\nrealistic latent. These alternating mechanisms collectively achieve\nartifact-free super-resolution. Extensive experiments demonstrate the\nsuperiority of our method, delivering detailed artifact-free high-resolution\nimages while reducing sampling steps by 2X. We release our code at\nhttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.\n","authors":["Qingping Zheng","Ling Zheng","Yuanfan Guo","Ying Li","Songcen Xu","Jiankang Deng","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16638v1","updated":"2024-03-25T11:26:18Z","published":"2024-03-25T11:26:18Z","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","summary":"  The advancement of generation models has led to the emergence of highly\nrealistic artificial intelligence (AI)-generated videos. Malicious users can\neasily create non-existent videos to spread false information. This letter\nproposes an effective AI-generated video detection (AIGVDet) scheme by\ncapturing the forensic traces with a two-branch spatio-temporal convolutional\nneural network (CNN). Specifically, two ResNet sub-detectors are learned\nseparately for identifying the anomalies in spatical and optical flow domains,\nrespectively. Results of such sub-detectors are fused to further enhance the\ndiscrimination ability. A large-scale generated video dataset (GVD) is\nconstructed as a benchmark for model training and evaluation. Extensive\nexperimental results verify the high generalization and robustness of our\nAIGVDet scheme. Code and dataset will be available at\nhttps://github.com/multimediaFor/AIGVDet.\n","authors":["Jianfa Bai","Man Lin","Gang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16476v4","updated":"2024-03-25T11:24:45Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\nshape over-smoothing, color over-saturation, limited diversity in results, and\nslow convergence in existing text-to-SVG generation methods. VPSD models SVGs\nas distributions of control points and colors to counteract over-smoothing and\nover-saturation. Furthermore, VPSD leverages a reward model to reweight vector\nparticles, which improves aesthetic appeal and accelerates convergence.\nExtensive experiments have been conducted to validate the effectiveness of\nSVGDreamer, demonstrating its superiority over baseline methods in terms of\neditability, visual quality, and diversity. The code and demo of SVGDreamer can\nbe found at https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v4.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2403.16635v1","updated":"2024-03-25T11:24:02Z","published":"2024-03-25T11:24:02Z","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","summary":"  The objective of the collaborative vehicle-to-everything perception task is\nto enhance the individual vehicle's perception capability through message\ncommunication among neighboring traffic agents. Previous methods focus on\nachieving optimal performance within bandwidth limitations and typically adopt\nBEV maps as the basic collaborative message units. However, we demonstrate that\ncollaboration with dense representations is plagued by object feature\ndestruction during message packing, inefficient message aggregation for\nlong-range collaboration, and implicit structure representation communication.\nTo tackle these issues, we introduce a brand new message unit, namely point\ncluster, designed to represent the scene sparsely with a combination of\nlow-level structure information and high-level semantic information. The point\ncluster inherently preserves object information while packing messages, with\nweak relevance to the collaboration range, and supports explicit structure\nmodeling. Building upon this representation, we propose a novel framework\nV2X-PC for collaborative perception. This framework includes a Point Cluster\nPacking (PCP) module to keep object feature and manage bandwidth through the\nmanipulation of cluster point numbers. As for effective message aggregation, we\npropose a Point Cluster Aggregation (PCA) module to match and merge point\nclusters associated with the same object. To further handle time latency and\npose errors encountered in real-world scenarios, we propose parameter-free\nsolutions that can adapt to different noisy levels without finetuning.\nExperiments on two widely recognized collaborative perception benchmarks\nshowcase the superior performance of our method compared to the previous\nstate-of-the-art approaches relying on BEV maps.\n","authors":["Si Liu","Zihan Ding","Jiahui Fu","Hongyu Li","Siheng Chen","Shifeng Zhang","Xu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16627v1","updated":"2024-03-25T11:16:23Z","published":"2024-03-25T11:16:23Z","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","summary":"  Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.\n","authors":["Yuda Song","Zehao Sun","Xuanwu Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17744v2","updated":"2024-03-25T11:04:17Z","published":"2023-11-29T15:49:31Z","title":"Variational Bayes image restoration with compressive autoencoders","summary":"  Regularization of inverse problems is of paramount importance in\ncomputational imaging. The ability of neural networks to learn efficient image\nrepresentations has been recently exploited to design powerful data-driven\nregularizers. While state-of-the-art plug-and-play methods rely on an implicit\nregularization provided by neural denoisers, alternative Bayesian approaches\nconsider Maximum A Posteriori (MAP) estimation in the latent space of a\ngenerative model, thus with an explicit regularization. However,\nstate-of-the-art deep generative models require a huge amount of training data\ncompared to denoisers. Besides, their complexity hampers the optimization\ninvolved in latent MAP derivation. In this work, we first propose to use\ncompressive autoencoders instead. These networks, which can be seen as\nvariational autoencoders with a flexible latent prior, are smaller and easier\nto train than state-of-the-art generative models. As a second contribution, we\nintroduce the Variational Bayes Latent Estimation (VBLE) algorithm, which\nperforms latent estimation within the framework of variational inference.\nThanks to a simple yet efficient parameterization of the variational posterior,\nVBLE allows for fast and easy (approximate) posterior sampling. Experimental\nresults on image datasets BSD and FFHQ demonstrate that VBLE reaches similar\nperformance than state-of-the-art plug-and-play methods, while being able to\nquantify uncertainties faster than other existing posterior sampling\ntechniques.\n","authors":["Maud Biquard","Marie Chabert","Thomas Oberlin"],"pdf_url":"https://arxiv.org/pdf/2311.17744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12198v2","updated":"2024-03-25T11:04:04Z","published":"2023-12-19T14:34:36Z","title":"Mask Grounding for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS) is a challenging task that requires an\nalgorithm to segment objects referred by free-form language expressions.\nDespite significant progress in recent years, most state-of-the-art (SOTA)\nmethods still suffer from considerable language-image modality gap at the pixel\nand word level. These methods generally 1) rely on sentence-level language\nfeatures for language-image alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they exhibit weak object-level\ncorrespondence between visual and language features. Without well-grounded\nfeatures, prior methods struggle to understand complex expressions that require\nstrong reasoning over relationships among multiple objects, especially when\ndealing with rarely used or ambiguous clauses. To tackle this challenge, we\nintroduce a novel Mask Grounding auxiliary task that significantly improves\nvisual grounding within language features, by explicitly teaching the model to\nlearn fine-grained correspondence between masked textual tokens and their\nmatching visual objects. Mask Grounding can be directly used on prior RIS\nmethods and consistently bring improvements. Furthermore, to holistically\naddress the modality gap, we also design a cross-modal alignment loss and an\naccompanying alignment module. These additions work synergistically with Mask\nGrounding. With all these techniques, our comprehensive approach culminates in\nMagNet (Mask-grounded Network), an architecture that significantly outperforms\nprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating\nour method's effectiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n","authors":["Yong Xien Chng","Henry Zheng","Yizeng Han","Xuchong Qiu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.12198v2.pdf","comment":"Accepted by CVPR2024; Project page:\n  https://yxchng.github.io/projects/mask-grounding"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16605v1","updated":"2024-03-25T10:30:22Z","published":"2024-03-25T10:30:22Z","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for\n  Aerial Semantic Segmentation","summary":"  In recent years, semantic segmentation has become a pivotal tool in\nprocessing and interpreting satellite imagery. Yet, a prevalent limitation of\nsupervised learning techniques remains the need for extensive manual\nannotations by experts. In this work, we explore the potential of generative\nimage diffusion to address the scarcity of annotated data in earth observation\ntasks. The main idea is to learn the joint data manifold of images and labels,\nleveraging recent advancements in denoising diffusion probabilistic models. To\nthe best of our knowledge, we are the first to generate both images and\ncorresponding masks for satellite segmentation. We find that the obtained pairs\nnot only display high quality in fine-scale features but also ensure a wide\nsampling diversity. Both aspects are crucial for earth observation data, where\nsemantic classes can vary severely in scale and occurrence frequency. We employ\nthe novel data instances for downstream segmentation, as a form of data\naugmentation. In our experiments, we provide comparisons to prior works based\non discriminative diffusion models or GANs. We demonstrate that integrating\ngenerated samples yields significant quantitative improvements for satellite\nsemantic segmentation -- both compared to baselines and when training only on\nthe original data.\n","authors":["Aysim Toker","Marvin Eisenberger","Daniel Cremers","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.16605v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14828v2","updated":"2024-03-25T10:12:46Z","published":"2024-03-21T20:43:10Z","title":"Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing","summary":"  Fashion illustration is a crucial medium for designers to convey their\ncreative vision and transform design concepts into tangible representations\nthat showcase the interplay between clothing and the human body. In the context\nof fashion design, computer vision techniques have the potential to enhance and\nstreamline the design process. Departing from prior research primarily focused\non virtual try-on, this paper tackles the task of multimodal-conditioned\nfashion image editing. Our approach aims to generate human-centric fashion\nimages guided by multimodal prompts, including text, human body poses, garment\nsketches, and fabric textures. To address this problem, we propose extending\nlatent diffusion models to incorporate these multiple modalities and modifying\nthe structure of the denoising network, taking multimodal prompts as input. To\ncondition the proposed architecture on fabric textures, we employ textual\ninversion techniques and let diverse cross-attention layers of the denoising\nnetwork attend to textual and texture information, thus incorporating different\ngranularity conditioning details. Given the lack of datasets for the task, we\nextend two existing fashion datasets, Dress Code and VITON-HD, with multimodal\nannotations. Experimental evaluations demonstrate the effectiveness of our\nproposed approach in terms of realism and coherence concerning the provided\nmultimodal inputs.\n","authors":["Alberto Baldrati","Davide Morelli","Marcella Cornia","Marco Bertini","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2403.14828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.16578v1","updated":"2024-03-25T09:43:56Z","published":"2024-03-25T09:43:56Z","title":"SegICL: A Universal In-context Learning Framework for Enhanced\n  Segmentation in Medical Imaging","summary":"  Medical image segmentation models adapting to new tasks in a training-free\nmanner through in-context learning is an exciting advancement. Universal\nsegmentation models aim to generalize across the diverse modality of medical\nimages, yet their effectiveness often diminishes when applied to\nout-of-distribution (OOD) data modalities and tasks, requiring intricate\nfine-tuning of model for optimal performance. For addressing this challenge, we\nintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) for\nimage segmentation. Unlike existing methods, SegICL has the capability to\nemploy text-guided segmentation and conduct in-context learning with a small\nset of image-mask pairs, eliminating the need for training the model from\nscratch or fine-tuning for OOD tasks (including OOD modality and dataset).\nExtensive experimental validation of SegICL demonstrates a positive correlation\nbetween the number of prompt samples and segmentation performance on OOD\nmodalities and tasks. This indicates that SegICL effectively address new\nsegmentation tasks based on contextual information. Additionally, SegICL also\nexhibits comparable segmentation performance to mainstream models on OOD and\nin-distribution tasks. Our code will be released soon.\n","authors":["Lingdong Shen","Fangxin Shang","Yehui Yang","Xiaoshuang Huang","Shining Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.15353v2","updated":"2024-03-25T09:36:42Z","published":"2024-03-22T17:08:03Z","title":"Fully automated workflow for the design of patient-specific orthopaedic\n  implants: application to total knee arthroplasty","summary":"  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing\npain and improving mobility. While arthroplasty has known several technical\nimprovements, a significant share of patients are still unsatisfied with their\nsurgery. Personalised arthroplasty improves surgical outcomes however current\nsolutions require delays, making it difficult to integrate in clinical routine.\nWe propose a fully automated workflow to design patient-specific implants,\npresented for total knee arthroplasty, the most widely performed arthroplasty\nin the world nowadays.\n  The proposed pipeline first uses artificial neural networks to segment the\nproximal and distal extremities of the femur and tibia. Then the full bones are\nreconstructed using augmented statistical shape models, combining shape and\nlandmarks information. Finally, 77 morphological parameters are computed to\ndesign patient-specific implants. The developed workflow has been trained using\n91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in\nterms of accuracy and execution time.\n  The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$\nfor the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 5 minutes.\n  The proposed workflow allows for a fast and reliable personalisation of knee\nimplants, directly from the patient CT image without requiring any manual\nintervention. It establishes a patient-specific pre-operative planning for TKA\nin a very short time making it easily available for all patients. Combined with\nefficient implant manufacturing techniques, this solution could help answer the\ngrowing number of arthroplasties while reducing complications and improving the\npatients' satisfaction.\n","authors":["Aziliz Guezou-Philippe","Arnaud Clavé","Ehouarn Maguet","Ludivine Maintier","Charles Garraud","Jean-Rassaire Fouefack","Valérie Burdin","Eric Stindel","Guillaume Dardenne"],"pdf_url":"https://arxiv.org/pdf/2403.15353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16558v1","updated":"2024-03-25T09:17:15Z","published":"2024-03-25T09:17:15Z","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset paired with novel tasks: Referring Single Object\nTracking (RSOT) and Video Referring Expression Generation (Video-REG).\nElysiumTrack-1M contains 1.27 million annotated video frames with corresponding\nobject boxes and descriptions. Leveraging this dataset, we conduct training of\nMLLMs and propose a token-compression model T-Selector to tackle the second\nchallenge. Our proposed approach, Elysium: Exploring Object-level Perception in\nVideos via MLLM, is an end-to-end trainable MLLM that makes the first attempt\nto conduct object-level tasks in videos without requiring any additional\nplug-in or expert models.\n","authors":["Han Wang","Yanjie Wang","Yongjie Ye","Yuxiang Nie","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16552v1","updated":"2024-03-25T08:57:27Z","published":"2024-03-25T08:57:27Z","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","summary":"  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with\nTransformer architectures, have attracted significant attention due to their\npotential for energy efficiency and high performance. However, existing models\nin this domain still suffer from suboptimal performance. We introduce several\ninnovations to improve the performance: i) We propose a novel spike-form Q-K\nattention mechanism, tailored for SNNs, which efficiently models the importance\nof token or channel dimensions through binary vectors with linear complexity.\nii) We incorporate the hierarchical structure, which significantly benefits the\nperformance of both the brain and artificial neural networks, into spiking\ntransformers to obtain multi-scale spiking representation. iii) We design a\nversatile and powerful patch embedding module with a deformed shortcut\nspecifically for spiking transformers. Together, we develop QKFormer, a\nhierarchical spiking transformer based on Q-K attention with direct training.\nQKFormer shows significantly superior performance over existing\nstate-of-the-art SNN models on various mainstream datasets. Notably, with\ncomparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a\ngroundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially\noutperforming Spikformer by 10.84%. To our best knowledge, this is the first\ntime that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The\ncode and models are publicly available at\nhttps://github.com/zhouchenlin2096/QKFormer\n","authors":["Chenlin Zhou","Han Zhang","Zhaokun Zhou","Liutao Yu","Liwei Huang","Xiaopeng Fan","Li Yuan","Zhengyu Ma","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2403.16552v1.pdf","comment":"10 pages, code: https://github.com/zhouchenlin2096/QKFormer"},{"id":"http://arxiv.org/abs/2403.11854v2","updated":"2024-03-25T08:54:33Z","published":"2024-03-18T15:03:56Z","title":"denoiSplit: a method for joint image splitting and unsupervised\n  denoising","summary":"  In this work we present denoiSplit, a method to tackle a new analysis task,\ni.e. the challenge of joint semantic image splitting and unsupervised\ndenoising. This dual approach has important applications in fluorescence\nmicroscopy, where semantic image splitting has important applications but noise\ndoes generally hinder the downstream analysis of image content. Image splitting\ninvolves dissecting an image into its distinguishable semantic structures. We\nshow that the current state-of-the-art method for this task struggles in the\npresence of image noise, inadvertently also distributing the noise across the\npredicted outputs. The method we present here can deal with image noise by\nintegrating an unsupervised denoising sub-task. This integration results in\nimproved semantic image unmixing, even in the presence of notable and realistic\nlevels of imaging noise. A key innovation in denoiSplit is the use of\nspecifically formulated noise models and the suitable adjustment of\nKL-divergence loss for the high-dimensional hierarchical latent space we are\ntraining. We showcase the performance of denoiSplit across 4 tasks on\nreal-world microscopy images. Additionally, we perform qualitative and\nquantitative evaluations and compare results to existing benchmarks,\ndemonstrating the effectiveness of using denoiSplit: a single Variational\nSplitting Encoder-Decoder (VSE) Network using two suitable noise models to\njointly perform semantic splitting and denoising.\n","authors":["Ashesh Ashesh","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2403.11854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2403.06904v2","updated":"2024-03-25T08:45:37Z","published":"2024-03-11T16:56:37Z","title":"FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks","summary":"  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n","authors":["Muhammad Saif Ullah Khan","Muhammad Ferjad Naeem","Federico Tombari","Luc Van Gool","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2403.06904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00247v4","updated":"2024-03-25T08:34:15Z","published":"2023-08-01T03:00:36Z","title":"Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive\n  Review","summary":"  The advent of deep learning has brought a revolutionary transformation to\nimage denoising techniques. However, the persistent challenge of acquiring\nnoise-clean pairs for supervised methods in real-world scenarios remains\nformidable, necessitating the exploration of more practical self-supervised\nimage denoising. This paper focuses on self-supervised image denoising methods\nthat offer effective solutions to address this challenge. Our comprehensive\nreview thoroughly analyzes the latest advancements in self-supervised image\ndenoising approaches, categorizing them into three distinct classes: General\nmethods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.\nFor each class, we provide a concise theoretical analysis along with their\npractical applications. To assess the effectiveness of these methods, we\npresent both quantitative and qualitative experimental results on various\ndatasets, utilizing classical algorithms as benchmarks. Additionally, we\ncritically discuss the current limitations of these methods and propose\npromising directions for future research. By offering a detailed overview of\nrecent developments in self-supervised image denoising, this review serves as\nan invaluable resource for researchers and practitioners in the field,\nfacilitating a deeper understanding of this emerging domain and inspiring\nfurther advancements.\n","authors":["Dan Zhang","Fangfang Zhou","Felix Albu","Yuanzhou Wei","Xiao Yang","Yuan Gu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.00247v4.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2403.16539v1","updated":"2024-03-25T08:31:14Z","published":"2024-03-25T08:31:14Z","title":"DOrA: 3D Visual Grounding with Order-Aware Referring","summary":"  3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. While previous works\nattempt to exploit the verbo-visual relation with proposed cross-modal\ntransformers, unstructured natural utterances and scattered objects might lead\nto undesirable performances. In this paper, we introduce DOrA, a novel 3D\nvisual grounding framework with Order-Aware referring. DOrA is designed to\nleverage Large Language Models (LLMs) to parse language description, suggesting\na referential order of anchor objects. Such ordered anchor objects allow DOrA\nto update visual features and locate the target object during the grounding\nprocess. Experimental results on the NR3D and ScanRefer datasets demonstrate\nour superiority in both low-resource and full-data scenarios. In particular,\nDOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding\naccuracy under 1% data and 10% data settings, respectively.\n","authors":["Tung-Yu Wu","Sheng-Yu Huang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08262v4","updated":"2024-03-25T08:29:52Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v4.pdf","comment":"Accepted by CVPR 2024, Project Page:\n  https://yunminjin2.github.io/projects/bitt/"},{"id":"http://arxiv.org/abs/2403.16536v1","updated":"2024-03-25T08:26:42Z","published":"2024-03-25T08:26:42Z","title":"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate\n  Spatiotemporal Forecasting","summary":"  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded\nunparalleled results in predicting temporal and spatial dynamics. However,\nmodeling extensive global information remains a formidable challenge; CNNs are\nlimited by their narrow receptive fields, and ViTs struggle with the intensive\ncomputational demands of their attention mechanisms. The emergence of recent\nMamba-based architectures has been met with enthusiasm for their exceptional\nlong-sequence modeling capabilities, surpassing established vision models in\nefficiency and accuracy, which motivates us to develop an innovative\narchitecture tailored for spatiotemporal forecasting. In this paper, we propose\nthe VMRNN cell, a new recurrent unit that integrates the strengths of Vision\nMamba blocks with LSTM. We construct a network centered on VMRNN cells to\ntackle spatiotemporal prediction tasks effectively. Our extensive evaluations\nshow that our proposed approach secures competitive results on a variety of\ntasks while maintaining a smaller model size. Our code is available at\nhttps://github.com/yyyujintang/VMRNN-PyTorch.\n","authors":["Yujin Tang","Peijie Dong","Zhenheng Tang","Xiaowen Chu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2403.16536v1.pdf","comment":"11 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2308.09891 by other authors"},{"id":"http://arxiv.org/abs/2403.16530v1","updated":"2024-03-25T08:16:06Z","published":"2024-03-25T08:16:06Z","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in\n  Diffusion Models","summary":"  Diffusion models have been widely used for conditional data cross-modal\ngeneration tasks such as text-to-image and text-to-video. However,\nstate-of-the-art models still fail to align the generated visual concepts with\nhigh-level semantics in a language such as object count, spatial relationship,\netc. We approach this problem from a multimodal data fusion perspective and\ninvestigate how different fusion strategies can affect vision-language\nalignment. We discover that compared to the widely used early fusion of\nconditioning text in a pretrained image feature space, a specially designed\nintermediate fusion can: (i) boost text-to-image alignment with improved\ngeneration quality and (ii) improve training and inference efficiency by\nreducing low-rank text-to-image attention calculations. We perform experiments\nusing a text-to-image generation task on the MS-COCO dataset. We compare our\nintermediate fusion mechanism with the classic early fusion mechanism on two\ncommon conditioning methods on a U-shaped ViT backbone. Our intermediate fusion\nmodel achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and\n50% increased training speed compared to a strong U-ViT baseline with an early\nfusion.\n","authors":["Zizhao Hu","Shaochong Jia","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2403.16530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16528v1","updated":"2024-03-25T08:14:22Z","published":"2024-03-25T08:14:22Z","title":"Open-Set Recognition in the Age of Vision-Language Models","summary":"  Are vision-language models (VLMs) open-set models because they are trained on\ninternet-scale datasets? We answer this question with a clear no - VLMs\nintroduce closed-set assumptions via their finite query set, making them\nvulnerable to open-set conditions. We systematically evaluate VLMs for open-set\nrecognition and find they frequently misclassify objects not contained in their\nquery set, leading to alarmingly low precision when tuned for high recall and\nvice versa. We show that naively increasing the size of the query set to\ncontain more and more classes does not mitigate this problem, but instead\ncauses diminishing task performance and open-set performance. We establish a\nrevised definition of the open-set problem for the age of VLMs, define a new\nbenchmark and evaluation protocol to facilitate standardised evaluation and\nresearch in this important area, and evaluate promising baseline approaches\nbased on predictive uncertainty and dedicated negative embeddings on a range of\nVLM classifiers and object detectors.\n","authors":["Dimity Miller","Niko Sünderhauf","Alex Kenna","Keita Mason"],"pdf_url":"https://arxiv.org/pdf/2403.16528v1.pdf","comment":"31 pages, under review"},{"id":"http://arxiv.org/abs/2403.16526v1","updated":"2024-03-25T08:09:22Z","published":"2024-03-25T08:09:22Z","title":"ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise\n  Optimization in Medical Image Registration","summary":"  Deformable image registration plays a crucial role in medical imaging, aiding\nin disease diagnosis and image-guided interventions. Traditional iterative\nmethods are slow, while deep learning (DL) accelerates solutions but faces\nusability and precision challenges. This study introduces a pyramid network\nwith the enhanced motion decomposition Transformer (ModeTv2) operator,\nshowcasing superior pairwise optimization (PO) akin to traditional methods. We\nre-implement ModeT operator with CUDA extensions to enhance its computational\nefficiency. We further propose RegHead module which refines deformation fields,\nimproves the realism of deformation and reduces parameters. By adopting the PO,\nthe proposed network balances accuracy, efficiency, and generalizability.\nExtensive experiments on two public brain MRI datasets and one abdominal CT\ndataset demonstrate the network's suitability for PO, providing a DL model with\nenhanced usability and interpretability. The code is publicly available.\n","authors":["Haiqiao Wang","Zhuoyuan Wang","Dong Ni","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v3","updated":"2024-03-25T08:05:16Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v3.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.16520v1","updated":"2024-03-25T08:02:41Z","published":"2024-03-25T08:02:41Z","title":"CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal\n  Representation Learning for AD classification","summary":"  Alzheimer's disease (AD) is an incurable neurodegenerative condition leading\nto cognitive and functional deterioration. Given the lack of a cure, prompt and\nprecise AD diagnosis is vital, a complex process dependent on multiple factors\nand multi-modal data. While successful efforts have been made to integrate\nmulti-modal representation learning into medical datasets, scant attention has\nbeen given to 3D medical images. In this paper, we propose Contrastive Masked\nVim Autoencoder (CMViM), the first efficient representation learning method\ntailored for 3D multi-modal data. Our proposed framework is built on a masked\nVim autoencoder to learn a unified multi-modal representation and\nlong-dependencies contained in 3D medical images. We also introduce an\nintra-modal contrastive learning module to enhance the capability of the\nmulti-modal Vim encoder for modeling the discriminative features in the same\nmodality, and an inter-modal contrastive learning module to alleviate\nmisaligned representation among modalities. Our framework consists of two main\nsteps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to\nreconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal\nrepresentations with contrastive learning mechanisms from both intra-modal and\ninter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset\nand validated on the downstream task for AD classification. The proposed CMViM\nyields 2.7\\% AUC performance improvement compared with other state-of-the-art\nmethods.\n","authors":["Guangqian Yang","Kangrui Du","Zhihan Yang","Ye Du","Yongping Zheng","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16520v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16513v1","updated":"2024-03-25T07:58:58Z","published":"2024-03-25T07:58:58Z","title":"Let Real Images be as a Judger, Spotting Fake Images Synthesized with\n  Generative Models","summary":"  In the last few years, generative models have shown their powerful\ncapabilities in synthesizing realistic images in both quality and diversity\n(i.e., facial images, and natural subjects). Unfortunately, the artifact\npatterns in fake images synthesized by different generative models are\ninconsistent, leading to the failure of previous research that relied on\nspotting subtle differences between real and fake. In our preliminary\nexperiments, we find that the artifacts in fake images always change with the\ndevelopment of the generative model, while natural images exhibit stable\nstatistical properties. In this paper, we employ natural traces shared only by\nreal images as an additional predictive target in the detector. Specifically,\nthe natural traces are learned from the wild real images and we introduce\nextended supervised contrastive learning to bring them closer to real images\nand further away from fake ones. This motivates the detector to make decisions\nbased on the proximity of images to the natural traces. To conduct a\ncomprehensive experiment, we built a high-quality and diverse dataset that\nincludes generative models comprising 6 GAN and 6 diffusion models, to evaluate\nthe effectiveness in generalizing unknown forgery techniques and robustness in\nsurviving different transformations. Experimental results show that our\nproposed method gives 96.1% mAP significantly outperforms the baselines.\nExtensive experiments conducted on the widely recognized platform Midjourney\nreveal that our proposed method achieves an accuracy exceeding 78.4%,\nunderscoring its practicality for real-world application deployment. The source\ncode and partial self-built dataset are available in supplementary material.\n","authors":["Ziyou Liang","Run Wang","Weifeng Liu","Yuyang Zhang","Wenyuan Yang","Lina Wang","Xingkai Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16510v1","updated":"2024-03-25T07:54:18Z","published":"2024-03-25T07:54:18Z","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","summary":"  Despite the remarkable process of talking-head-based avatar-creating\nsolutions, directly generating anchor-style videos with full-body motions\nremains challenging. In this study, we propose Make-Your-Anchor, a novel system\nnecessitating only a one-minute video clip of an individual for training,\nsubsequently enabling the automatic generation of anchor-style videos with\nprecise torso and hand movements. Specifically, we finetune a proposed\nstructure-guided diffusion model on input video to render 3D mesh conditions\ninto human appearances. We adopt a two-stage training strategy for the\ndiffusion model, effectively binding movements with specific appearances. To\nproduce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise\ndiffusion model to a 3D style without additional training cost, and a simple\nyet effective batch-overlapped temporal denoising module is proposed to bypass\nthe constraints on video length during inference. Finally, a novel\nidentity-specific face enhancement module is introduced to improve the visual\nquality of facial regions in the output videos. Comparative experiments\ndemonstrate the effectiveness and superiority of the system in terms of visual\nquality, temporal coherence, and identity preservation, outperforming SOTA\ndiffusion/non-diffusion methods. Project page:\n\\url{https://github.com/ICTMCG/Make-Your-Anchor}.\n","authors":["Ziyao Huang","Fan Tang","Yong Zhang","Xiaodong Cun","Juan Cao","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16510v1.pdf","comment":"accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2305.01309v2","updated":"2024-03-25T07:53:54Z","published":"2023-05-02T10:35:20Z","title":"Geometric Prior Based Deep Human Point Cloud Geometry Compression","summary":"  The emergence of digital avatars has raised an exponential increase in the\ndemand for human point clouds with realistic and intricate details. The\ncompression of such data becomes challenging with overwhelming data amounts\ncomprising millions of points. Herein, we leverage the human geometric prior in\ngeometry redundancy removal of point clouds, greatly promoting the compression\nperformance. More specifically, the prior provides topological constraints as\ngeometry initialization, allowing adaptive adjustments with a compact parameter\nset that could be represented with only a few bits. Therefore, we can envisage\nhigh-resolution human point clouds as a combination of geometric priors and\nstructural deviations. The priors could first be derived with an aligned point\ncloud, and subsequently the difference of features is compressed into a compact\nlatent code. The proposed framework can operate in a play-and-plug fashion with\nexisting learning based point cloud compression methods. Extensive experimental\nresults show that our approach significantly improves the compression\nperformance without deteriorating the quality, demonstrating its promise in a\nvariety of applications.\n","authors":["Xinju Wu","Pingping Zhang","Meng Wang","Peilin Chen","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2305.01309v2.pdf","comment":"Accepted by TCSVT 2024"},{"id":"http://arxiv.org/abs/2311.17315v3","updated":"2024-03-25T07:51:14Z","published":"2023-11-29T02:10:31Z","title":"Explaining CLIP's performance disparities on data from blind/low vision\n  users","summary":"  Large multi-modal models (LMMs) hold the potential to usher in a new era of\nautomated visual assistance for people who are blind or low vision (BLV). Yet,\nthese models have not been systematically evaluated on data captured by BLV\nusers. We address this by empirically assessing CLIP, a widely-used LMM likely\nto underpin many assistive technologies. Testing 25 CLIP variants in a\nzero-shot classification task, we find that their accuracy is 15 percentage\npoints lower on average for images captured by BLV users than web-crawled\nimages. This disparity stems from CLIP's sensitivities to 1) image content\n(e.g. not recognizing disability objects as well as other objects); 2) image\nquality (e.g. not being robust to lighting variation); and 3) text content\n(e.g. not recognizing objects described by tactile adjectives as well as visual\nones). We delve deeper with a textual analysis of three common pre-training\ndatasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content\nis rarely mentioned. We then provide three examples that illustrate how the\nperformance disparities extend to three downstream models underpinned by CLIP:\nOWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5\nimages can mitigate CLIP's quality-of-service disparities for BLV users in some\nscenarios, which we discuss alongside a set of other possible mitigations.\n","authors":["Daniela Massiceti","Camilla Longden","Agnieszka Słowik","Samuel Wills","Martin Grayson","Cecily Morrison"],"pdf_url":"https://arxiv.org/pdf/2311.17315v3.pdf","comment":"Accepted at 2024 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.16502v1","updated":"2024-03-25T07:35:28Z","published":"2024-03-25T07:35:28Z","title":"Medical Image Registration and Its Application in Retinal Images: A\n  Review","summary":"  Medical image registration is vital for disease diagnosis and treatment with\nits ability to merge diverse information of images, which may be captured under\ndifferent times, angles, or modalities. Although several surveys have reviewed\nthe development of medical image registration, these surveys have not\nsystematically summarized methodologies of existing medical image registration\nmethods. To this end, we provide a comprehensive review of these methods from\ntraditional and deep learning-based directions, aiming to help audiences\nunderstand the development of medical image registration quickly. In\nparticular, we review recent advances in retinal image registration at the end\nof each section, which has not attracted much attention. Additionally, we also\ndiscuss the current challenges of retinal image registration and provide\ninsights and prospects for future research.\n","authors":["Qiushi Nie","Xiaoqing Zhang","Yan Hu","Mingdao Gong","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16499v1","updated":"2024-03-25T07:34:06Z","published":"2024-03-25T07:34:06Z","title":"Self-Supervised Learning for Medical Image Data with Anatomy-Oriented\n  Imaging Planes","summary":"  Self-supervised learning has emerged as a powerful tool for pretraining deep\nnetworks on unlabeled data, prior to transfer learning of target tasks with\nlimited annotation. The relevance between the pretraining pretext and target\ntasks is crucial to the success of transfer learning. Various pretext tasks\nhave been proposed to utilize properties of medical image data (e.g., three\ndimensionality), which are more relevant to medical image analysis than generic\nones for natural images. However, previous work rarely paid attention to data\nwith anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance\nimaging views. As these imaging planes are defined according to the anatomy of\nthe imaged organ, pretext tasks effectively exploiting this information can\npretrain the networks to gain knowledge on the organ of interest. In this work,\nwe propose two complementary pretext tasks for this group of medical image data\nbased on the spatial relationship of the imaging planes. The first is to learn\nthe relative orientation between the imaging planes and implemented as\nregressing their intersecting lines. The second exploits parallel imaging\nplanes to regress their relative slice locations within a stack. Both pretext\ntasks are conceptually straightforward and easy to implement, and can be\ncombined in multitask learning for better representation learning. Thorough\nexperiments on two anatomical structures (heart and knee) and representative\ntarget tasks (semantic segmentation and classification) demonstrate that the\nproposed pretext tasks are effective in pretraining deep networks for\nremarkably boosted performance on the target tasks, and superior to other\nrecent approaches.\n","authors":["Tianwei Zhang","Dong Wei","Mengmeng Zhua","Shi Gu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.16499v1.pdf","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16494v1","updated":"2024-03-25T07:22:22Z","published":"2024-03-25T07:22:22Z","title":"CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid\n  Convolution and Transformer Neural Networks","summary":"  We present CT-Bound, a fast boundary estimation method for noisy images using\na hybrid Convolution and Transformer neural network. The proposed architecture\ndecomposes boundary estimation into two tasks: local detection and global\nregularization of image boundaries. It first estimates a parametric\nrepresentation of boundary structures only using the input image within a small\nreceptive field and then refines the boundary structure in the parameter domain\nwithout accessing the input image. Because of this, a part of the network can\nbe easily trained using naive, synthetic images and still generalized to real\nimages, and the entire architecture is computationally efficient as the\nboundary refinement is non-iterative and not in the image domain. Compared with\nthe previous highest accuracy methods, our experiment shows that CT-Bound is\n100 times faster, producing comparably accurate, high-quality boundary and\ncolor maps. We also demonstrate that CT-Bound can produce boundary and color\nmaps on real captured images without extra fine-tuning and real-time boundary\nmap and color map videos at ten frames per second.\n","authors":["Wei Xu","Junjie Luo","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.16494v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16481v1","updated":"2024-03-25T07:07:50Z","published":"2024-03-25T07:07:50Z","title":"REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices","summary":"  This work tackles the challenging task of achieving real-time novel view\nsynthesis on various scenes, including highly reflective objects and unbounded\noutdoor scenes. Existing real-time rendering methods, especially those based on\nmeshes, often have subpar performance in modeling surfaces with rich\nview-dependent appearances. Our key idea lies in leveraging meshes for\nrendering acceleration while incorporating a novel approach to parameterize\nview-dependent information. We decompose the color into diffuse and specular,\nand model the specular color in the reflected direction based on a neural\nenvironment map. Our experiments demonstrate that our method achieves\ncomparable reconstruction quality for highly reflective surfaces compared to\nstate-of-the-art offline methods, while also efficiently enabling real-time\nrendering on edge devices such as smartphones.\n","authors":["Chaojie Ji","Yufeng Li","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.16481v1.pdf","comment":"Project Page:https://xdimlab.github.io/REFRAME/"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.10066v2","updated":"2024-03-25T06:27:57Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16450v1","updated":"2024-03-25T06:22:27Z","published":"2024-03-25T06:22:27Z","title":"Camera-aware Label Refinement for Unsupervised Person Re-identification","summary":"  Unsupervised person re-identification aims to retrieve images of a specified\nperson without identity labels. Many recent unsupervised Re-ID approaches adopt\nclustering-based methods to measure cross-camera feature similarity to roughly\ndivide images into clusters. They ignore the feature distribution discrepancy\ninduced by camera domain gap, resulting in the unavoidable performance\ndegradation. Camera information is usually available, and the feature\ndistribution in the single camera usually focuses more on the appearance of the\nindividual and has less intra-identity variance. Inspired by the observation,\nwe introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel\n\\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by\nclustering intra-camera similarity. Specifically, we employ intra-camera\ntraining to obtain reliable local pseudo labels within each camera, and then\nrefine global labels generated by inter-camera clustering and train the\ndiscriminative model using more reliable global pseudo labels in a self-paced\nmanner. Meanwhile, we develop a camera-alignment module to align feature\ndistributions under different cameras, which could help deal with the camera\nvariance further. Extensive experiments validate the superiority of our\nproposed method over state-of-the-art approaches. The code is accessible at\nhttps://github.com/leeBooMla/CALR.\n","authors":["Pengna Li","Kangyi Wu","Wenli Huang","Sanping Zhou","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16450v1.pdf","comment":"submitted to IEEE TMM"},{"id":"http://arxiv.org/abs/2312.02480v2","updated":"2024-03-25T06:22:09Z","published":"2023-12-05T04:13:31Z","title":"Differentiable Point-based Inverse Rendering","summary":"  We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting.\n","authors":["Hoon-Gyu Chung","Seokjun Choi","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2312.02480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16440v1","updated":"2024-03-25T06:02:05Z","published":"2024-03-25T06:02:05Z","title":"RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection","summary":"  Three-dimensional object detection is one of the key tasks in autonomous\ndriving. To reduce costs in practice, low-cost multi-view cameras for 3D object\ndetection are proposed to replace the expansive LiDAR sensors. However, relying\nsolely on cameras is difficult to achieve highly accurate and robust 3D object\ndetection. An effective solution to this issue is combining multi-view cameras\nwith the economical millimeter-wave radar sensor to achieve more reliable\nmulti-modal 3D object detection. In this paper, we introduce RCBEVDet, a\nradar-camera fusion 3D object detection method in the bird's eye view (BEV).\nSpecifically, we first design RadarBEVNet for radar BEV feature extraction.\nRadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section\n(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based\nencoder and a transformer-based encoder are proposed to extract radar features,\nwith an injection and extraction module to facilitate communication between the\ntwo encoders. The RCS-aware BEV encoder takes RCS as the object size prior to\nscattering the point feature in BEV. Besides, we present the Cross-Attention\nMulti-layer Fusion module to automatically align the multi-modal BEV feature\nfrom radar and camera with the deformable attention mechanism, and then fuse\nthe feature with channel and spatial fusion layers. Experimental results show\nthat RCBEVDet achieves new state-of-the-art radar-camera fusion results on\nnuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,\nRCBEVDet achieves better 3D detection results than all real-time camera-only\nand radar-camera 3D object detectors with a faster inference speed at 21~28\nFPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.\n","authors":["Zhiwei Lin","Zhe Liu","Zhongyu Xia","Xinhao Wang","Yongtao Wang","Shengxiang Qi","Yang Dong","Nan Dong","Le Zhang","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16440v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07371v2","updated":"2024-03-25T05:48:28Z","published":"2024-03-12T07:15:29Z","title":"Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\n  Altered Diffusion Models","summary":"  This study discusses the critical issues of Virtual Try-On in contemporary\ne-commerce and the prospective metaverse, emphasizing the challenges of\npreserving intricate texture details and distinctive features of the target\nperson and the clothes in various scenarios, such as clothing texture and\nidentity characteristics like tattoos or accessories. In addition to the\nfidelity of the synthesized images, the efficiency of the synthesis process\npresents a significant hurdle. Various existing approaches are explored,\nhighlighting the limitations and unresolved aspects, e.g., identity information\nomission, uncontrollable artifacts, and low synthesis speed. It then proposes a\nnovel diffusion-based solution that addresses garment texture preservation and\nuser identity retention during virtual try-on. The proposed network comprises\ntwo primary modules - a warping module aligning clothing with individual\nfeatures and a try-on module refining the attire and generating missing parts\nintegrated with a mask-aware post-processing technique ensuring the integrity\nof the individual's identity. It demonstrates impressive results, surpassing\nthe state-of-the-art in speed by nearly 20 times during inference, with\nsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\ncomparable performance with the recent SOTA method on the VITON-HD and\nDresscode datasets.\n","authors":["Phuong Dam","Jihoon Jeong","Anh Tran","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.07371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16438v1","updated":"2024-03-25T05:46:06Z","published":"2024-03-25T05:46:06Z","title":"Real-time Neuron Segmentation for Voltage Imaging","summary":"  In voltage imaging, where the membrane potentials of individual neurons are\nrecorded at from hundreds to thousand frames per second using fluorescence\nmicroscopy, data processing presents a challenge. Even a fraction of a minute\nof recording with a limited image size yields gigabytes of video data\nconsisting of tens of thousands of frames, which can be time-consuming to\nprocess. Moreover, millisecond-level short exposures lead to noisy video\nframes, obscuring neuron footprints especially in deep-brain samples where\nnoisy signals are buried in background fluorescence. To address this challenge,\nwe propose a fast neuron segmentation method able to detect multiple,\npotentially overlapping, spiking neurons from noisy video frames, and implement\na data processing pipeline incorporating the proposed segmentation method along\nwith GPU-accelerated motion correction. By testing on existing datasets as well\nas on new datasets we introduce, we show that our pipeline extracts neuron\nfootprints that agree well with human annotation even from cluttered datasets,\nand demonstrate real-time processing of voltage imaging data on a single\ndesktop computer for the first time.\n","authors":["Yosuke Bando","Ramdas Pillai","Atsushi Kajita","Farhan Abdul Hakeem","Yves Quemener","Hua-an Tseng","Kiryl D. Piatkevich","Changyang Linghu","Xue Han","Edward S. Boyden"],"pdf_url":"https://arxiv.org/pdf/2403.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18287v2","updated":"2024-03-25T05:34:58Z","published":"2023-11-30T06:45:52Z","title":"Dispersed Structured Light for Hyperspectral 3D Imaging","summary":"  Hyperspectral 3D imaging aims to acquire both depth and spectral information\nof a scene. However, existing methods are either prohibitively expensive and\nbulky or compromise on spectral and depth accuracy. In this work, we present\nDispersed Structured Light (DSL), a cost-effective and compact method for\naccurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera\nsystem by placing a sub-millimeter thick diffraction grating film front of the\nprojector. The grating disperses structured light based on light wavelength. To\nutilize the dispersed structured light, we devise a model for dispersive\nprojection image formation and a per-pixel hyperspectral 3D reconstruction\nmethod. We validate DSL by instantiating a compact experimental prototype. DSL\nachieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth\nerror of 1mm. We demonstrate that DSL outperforms prior work on practical\nhyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D\nimaging for diverse application domains, including computer vision and\ngraphics, cultural heritage, geology, and biology.\n","authors":["Suhyun Shin","Seokjun Choi","Felix Heide","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2311.18287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16431v1","updated":"2024-03-25T05:22:34Z","published":"2024-03-25T05:22:34Z","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene\n  Understanding","summary":"  Point scene understanding is a challenging task to process real-world scene\npoint cloud, which aims at segmenting each object, estimating its pose, and\nreconstructing its mesh simultaneously. Recent state-of-the-art method first\nsegments each object and then processes them independently with multiple stages\nfor the different sub-tasks. This leads to a complex pipeline to optimize and\nmakes it hard to leverage the relationship constraints between multiple\nobjects. In this work, we propose a novel Disentangled Object-Centric\nTRansformer (DOCTR) that explores object-centric representation to facilitate\nlearning with multiple objects for the multiple sub-tasks in a unified manner.\nEach object is represented as a query, and a Transformer decoder is adapted to\niteratively optimize all the queries involving their relationship. In\nparticular, we introduce a semantic-geometry disentangled query (SGDQ) design\nthat enables the query features to attend separately to semantic information\nand geometric information relevant to the corresponding sub-tasks. A hybrid\nbipartite matching module is employed to well use the supervisions from all the\nsub-tasks during training. Qualitative and quantitative experimental results\ndemonstrate that our method achieves state-of-the-art performance on the\nchallenging ScanNet dataset. Code is available at\nhttps://github.com/SAITPublic/DOCTR.\n","authors":["Xiaoxuan Yu","Hao Wang","Weiming Li","Qiang Wang","Soonyong Cho","Younghun Sung"],"pdf_url":"https://arxiv.org/pdf/2403.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v3","updated":"2024-03-25T05:18:04Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v3.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.16428v1","updated":"2024-03-25T05:12:21Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3D understanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Liu Zheng","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16425v1","updated":"2024-03-25T05:10:34Z","published":"2024-03-25T05:10:34Z","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in\n  Event Cameras","summary":"  Event cameras are increasingly popular in robotics due to their beneficial\nfeatures, such as low latency, energy efficiency, and high dynamic range.\nNevertheless, their downstream task performance is greatly influenced by the\noptimization of bias parameters. These parameters, for instance, regulate the\nnecessary change in light intensity to trigger an event, which in turn depends\non factors such as the environment lighting and camera motion. This paper\nintroduces feedback control algorithms that automatically tune the bias\nparameters through two interacting methods: 1) An immediate, on-the-fly fast\nadaptation of the refractory period, which sets the minimum interval between\nconsecutive events, and 2) if the event rate exceeds the specified bounds even\nafter changing the refractory period repeatedly, the controller adapts the\npixel bandwidth and event thresholds, which stabilizes after a short period of\nnoise events across all pixels (slow adaptation). Our evaluation focuses on the\nvisual place recognition task, where incoming query images are compared to a\ngiven reference database. We conducted comprehensive evaluations of our\nalgorithms' adaptive feedback control in real-time. To do so, we collected the\nQCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366\nrepeated traversals of a Scout Mini robot navigating through a 100 meter long\nindoor lab setting (totaling over 35km distance traveled) in varying brightness\nconditions with ground truth location information. Our proposed feedback\ncontrollers result in superior performance when compared to the standard bias\nsettings and prior feedback control methods. Our findings also detail the\nimpact of bias adjustments on task performance and feature ablation studies on\nthe fast and slow adaptation mechanisms.\n","authors":["Gokul B. Nair","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2403.16425v1.pdf","comment":"8 pages, 9 figures, paper under review"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16422v1","updated":"2024-03-25T04:54:49Z","published":"2024-03-25T04:54:49Z","title":"Refining Text-to-Image Generation: Towards Accurate Training-Free\n  Glyph-Enhanced Image Generation","summary":"  Over the past few years, Text-to-Image (T2I) generation approaches based on\ndiffusion models have gained significant attention. However, vanilla diffusion\nmodels often suffer from spelling inaccuracies in the text displayed within the\ngenerated images. The capability to generate visual text is crucial, offering\nboth academic interest and a wide range of practical applications. To produce\naccurate visual text images, state-of-the-art techniques adopt a\nglyph-controlled image generation approach, consisting of a text layout\ngenerator followed by an image generator that is conditioned on the generated\ntext layout. Nevertheless, our study reveals that these models still face three\nprimary challenges, prompting us to develop a testbed to facilitate future\nresearch. We introduce a benchmark, LenCom-Eval, specifically designed for\ntesting models' capability in generating images with Lengthy and Complex visual\ntext. Subsequently, we introduce a training-free framework to enhance the\ntwo-stage generation approaches. We examine the effectiveness of our approach\non both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable\nimprovements across a range of evaluation metrics, including CLIPScore, OCR\nprecision, recall, F1 score, accuracy, and edit distance scores. For instance,\nour proposed framework improves the backbone model, TextDiffuser, by more than\n23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,\nrespectively. Our work makes a unique contribution to the field by focusing on\ngenerating images with long and rare text sequences, a niche previously\nunexplored by existing literature\n","authors":["Sanyam Lakhanpal","Shivang Chopra","Vinija Jain","Aman Chadha","Man Luo"],"pdf_url":"https://arxiv.org/pdf/2403.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03173v3","updated":"2024-03-25T04:42:22Z","published":"2024-03-05T18:08:29Z","title":"Solving the bongard-logo problem by modeling a probabilistic model","summary":"  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03173v3.pdf","comment":"14 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.03190v4","updated":"2024-03-25T04:40:39Z","published":"2024-03-05T18:29:17Z","title":"Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract\n  Reasoning process","summary":"  Abstract reasoning problems pose significant challenges to artificial\nintelligence algorithms, demanding cognitive capabilities beyond those required\nfor perception tasks. This study introduces the Triple-CFN approach to tackle\nthe Bongard-Logo problem, achieving notable reasoning accuracy by implicitly\nreorganizing the concept space of conflicting instances. Additionally, the\nTriple-CFN paradigm proves effective for the RPM problem with necessary\nmodifications, yielding competitive results. To further enhance performance on\nthe RPM issue, we develop the Meta Triple-CFN network, which explicitly\nstructures the problem space while maintaining interpretability on progressive\npatterns. The success of Meta Triple-CFN is attributed to its paradigm of\nmodeling the conceptual space, equivalent to normalizing reasoning information.\nBased on this ideology, we introduce the Re-space layer, enhancing the\nperformance of both Meta Triple-CFN and Triple-CFN. This paper aims to\ncontribute to advancements in machine intelligence by exploring innovative\nnetwork designs for addressing abstract reasoning problems, paving the way for\nfurther breakthroughs in this domain.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03190v4.pdf","comment":"14 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.03452v4","updated":"2024-03-25T04:38:42Z","published":"2024-03-06T04:36:43Z","title":"D4C glove-train: solving the RPM and Bongard-logo problem by\n  distributing and Circumscribing concepts","summary":"  This paper achieves noteworthy progress in the realm of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nchallenges. Initially, we introduce Lico-Net, a novel baseline model that\nresolves RPM problems with remarkable accuracy. Leveraging this foundation, we\nadvance with the D3C approach, which advocates representing the underlying\nconcepts in abstract reasoning problems through distributions. This perspective\nenhances the performance of both Lico-Net and a baseline model excelling in\nBongard-Logo tasks. To bolster the computational efficiency of D3C, we present\nthe D3C-cos variant, offering a streamlined yet precise solution. Furthermore,\nwe propose the D2C method, redefining conceptual boundaries within these\ndomains and bridging the divide between high-level abstractions and their\nlower-dimensional counterparts. Finally, we extend our methodology to D4C,\nemploying adversarial techniques to refine conceptual boundaries further and\ndemonstrate substantial improvements in both RPM and Bongard-Logo challenges.\nOverall, our contributions present a fresh outlook and practical advancements\nin the field of abstract reasoning.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03452v4.pdf","comment":"18 pages, 19 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16412v1","updated":"2024-03-25T04:14:07Z","published":"2024-03-25T04:14:07Z","title":"Unsupervised Template-assisted Point Cloud Shape Correspondence Network","summary":"  Unsupervised point cloud shape correspondence aims to establish point-wise\ncorrespondences between source and target point clouds. Existing methods obtain\ncorrespondences directly by computing point-wise feature similarity between\npoint clouds. However, non-rigid objects possess strong deformability and\nunusual shapes, making it a longstanding challenge to directly establish\ncorrespondences between point clouds with unconventional shapes. To address\nthis challenge, we propose an unsupervised Template-Assisted point cloud shape\ncorrespondence Network, termed TANet, including a template generation module\nand a template assistance module. The proposed TANet enjoys several merits.\nFirstly, the template generation module establishes a set of learnable\ntemplates with explicit structures. Secondly, we introduce a template\nassistance module that extensively leverages the generated templates to\nestablish more accurate shape correspondences from multiple perspectives.\nExtensive experiments on four human and animal datasets demonstrate that TANet\nachieves favorable performance against state-of-the-art methods.\n","authors":["Jiacheng Deng","Jiahao Lu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16412v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16410v1","updated":"2024-03-25T04:05:23Z","published":"2024-03-25T04:05:23Z","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","summary":"  As a neuromorphic sensor with high temporal resolution, spike cameras offer\nnotable advantages over traditional cameras in high-speed vision applications\nsuch as high-speed optical estimation, depth estimation, and object tracking.\nInspired by the success of the spike camera, we proposed Spike-NeRF, the first\nNeural Radiance Field derived from spike data, to achieve 3D reconstruction and\nnovel viewpoint synthesis of high-speed scenes. Instead of the multi-view\nimages at the same time of NeRF, the inputs of Spike-NeRF are continuous spike\nstreams captured by a moving spike camera in a very short time. To reconstruct\na correct and stable 3D scene from high-frequency but unstable spike data, we\ndevised spike masks along with a distinctive loss function. We evaluate our\nmethod qualitatively and numerically on several challenging synthetic scenes\ngenerated by blender with the spike camera simulator. Our results demonstrate\nthat Spike-NeRF produces more visually appealing results than the existing\nmethods and the baseline we proposed in high-speed scenes. Our code and data\nwill be released soon.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Mianzhi Liu","Ziyi Guo","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16410v1.pdf","comment":"This paper is accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.16407v1","updated":"2024-03-25T03:47:53Z","published":"2024-03-25T03:47:53Z","title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects","summary":"  Video generation is a rapidly advancing research area, garnering significant\nattention due to its broad range of applications. One critical aspect of this\nfield is the generation of long-duration videos, which presents unique\nchallenges and opportunities. This paper presents the first survey of recent\nadvancements in long video generation and summarises them into two key\nparadigms: divide and conquer temporal autoregressive.\n  We delve into the common models employed in each paradigm, including aspects\nof network design and conditioning techniques. Furthermore, we offer a\ncomprehensive overview and classification of the datasets and evaluation\nmetrics which are crucial for advancing long video generation research.\nConcluding with a summary of existing studies, we also discuss the emerging\nchallenges and future directions in this dynamic field. We hope that this\nsurvey will serve as an essential reference for researchers and practitioners\nin the realm of long video generation.\n","authors":["Chengxuan Li","Di Huang","Zeyu Lu","Yang Xiao","Qingqi Pei","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2403.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16405v1","updated":"2024-03-25T03:44:36Z","published":"2024-03-25T03:44:36Z","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low\n  Curvature Models","summary":"  The integration of an ensemble of deep learning models has been extensively\nexplored to enhance defense against adversarial attacks. The diversity among\nsub-models increases the attack cost required to deceive the majority of the\nensemble, thereby improving the adversarial robustness. While existing\napproaches mainly center on increasing diversity in feature representations or\ndispersion of first-order gradients with respect to input, the limited\ncorrelation between these diversity metrics and adversarial robustness\nconstrains the performance of ensemble adversarial defense. In this work, we\naim to enhance ensemble diversity by reducing attack transferability. We\nidentify second-order gradients, which depict the loss curvature, as a key\nfactor in adversarial robustness. Computing the Hessian matrix involved in\nsecond-order gradients is computationally expensive. To address this, we\napproximate the Hessian-vector product using differential approximation. Given\nthat low curvature provides better robustness, our ensemble model was designed\nto consider the influence of curvature among different sub-models. We introduce\na novel regularizer to train multiple more-diverse low-curvature network\nmodels. Extensive experiments across various datasets demonstrate that our\nensemble model exhibits superior robustness against a range of attacks,\nunderscoring the effectiveness of our approach.\n","authors":["Kaikang Zhao","Xi Chen","Wei Huang","Liuxin Ding","Xianglong Kong","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16405v1.pdf","comment":"Accepted to The 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2304.06928v2","updated":"2024-03-25T03:40:19Z","published":"2023-04-14T05:25:52Z","title":"CiPR: An Efficient Framework with Cross-instance Positive Relations for\n  Generalized Category Discovery","summary":"  We tackle the issue of generalized category discovery (GCD). GCD considers\nthe open-world problem of automatically clustering a partially labelled\ndataset, in which the unlabelled data may contain instances from both novel\ncategories and labelled classes. In this paper, we address the GCD problem with\nan unknown category number for the unlabelled data. We propose a framework,\nnamed CiPR, to bootstrap the representation by exploiting Cross-instance\nPositive Relations in the partially labelled data for contrastive learning,\nwhich have been neglected in existing methods. To obtain reliable\ncross-instance relations to facilitate representation learning, we introduce a\nsemi-supervised hierarchical clustering algorithm, named selective neighbor\nclustering (SNC), which can produce a clustering hierarchy directly from the\nconnected components of a graph constructed from selective neighbors. We\nfurther present a method to estimate the unknown class number using SNC with a\njoint reference score that considers clustering indexes of both labelled and\nunlabelled data, and extend SNC to allow label assignment for the unlabelled\ninstances with a given class number. We thoroughly evaluate our framework on\npublic generic image recognition datasets and challenging fine-grained\ndatasets, and establish a new state-of-the-art. Code:\nhttps://github.com/haoosz/CiPR\n","authors":["Shaozhe Hao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2304.06928v2.pdf","comment":"Accepted to TMLR. Code: https://github.com/haoosz/CiPR"},{"id":"http://arxiv.org/abs/2311.13614v2","updated":"2024-03-25T03:39:45Z","published":"2023-11-22T04:52:58Z","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction\n  Data","summary":"  Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.\n","authors":["Qifan Yu","Juncheng Li","Longhui Wei","Liang Pang","Wentao Ye","Bosheng Qin","Siliang Tang","Qi Tian","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.13614v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16400v1","updated":"2024-03-25T03:30:37Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17460v3","updated":"2024-03-25T03:21:39Z","published":"2023-11-29T09:02:07Z","title":"W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera\n  Calibration and Orientation Correction","summary":"  For a long time, in reconstructing 3D human bodies from monocular images,\nmost methods opted to simplify the task by minimizing the influence of the\ncamera. Using a coarse focal length setting results in the reconstructed bodies\nnot aligning well with distorted images. Ignoring camera rotation leads to an\nunrealistic reconstructed body pose in world space. Consequently, the\napplication scenarios of existing methods are confined to controlled\nenvironments. When confronted with complex and diverse in-the-wild images, they\nstruggle to achieve accurate and reasonable reconstruction in world space. To\naddress the above issues, we propose W-HMR, which decouples global body\nrecovery into camera calibration, local body recovery, and global body\norientation correction. We design the first weak-supervised camera calibration\nmethod for body distortion, eliminating dependence on focal length labels and\nachieving finer mesh-image alignment. We propose a novel orientation correction\nmodule to allow the reconstructed human body to remain normal in world space.\nDecoupling body orientation and body pose enables our model to consider the\naccuracy in camera coordinate and the reasonableness in world coordinate\nsimultaneously, expanding the range of applications. As a result, W-HMR\nachieves high-quality reconstruction in dual coordinate systems, particularly\nin challenging scenes. Codes and demos have been released on the project page\nhttps://yw0208.github.io/w-hmr/.\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2311.17460v3.pdf","comment":"Project Page: https://yw0208.github.io/w-hmr/"},{"id":"http://arxiv.org/abs/2403.16395v1","updated":"2024-03-25T03:18:58Z","published":"2024-03-25T03:18:58Z","title":"Multi-attention Associate Prediction Network for Visual Tracking","summary":"  Classification-regression prediction networks have realized impressive\nsuccess in several modern deep trackers. However, there is an inherent\ndifference between classification and regression tasks, so they have diverse\neven opposite demands for feature matching. Existed models always ignore the\nkey issue and only employ a unified matching block in two task branches,\ndecaying the decision quality. Besides, these models also struggle with\ndecision misalignment situation. In this paper, we propose a multi-attention\nassociate prediction network (MAPNet) to tackle the above problems. Concretely,\ntwo novel matchers, i.e., category-aware matcher and spatial-aware matcher, are\nfirst designed for feature comparison by integrating self, cross, channel or\nspatial attentions organically. They are capable of fully capturing the\ncategory-related semantics for classification and the local spatial contexts\nfor regression, respectively. Then, we present a dual alignment module to\nenhance the correspondences between two branches, which is useful to find the\noptimal tracking solution. Finally, we describe a Siamese tracker built upon\nthe proposed prediction network, which achieves the leading performance on five\ntracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and\nUAV123, and surpasses other state-of-the-art approaches.\n","authors":["Xinglong Sun","Haijiang Sun","Shan Jiang","Jiacheng Wang","Xilai Wei","Zhonghe Hu"],"pdf_url":"https://arxiv.org/pdf/2403.16395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16387v1","updated":"2024-03-25T03:06:45Z","published":"2024-03-25T03:06:45Z","title":"Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and\n  Interactive Image Fusion","summary":"  Image fusion aims to combine information from different source images to\ncreate a comprehensively representative image. Existing fusion methods are\ntypically helpless in dealing with degradations in low-quality source images\nand non-interactive to multiple subjective and objective needs. To solve them,\nwe introduce a novel approach that leverages semantic text guidance image\nfusion model for degradation-aware and interactive image fusion task, termed as\nText-IF. It innovatively extends the classical image fusion to the text guided\nimage fusion along with the ability to harmoniously address the degradation and\ninteraction issues during fusion. Through the text semantic encoder and\nsemantic interaction fusion decoder, Text-IF is accessible to the all-in-one\ninfrared and visible image degradation-aware processing and the interactive\nflexible fusion outcomes. In this way, Text-IF achieves not only multi-modal\nimage fusion, but also multi-modal information fusion. Extensive experiments\nprove that our proposed text guided image fusion strategy has obvious\nadvantages over SOTA methods in the image fusion performance and degradation\ntreatment. The code is available at https://github.com/XunpengYi/Text-IF.\n","authors":["Xunpeng Yi","Han Xu","Hao Zhang","Linfeng Tang","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16387v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09065v3","updated":"2024-03-25T03:04:44Z","published":"2024-03-14T03:12:02Z","title":"When Semantic Segmentation Meets Frequency Aliasing","summary":"  Despite recent advancements in semantic segmentation, where and what pixels\nare hard to segment remains largely unexplored. Existing research only\nseparates an image into easy and hard regions and empirically observes the\nlatter are associated with object boundaries. In this paper, we conduct a\ncomprehensive analysis of hard pixel errors, categorizing them into three\ntypes: false responses, merging mistakes, and displacements. Our findings\nreveal a quantitative association between hard pixels and aliasing, which is\ndistortion caused by the overlapping of frequency components in the Fourier\ndomain during downsampling. To identify the frequencies responsible for\naliasing, we propose using the equivalent sampling rate to calculate the\nNyquist frequency, which marks the threshold for aliasing. Then, we introduce\nthe aliasing score as a metric to quantify the extent of aliasing. While\npositively correlated with the proposed aliasing score, three types of hard\npixels exhibit different patterns. Here, we propose two novel de-aliasing\nfilter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\ndegradation by accurately removing or adjusting frequencies higher than the\nNyquist frequency. The DAF precisely removes the frequencies responsible for\naliasing before downsampling, while the FreqMix dynamically selects\nhigh-frequency components within the encoder block. Experimental results\ndemonstrate consistent improvements in semantic segmentation and low-light\ninstance segmentation tasks. The code is available at:\nhttps://github.com/Linwei-Chen/Seg-Aliasing.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09065v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16386v1","updated":"2024-03-25T03:02:51Z","published":"2024-03-25T03:02:51Z","title":"Dia-LLaMA: Towards Large Language Model-driven CT Report Generation","summary":"  Medical report generation has achieved remarkable advancements yet has still\nbeen faced with several challenges. First, the inherent imbalance in the\ndistribution of normal and abnormal cases may lead models to exhibit a biased\nfocus on normal samples, resulting in unreliable diagnoses. Second, the\nfrequent occurrence of common template sentences in the reports may overwhelm\nthe critical abnormal information. Moreover, existing works focus on 2D chest\nX-rays, leaving CT report generation underexplored due to the high-dimensional\nnature of CT images and the limited availability of CT-report pairs. Recently,\nLLM has shown a great ability to generate reliable answers with appropriate\nprompts, which shed light on addressing the aforementioned challenges. In this\npaper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report\ngeneration by incorporating diagnostic information as guidance prompts.\nConsidering the high dimension of CT, we leverage a pre-trained ViT3D with\nperceiver to extract the visual information. To tailor the LLM for report\ngeneration and emphasize abnormality, we extract additional diagnostic\ninformation by referring to a disease prototype memory bank, which is updated\nduring training to capture common disease representations. Furthermore, we\nintroduce disease-aware attention to enable the model to adjust attention for\ndifferent diseases. Experiments on the chest CT dataset demonstrated that our\nproposed method outperformed previous methods and achieved state-of-the-art on\nboth clinical efficacy performance and natural language generation metrics. The\ncode will be made publically available.\n","authors":["Zhixuan Chen","Luyang Luo","Yequan Bie","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16386v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.16385v1","updated":"2024-03-25T03:02:27Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Li Zhuowan","Jasani Bhavan","Tang Peng","Ghadar Shabnam"],"pdf_url":"https://arxiv.org/pdf/2403.16385v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16384v1","updated":"2024-03-25T03:01:53Z","published":"2024-03-25T03:01:53Z","title":"Residual Dense Swin Transformer for Continuous Depth-Independent\n  Ultrasound Imaging","summary":"  Ultrasound imaging is crucial for evaluating organ morphology and function,\nyet depth adjustment can degrade image quality and field-of-view, presenting a\ndepth-dependent dilemma. Traditional interpolation-based zoom-in techniques\noften sacrifice detail and introduce artifacts. Motivated by the potential of\narbitrary-scale super-resolution to naturally address these inherent\nchallenges, we present the Residual Dense Swin Transformer Network (RDSTN),\ndesigned to capture the non-local characteristics and long-range dependencies\nintrinsic to ultrasound images. It comprises a linear embedding module for\nfeature enhancement, an encoder with shifted-window attention for modeling\nnon-locality, and an MLP decoder for continuous detail reconstruction. This\nstrategy streamlines balancing image quality and field-of-view, which offers\nsuperior textures over traditional methods. Experimentally, RDSTN outperforms\nexisting approaches while requiring fewer parameters. In conclusion, RDSTN\nshows promising potential for ultrasound image enhancement by overcoming the\nlimitations of conventional interpolation-based methods and achieving\ndepth-independent imaging.\n","authors":["Jintong Hu","Hui Che","Zishuo Li","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16384v1.pdf","comment":"Accepted by ICASSP2024, https://ieeexplore.ieee.org/document/10447712"},{"id":"http://arxiv.org/abs/2403.16379v1","updated":"2024-03-25T02:53:32Z","published":"2024-03-25T02:53:32Z","title":"FlashEval: Towards Fast and Accurate Evaluation of Text-to-image\n  Diffusion Generative Models","summary":"  In recent years, there has been significant progress in the development of\ntext-to-image generative models. Evaluating the quality of the generative\nmodels is one essential step in the development process. Unfortunately, the\nevaluation process could consume a significant amount of computational\nresources, making the required periodic evaluation of model performance (e.g.,\nmonitoring training progress) impractical. Therefore, we seek to improve the\nevaluation efficiency by selecting the representative subset of the text-image\ndataset. We systematically investigate the design choices, including the\nselection criteria (textural features or image-based metrics) and the selection\ngranularity (prompt-level or set-level). We find that the insights from prior\nwork on subset selection for training data do not generalize to this problem,\nand we propose FlashEval, an iterative search algorithm tailored to evaluation\ndata selection. We demonstrate the effectiveness of FlashEval on ranking\ndiffusion models with various configurations, including architectures,\nquantization levels, and sampler schedules on COCO and DiffusionDB datasets.\nOur searched 50-item subset could achieve comparable evaluation quality to the\nrandomly sampled 500-item subset for COCO annotations on unseen models,\nachieving a 10x evaluation speedup. We release the condensed subset of these\ncommonly used datasets to help facilitate diffusion algorithm design and\nevaluation, and open-source FlashEval as a tool for condensing future datasets,\naccessible at https://github.com/thu-nics/FlashEval.\n","authors":["Lin Zhao","Tianchen Zhao","Zinan Lin","Xuefei Ning","Guohao Dai","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16379v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15082v2","updated":"2024-03-25T02:50:07Z","published":"2024-03-22T10:06:31Z","title":"Cell Variational Information Bottleneck Network","summary":"  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n","authors":["Zhonghua Zhai","Chen Ju","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.15082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16376v1","updated":"2024-03-25T02:46:57Z","published":"2024-03-25T02:46:57Z","title":"Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and\n  Distance-Aware Bi-Projection Fusion","summary":"  360 depth estimation has recently received great attention for 3D\nreconstruction owing to its omnidirectional field of view (FoV). Recent\napproaches are predominantly focused on cross-projection fusion with\ngeometry-based re-projection: they fuse 360 images with equirectangular\nprojection (ERP) and another projection type, e.g., cubemap projection to\nestimate depth with the ERP format. However, these methods suffer from 1)\nlimited local receptive fields, making it hardly possible to capture large FoV\nscenes, and 2) prohibitive computational cost, caused by the complex\ncross-projection fusion module design. In this paper, we propose Elite360D, a\nnovel framework that inputs the ERP image and icosahedron projection (ICOSAP)\npoint set, which is undistorted and spatially continuous. Elite360D is superior\nin its capacity in learning a representation from a local-with-global\nperspective. With a flexible ERP image encoder, it includes an ICOSAP point\nencoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M\nparameters). Specifically, the ERP image encoder can take various perspective\nimage-trained backbones (e.g., ResNet, Transformer) to extract local features.\nThe point encoder extracts the global features from the ICOSAP. Then, the B2F\nmodule captures the semantic- and distance-aware dependencies between each\npixel of the ERP feature and the entire ICOSAP feature set. Without specific\nbackbone design and obvious computational cost increase, Elite360D outperforms\nthe prior arts on several benchmark datasets.\n","authors":["Hao Ai","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16376v1.pdf","comment":"8 pages, accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.09506v2","updated":"2024-03-25T02:45:35Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: Towards Motion Coherent Video Representation","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v2.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.16370v1","updated":"2024-03-25T02:30:32Z","published":"2024-03-25T02:30:32Z","title":"GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model\n  for Distortion-aware Panoramic Semantic Segmentation","summary":"  This paper tackles a novel yet challenging problem: how to transfer knowledge\nfrom the emerging Segment Anything Model (SAM) -- which reveals impressive\nzero-shot instance segmentation capacity -- to learn a compact panoramic\nsemantic segmentation model, i.e., student, without requiring any labeled data.\nThis poses considerable challenges due to SAM's inability to provide semantic\nlabels and the large capacity gap between SAM and the student. To this end, we\npropose a novel framework, called GoodSAM, that introduces a teacher assistant\n(TA) to provide semantic information, integrated with SAM to generate ensemble\nlogits to achieve knowledge transfer. Specifically, we propose a\nDistortion-Aware Rectification (DAR) module that first addresses the distortion\nproblem of panoramic images by imposing prediction-level consistency and\nboundary enhancement. This subtly enhances TA's prediction capacity on\npanoramic images. DAR then incorporates a cross-task complementary fusion block\nto adaptively merge the predictions of SAM and TA to obtain more reliable\nensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation\n(MKA) module to efficiently transfer the multi-level feature knowledge from TA\nand ensemble logits to learn a compact student model. Extensive experiments on\ntwo benchmarks show that our GoodSAM achieves a remarkable +3.75\\% mIoU\nimprovement over the state-of-the-art (SOTA) domain adaptation methods. Also,\nour most lightweight model achieves comparable performance to the SOTA methods\nwith only 3.7M parameters.\n","authors":["Weiming Zhang","Yexin Liu","Xu Zheng","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16370v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16368v1","updated":"2024-03-25T02:17:20Z","published":"2024-03-25T02:17:20Z","title":"Distilling Semantic Priors from SAM to Efficient Image Restoration\n  Models","summary":"  In image restoration (IR), leveraging semantic priors from segmentation\nmodels has been a common approach to improve performance. The recent segment\nanything model (SAM) has emerged as a powerful tool for extracting advanced\nsemantic priors to enhance IR tasks. However, the computational cost of SAM is\nprohibitive for IR, compared to existing smaller IR models. The incorporation\nof SAM for extracting semantic priors considerably hampers the model inference\nefficiency. To address this issue, we propose a general framework to distill\nSAM's semantic knowledge to boost exiting IR models without interfering with\ntheir inference process. Specifically, our proposed framework consists of the\nsemantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)\nscheme. SPF fuses two kinds of information between the restored image predicted\nby the original IR model and the semantic mask predicted by SAM for the refined\nrestored image. SPD leverages a self-distillation manner to distill the fused\nsemantic priors to boost the performance of original IR models. Additionally,\nwe design a semantic-guided relation (SGR) module for SPD, which ensures\nsemantic feature representation space consistency to fully distill the priors.\nWe demonstrate the effectiveness of our framework across multiple IR models and\ntasks, including deraining, deblurring, and denoising.\n","authors":["Quan Zhang","Xiaoyu Liu","Wei Li","Hanting Chen","Junchao Liu","Jie Hu","Zhiwei Xiong","Chun Yuan","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.16365v1","updated":"2024-03-25T02:03:38Z","published":"2024-03-25T02:03:38Z","title":"Generating Potent Poisons and Backdoors from Scratch with Guided\n  Diffusion","summary":"  Modern neural networks are often trained on massive datasets that are web\nscraped with minimal human inspection. As a result of this insecure curation\npipeline, an adversary can poison or backdoor the resulting model by uploading\nmalicious data to the internet and waiting for a victim to scrape and train on\nit. Existing approaches for creating poisons and backdoors start with randomly\nsampled clean data, called base samples, and then modify those samples to craft\npoisons. However, some base samples may be significantly more amenable to\npoisoning than others. As a result, we may be able to craft more potent poisons\nby carefully choosing the base samples. In this work, we use guided diffusion\nto synthesize base samples from scratch that lead to significantly more potent\npoisons and backdoors than previous state-of-the-art attacks. Our Guided\nDiffusion Poisoning (GDP) base samples can be combined with any downstream\npoisoning or backdoor attack to boost its effectiveness. Our implementation\ncode is publicly available at: https://github.com/hsouri/GDP .\n","authors":["Hossein Souri","Arpit Bansal","Hamid Kazemi","Liam Fowl","Aniruddha Saha","Jonas Geiping","Andrew Gordon Wilson","Rama Chellappa","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2403.16365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17516v3","updated":"2024-03-25T01:55:03Z","published":"2023-11-29T10:39:53Z","title":"MMA-Diffusion: MultiModal Attack on Diffusion Models","summary":"  In recent years, Text-to-Image (T2I) models have seen remarkable\nadvancements, gaining widespread adoption. However, this progress has\ninadvertently opened avenues for potential misuse, particularly in generating\ninappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces\nMMA-Diffusion, a framework that presents a significant and realistic threat to\nthe security of T2I models by effectively circumventing current defensive\nmeasures in both open-source models and commercial online services. Unlike\nprevious approaches, MMA-Diffusion leverages both textual and visual modalities\nto bypass safeguards like prompt filters and post-hoc safety checkers, thus\nexposing and highlighting the vulnerabilities in existing defense mechanisms.\n","authors":["Yijun Yang","Ruiyuan Gao","Xiaosen Wang","Tsung-Yi Ho","Nan Xu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2311.17516v3.pdf","comment":"CVPR 2024. Code is available at\n  https://github.com/yangyijune/MMA-Diffusion"},{"id":"http://arxiv.org/abs/2403.16361v1","updated":"2024-03-25T01:54:57Z","published":"2024-03-25T01:54:57Z","title":"RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable\n  and Circular Convolutions","summary":"  Four-dimensional cone-beam computed tomography (4D CBCT) provides\nrespiration-resolved images and can be used for image-guided radiation therapy.\nHowever, the ability to reveal respiratory motion comes at the cost of image\nartifacts. As raw projection data are sorted into multiple respiratory phases,\nthere is a limited number of cone-beam projections available for image\nreconstruction. Consequently, the 4D CBCT images are covered by severe streak\nartifacts. Although several deep learning-based methods have been proposed to\naddress this issue, most algorithms employ ordinary network models, neglecting\nthe intrinsic structural prior within 4D CBCT images. In this paper, we first\nexplore the origin and appearance of streak artifacts in 4D CBCT\nimages.Specifically, we find that streak artifacts exhibit a periodic\nrotational motion along with the patient's respiration. This unique motion\npattern inspires us to distinguish the artifacts from the desired anatomical\nstructures in the spatiotemporal domain. Thereafter, we propose a\nspatiotemporal neural network named RSTAR-Net with separable and circular\nconvolutions for Rotational Streak Artifact Reduction. The specially designed\nmodel effectively encodes dynamic image features, facilitating the recovery of\n4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally\nefficient. Extensive experiments substantiate the effectiveness of our proposed\nmethod, and RSTAR-Net shows superior performance to comparison methods.\n","authors":["Ziheng Deng","Hua Chen","Haibo Hu","Zhiyong Xu","Tianling Lyu","Yan Xi","Yang Chen","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09911v2","updated":"2024-03-25T01:54:41Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16358v1","updated":"2024-03-25T01:44:34Z","published":"2024-03-25T01:44:34Z","title":"ChebMixer: Efficient Graph Representation Learning with MLP Mixer","summary":"  Graph neural networks have achieved remarkable success in learning graph\nrepresentations, especially graph Transformer, which has recently shown\nsuperior performance on various graph mining tasks. However, graph Transformer\ngenerally treats nodes as tokens, which results in quadratic complexity\nregarding the number of nodes during self-attention computation. The graph MLP\nMixer addresses this challenge by using the efficient MLP Mixer technique from\ncomputer vision. However, the time-consuming process of extracting graph tokens\nlimits its performance. In this paper, we present a novel architecture named\nChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based\nspectral filtering to extract a sequence of tokens. Firstly, we produce\nmultiscale representations of graph nodes via fast Chebyshev polynomial-based\nspectral filtering. Next, we consider each node's multiscale representations as\na sequence of tokens and refine the node representation with an effective MLP\nMixer. Finally, we aggregate the multiscale representations of nodes through\nChebyshev interpolation. Owing to the powerful representation capabilities and\nfast computational properties of MLP Mixer, we can quickly extract more\ninformative node representations to improve the performance of downstream\ntasks. The experimental results prove our significant improvements in a variety\nof scenarios ranging from graph node classification to medical image\nsegmentation.\n","authors":["Xiaoyan Kui","Haonan Yan","Qinsong Li","Liming Chen","Beiji Zou"],"pdf_url":"https://arxiv.org/pdf/2403.16358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11614v2","updated":"2024-03-25T01:23:07Z","published":"2024-03-18T09:44:44Z","title":"CRS-Diff: Controllable Generative Remote Sensing Foundation Model","summary":"  The emergence of diffusion models has revolutionized the field of image\ngeneration, providing new methods for creating high-quality, high-resolution\nimages across various applications. However, the potential of these models for\ngenerating domain-specific images, particularly remote sensing (RS) images,\nremains largely untapped. RS images that are notable for their high resolution,\nextensive coverage, and rich information content, bring new challenges that\ngeneral diffusion models may not adequately address. This paper proposes\nCRS-Diff, a pioneering diffusion modeling framework specifically tailored for\ngenerating remote sensing imagery, leveraging the inherent advantages of\ndiffusion models while integrating advanced control mechanisms to ensure that\nthe imagery is not only visually clear but also enriched with geographic and\ntemporal information. The model integrates global and local control inputs,\nenabling precise combinations of generation conditions to refine the generation\nprocess. A comprehensive evaluation of CRS-Diff has demonstrated its superior\ncapability to generate RS imagery both in a single condition and multiple\nconditions compared with previous methods in terms of image quality and\ndiversity.\n","authors":["Datao Tang","Xiangyong Cao","Xingsong Hou","Zhongyuan Jiang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.11614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17034v2","updated":"2024-03-25T01:21:18Z","published":"2023-11-28T18:45:13Z","title":"Telling Left from Right: Identifying Geometry-Aware Semantic\n  Correspondence","summary":"  While pre-trained large-scale vision models have shown significant promise\nfor semantic correspondence, their features often struggle to grasp the\ngeometry and orientation of instances. This paper identifies the importance of\nbeing geometry-aware for semantic correspondence and reveals a limitation of\nthe features of current foundation models under simple post-processing. We show\nthat incorporating this information can markedly enhance semantic\ncorrespondence performance with simple but effective solutions in both\nzero-shot and supervised settings. We also construct a new challenging\nbenchmark for semantic correspondence built from an existing animal pose\nestimation dataset, for both pre-training validating models. Our method\nachieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the\nchallenging SPair-71k dataset, outperforming the state of the art by 5.5p and\n11.0p absolute gains, respectively. Our code and datasets are publicly\navailable at: https://telling-left-from-right.github.io/.\n","authors":["Junyi Zhang","Charles Herrmann","Junhwa Hur","Eric Chen","Varun Jampani","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2311.17034v2.pdf","comment":"Accepted by CVPR 24, project page:\n  https://telling-left-from-right.github.io/"},{"id":"http://arxiv.org/abs/2403.14743v2","updated":"2024-03-25T01:18:37Z","published":"2024-03-21T18:00:00Z","title":"VURF: A General-purpose Reasoning and Self-refinement Framework for\n  Video Understanding","summary":"  Recent studies have demonstrated the effectiveness of Large Language Models\n(LLMs) as reasoning modules that can deconstruct complex tasks into more\nmanageable sub-tasks, particularly when applied to visual reasoning tasks for\nimages. In contrast, this paper introduces a Video Understanding and Reasoning\nFramework (VURF) based on the reasoning power of LLMs. Ours is a novel approach\nto extend the utility of LLMs in the context of video tasks, leveraging their\ncapacity to generalize from minimal input and output demonstrations within a\ncontextual framework. By presenting LLMs with pairs of instructions and their\ncorresponding high-level programs, we harness their contextual learning\ncapabilities to generate executable visual programs for video understanding. To\nenhance program's accuracy and robustness, we implement two important\nstrategies. Firstly, we employ a feedback-generation approach, powered by\nGPT-3.5, to rectify errors in programs utilizing unsupported functions.\nSecondly, taking motivation from recent works on self refinement of LLM\noutputs, we introduce an iterative procedure for improving the quality of the\nin-context examples by aligning the initial outputs to the outputs that would\nhave been generated had the LLM not been bound by the structure of the\nin-context examples. Our results on several video-specific tasks, including\nvisual QA, video anticipation, pose estimation and multi-video QA illustrate\nthe efficacy of these enhancements in improving the performance of visual\nprogramming approaches for video tasks.\n","authors":["Ahmad Mahmood","Ashmal Vayani","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10119v2","updated":"2024-03-25T01:08:14Z","published":"2024-03-15T09:08:27Z","title":"URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural\n  Radiance Fields","summary":"  We propose a novel rolling shutter bundle adjustment method for neural\nradiance fields (NeRF), which utilizes the unordered rolling shutter (RS)\nimages to obtain the implicit 3D representation. Existing NeRF methods suffer\nfrom low-quality images and inaccurate initial camera poses due to the RS\neffect in the image, whereas, the previous method that incorporates the RS into\nNeRF requires strict sequential data input, limiting its widespread\napplicability. In constant, our method recovers the physical formation of RS\nimages by estimating camera poses and velocities, thereby removing the input\nconstraints on sequential data. Moreover, we adopt a coarse-to-fine training\nstrategy, in which the RS epipolar constraints of the pairwise frames in the\nscene graph are used to detect the camera poses that fall into local minima.\nThe poses detected as outliers are corrected by the interpolation method with\nneighboring poses. The experimental results validate the effectiveness of our\nmethod over state-of-the-art works and demonstrate that the reconstruction of\n3D representations is not constrained by the requirement of video sequence\ninput.\n","authors":["Bo Xu","Ziao Liu","Mengqi Guo","Jiancheng Li","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16350v1","updated":"2024-03-25T00:59:35Z","published":"2024-03-25T00:59:35Z","title":"3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical\n  Image Segmentation","summary":"  Medical image segmentation (MIS) aims to finely segment various organs. It\nrequires grasping global information from both parts and the entire image for\nbetter segmenting, and clinically there are often certain requirements for\nsegmentation efficiency. Convolutional neural networks (CNNs) have made\nconsiderable achievements in MIS. However, they are difficult to fully collect\nglobal context information and their pooling layer may cause information loss.\nCapsule networks, which combine the benefits of CNNs while taking into account\nadditional information such as relative location that CNNs do not, have lately\ndemonstrated some advantages in MIS. Vision Transformer (ViT) employs\ntransformers in visual tasks. Transformer based on attention mechanism has\nexcellent global inductive modeling capabilities and is expected to capture\nlongrange information. Moreover, there have been resent studies on making ViT\nmore lightweight to minimize model complexity and increase efficiency. In this\npaper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps,\nwhich combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our\nencoder uses capsule blocks and EfficientViT blocks to jointly capture local\nand global semantic information more effectively and efficiently with less\ninformation loss, while the decoder employs CNN blocks and EfficientViT blocks\nto catch ffner details for segmentation. We conduct experiments on various\ndatasets, including iSeg-2017, Hippocampus and Cardiac to verify the\nperformance and efficiency of 3D-EffiViTCaps, which performs better than\nprevious 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We\nfurther implement a series of ablation experiments on the main blocks. Our code\nis available at: https://github.com/HidNeuron/3D-EffiViTCaps.\n","authors":["Dongwei Gan","Ming Chang","Juan Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16350v1.pdf","comment":"15 pages, 4 figures, submitted to ICPR2024"},{"id":"http://arxiv.org/abs/2301.06626v2","updated":"2024-03-25T00:45:30Z","published":"2023-01-16T22:30:53Z","title":"Masked Vector Quantization","summary":"  Generative models with discrete latent representations have recently\ndemonstrated an impressive ability to learn complex high-dimensional data\ndistributions. However, their performance relies on a long sequence of tokens\nper instance and a large number of codebook entries, resulting in long sampling\ntimes and considerable computation to fit the categorical posterior. To address\nthese issues, we propose the Masked Vector Quantization (MVQ) framework which\nincreases the representational capacity of each code vector by learning mask\nconfigurations via a stochastic winner-takes-all training regime called\nMultiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces\nFID in existing vector quantization architectures by up to $68\\%$ at 2 tokens\nper instance and $57\\%$ at 5 tokens. These improvements widen as codebook\nentries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token\nsampling during inference. As an additional benefit, we find that smaller\nlatent spaces lead to MVQ identifying transferable visual representations where\nmultiple can be smoothly combined.\n","authors":["David D. Nguyen","David Leibowitz","Surya Nepal","Salil S. Kanhere"],"pdf_url":"https://arxiv.org/pdf/2301.06626v2.pdf","comment":"A newer version of this manuscript was archived under 2312.11735"},{"id":"http://arxiv.org/abs/2403.16338v1","updated":"2024-03-25T00:24:10Z","published":"2024-03-25T00:24:10Z","title":"Impact of Video Compression Artifacts on Fisheye Camera Visual\n  Perception Tasks","summary":"  Autonomous driving systems require extensive data collection schemes to cover\nthe diverse scenarios needed for building a robust and safe system. The data\nvolumes are in the order of Exabytes and have to be stored for a long period of\ntime (i.e., more than 10 years of the vehicle's life cycle). Lossless\ncompression doesn't provide sufficient compression ratios, hence, lossy video\ncompression has been explored. It is essential to prove that lossy video\ncompression artifacts do not impact the performance of the perception\nalgorithms. However, there is limited work in this area to provide a solid\nconclusion. In particular, there is no such work for fisheye cameras, which\nhave high radial distortion and where compression may have higher artifacts.\nFisheye cameras are commonly used in automotive systems for 3D object detection\ntask. In this work, we provide the first analysis of the impact of standard\nvideo compression codecs on wide FOV fisheye camera images. We demonstrate that\nthe achievable compression with negligible impact depends on the dataset and\ntemporal prediction of the video codec. We propose a radial distortion-aware\nzonal metric to evaluate the performance of artifacts in fisheye images. In\naddition, we present a novel method for estimating affine mode parameters of\nthe latest VVC codec, and suggest some areas for improvement in video codecs\nfor the application to fisheye imagery.\n","authors":["Madhumitha Sakthi","Louis Kerofsky","Varun Ravi Kumar","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2403.16338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16335v1","updated":"2024-03-25T00:17:43Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v1.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"},{"id":"http://arxiv.org/abs/2312.02365v2","updated":"2024-03-25T23:52:15Z","published":"2023-12-04T21:46:39Z","title":"MEDPSeg: Hierarchical polymorphic multitask learning for the\n  segmentation of ground-glass opacities, consolidation, and pulmonary\n  structures on computed tomography","summary":"  The COVID-19 pandemic response highlighted the potential of deep learning\nmethods in facilitating the diagnosis, prognosis and understanding of lung\ndiseases through automated segmentation of pulmonary structures and lesions in\nchest computed tomography (CT). Automated separation of lung lesion into\nground-glass opacity (GGO) and consolidation is hindered due to the\nlabor-intensive and subjective nature of this task, resulting in scarce\navailability of ground truth for supervised learning. To tackle this problem,\nwe propose MEDPSeg. MEDPSeg learns from heterogeneous chest CT targets through\nhierarchical polymorphic multitask learning (HPML). HPML explores the\nhierarchical nature of GGO and consolidation, lung lesions, and the lungs, with\nfurther benefits achieved through multitasking airway and pulmonary artery\nsegmentation. Over 6000 volumetric CT scans from different partially labeled\nsources were used for training and testing. Experiments show PML enabling new\nstate-of-the-art performance for GGO and consolidation segmentation tasks. In\naddition, MEDPSeg simultaneously performs segmentation of the lung parenchyma,\nairways, pulmonary artery, and lung lesions, all in a single forward\nprediction, with performance comparable to state-of-the-art methods specialized\nin each of those targets. Finally, we provide an open-source implementation\nwith a graphical user interface at https://github.com/MICLab-Unicamp/medpseg.\n","authors":["Diedre S. Carmo","Jean A. Ribeiro","Alejandro P. Comellas","Joseph M. Reinhardt","Sarah E. Gerard","Letícia Rittner","Roberto A. Lotufo"],"pdf_url":"https://arxiv.org/pdf/2312.02365v2.pdf","comment":"This manuscript is under review and might change in the future"},{"id":"http://arxiv.org/abs/2312.00412v2","updated":"2024-03-25T23:40:29Z","published":"2023-12-01T08:22:34Z","title":"SCHEME: Scalable Channer Mixer for Vision Transformers","summary":"  Vision Transformers have received significant attention due to their\nimpressive performance in many vision tasks. While the token mixer or attention\nblock has been studied in great detail, the channel mixer or feature mixing\nblock (FFN or MLP) has not been explored in depth albeit it accounts for a bulk\nof the parameters and computation in a model. In this work, we study whether\nsparse feature mixing can replace the dense connections and confirm this with a\nblock diagonal MLP structure that improves the accuracy by supporting larger\nexpansion ratios. To improve the feature clusters formed by this structure and\nthereby further improve the accuracy, a lightweight, parameter-free, channel\ncovariance attention (CCA) mechanism is introduced as a parallel branch during\ntraining. This design of CCA enables gradual feature mixing across channel\ngroups during training whose contribution decays to zero as the training\nprogresses to convergence. This allows the CCA block to be discarded during\ninference, thus enabling enhanced performance with no additional computational\ncost. The resulting $\\textit{Scalable CHannEl MixEr}$ (SCHEME) can be plugged\ninto any ViT architecture to obtain a gamut of models with different trade-offs\nbetween complexity and performance by controlling the block diagonal structure\nsize in the MLP. This is shown by the introduction of a new family of\nSCHEMEformer models that is shown to establish new Pareto frontiers for\naccuracy vs FLOPS, accuracy vs model size, and accuracy vs throughput,\nespecially for fast transformers of small model size. For example, the\nSCHEMEformer establishes a new SOTA of 79.7% accuracy for ViTs using pure\nattention mixers on ImageNet-1K at 1.77G FLOPs.\n","authors":["Deepak Sridhar","Yunsheng Li","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2312.00412v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.10971v2","updated":"2024-03-25T23:14:28Z","published":"2023-10-17T03:35:27Z","title":"Context-Aware Meta-Learning","summary":"  Large Language Models like ChatGPT demonstrate a remarkable capacity to learn\nnew concepts during inference without any fine-tuning. However, visual models\ntrained to detect new objects during inference have been unable to replicate\nthis ability, and instead either perform poorly or require meta-training and/or\nfine-tuning on similar objects. In this work, we propose a meta-learning\nalgorithm that emulates Large Language Models by learning new visual concepts\nduring inference without fine-tuning. Our approach leverages a frozen\npre-trained feature extractor, and analogous to in-context learning, recasts\nvisual meta-learning as sequence modeling over datapoints with known labels and\na test datapoint with an unknown label. On 8 out of 11 meta-learning\nbenchmarks, our approach -- without meta-training or fine-tuning -- exceeds or\nmatches the state-of-the-art algorithm, P>M>F, which is meta-trained on these\nbenchmarks. Our code is available at https://github.com/cfifty/CAML.\n","authors":["Christopher Fifty","Dennis Duan","Ronald G. Junkins","Ehsan Amid","Jure Leskovec","Christopher Re","Sebastian Thrun"],"pdf_url":"https://arxiv.org/pdf/2310.10971v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17255v1","updated":"2024-03-25T23:03:51Z","published":"2024-03-25T23:03:51Z","title":"Decoding the visual attention of pathologists to reveal their level of\n  expertise","summary":"  We present a method for classifying the expertise of a pathologist based on\nhow they allocated their attention during a cancer reading. We engage this\ndecoding task by developing a novel method for predicting the attention of\npathologists as they read whole-slide Images (WSIs) of prostate and make cancer\ngrade classifications. Our ground truth measure of a pathologists' attention is\nthe x, y and z (magnification) movement of their viewport as they navigated\nthrough WSIs during readings, and to date we have the attention behavior of 43\npathologists reading 123 WSIs. These data revealed that specialists have higher\nagreement in both their attention and cancer grades compared to general\npathologists and residents, suggesting that sufficient information may exist in\ntheir attention behavior to classify their expertise level. To attempt this, we\ntrained a transformer-based model to predict the visual attention heatmaps of\nresident, general, and specialist (GU) pathologists during Gleason grading.\nBased solely on a pathologist's attention during a reading, our model was able\nto predict their level of expertise with 75.3%, 56.1%, and 77.2% accuracy,\nrespectively, better than chance and baseline models. Our model therefore\nenables a pathologist's expertise level to be easily and objectively evaluated,\nimportant for pathology training and competency assessment. Tools developed\nfrom our model could also be used to help pathology trainees learn how to read\nWSIs like an expert.\n","authors":["Souradeep Chakraborty","Dana Perez","Paul Friedman","Natallia Sheuka","Constantin Friedman","Oksana Yaskiv","Rajarsi Gupta","Gregory J. Zelinsky","Joel H. Saltz","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.17255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17237v1","updated":"2024-03-25T22:34:05Z","published":"2024-03-25T22:34:05Z","title":"DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric\n  Diffusion","summary":"  We present DreamPolisher, a novel Gaussian Splatting based method with\ngeometric guidance, tailored to learn cross-view consistency and intricate\ndetail from textual descriptions. While recent progress on text-to-3D\ngeneration methods have been promising, prevailing methods often fail to ensure\nview-consistency and textural richness. This problem becomes particularly\nnoticeable for methods that work with text input alone. To address this, we\npropose a two-stage Gaussian Splatting based approach that enforces geometric\nconsistency among views. Initially, a coarse 3D generation undergoes refinement\nvia geometric optimization. Subsequently, we use a ControlNet driven refiner\ncoupled with the geometric consistency term to improve both texture fidelity\nand overall consistency of the generated 3D asset. Empirical evaluations across\ndiverse textual prompts spanning various object categories demonstrate the\nefficacy of DreamPolisher in generating consistent and realistic 3D objects,\naligning closely with the semantics of the textual instructions.\n","authors":["Yuanze Lin","Ronald Clark","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2403.17237v1.pdf","comment":"Project webpage: https://yuanze-lin.me/DreamPolisher_page/"},{"id":"http://arxiv.org/abs/2312.12735v2","updated":"2024-03-25T22:25:35Z","published":"2023-12-20T03:16:34Z","title":"MetaSegNet: Metadata-collaborative Vision-Language Representation\n  Learning for Semantic Segmentation of Remote Sensing Images","summary":"  Semantic segmentation of remote sensing images plays a vital role in a wide\nrange of Earth Observation (EO) applications, such as land use land cover\nmapping, environment monitoring, and sustainable development. Driven by rapid\ndevelopments in Artificial Intelligence (AI), deep learning (DL) has emerged as\nthe mainstream tool for semantic segmentation and has achieved many\nbreakthroughs in the field of remote sensing. However, the existing DL-based\nmethods mainly focus on unimodal visual data while ignoring the rich multimodal\ninformation involved in the real world, usually demonstrating weak reliability\nand generlization. Inspired by the success of Vision Transformers and large\nlanguage models, we propose a novel metadata-collaborative multimodal\nsegmentation network (MetaSegNet) that applies vision-language representation\nlearning for semantic segmentation of remote sensing images. Unlike the common\nmodel structure that only uses unimodal visual data, we extract the key\ncharacteristic (e.g. the climate zone) from freely available remote sensing\nimage metadata and transfer it into knowledge-based text prompts via the\ngeneric ChatGPT. Then, we construct an image encoder, a text encoder and a\ncross-modal attention fusion subnetwork to extract the image and text feature\nand apply image-text interaction. Benefiting from such a design, the proposed\nMetaSegNet demonstrates superior generalization and achieves competitive\naccuracy with the state-of-the-art semantic segmentation methods on the\nlarge-scale OpenEarthMap dataset (68.6% mIoU) and Potsdam dataset (93.3% mean\nF1 score) as well as LoveDA dataset (52.2% mIoU).\n","authors":["Libo Wang","Sijun Dong","Ying Chen","Xiaoliang Meng","Shenghui Fang","Ayman Habib","Songlin Fei"],"pdf_url":"https://arxiv.org/pdf/2312.12735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15101v3","updated":"2024-03-25T22:13:44Z","published":"2023-12-22T22:46:48Z","title":"Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model\n  Conversions between Frameworks","summary":"  Converting deep learning models between frameworks is a common step to\nmaximize model compatibility across devices and leverage optimization features\nthat may be exclusively provided in one deep learning framework. However, this\nconversion process may be riddled with bugs, making the converted models either\nundeployable or problematic, considerably degrading their prediction\ncorrectness.\n  In this paper we propose an automated approach for fault localization and\nrepair, Fix-Con, during model conversion between deep learning frameworks.\nFix-Con is capable of detecting and fixing faults introduced in model input,\nparameters, hyperparameters, and the model graph during conversion.\n  Fix-Con uses a set of fault types (mined from surveying conversion issues\nreported \\nick{in code repositories and forums}) to localize potential\nconversion faults in the converted target model and then repair them\nappropriately, e.g., replacing the parameters of the target model with those\nfrom the source model. This is done iteratively for every image in the dataset,\ncomparing output label differences between the source model and the converted\ntarget model until all differences are resolved. We evaluate the effectiveness\nof Fix-Con in fixing model conversion bugs of three widely used image\nrecognition models converted across four different deep learning frameworks.\nOverall, Fix-Con was able to fix $462$ out of $755$ detected conversion faults,\neither completely repairing or significantly improving the performance of $14$\nout of the $15$ erroneous conversion cases.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2312.15101v3.pdf","comment":"12 pages, 4 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/1905.10711v5","updated":"2024-03-25T22:10:45Z","published":"2019-05-26T01:58:28Z","title":"DISN: Deep Implicit Surface Network for High-quality Single-view 3D\n  Reconstruction","summary":"  Reconstructing 3D shapes from single-view images has been a long-standing\nresearch problem. In this paper, we present DISN, a Deep Implicit Surface\nNetwork which can generate a high-quality detail-rich 3D mesh from an 2D image\nby predicting the underlying signed distance fields. In addition to utilizing\nglobal image features, DISN predicts the projected location for each 3D point\non the 2D image, and extracts local features from the image feature maps.\nCombining global and local features significantly improves the accuracy of the\nsigned distance field prediction, especially for the detail-rich areas. To the\nbest of our knowledge, DISN is the first method that constantly captures\ndetails such as holes and thin structures present in 3D shapes from single-view\nimages. DISN achieves the state-of-the-art single-view reconstruction\nperformance on a variety of shape categories reconstructed from both synthetic\nand real images. Code is available at https://github.com/xharlie/DISN The\nsupplementary can be found at\nhttps://xharlie.github.io/images/neurips_2019_supp.pdf\n","authors":["Qiangeng Xu","Weiyue Wang","Duygu Ceylan","Radomir Mech","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/1905.10711v5.pdf","comment":"This project was in part supported by the gift funding to the\n  University of Southern California from Adobe Research"},{"id":"http://arxiv.org/abs/2403.17223v1","updated":"2024-03-25T21:53:36Z","published":"2024-03-25T21:53:36Z","title":"Co-Occurring of Object Detection and Identification towards unlabeled\n  object discovery","summary":"  In this paper, we propose a novel deep learning based approach for\nidentifying co-occurring objects in conjunction with base objects in multilabel\nobject categories. Nowadays, with the advancement in computer vision based\ntechniques we need to know about co-occurring objects with respect to base\nobject for various purposes. The pipeline of the proposed work is composed of\ntwo stages: in the first stage of the proposed model we detect all the bounding\nboxes present in the image and their corresponding labels, then in the second\nstage we perform co-occurrence matrix analysis. In co-occurrence matrix\nanalysis, we set base classes based on the maximum occurrences of the labels\nand build association rules and generate frequent patterns. These frequent\npatterns will show base classes and their corresponding co-occurring classes.\nWe performed our experiments on two publicly available datasets: Pascal VOC and\nMS-COCO. The experimental results on public benchmark dataset is reported in\nSec 4. Further we extend this work by considering all frequently objects as\nunlabeled and what if they are occluded as well.\n","authors":["Binay Kumar Singh","Niels Da Vitoria Lobo"],"pdf_url":"https://arxiv.org/pdf/2403.17223v1.pdf","comment":"6 pages, 2 figures,"},{"id":"http://arxiv.org/abs/2403.17217v1","updated":"2024-03-25T21:46:53Z","published":"2024-03-25T21:46:53Z","title":"DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face\n  Reenactment","summary":"  Video-driven neural face reenactment aims to synthesize realistic facial\nimages that successfully preserve the identity and appearance of a source face,\nwhile transferring the target head pose and facial expressions. Existing\nGAN-based methods suffer from either distortions and visual artifacts or poor\nreconstruction quality, i.e., the background and several important appearance\ndetails, such as hair style/color, glasses and accessories, are not faithfully\nreconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable\nthe generation of high-quality realistic images. To this end, in this paper we\npresent DiffusionAct, a novel method that leverages the photo-realistic image\ngeneration of diffusion models to perform neural face reenactment.\nSpecifically, we propose to control the semantic space of a Diffusion\nAutoencoder (DiffAE), in order to edit the facial pose of the input images,\ndefined as the head pose orientation and the facial expressions. Our method\nallows one-shot, self, and cross-subject reenactment, without requiring\nsubject-specific fine-tuning. We compare against state-of-the-art GAN-,\nStyleGAN2-, and diffusion-based methods, showing better or on-par reenactment\nperformance.\n","authors":["Stella Bounareli","Christos Tzelepis","Vasileios Argyriou","Ioannis Patras","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.17217v1.pdf","comment":"Project page: https://stelabou.github.io/diffusionact/"},{"id":"http://arxiv.org/abs/2403.17213v1","updated":"2024-03-25T21:40:44Z","published":"2024-03-25T21:40:44Z","title":"AnimateMe: 4D Facial Expressions via Diffusion Models","summary":"  The field of photorealistic 3D avatar reconstruction and generation has\ngarnered significant attention in recent years; however, animating such avatars\nremains challenging. Recent advances in diffusion models have notably enhanced\nthe capabilities of generative models in 2D animation. In this work, we\ndirectly utilize these models within the 3D domain to achieve controllable and\nhigh-fidelity 4D facial animation. By integrating the strengths of diffusion\nprocesses and geometric deep learning, we employ Graph Neural Networks (GNNs)\nas denoising diffusion models in a novel approach, formulating the diffusion\nprocess directly on the mesh space and enabling the generation of 3D facial\nexpressions. This facilitates the generation of facial deformations through a\nmesh-diffusion-based model. Additionally, to ensure temporal coherence in our\nanimations, we propose a consistent noise sampling method. Under a series of\nboth quantitative and qualitative experiments, we showcase that the proposed\nmethod outperforms prior work in 4D expression synthesis by generating\nhigh-fidelity extreme expressions. Furthermore, we applied our method to\ntextured 4D facial expression generation, implementing a straightforward\nextension that involves training on a large-scale textured 4D facial expression\ndatabase.\n","authors":["Dimitrios Gerogiannis","Foivos Paraperas Papantoniou","Rolandos Alexandros Potamias","Alexandros Lattas","Stylianos Moschoglou","Stylianos Ploumpis","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2403.17213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06157v5","updated":"2024-03-25T21:23:11Z","published":"2023-06-10T23:50:02Z","title":"Fault Localization for Buggy Deep Learning Framework Conversions in\n  Image Recognition","summary":"  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 100%.\nTo mitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06157v5.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.17192v1","updated":"2024-03-25T21:08:26Z","published":"2024-03-25T21:08:26Z","title":"Strategies to Improve Real-World Applicability of Laparoscopic Anatomy\n  Segmentation Models","summary":"  Accurate identification and localization of anatomical structures of varying\nsize and appearance in laparoscopic imaging are necessary to leverage the\npotential of computer vision techniques for surgical decision support.\nSegmentation performance of such models is traditionally reported using metrics\nof overlap such as IoU. However, imbalanced and unrealistic representation of\nclasses in the training data and suboptimal selection of reported metrics have\nthe potential to skew nominal segmentation performance and thereby ultimately\nlimit clinical translation. In this work, we systematically analyze the impact\nof class characteristics (i.e., organ size differences), training and test data\ncomposition (i.e., representation of positive and negative examples), and\nmodeling parameters (i.e., foreground-to-background class weight) on eight\nsegmentation metrics: accuracy, precision, recall, IoU, F1 score, specificity,\nHausdorff Distance, and Average Symmetric Surface Distance. Based on our\nfindings, we propose two simple yet effective strategies to improve real-world\napplicability of image segmentation models in laparoscopic surgical data: (1)\ninclusion of negative examples in the training process and (2) adaptation of\nforeground-background weights in segmentation models to maximize model\nperformance with respect to specific metrics of interest, depending on the\nclinical use case.\n","authors":["Fiona R. Kolbinger","Jiangpeng He","Jinge Ma","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17192v1.pdf","comment":"13 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2306.06208v5","updated":"2024-03-25T21:08:25Z","published":"2023-06-05T23:07:01Z","title":"DeltaNN: Assessing the Impact of Computational Environment Parameters on\n  the Performance of Image Recognition Models","summary":"  Image recognition tasks typically use deep learning and require enormous\nprocessing power, thus relying on hardware accelerators like GPUs and TPUs for\nfast, timely processing. Failure in real-time image recognition tasks can occur\ndue to sub-optimal mapping on hardware accelerators during model deployment,\nwhich may lead to timing uncertainty and erroneous behavior. Mapping on\nhardware accelerators is done using multiple software components like deep\nlearning frameworks, compilers, and device libraries, that we refer to as the\ncomputational environment. Owing to the increased use of image recognition\ntasks in safety-critical applications like autonomous driving and medical\nimaging, it is imperative to assess their robustness to changes in the\ncomputational environment, as the impact of parameters like deep learning\nframeworks, compiler optimizations, and hardware devices on model performance\nand correctness is not yet well understood.\n  In this paper we present a differential testing framework, DeltaNN, that\nallows us to assess the impact of different computational environment\nparameters on the performance of image recognition models during deployment,\npost training. DeltaNN generates different implementations of a given image\nrecognition model for variations in environment parameters, namely, deep\nlearning frameworks, compiler optimizations and hardware devices and analyzes\ndifferences in model performance as a result. Using DeltaNN, we conduct an\nempirical study of robustness analysis of three popular image recognition\nmodels using the ImageNet dataset. We report the impact in terms of\nmisclassifications and inference time differences across different settings. In\ntotal, we observed up to 100% output label differences across deep learning\nframeworks, and up to 81% unexpected performance degradation in terms of\ninference time, when applying compiler optimizations.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06208v5.pdf","comment":"11 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.17188v1","updated":"2024-03-25T21:01:29Z","published":"2024-03-25T21:01:29Z","title":"LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning","summary":"  Backdoor attack poses a significant security threat to Deep Learning\napplications. Existing attacks are often not evasive to established backdoor\ndetection techniques. This susceptibility primarily stems from the fact that\nthese attacks typically leverage a universal trigger pattern or transformation\nfunction, such that the trigger can cause misclassification for any input. In\nresponse to this, recent papers have introduced attacks using sample-specific\ninvisible triggers crafted through special transformation functions. While\nthese approaches manage to evade detection to some extent, they reveal\nvulnerability to existing backdoor mitigation techniques. To address and\nenhance both evasiveness and resilience, we introduce a novel backdoor attack\nLOTUS. Specifically, it leverages a secret function to separate samples in the\nvictim class into a set of partitions and applies unique triggers to different\npartitions. Furthermore, LOTUS incorporates an effective trigger focusing\nmechanism, ensuring only the trigger corresponding to the partition can induce\nthe backdoor behavior. Extensive experimental results show that LOTUS can\nachieve high attack success rate across 4 datasets and 7 model structures, and\neffectively evading 13 backdoor detection and mitigation techniques. The code\nis available at https://github.com/Megum1/LOTUS.\n","authors":["Siyuan Cheng","Guanhong Tao","Yingqi Liu","Guangyu Shen","Shengwei An","Shiwei Feng","Xiangzhe Xu","Kaiyuan Zhang","Shiqing Ma","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17188v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\n  2024)"},{"id":"http://arxiv.org/abs/2403.17177v1","updated":"2024-03-25T20:44:01Z","published":"2024-03-25T20:44:01Z","title":"Brain Stroke Segmentation Using Deep Learning Models: A Comparative\n  Study","summary":"  Stroke segmentation plays a crucial role in the diagnosis and treatment of\nstroke patients by providing spatial information about affected brain regions\nand the extent of damage. Segmenting stroke lesions accurately is a challenging\ntask, given that conventional manual techniques are time consuming and prone to\nerrors. Recently, advanced deep models have been introduced for general medical\nimage segmentation, demonstrating promising results that surpass many state of\nthe art networks when evaluated on specific datasets. With the advent of the\nvision Transformers, several models have been introduced based on them, while\nothers have aimed to design better modules based on traditional convolutional\nlayers to extract long-range dependencies like Transformers. The question of\nwhether such high-level designs are necessary for all segmentation cases to\nachieve the best results remains unanswered. In this study, we selected four\ntypes of deep models that were recently proposed and evaluated their\nperformance for stroke segmentation: a pure Transformer-based architecture\n(DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention\nmechanisms in their design, an advanced hybrid model that incorporates CNNs\nwith Transformers (FCT), and the well- known self-adaptive nnUNet framework\nwith its configuration based on given data. We examined their performance on\ntwo publicly available datasets, and found that the nnUNet achieved the best\nresults with the simplest design among all. Revealing the robustness issue of\nTransformers to such variabilities serves as a potential reason for their\nweaker performance. Furthermore, nnUNet's success underscores the significant\nimpact of preprocessing and postprocessing techniques in enhancing segmentation\nresults, surpassing the focus solely on architectural designs\n","authors":["Ahmed Soliman","Yousif Yousif","Ahmed Ibrahim","Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.17177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17176v1","updated":"2024-03-25T20:43:48Z","published":"2024-03-25T20:43:48Z","title":"Histogram Layers for Neural Engineered Features","summary":"  In the computer vision literature, many effective histogram-based features\nhave been developed. These engineered features include local binary patterns\nand edge histogram descriptors among others and they have been shown to be\ninformative features for a variety of computer vision tasks. In this paper, we\nexplore whether these features can be learned through histogram layers embedded\nin a neural network and, therefore, be leveraged within deep learning\nframeworks. By using histogram features, local statistics of the feature maps\nfrom the convolution neural networks can be used to better represent the data.\nWe present neural versions of local binary pattern and edge histogram\ndescriptors that jointly improve the feature representation and perform image\nclassification. Experiments are presented on benchmark and real-world datasets.\n","authors":["Joshua Peeples","Salim Al Kharsa","Luke Saleh","Alina Zare"],"pdf_url":"https://arxiv.org/pdf/2403.17176v1.pdf","comment":"11 pages, 7 figures, submitted for review"},{"id":"http://arxiv.org/abs/2403.17175v1","updated":"2024-03-25T20:43:23Z","published":"2024-03-25T20:43:23Z","title":"Engagement Measurement Based on Facial Landmarks and Spatial-Temporal\n  Graph Convolutional Networks","summary":"  Engagement in virtual learning is crucial for a variety of factors including\nlearner satisfaction, performance, and compliance with learning programs, but\nmeasuring it is a challenging task. There is therefore considerable interest in\nutilizing artificial intelligence and affective computing to measure engagement\nin natural settings as well as on a large scale. This paper introduces a novel,\nprivacy-preserving method for engagement measurement from videos. It uses\nfacial landmarks, which carry no personally identifiable information, extracted\nfrom videos via the MediaPipe deep learning solution. The extracted facial\nlandmarks are fed to a Spatial-Temporal Graph Convolutional Network (ST-GCN) to\noutput the engagement level of the learner in the video. To integrate the\nordinal nature of the engagement variable into the training process, ST-GCNs\nundergo training in a novel ordinal learning framework based on transfer\nlearning. Experimental results on two video student engagement measurement\ndatasets show the superiority of the proposed method compared to previous\nmethods with improved state-of-the-art on the EngageNet dataset with a %3.1\nimprovement in four-class engagement level classification accuracy and on the\nOnline Student Engagement dataset with a %1.5 improvement in binary engagement\nclassification accuracy. The relatively lightweight ST-GCN and its integration\nwith the real-time MediaPipe deep learning solution make the proposed approach\ncapable of being deployed on virtual learning platforms and measuring\nengagement in real time.\n","authors":["Ali Abedi","Shehroz S. Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17173v1","updated":"2024-03-25T20:39:58Z","published":"2024-03-25T20:39:58Z","title":"Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships","summary":"  Modeling and visualizing relationships between tasks or datasets is an\nimportant step towards solving various meta-tasks such as dataset discovery,\nmulti-tasking, and transfer learning. However, many relationships, such as\ncontainment and transferability, are naturally asymmetric and current\napproaches for representation and visualization (e.g., t-SNE do not readily\nsupport this. We propose Task2Box, an approach to represent tasks using box\nembeddings -- axis-aligned hyperrectangles in low dimensional spaces -- that\ncan capture asymmetric relationships between them through volumetric overlaps.\nWe show that Task2Box accurately predicts unseen hierarchical relationships\nbetween nodes in ImageNet and iNaturalist datasets, as well as transferability\nbetween tasks in the Taskonomy benchmark. We also show that box embeddings\nestimated from task representations (e.g., CLIP, Task2Vec, or attribute based)\ncan be used to predict relationships between unseen tasks more accurately than\nclassifiers trained on the same representations, as well as handcrafted\nasymmetric distances (e.g., KL divergence). This suggests that low-dimensional\nbox embeddings can effectively capture these task relationships and have the\nadded advantage of being interpretable. We use the approach to visualize\nrelationships among publicly available image classification datasets on popular\ndataset hosting platform called Hugging Face.\n","authors":["Rangel Daroya","Aaron Sun","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2403.17173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09857v2","updated":"2024-03-25T20:08:07Z","published":"2024-03-14T20:34:53Z","title":"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive\n  Prompt","summary":"  Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn\nnew classes with scarce samples while preserving knowledge of old ones.\nExisting FSCIL methods usually fine-tune the entire backbone, leading to\noverfitting and hindering the potential to learn new classes. On the other\nhand, recent prompt-based CIL approaches alleviate forgetting by training\nprompts with sufficient data in each task. In this work, we propose a novel\nframework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages\ntask-invariant prompts to capture shared knowledge by reducing specific\ninformation from the attention aspect. Additionally, self-adaptive\ntask-specific prompts in ASP provide specific information and transfer\nknowledge from old classes to new classes with an Information Bottleneck\nlearning objective. In summary, ASP prevents overfitting on base task and does\nnot require enormous data in few-shot incremental tasks. Extensive experiments\non three benchmark datasets validate that ASP consistently outperforms\nstate-of-the-art FSCIL and prompt-based CIL methods in terms of both learning\nnew classes and mitigating forgetting.\n","authors":["Chenxi Liu","Zhenyi Wang","Tianyi Xiong","Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2403.09857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07021v2","updated":"2024-03-25T19:46:25Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v2.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.17128v1","updated":"2024-03-25T19:13:12Z","published":"2024-03-25T19:13:12Z","title":"Benchmarking Video Frame Interpolation","summary":"  Video frame interpolation, the task of synthesizing new frames in between two\nor more given ones, is becoming an increasingly popular research target.\nHowever, the current evaluation of frame interpolation techniques is not ideal.\nDue to the plethora of test datasets available and inconsistent computation of\nerror metrics, a coherent and fair comparison across papers is very\nchallenging. Furthermore, new test sets have been proposed as part of method\npapers so they are unable to provide the in-depth evaluation of a dedicated\nbenchmarking paper. Another severe downside is that these test sets violate the\nassumption of linearity when given two input frames, making it impossible to\nsolve without an oracle. We hence strongly believe that the community would\ngreatly benefit from a benchmarking paper, which is what we propose.\nSpecifically, we present a benchmark which establishes consistent error metrics\nby utilizing a submission website that computes them, provides insights by\nanalyzing the interpolation quality with respect to various per-pixel\nattributes such as the motion magnitude, contains a carefully designed test set\nadhering to the assumption of linearity by utilizing synthetic data, and\nevaluates the computational efficiency in a coherent manner.\n","authors":["Simon Kiefhaber","Simon Niklaus","Feng Liu","Simone Schaub-Meyer"],"pdf_url":"https://arxiv.org/pdf/2403.17128v1.pdf","comment":"http://sniklaus.com/vfibench"}],"Graphics":[{"id":"http://arxiv.org/abs/2403.16653v1","updated":"2024-03-25T11:40:47Z","published":"2024-03-25T11:40:47Z","title":"Instantaneous Visual Analysis of Blood Flow in Stenoses Using\n  Morphological Similarity","summary":"  The emergence of computational fluid dynamics (CFD) enabled the simulation of\nintricate transport processes, including flow in physiological structures, such\nas blood vessels. While these so-called hemodynamic simulations offer\ngroundbreaking opportunities to solve problems at the clinical forefront, a\nsuccessful translation of CFD to clinical decision-making is challenging.\nHemodynamic simulations are intrinsically complex, time-consuming, and\nresource-intensive, which conflicts with the time-sensitive nature of clinical\nworkflows and the fact that hospitals usually do not have the necessary\nresources or infrastructure to support CFD simulations. To address these\ntransfer challenges, we propose a novel visualization system which enables\ninstant flow exploration without performing on-site simulation. To gain\ninsights into the viability of the approach, we focus on hemodynamic\nsimulations of the carotid bifurcation, which is a highly relevant arterial\nsubtree in stroke diagnostics and prevention. We created an initial database of\n120 high-resolution carotid bifurcation flow models and developed a set of\nsimilarity metrics used to place a new carotid surface model into a\nneighborhood of simulated cases with the highest geometric similarity. The\nneighborhood can be immediately explored and the flow fields analyzed. We found\nthat if the artery models are similar enough in the regions of interest, a new\nsimulation leads to coinciding results, allowing the user to circumvent\nindividual flow simulations. We conclude that similarity-based visual analysis\nis a promising approach toward the usability of CFD in medical practice.\n","authors":["Pepe Eulzer","Kevin Richter","Anna Hundertmark","Ralf Wickenhöfer","Carsten M. Klingner","Kai Lawonn"],"pdf_url":"https://arxiv.org/pdf/2403.16653v1.pdf","comment":"13 pages, Eurographics Conference on Visualization (EuroVis) 2024"},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2311.18287v2","updated":"2024-03-25T05:34:58Z","published":"2023-11-30T06:45:52Z","title":"Dispersed Structured Light for Hyperspectral 3D Imaging","summary":"  Hyperspectral 3D imaging aims to acquire both depth and spectral information\nof a scene. However, existing methods are either prohibitively expensive and\nbulky or compromise on spectral and depth accuracy. In this work, we present\nDispersed Structured Light (DSL), a cost-effective and compact method for\naccurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera\nsystem by placing a sub-millimeter thick diffraction grating film front of the\nprojector. The grating disperses structured light based on light wavelength. To\nutilize the dispersed structured light, we devise a model for dispersive\nprojection image formation and a per-pixel hyperspectral 3D reconstruction\nmethod. We validate DSL by instantiating a compact experimental prototype. DSL\nachieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth\nerror of 1mm. We demonstrate that DSL outperforms prior work on practical\nhyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D\nimaging for diverse application domains, including computer vision and\ngraphics, cultural heritage, geology, and biology.\n","authors":["Suhyun Shin","Seokjun Choi","Felix Heide","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2311.18287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17261v1","updated":"2024-03-25T23:09:22Z","published":"2024-03-25T23:09:22Z","title":"Distributed Simulation of Large Multi-body Systems","summary":"  We present a technique designed for parallelizing large rigid body\nsimulations, capable of exploiting multiple CPU cores within a computer and\nacross a network. Our approach can be applied to simulate both unilateral and\nbilateral constraints, requiring straightforward modifications to the\nunderlying physics engine. Starting from an approximate partitioning, we\nidentify interface bodies and add them to overlapping sets such that they are\nsimulated by multiple workers. At each timestep, we blend the states of overlap\nbodies using weights based on graph geodesic distances within the constraint\ngraph. The use of overlap simulation also allows us to perform load balancing\nusing efficient local evaluations of the constraint graph. We demonstrate our\ntechnique's scalability and load-balancing capabilities using several\nlarge-scale scenes.\n","authors":["Manas Kale","Paul G. Kry"],"pdf_url":"https://arxiv.org/pdf/2403.17261v1.pdf","comment":"For associated video, see https://www.youtube.com/watch?v=2gg-YnIGJ-w"},{"id":"http://arxiv.org/abs/2403.17237v1","updated":"2024-03-25T22:34:05Z","published":"2024-03-25T22:34:05Z","title":"DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric\n  Diffusion","summary":"  We present DreamPolisher, a novel Gaussian Splatting based method with\ngeometric guidance, tailored to learn cross-view consistency and intricate\ndetail from textual descriptions. While recent progress on text-to-3D\ngeneration methods have been promising, prevailing methods often fail to ensure\nview-consistency and textural richness. This problem becomes particularly\nnoticeable for methods that work with text input alone. To address this, we\npropose a two-stage Gaussian Splatting based approach that enforces geometric\nconsistency among views. Initially, a coarse 3D generation undergoes refinement\nvia geometric optimization. Subsequently, we use a ControlNet driven refiner\ncoupled with the geometric consistency term to improve both texture fidelity\nand overall consistency of the generated 3D asset. Empirical evaluations across\ndiverse textual prompts spanning various object categories demonstrate the\nefficacy of DreamPolisher in generating consistent and realistic 3D objects,\naligning closely with the semantics of the textual instructions.\n","authors":["Yuanze Lin","Ronald Clark","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2403.17237v1.pdf","comment":"Project webpage: https://yuanze-lin.me/DreamPolisher_page/"},{"id":"http://arxiv.org/abs/2311.16682v2","updated":"2024-03-25T18:54:18Z","published":"2023-11-28T10:53:55Z","title":"ContextSeg: Sketch Semantic Segmentation by Querying the Context with\n  Attention","summary":"  Sketch semantic segmentation is a well-explored and pivotal problem in\ncomputer vision involving the assignment of pre-defined part labels to\nindividual strokes. This paper presents ContextSeg - a simple yet highly\neffective approach to tackling this problem with two stages. In the first\nstage, to better encode the shape and positional information of strokes, we\npropose to predict an extra dense distance field in an autoencoder network to\nreinforce structural information learning. In the second stage, we treat an\nentire stroke as a single entity and label a group of strokes within the same\nsemantic part using an auto-regressive Transformer with the default attention\nmechanism. By group-based labeling, our method can fully leverage the context\ninformation when making decisions for the remaining groups of strokes. Our\nmethod achieves the best segmentation accuracy compared with state-of-the-art\napproaches on two representative datasets and has been extensively evaluated\ndemonstrating its superior performance. Additionally, we offer insights into\nsolving part imbalance in training data and the preliminary experiment on\ncross-category training, which can inspire future research in this field.\n","authors":["Jiawei Wang","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2311.16682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17083v1","updated":"2024-03-25T18:16:34Z","published":"2024-03-25T18:16:34Z","title":"A Study in Dataset Pruning for Image Super-Resolution","summary":"  In image Super-Resolution (SR), relying on large datasets for training is a\ndouble-edged sword. While offering rich training material, they also demand\nsubstantial computational and storage resources. In this work, we analyze\ndataset pruning as a solution to these challenges. We introduce a novel\napproach that reduces a dataset to a core-set of training samples, selected\nbased on their loss values as determined by a simple pre-trained SR model. By\nfocusing the training on just 50% of the original dataset, specifically on the\nsamples characterized by the highest loss values, we achieve results comparable\nto or even surpassing those obtained from training on the entire dataset.\nInterestingly, our analysis reveals that the top 5% of samples with the highest\nloss values negatively affect the training process. Excluding these samples and\nadjusting the selection to favor easier samples further enhances training\noutcomes. Our work opens new perspectives to the untapped potential of dataset\npruning in image SR. It suggests that careful selection of training data based\non loss-value metrics can lead to better SR models, challenging the\nconventional wisdom that more data inevitably leads to better performance.\n","authors":["Brian B. Moser","Federico Raue","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.17083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16703v3","updated":"2024-03-25T18:03:41Z","published":"2023-11-28T11:27:48Z","title":"CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD\n  Programs","summary":"  CAD programs are a popular way to compactly encode shapes as a sequence of\noperations that are easy to parametrically modify. However, without sufficient\nsemantic comments and structure, such programs can be challenging to\nunderstand, let alone modify. We introduce the problem of semantic commenting\nCAD programs, wherein the goal is to segment the input program into code blocks\ncorresponding to semantically meaningful shape parts and assign a semantic\nlabel to each block. We solve the problem by combining program parsing with\nvisual-semantic analysis afforded by recent advances in foundational language\nand vision models. Specifically, by executing the input programs, we create\nshapes, which we use to generate conditional photorealistic images to make use\nof semantic annotators for such images. We then distill the information across\nthe images and link back to the original programs to semantically comment on\nthem. Additionally, we collected and annotated a benchmark dataset, CADTalk,\nconsisting of 5,288 machine-made programs and 45 human-made programs with\nground truth semantic comments. We extensively evaluated our approach, compared\nit to a GPT-based baseline, and an open-set shape segmentation baseline, and\nreported an 83.24% accuracy on the new CADTalk dataset. Code and data:\nhttps://enigma-li.github.io/CADTalk/.\n","authors":["Haocheng Yuan","Jing Xu","Hao Pan","Adrien Bousseau","Niloy J. Mitra","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2311.16703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16990v1","updated":"2024-03-25T17:52:07Z","published":"2024-03-25T17:52:07Z","title":"Be Yourself: Bounded Attention for Multi-Subject Text-to-Image\n  Generation","summary":"  Text-to-image diffusion models have an unprecedented ability to generate\ndiverse and high-quality images. However, they often struggle to faithfully\ncapture the intended semantics of complex input prompts that include multiple\nsubjects. Recently, numerous layout-to-image extensions have been introduced to\nimprove user control, aiming to localize subjects represented by specific\ntokens. Yet, these methods often produce semantically inaccurate images,\nespecially when dealing with multiple semantically or visually similar\nsubjects. In this work, we study and analyze the causes of these limitations.\nOur exploration reveals that the primary issue stems from inadvertent semantic\nleakage between subjects in the denoising process. This leakage is attributed\nto the diffusion model's attention layers, which tend to blend the visual\nfeatures of different subjects. To address these issues, we introduce Bounded\nAttention, a training-free method for bounding the information flow in the\nsampling process. Bounded Attention prevents detrimental leakage among subjects\nand enables guiding the generation to promote each subject's individuality,\neven with complex multi-subject conditioning. Through extensive\nexperimentation, we demonstrate that our method empowers the generation of\nmultiple subjects that better align with given prompts and layouts.\n","authors":["Omer Dahary","Or Patashnik","Kfir Aberman","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.16990v1.pdf","comment":"Project page: https://omer11a.github.io/bounded-attention/"},{"id":"http://arxiv.org/abs/2403.16862v1","updated":"2024-03-25T15:26:32Z","published":"2024-03-25T15:26:32Z","title":"INPC: Implicit Neural Point Clouds for Radiance Field Rendering","summary":"  We introduce a new approach for reconstruction and novel-view synthesis of\nunbounded real-world scenes. In contrast to previous methods using either\nvolumetric fields, grid-based models, or discrete point cloud proxies, we\npropose a hybrid scene representation, which implicitly encodes a point cloud\nin a continuous octree-based probability field and a multi-resolution hash\ngrid. In doing so, we combine the benefits of both worlds by retaining\nfavorable behavior during optimization: Our novel implicit point cloud\nrepresentation and differentiable bilinear rasterizer enable fast rendering\nwhile preserving fine geometric detail without depending on initial priors like\nstructure-from-motion point clouds. Our method achieves state-of-the-art image\nquality on several common benchmark datasets. Furthermore, we achieve fast\ninference at interactive frame rates, and can extract explicit point clouds to\nfurther enhance performance.\n","authors":["Florian Hahlbohm","Linus Franke","Moritz Kappel","Susana Castillo","Marc Stamminger","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2403.16862v1.pdf","comment":"Project page: https://fhahlbohm.github.io/inpc/"}]},"2024-03-24T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2311.02787v2","updated":"2024-03-24T23:36:06Z","published":"2023-11-05T22:43:29Z","title":"Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable\n  Manipulation with Tools","summary":"  Deformable object manipulation stands as one of the most captivating yet\nformidable challenges in robotics. While previous techniques have predominantly\nrelied on learning latent dynamics through demonstrations, typically\nrepresented as either particles or images, there exists a pertinent limitation:\nacquiring suitable demonstrations, especially for long-horizon tasks, can be\nelusive. Moreover, basing learning entirely on demonstrations can hamper the\nmodel's ability to generalize beyond the demonstrated tasks. In this work, we\nintroduce a demonstration-free hierarchical planning approach capable of\ntackling intricate long-horizon tasks without necessitating any training. We\nemploy large language models (LLMs) to articulate a high-level, stage-by-stage\nplan corresponding to a specified task. For every individual stage, the LLM\nprovides both the tool's name and the Python code to craft intermediate subgoal\npoint clouds. With the tool and subgoal for a particular stage at our disposal,\nwe present a granular closed-loop model predictive control strategy. This\nleverages Differentiable Physics with Point-to-Point correspondence\n(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied\niteratively. Experimental findings affirm that our technique surpasses multiple\nbenchmarks in dough manipulation, spanning both short and long horizons.\nRemarkably, our model demonstrates robust generalization capabilities to novel\nand previously unencountered complex tasks without any preliminary\ndemonstrations. We further substantiate our approach with experimental trials\non real-world robotic platforms. Our project page:\nhttps://qq456cvb.github.io/projects/donut.\n","authors":["Yang You","Bokui Shen","Congyue Deng","Haoran Geng","Songlin Wei","He Wang","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2311.02787v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.13332v3","updated":"2024-03-24T23:18:18Z","published":"2022-12-27T01:06:26Z","title":"Development and Evaluation of a Learning-based Model for Real-time\n  Haptic Texture Rendering","summary":"  Current Virtual Reality (VR) environments lack the rich haptic signals that\nhumans experience during real-life interactions, such as the sensation of\ntexture during lateral movement on a surface. Adding realistic haptic textures\nto VR environments requires a model that generalizes to variations of a user's\ninteraction and to the wide variety of existing textures in the world. Current\nmethodologies for haptic texture rendering exist, but they usually develop one\nmodel per texture, resulting in low scalability. We present a deep\nlearning-based action-conditional model for haptic texture rendering and\nevaluate its perceptual performance in rendering realistic texture vibrations\nthrough a multi part human user study. This model is unified over all materials\nand uses data from a vision-based tactile sensor (GelSight) to render the\nappropriate surface conditioned on the user's action in real time. For\nrendering texture, we use a high-bandwidth vibrotactile transducer attached to\na 3D Systems Touch device. The result of our user study shows that our\nlearning-based method creates high-frequency texture renderings with comparable\nor better quality than state-of-the-art methods without the need for learning a\nseparate model per texture. Furthermore, we show that the method is capable of\nrendering previously unseen textures using a single GelSight image of their\nsurface.\n","authors":["Negin Heravi","Heather Culbertson","Allison M. Okamura","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2212.13332v3.pdf","comment":"Accepted for publication in IEEE Transactions on Haptics 2024. 12\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16320v1","updated":"2024-03-24T22:56:08Z","published":"2024-03-24T22:56:08Z","title":"Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable\n  Grasping Configurations","summary":"  This study proposes a novel robotic gripper with variable grasping\nconfigurations for grasping various objects. The fingers of the developed\ngripper incorporate multiple different surfaces. The gripper possesses the\nfunction of altering the finger surfaces facing a target object by rotating the\nfingers in its longitudinal direction. In the proposed design equipped with two\nfingers, the two fingers incorporate three and four surfaces, respectively,\nresulting in the nine available grasping configurations by the combination of\nthese finger surfaces. The developed gripper is equipped with the functions of\nopening/closing its fingers for grasping and rotating its fingers to alter the\ngrasping configuration -all achieved with a single motor. To enable the two\nmotions using a single motor, this study introduces a self-motion switching\nmechanism utilizing magnets. This mechanism automatically transitions between\ngripper motions based on the direction of the motor rotation when the gripper\nis fully opened. In this state, rotating the motor towards closing initiates\nthe finger closing action, while further opening the fingers from the fully\nopened state activates the finger rotation. This letter presents the gripper\ndesign, the mechanics of the self-motion switching mechanism, the control\nmethod, and the grasping configuration selection strategy. The performance of\nthe gripper is experimentally demonstrated.\n","authors":["Toshihiro Nishimura","Yosuke Suzuki","Tokuo Tsuj","Tetsuyou Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.16320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16291v1","updated":"2024-03-24T20:43:29Z","published":"2024-03-24T20:43:29Z","title":"Guessing human intentions to avoid dangerous situations in caregiving\n  robots","summary":"  For robots to interact socially, they must interpret human intentions and\nanticipate their potential outcomes accurately. This is particularly important\nfor social robots designed for human care, which may face potentially dangerous\nsituations for people, such as unseen obstacles in their way, that should be\navoided. This paper explores the Artificial Theory of Mind (ATM) approach to\ninferring and interpreting human intentions. We propose an algorithm that\ndetects risky situations for humans, selecting a robot action that removes the\ndanger in real time. We use the simulation-based approach to ATM and adopt the\n'like-me' policy to assign intentions and actions to people. Using this\nstrategy, the robot can detect and act with a high rate of success under\ntime-constrained situations. The algorithm has been implemented as part of an\nexisting robotics cognitive architecture and tested in simulation scenarios.\nThree experiments have been conducted to test the implementation's robustness,\nprecision and real-time response, including a simulated scenario, a\nhuman-in-the-loop hybrid configuration and a real-world scenario.\n","authors":["Noé Zapata","Gerardo Pérez","Lucas Bonilla","Pedro Núñez","Pilar Bachiller","Pablo Bustos"],"pdf_url":"https://arxiv.org/pdf/2403.16291v1.pdf","comment":"8 pages, 6 figures. Submitted to IROS"},{"id":"http://arxiv.org/abs/2403.16277v1","updated":"2024-03-24T19:52:53Z","published":"2024-03-24T19:52:53Z","title":"Combined Task and Motion Planning Via Sketch Decompositions (Extended\n  Version with Supplementary Material)","summary":"  The challenge in combined task and motion planning (TAMP) is the effective\nintegration of a search over a combinatorial space, usually carried out by a\ntask planner, and a search over a continuous configuration space, carried out\nby a motion planner. Using motion planners for testing the feasibility of task\nplans and filling out the details is not effective because it makes the\ngeometrical constraints play a passive role. This work introduces a new\ninterleaved approach for integrating the two dimensions of TAMP that makes use\nof sketches, a recent simple but powerful language for expressing the\ndecomposition of problems into subproblems. A sketch has width 1 if it\ndecomposes the problem into subproblems that can be solved greedily in linear\ntime. In the paper, a general sketch is introduced for several classes of TAMP\nproblems which has width 1 under suitable assumptions. While sketch\ndecompositions have been developed for classical planning, they offer two\nimportant benefits in the context of TAMP. First, when a task plan is found to\nbe unfeasible due to the geometric constraints, the combinatorial search\nresumes in a specific sub-problem. Second, the sampling of object\nconfigurations is not done once, globally, at the start of the search, but\nlocally, at the start of each subproblem. Optimizations of this basic setting\nare also considered and experimental results over existing and new\npick-and-place benchmarks are reported.\n","authors":["Magí Dalmau-Moreno","Néstor García","Vicenç Gómez","Héctor Geffner"],"pdf_url":"https://arxiv.org/pdf/2403.16277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16275v1","updated":"2024-03-24T19:47:37Z","published":"2024-03-24T19:47:37Z","title":"M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and\n  Scheduling","summary":"  In this paper, we present a novel problem coined multi-robot,\nmulti-objective, and multi-mode routing and scheduling (M^3RS). The formulation\nfor M^3RS is introduced for time-bound multi-robot, multi-objective routing and\nscheduling missions where each task has multiple execution modes. Different\nexecution modes have distinct resource consumption, associated execution time,\nand quality. M^3RS assigns the optimal sequence of tasks and the execution\nmodes to each agent. The routes and associated modes depend on user preferences\nfor different objective criteria. The need for M^3RS comes from multi-robot\napplications in which a trade-off between multiple criteria arises from\ndifferent task execution modes. We use M^3RS for the application of multi-robot\ndisinfection in public locations. The objectives considered for disinfection\napplication are disinfection quality and number of tasks completed. A\nmixed-integer linear programming model is proposed for M^3RS. Then, a\ntime-efficient column generation scheme is presented to tackle the issue of\ncomputation times for larger problem instances. The advantage of using multiple\nmodes over fixed execution mode is demonstrated using experiments on synthetic\ndata. The results suggest that M^3RS provides flexibility to the user in terms\nof available solutions and performs well in joint performance metrics. The\napplication of the proposed problem is shown for a team of disinfection\nrobots.} The videos for the experiments are available on the project website:\nhttps://sites.google.com/view/g-robot/m3rs/ .\n","authors":["Ishaan Mehta","Junseo Kim","Sharareh Taghipour","Sajad Saeedi"],"pdf_url":"https://arxiv.org/pdf/2403.16275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08812v4","updated":"2024-03-24T19:25:20Z","published":"2022-09-19T07:52:02Z","title":"Generative Graphical Inverse Kinematics","summary":"  Quickly and reliably finding accurate inverse kinematics (IK) solutions\nremains a challenging problem for many robot manipulators. Existing numerical\nsolvers are broadly applicable but typically only produce a single solution and\nrely on local search techniques to minimize nonconvex objective functions. More\nrecent learning-based approaches that approximate the entire feasible set of\nsolutions have shown promise as a means to generate multiple fast and accurate\nIK results in parallel. However, existing learning-based techniques have a\nsignificant drawback: each robot of interest requires a specialized model that\nmust be trained from scratch. To address this key shortcoming, we propose a\nnovel distance-geometric robot representation coupled with a graph structure\nthat allows us to leverage the sample efficiency of Euclidean equivariant\nfunctions and the generalizability of graph neural networks (GNNs). Our\napproach is generative graphical inverse kinematics (GGIK), the first learned\nIK solver able to accurately and efficiently produce a large number of diverse\nsolutions in parallel while also displaying the ability to generalize -- a\nsingle learned model can be used to produce IK solutions for a variety of\ndifferent robots. When compared to several other learned IK methods, GGIK\nprovides more accurate solutions with the same amount of data. GGIK can\ngeneralize reasonably well to robot manipulators unseen during training.\nAdditionally, GGIK can learn a constrained distribution that encodes joint\nlimits and scales efficiently to larger robots and a high number of sampled\nsolutions. Finally, GGIK can be used to complement local IK solvers by\nproviding reliable initializations for a local optimization process.\n","authors":["Oliver Limoyo","Filip Marić","Matthew Giamou","Petra Alexson","Ivan Petrović","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2209.08812v4.pdf","comment":"Submitted to IEEE Transactions on Robotics, June 2023"},{"id":"http://arxiv.org/abs/2403.16262v1","updated":"2024-03-24T18:49:16Z","published":"2024-03-24T18:49:16Z","title":"HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under\n  Unknown Vertical Ground Motion","summary":"  This paper presents a hierarchical control framework that enables robust\nquadrupedal locomotion on a dynamic rigid surface (DRS) with general and\nunknown vertical motions. The key novelty of the framework lies in its higher\nlayer, which is a discrete-time, provably stabilizing footstep controller. The\nbasis of the footstep controller is a new hybrid, time-varying, linear inverted\npendulum (HT-LIP) model that is low-dimensional and accurately captures the\nessential robot dynamics during DRS locomotion. A new set of sufficient\nstability conditions are then derived to directly guide the controller design\nfor ensuring the asymptotic stability of the HT-LIP model under general,\nunknown, vertical DRS motions. Further, the footstep controller is cast as a\ncomputationally efficient quadratic program that incorporates the proposed\nHT-LIP model and stability conditions. The middle layer takes the desired\nfootstep locations generated by the higher layer as input to produce\nkinematically feasible full-body reference trajectories, which are then\naccurately tracked by a lower-layer torque controller. Hardware experiments on\na Unitree Go1 quadrupedal robot confirm the robustness of the proposed\nframework under various unknown, aperiodic, vertical DRS motions and\nuncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and\nsudden pushes).\n","authors":["Amir Iqbal","Sushant Veer","Christopher Niezrecki","Yan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16252v1","updated":"2024-03-24T18:10:30Z","published":"2024-03-24T18:10:30Z","title":"Legged Robot State Estimation within Non-inertial Environments","summary":"  This paper investigates the robot state estimation problem within a\nnon-inertial environment. The proposed state estimation approach relaxes the\ncommon assumption of static ground in the system modeling. The process and\nmeasurement models explicitly treat the movement of the non-inertial\nenvironments without requiring knowledge of its motion in the inertial frame or\nrelying on GPS or sensing environmental landmarks. Further, the proposed state\nestimator is formulated as an invariant extended Kalman filter (InEKF) with the\ndeterministic part of its process model obeying the group-affine property,\nleading to log-linear error dynamics. The observability analysis of the filter\nconfirms that the robot's pose (i.e., position and orientation) and velocity\nrelative to the non-inertial environment are observable. Hardware experiments\non a humanoid robot moving on a rotating and translating treadmill demonstrate\nthe high convergence rate and accuracy of the proposed InEKF even under\nsignificant treadmill pitch sway, as well as large estimation errors.\n","authors":["Zijian He","Sangli Teng","Tzu-Yuan Lin","Maani Ghaffari","Yan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12188v2","updated":"2024-03-24T17:19:14Z","published":"2023-09-21T15:54:33Z","title":"SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on\n  Scene Graphs","summary":"  Object rearrangement is pivotal in robotic-environment interactions,\nrepresenting a significant capability in embodied AI. In this paper, we present\nSG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme\nwith a scene graph as the scene representation. Unlike previous methods that\nrely on either known goal priors or zero-shot large models, SG-Bot exemplifies\nlightweight, real-time, and user-controllable characteristics, seamlessly\nblending the consideration of commonsense knowledge with automatic generation\ncapabilities. SG-Bot employs a three-fold procedure--observation, imagination,\nand execution--to adeptly address the task. Initially, objects are discerned\nand extracted from a cluttered scene during the observation. These objects are\nfirst coarsely organized and depicted within a scene graph, guided by either\ncommonsense or user-defined criteria. Then, this scene graph subsequently\ninforms a generative model, which forms a fine-grained goal scene considering\nthe shape information from the initial scene and object semantics. Finally, for\nexecution, the initial and envisioned goal scenes are matched to formulate\nrobotic action policies. Experimental results demonstrate that SG-Bot\noutperforms competitors by a large margin.\n","authors":["Guangyao Zhai","Xiaoni Cai","Dianye Huang","Yan Di","Fabian Manhardt","Federico Tombari","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.12188v2.pdf","comment":"ICRA 2024 accepted. Project website:\n  https://sites.google.com/view/sg-bot"},{"id":"http://arxiv.org/abs/2403.16238v1","updated":"2024-03-24T17:00:01Z","published":"2024-03-24T17:00:01Z","title":"KITchen: A Real-World Benchmark and Dataset for 6D Object Pose\n  Estimation in Kitchen Environments","summary":"  Despite the recent progress on 6D object pose estimation methods for robotic\ngrasping, a substantial performance gap persists between the capabilities of\nthese methods on existing datasets and their efficacy in real-world mobile\nmanipulation tasks, particularly when robots rely solely on their monocular\negocentric field of view (FOV). Existing real-world datasets primarily focus on\ntable-top grasping scenarios, where a robotic arm is placed in a fixed position\nand the objects are centralized within the FOV of fixed external camera(s).\nAssessing performance on such datasets may not accurately reflect the\nchallenges encountered in everyday mobile manipulation tasks within kitchen\nenvironments such as retrieving objects from higher shelves, sinks,\ndishwashers, ovens, refrigerators, or microwaves. To address this gap, we\npresent Kitchen, a novel benchmark designed specifically for estimating the 6D\nposes of objects located in diverse positions within kitchen settings. For this\npurpose, we recorded a comprehensive dataset comprising around 205k real-world\nRGBD images for 111 kitchen objects captured in two distinct kitchens,\nutilizing one humanoid robot with its egocentric perspectives. Subsequently, we\ndeveloped a semi-automated annotation pipeline, to streamline the labeling\nprocess of such datasets, resulting in the generation of 2D object labels, 2D\nobject segmentation masks, and 6D object poses with minimized human effort. The\nbenchmark, the dataset, and the annotation pipeline are available at\nhttps://kitchen-dataset.github.io/KITchen.\n","authors":["Abdelrahman Younes","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.16238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16178v1","updated":"2024-03-24T14:38:18Z","published":"2024-03-24T14:38:18Z","title":"Mixed-Initiative Human-Robot Teaming under Suboptimality with Online\n  Bayesian Adaptation","summary":"  For effective human-agent teaming, robots and other artificial intelligence\n(AI) agents must infer their human partner's abilities and behavioral response\npatterns and adapt accordingly. Most prior works make the unrealistic\nassumption that one or more teammates can act near-optimally. In real-world\ncollaboration, humans and autonomous agents can be suboptimal, especially when\neach only has partial domain knowledge. In this work, we develop computational\nmodeling and optimization techniques for enhancing the performance of\nsuboptimal human-agent teams, where the human and the agent have asymmetric\ncapabilities and act suboptimally due to incomplete environmental knowledge. We\nadopt an online Bayesian approach that enables a robot to infer people's\nwillingness to comply with its assistance in a sequential decision-making game.\nOur user studies show that user preferences and team performance indeed vary\nwith robot intervention styles, and our approach for mixed-initiative\ncollaborations enhances objective team performance ($p<.001$) and subjective\nmeasures, such as user's trust ($p<.001$) and perceived likeability of the\nrobot ($p<.001$).\n","authors":["Manisha Natarajan","Chunyue Xue","Sanne van Waveren","Karen Feigh","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2403.16178v1.pdf","comment":"8 pages, 4 pages for supplementary"},{"id":"http://arxiv.org/abs/2311.12261v2","updated":"2024-03-24T14:18:36Z","published":"2023-11-21T00:45:13Z","title":"EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic\n  Under Real-World Perturbations Via Reinforcement Learning","summary":"  Human-driven vehicles (HVs) amplify naturally occurring perturbations in\ntraffic, leading to congestion--a major contributor to increased fuel\nconsumption, higher collision risks, and reduced road capacity utilization.\nWhile previous research demonstrates that Robot Vehicles (RVs) can be leveraged\nto mitigate these issues, most such studies rely on simulations with simplistic\nmodels of human car-following behaviors. In this work, we analyze real-world\ndriving trajectories and extract a wide range of acceleration profiles. We then\nincorporates these profiles into simulations for training RVs to mitigate\ncongestion. We evaluate the safety, efficiency, and stability of mixed traffic\nvia comprehensive experiments conducted in two mixed traffic environments (Ring\nand Bottleneck) at various traffic densities, configurations, and RV\npenetration rates. The results show that under real-world perturbations, prior\nRV controllers experience performance degradation on all three objectives\n(sometimes even lower than 100% HVs). To address this, we introduce a\nreinforcement learning based RV that employs a congestion stage classifier to\noptimize the safety, efficiency, and stability of mixed traffic. Our RVs\ndemonstrate significant improvements: safety by up to 66%, efficiency by up to\n54%, and stability by up to 97%.\n","authors":["Bibek Poudel","Weizi Li","Kevin Heaslip"],"pdf_url":"https://arxiv.org/pdf/2311.12261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04846v2","updated":"2024-03-24T14:18:28Z","published":"2023-10-07T15:11:31Z","title":"Soft finger rotational stability for precision grasps","summary":"  Soft robotic fingers can safely grasp fragile or variable form objects, but\ntheir force capacity is limited, especially with less contact area: precision\ngrasps and when objects are smaller or not spherical. Current research is\nimproving force capacity through mechanical design by increasing contact area\nor stiffness, typically without models which explain soft finger force\nlimitations. To address this, this paper considers two types of soft grip\nfailure, slip and dynamic rotational stability. For slip, the validity of a\nCoulomb model investigated, identifying the effect of contact area, pressure,\nand relative pose. For rotational stability, bulk linear stiffness of the\nfingers is used to develop conditions for dynamic stability and identify when\nrotation leads to slip. Together, these models suggest contact area improves\nforce capacity by increasing transverse stiffness and normal force. The models\nare validated on pneumatic fingers, both custom PneuNets-based and commercially\navailable. The models are used to find grip parameters which increase force\ncapacity without failure.\n","authors":["Hun Jang","Valentyn Petrichenko","Joonbum Bae","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04846v2.pdf","comment":"Submitted IROS24"},{"id":"http://arxiv.org/abs/2310.04822v2","updated":"2024-03-24T13:57:35Z","published":"2023-10-07T14:21:43Z","title":"Combining Sampling- and Gradient-based Planning for Contact-rich\n  Manipulation","summary":"  Planning over discontinuous dynamics is needed for robotics tasks like\ncontact-rich manipulation, which presents challenges in the numerical stability\nand speed of planning methods when either neural network or analytical models\nare used. On the one hand, sampling-based planners require higher sample\ncomplexity in high-dimensional problems and cannot describe safety constraints\nsuch as force limits. On the other hand, gradient-based solvers can suffer from\nlocal optima and convergence issues when the Hessian is poorly conditioned. We\npropose a planning method with both sampling- and gradient-based elements,\nusing the Cross-entropy Method to initialize a gradient-based solver, providing\nbetter search over local minima and the ability to handle explicit constraints.\nWe show the approach allows smooth, stable contact-rich planning for an\nimpedance-controlled robot making contact with a stiff environment,\nbenchmarking against gradient-only MPC and CEM.\n","authors":["Filippo Rozzi","Loris Roveda","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04822v2.pdf","comment":"Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw\n  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc"},{"id":"http://arxiv.org/abs/2403.16146v1","updated":"2024-03-24T13:36:23Z","published":"2024-03-24T13:36:23Z","title":"Realtime Robust Shape Estimation of Deformable Linear Object","summary":"  Realtime shape estimation of continuum objects and manipulators is essential\nfor developing accurate planning and control paradigms. The existing methods\nthat create dense point clouds from camera images, and/or use distinguishable\nmarkers on a deformable body have limitations in realtime tracking of large\ncontinuum objects/manipulators. The physical occlusion of markers can often\ncompromise accurate shape estimation. We propose a robust method to estimate\nthe shape of linear deformable objects in realtime using scattered and\nunordered key points. By utilizing a robust probability-based labeling\nalgorithm, our approach identifies the true order of the detected key points\nand then reconstructs the shape using piecewise spline interpolation. The\napproach only relies on knowing the number of the key points and the interval\nbetween two neighboring points. We demonstrate the robustness of the method\nwhen key points are partially occluded. The proposed method is also integrated\ninto a simulation in Unity for tracking the shape of a cable with a length of\n1m and a radius of 5mm. The simulation results show that our proposed approach\nachieves an average length error of 1.07% over the continuum's centerline and\nan average cross-section error of 2.11mm. The real-world experiments of\ntracking and estimating a heavy-load cable prove that the proposed approach is\nrobust under occlusion and complex entanglement scenarios.\n","authors":["Jiaming Zhang","Zhaomeng Zhang","Yihao Liu","Yaqian Chen","Amir Kheradmand","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2403.16146v1.pdf","comment":"This paper has been accepted to IEEE ICRA 2024 as a contributed paper"},{"id":"http://arxiv.org/abs/2403.16095v1","updated":"2024-03-24T11:19:59Z","published":"2024-03-24T11:19:59Z","title":"CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D\n  Gaussian Field","summary":"  Recently neural radiance fields (NeRF) have been widely exploited as 3D\nrepresentations for dense simultaneous localization and mapping (SLAM). Despite\ntheir notable successes in surface modeling and novel view synthesis, existing\nNeRF-based methods are hindered by their computationally intensive and\ntime-consuming volume rendering pipeline. This paper presents an efficient\ndense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D\nGaussian field with high consistency and geometric stability. Through an\nin-depth analysis of Gaussian Splatting, we propose several techniques to\nconstruct a consistent and stable 3D Gaussian field suitable for tracking and\nmapping. Additionally, a novel depth uncertainty model is proposed to ensure\nthe selection of valuable Gaussian primitives during optimization, thereby\nimproving tracking efficiency and accuracy. Experiments on various datasets\ndemonstrate that CG-SLAM achieves superior tracking and mapping performance\nwith a notable tracking speed of up to 15 Hz. We will make our source code\npublicly available. Project page: https://zju3dv.github.io/cg-slam.\n","authors":["Jiarui Hu","Xianhao Chen","Boyin Feng","Guanglin Li","Liangjing Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16095v1.pdf","comment":"Project Page: https://zju3dv.github.io/cg-slam"},{"id":"http://arxiv.org/abs/2403.16092v1","updated":"2024-03-24T11:09:41Z","published":"2024-03-24T11:09:41Z","title":"Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap","summary":"  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different pre-training strategies.\nOur results show notable improvements in model robustness to simulated data,\neven improving real-world performance in some cases. Last, we delve into the\ncorrelation between the real-to-simulated gap and image reconstruction metrics,\nidentifying FID and LPIPS as strong indicators.\n","authors":["Carl Lindström","Georg Hess","Adam Lilja","Maryam Fatemi","Lars Hammarstrand","Christoffer Petersson","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2403.16092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16699v2","updated":"2024-03-24T09:46:04Z","published":"2024-02-26T16:13:09Z","title":"SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm\n  Robotic Systems","summary":"  Large-scale swarm robotic systems consisting of numerous cooperative agents\nshow considerable promise for performing autonomous tasks across various\nsectors. Nonetheless, traditional motion planning approaches often face a\ntrade-off between scalability and solution quality due to the exponential\ngrowth of the joint state space of robots. In response, this work proposes\nSwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware\nsampling-based motion planning approach for large-scale swarm robots. SwarmPRM\nutilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic\nstate and constructs a Probabilistic Roadmap in Gaussian space, referred to as\nthe Gaussian roadmap, to generate a transport trajectory of GMM. This\ntrajectory is then followed by each robot at the microscopic stage. To enhance\ntrajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR)\nin the collision checking process to impart the property of risk awareness to\nthe constructed Gaussian roadmap. SwarmPRM then crafts a linear programming\nformulation to compute the optimal GMM transport trajectory within this\nroadmap. Extensive simulations demonstrate that SwarmPRM outperforms\nstate-of-the-art methods in computational efficiency, scalability, and\ntrajectory quality while offering the capability to adjust the risk tolerance\nof generated trajectories.\n","authors":["Yunze Hu","Xuru Yang","Kangjie Zhou","Qinghang Liu","Kang Ding","Han Gao","Pingping Zhu","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16699v2.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.16023v1","updated":"2024-03-24T05:55:39Z","published":"2024-03-24T05:55:39Z","title":"RPMArt: Towards Robust Perception and Manipulation for Articulated\n  Objects","summary":"  Articulated objects are commonly found in daily life. It is essential that\nrobots can exhibit robust perception and manipulation skills for articulated\nobjects in real-world robotic applications. However, existing methods for\narticulated objects insufficiently address noise in point clouds and struggle\nto bridge the gap between simulation and reality, thus limiting the practical\ndeployment in real-world scenarios. To tackle these challenges, we propose a\nframework towards Robust Perception and Manipulation for Articulated Objects\n(RPMArt), which learns to estimate the articulation parameters and manipulate\nthe articulation part from the noisy point cloud. Our primary contribution is a\nRobust Articulation Network (RoArtNet) that is able to predict both joint\nparameters and affordable points robustly by local feature learning and point\ntuple voting. Moreover, we introduce an articulation-aware classification\nscheme to enhance its ability for sim-to-real transfer. Finally, with the\nestimated affordable point and articulation joint constraint, the robot can\ngenerate robust actions to manipulate articulated objects. After learning only\nfrom synthetic data, RPMArt is able to transfer zero-shot to real-world\narticulated objects. Experimental results confirm our approach's effectiveness,\nwith our framework achieving state-of-the-art performance in both noise-added\nsimulation and real-world environments. The code and data will be open-sourced\nfor reproduction. More results are published on the project website at\nhttps://r-pmart.github.io .\n","authors":["Junbo Wang","Wenhai Liu","Qiaojun Yu","Yang You","Liu Liu","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.16023v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024), project website at\n  https://r-pmart.github.io"},{"id":"http://arxiv.org/abs/2403.16015v1","updated":"2024-03-24T05:13:37Z","published":"2024-03-24T05:13:37Z","title":"MQE: Unleashing the Power of Interaction with Multi-agent Quadruped\n  Environment","summary":"  The advent of deep reinforcement learning (DRL) has significantly advanced\nthe field of robotics, particularly in the control and coordination of\nquadruped robots. However, the complexity of real-world tasks often\nnecessitates the deployment of multi-robot systems capable of sophisticated\ninteraction and collaboration. To address this need, we introduce the\nMulti-agent Quadruped Environment (MQE), a novel platform designed to\nfacilitate the development and evaluation of multi-agent reinforcement learning\n(MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex\ninteractions between robots and objects, hierarchical policy structures, and\nchallenging evaluation scenarios that reflect real-world applications. We\npresent a series of collaborative and competitive tasks within MQE, ranging\nfrom simple coordination to complex adversarial interactions, and benchmark\nstate-of-the-art MARL algorithms. Our findings indicate that hierarchical\nreinforcement learning can simplify task learning, but also highlight the need\nfor advanced algorithms capable of handling the intricate dynamics of\nmulti-agent interactions. MQE serves as a stepping stone towards bridging the\ngap between simulation and practical deployment, offering a rich environment\nfor future research in multi-agent systems and robot learning. For open-sourced\ncode and more details of MQE, please refer to\nhttps://ziyanx02.github.io/multiagent-quadruped-environment/ .\n","authors":["Ziyan Xiong","Bo Chen","Shiyu Huang","Wei-Wei Tu","Zhaofeng He","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2403.16015v1.pdf","comment":"Open-source code is available at\n  https://github.com/ziyanx02/multiagent-quadruped-environment"},{"id":"http://arxiv.org/abs/2308.15991v3","updated":"2024-03-24T04:45:03Z","published":"2023-08-30T12:24:30Z","title":"DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous\n  Driving","summary":"  Autonomous driving systems are always built on motion-related modules such as\nthe planner and the controller. An accurate and robust trajectory tracking\nmethod is indispensable for these motion-related modules as a primitive\nroutine. Current methods often make strong assumptions about the model such as\nthe context and the dynamics, which are not robust enough to deal with the\nchanging scenarios in a real-world system. In this paper, we propose a Deep\nReinforcement Learning (DRL)-based trajectory tracking method for the\nmotion-related modules in autonomous driving systems. The representation\nlearning ability of DL and the exploration nature of RL bring strong robustness\nand improve accuracy. Meanwhile, it enhances versatility by running the\ntrajectory tracking in a model-free and data-driven manner. Through extensive\nexperiments, we demonstrate both the efficiency and effectiveness of our method\ncompared to current methods. Code and documentation are released to facilitate\nboth further research and industrial deployment.\n","authors":["Yinda Xu","Lidong Yu"],"pdf_url":"https://arxiv.org/pdf/2308.15991v3.pdf","comment":"Technical report. Code:\n  https://github.com/MARMOTatZJU/drl-based-trajectory-tracking Documentation:\n  https://drl-based-trajectory-tracking.readthedocs.io"},{"id":"http://arxiv.org/abs/2403.15993v1","updated":"2024-03-24T03:10:18Z","published":"2024-03-24T03:10:18Z","title":"Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion\n  via Signal Temporal Logic Guided Model Predictive Control","summary":"  This study introduces a robust planning framework that utilizes a model\npredictive control (MPC) approach, enhanced by incorporating signal temporal\nlogic (STL) specifications. This marks the first-ever study to apply STL-guided\ntrajectory optimization for bipedal locomotion, specifically designed to handle\nboth translational and orientational perturbations. Existing recovery\nstrategies often struggle with reasoning complex task logic and evaluating\nlocomotion robustness systematically, making them susceptible to failures\ncaused by inappropriate recovery strategies or lack of robustness. To address\nthese issues, we design an analytical robustness metric for bipedal locomotion\nand quantify this metric using STL specifications, which guide the generation\nof recovery trajectories to achieve maximum locomotion robustness. To enable\nsafe and computational-efficient crossed-leg maneuver, we design data-driven\nself-leg-collision constraints that are $1000$ times faster than the\ntraditional inverse-kinematics-based approach. Our framework outperforms a\nstate-of-the-art locomotion controller, a standard MPC without STL, and a\nlinear-temporal-logic-based planner in a high-fidelity dynamic simulation,\nespecially in scenarios involving crossed-leg maneuvers. Additionally, the\nCassie bipedal robot achieves robust performance under horizontal and\norientational perturbations such as those observed in ship motions. These\nenvironments are validated in simulations and deployed on hardware.\nFurthermore, our proposed method demonstrates versatility on stepping stones\nand terrain-agnostic features on inclined terrains.\n","authors":["Zhaoyuan Gu","Yuntian Zhao","Yipu Chen","Rongming Guo","Jennifer K. Leestma","Gregory S. Sawicki","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.04132v4","updated":"2024-03-24T01:23:11Z","published":"2020-12-08T00:37:35Z","title":"A Number Sense as an Emergent Property of the Manipulating Brain","summary":"  The ability to understand and manipulate numbers and quantities emerges\nduring childhood, but the mechanism through which humans acquire and develop\nthis ability is still poorly understood. We explore this question through a\nmodel, assuming that the learner is able to pick up and place small objects\nfrom, and to, locations of its choosing, and will spontaneously engage in such\nundirected manipulation. We further assume that the learner's visual system\nwill monitor the changing arrangements of objects in the scene and will learn\nto predict the effects of each action by comparing perception with a\nsupervisory signal from the motor system. We model perception using standard\ndeep networks for feature extraction and classification, and gradient descent\nlearning. Our main finding is that, from learning the task of action\nprediction, an unexpected image representation emerges exhibiting regularities\nthat foreshadow the perception and representation of numbers and quantity.\nThese include distinct categories for zero and the first few natural numbers, a\nstrict ordering of the numbers, and a one-dimensional signal that correlates\nwith numerical quantity. As a result, our model acquires the ability to\nestimate numerosity, i.e. the number of objects in the scene, as well as\nsubitization, i.e. the ability to recognize at a glance the exact number of\nobjects in small scenes. Remarkably, subitization and numerosity estimation\nextrapolate to scenes containing many objects, far beyond the three objects\nused during training. We conclude that important aspects of a facility with\nnumbers and quantities may be learned with supervision from a simple\npre-training task. Our observations suggest that cross-modal learning is a\npowerful learning mechanism that may be harnessed in artificial intelligence.\n","authors":["Neehar Kondapaneni","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2012.04132v4.pdf","comment":"16 pages, 5 figures, 15 supplemental figures"},{"id":"http://arxiv.org/abs/2205.11624v5","updated":"2024-03-24T20:03:38Z","published":"2022-05-23T20:49:40Z","title":"Effective Integration of Weighted Cost-to-go and Conflict Heuristic\n  within Suboptimal CBS","summary":"  Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)\nsolver that employs a low-level single agent planner and a high-level\nconstraint tree to resolve conflicts. The vast majority of modern MAPF solvers\nfocus on improving CBS by reducing the size of this tree through various\nstrategies with few methods modifying the low level planner. Typically low\nlevel planners in existing CBS methods use an unweighted cost-to-go heuristic,\nwith suboptimal CBS methods also using a conflict heuristic to help the high\nlevel search. In this paper, we show that, contrary to prevailing CBS beliefs,\na weighted cost-to-go heuristic can be used effectively alongside the conflict\nheuristic in two possible variants. In particular, one of these variants can\nobtain large speedups, 2-100x, across several scenarios and suboptimal CBS\nmethods. Importantly, we discover that performance is related not to the\nweighted cost-to-go heuristic but rather to the relative conflict heuristic\nweight's ability to effectively balance low-level and high-level work.\nAdditionally, to the best of our knowledge, we show the first theoretical\nrelation of prioritized planning and bounded suboptimal CBS and demonstrate\nthat our methods are their natural generalization. Update March 2024: We found\nthat the relative speedup decreases to around 1.2-10x depending on how the\nconflict heuristic is computed (see appendix for more details).\n","authors":["Rishi Veerapaneni","Tushar Kusnur","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2205.11624v5.pdf","comment":"Published in AAAI 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.15168v3","updated":"2024-03-24T23:32:50Z","published":"2023-10-23T17:59:52Z","title":"Ghost on the Shell: An Expressive Representation of General 3D Shapes","summary":"  The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.\n","authors":["Zhen Liu","Yao Feng","Yuliang Xiu","Weiyang Liu","Liam Paull","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2310.15168v3.pdf","comment":"ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:\n  https://gshell3d.github.io/)"},{"id":"http://arxiv.org/abs/2311.01623v3","updated":"2024-03-24T23:13:06Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v3.pdf","comment":"MLSys'24"},{"id":"http://arxiv.org/abs/2403.16318v1","updated":"2024-03-24T22:53:16Z","published":"2024-03-24T22:53:16Z","title":"AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans","summary":"  Recently, progress in acquisition equipment such as LiDAR sensors has enabled\nsensing increasingly spacious outdoor 3D environments. Making sense of such 3D\nacquisitions requires fine-grained scene understanding, such as constructing\ninstance-based 3D scene segmentations. Commonly, a neural network is trained\nfor this task; however, this requires access to a large, densely annotated\ndataset, which is widely known to be challenging to obtain. To address this\nissue, in this work we propose to predict instance segmentations for 3D scenes\nin an unsupervised way, without relying on ground-truth annotations. To this\nend, we construct a learning framework consisting of two components: (1) a\npseudo-annotation scheme for generating initial unsupervised pseudo-labels; and\n(2) a self-training algorithm for instance segmentation to fit robust, accurate\ninstances from initial noisy proposals. To enable generating 3D instance mask\nproposals, we construct a weighted proxy-graph by connecting 3D points with\nedges integrating multi-modal image- and point-based self-supervised features,\nand perform graph-cuts to isolate individual pseudo-instances. We then build on\na state-of-the-art point-based architecture and train a 3D instance\nsegmentation model, resulting in significant refinement of initial proposals.\nTo scale to arbitrary complexity 3D scenes, we design our algorithm to operate\non local 3D point chunks and construct a merging step to generate scene-level\ninstance segmentations. Experiments on the challenging SemanticKITTI benchmark\ndemonstrate the potential of our approach, where it attains 13.3% higher\nAverage Precision and 9.1% higher F1 score compared to the best-performing\nbaseline. The code will be made publicly available at\nhttps://github.com/artonson/autoinst.\n","authors":["Cedric Perauer","Laurenz Adrian Heidrich","Haifan Zhang","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.16318v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.03881v2","updated":"2024-03-24T21:38:49Z","published":"2024-03-06T17:41:41Z","title":"Latent Dataset Distillation with Diffusion Models","summary":"  The efficacy of machine learning has traditionally relied on the availability\nof increasingly larger datasets. However, large datasets pose storage\nchallenges and contain non-influential samples, which could be ignored during\ntraining without impacting the final accuracy of the model. In response to\nthese limitations, the concept of distilling the information on a dataset into\na condensed set of (synthetic) samples, namely a distilled dataset, emerged.\nOne crucial aspect is the selected architecture (usually ConvNet) for linking\nthe original and synthetic datasets. However, the final accuracy is lower if\nthe employed model architecture differs from the model used during\ndistillation. Another challenge is the generation of high-resolution images,\ne.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation\nwith Diffusion Models (LD3M) that combine diffusion in latent space with\ndataset distillation to tackle both challenges. LD3M incorporates a novel\ndiffusion process tailored for dataset distillation, which improves the\ngradient norms for learning synthetic images. By adjusting the number of\ndiffusion steps, LD3M also offers a straightforward way of controlling the\ntrade-off between speed and accuracy. We evaluate our approach in several\nImageNet subsets and for high-resolution images (128x128 and 256x256). As a\nresult, LD3M consistently outperforms state-of-the-art distillation techniques\nby up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.\n","authors":["Brian B. Moser","Federico Raue","Sebastian Palacio","Stanislav Frolov","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.03881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16292v1","updated":"2024-03-24T20:48:36Z","published":"2024-03-24T20:48:36Z","title":"latentSplat: Autoencoding Variational Gaussians for Fast Generalizable\n  3D Reconstruction","summary":"  We present latentSplat, a method to predict semantic Gaussians in a 3D latent\nspace that can be splatted and decoded by a light-weight generative 2D\narchitecture. Existing methods for generalizable 3D reconstruction either do\nnot enable fast inference of high resolution novel views due to slow volume\nrendering, or are limited to interpolation of close input views, even in\nsimpler settings with a single central object, where 360-degree generalization\nis possible. In this work, we combine a regression-based approach with a\ngenerative model, moving towards both of these capabilities within the same\nmethod, trained purely on readily available real video data. The core of our\nmethod are variational 3D Gaussians, a representation that efficiently encodes\nvarying uncertainty within a latent space consisting of 3D feature Gaussians.\nFrom these Gaussians, specific instances can be sampled and rendered via\nefficient Gaussian splatting and a fast, generative decoder network. We show\nthat latentSplat outperforms previous works in reconstruction quality and\ngeneralization, while being fast and scalable to high-resolution data.\n","authors":["Christopher Wewer","Kevin Raj","Eddy Ilg","Bernt Schiele","Jan Eric Lenssen"],"pdf_url":"https://arxiv.org/pdf/2403.16292v1.pdf","comment":"Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/"},{"id":"http://arxiv.org/abs/2403.16286v1","updated":"2024-03-24T20:31:42Z","published":"2024-03-24T20:31:42Z","title":"HemoSet: The First Blood Segmentation Dataset for Automation of\n  Hemostasis Management","summary":"  Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly\nadapt to the visual interference that results from blood rapidly filling the\nsurgical field. Introducing automation into the crucial surgical task of\nhemostasis management would offload mental and physical tasks from the surgeon\nand surgical assistants while simultaneously increasing the efficiency and\nsafety of the operation. The first step in automation of hemostasis management\nis detection of blood in the surgical field. To propel the development of blood\ndetection algorithms in surgeries, we present HemoSet, the first blood\nsegmentation dataset based on bleeding during a live animal robotic surgery.\nOur dataset features vessel hemorrhage scenarios where turbulent flow leads to\nabnormal pooling geometries in surgical fields. These pools are formed in\nconditions endemic to surgical procedures -- uneven heterogeneous tissue, under\nglossy lighting conditions and rapid tool movement. We benchmark several\nstate-of-the-art segmentation models and provide insight into the difficulties\nspecific to blood detection. We intend for HemoSet to spur development of\nautonomous blood suction tools by providing a platform for training and\nrefining blood segmentation models, addressing the precision needed for such\nrobotics.\n","authors":["Albert J. Miao Shan Lin","Jingpei Lu","Florian Richter","Benjamin Ostrander","Emily K. Funk","Ryan K. Orosco","Michael C. Yip"],"pdf_url":"https://arxiv.org/pdf/2403.16286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04926v2","updated":"2024-03-24T20:25:03Z","published":"2024-03-07T22:21:08Z","title":"BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel\n  Modeling","summary":"  Recent efforts in using 3D Gaussians for scene reconstruction and novel view\nsynthesis can achieve impressive results on curated benchmarks; however, images\ncaptured in real life are often blurry. In this work, we analyze the robustness\nof Gaussian-Splatting-based methods against various image blur, such as motion\nblur, defocus blur, downscaling blur, \\etc. Under these degradations,\nGaussian-Splatting-based methods tend to overfit and produce worse results than\nNeural-Radiance-Field-based methods. To address this issue, we propose Blur\nAgnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling\ncapacities such that a 3D-consistent and high quality scene can be\nreconstructed despite image-wise blur. Specifically, we model blur by\nestimating per-pixel convolution kernels from a Blur Proposal Network (BPN).\nBPN is designed to consider spatial, color, and depth variations of the scene\nto maximize modeling capacity. Additionally, BPN also proposes a\nquality-assessing mask, which indicates regions where blur occur. Finally, we\nintroduce a coarse-to-fine kernel optimization scheme; this optimization scheme\nis fast and avoids sub-optimal solutions due to a sparse point cloud\ninitialization, which often occurs when we apply Structure-from-Motion on\nblurry images. We demonstrate that BAGS achieves photorealistic renderings\nunder various challenging blur conditions and imaging geometry, while\nsignificantly improving upon existing approaches.\n","authors":["Cheng Peng","Yutao Tang","Yifan Zhou","Nengyu Wang","Xijun Liu","Deming Li","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2403.04926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16276v1","updated":"2024-03-24T19:50:49Z","published":"2024-03-24T19:50:49Z","title":"AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary\n  Alignment for Temporal Referential Dialogue","summary":"  In everyday communication, humans frequently use speech and gestures to refer\nto specific areas or objects, a process known as Referential Dialogue (RD).\nWhile prior studies have investigated RD through Large Language Models (LLMs)\nor Large Multimodal Models (LMMs) in static contexts, the exploration of\nTemporal Referential Dialogue (TRD) within audio-visual media remains limited.\nTwo primary challenges hinder progress in this field: (1) the absence of\ncomprehensive, untrimmed audio-visual video datasets with precise temporal\nannotations, and (2) the need for methods to integrate complex temporal\nauditory and visual cues effectively. To address these challenges, we introduce\na novel framework to generate PU-VALOR, an extensive audio-visual dataset\ncomprising over 114,000 untrimmed videos with accurate temporal demarcations.\nWe also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI)\nthat ensures the temporal alignment of audio-visual information. Additionally,\nwe develop the A5-222K dataset, encompassing more than 200,000 audio-text\npairings, to facilitate the audio and text alignments. Our experiments\ndemonstrate that AVicuna can effectively handle TRD in audio-visual videos and\nachieve state-of-the-art performance on various audio-visual video\nunderstanding tasks, particularly in untrimmed videos. We further investigate\nthe optimal audio-interleaving rate for interleaved audio-visual inputs, which\nmaximizes performance on the Audio-Visual Event Dense Localization task.\n","authors":["Yunlong Tang","Daiki Shimada","Jing Bi","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.00915v3","updated":"2024-03-24T19:39:00Z","published":"2022-09-02T09:50:31Z","title":"Detection of diabetic retinopathy using longitudinal self-supervised\n  learning","summary":"  Longitudinal imaging is able to capture both static anatomical structures and\ndynamic changes in disease progression towards earlier and better\npatient-specific pathology management. However, conventional approaches for\ndetecting diabetic retinopathy (DR) rarely take advantage of longitudinal\ninformation to improve DR analysis. In this work, we investigate the benefit of\nexploiting self-supervised learning with a longitudinal nature for DR diagnosis\npurposes. We compare different longitudinal self-supervised learning (LSSL)\nmethods to model the disease progression from longitudinal retinal color fundus\nphotographs (CFP) to detect early DR severity changes using a pair of\nconsecutive exams. The experiments were conducted on a longitudinal DR\nscreening dataset with or without those trained encoders (LSSL) acting as a\nlongitudinal pretext task. Results achieve an AUC of 0.875 for the baseline\n(model trained from scratch) and an AUC of 0.96 (95% CI: 0.9593-0.9655 DeLong\ntest) with a p-value < 2.2e-16 on early fusion using a simple ResNet alike\narchitecture with frozen LSSL weights, suggesting that the LSSL latent space\nenables to encode the dynamic of DR progression.\n","authors":["Rachid Zeghlache","Pierre-Henri Conze","Mostafa El Habib Daho","Ramin Tadayoni","Pascal Massin","Béatrice Cochener","Gwenolé Quellec","Mathieu Lamard"],"pdf_url":"https://arxiv.org/pdf/2209.00915v3.pdf","comment":"Accepted preprint for presentation at MICCAI-OMIA"},{"id":"http://arxiv.org/abs/2403.16272v1","updated":"2024-03-24T19:34:33Z","published":"2024-03-24T19:34:33Z","title":"L-MAE: Longitudinal masked auto-encoder with time and severity-aware\n  encoding for diabetic retinopathy progression prediction","summary":"  Pre-training strategies based on self-supervised learning (SSL) have proven\nto be effective pretext tasks for many downstream tasks in computer vision. Due\nto the significant disparity between medical and natural images, the\napplication of typical SSL is not straightforward in medical imaging.\nAdditionally, those pretext tasks often lack context, which is critical for\ncomputer-aided clinical decision support. In this paper, we developed a\nlongitudinal masked auto-encoder (MAE) based on the well-known\nTransformer-based MAE. In particular, we explored the importance of time-aware\nposition embedding as well as disease progression-aware masking. Taking into\naccount the time between examinations instead of just scheduling them offers\nthe benefit of capturing temporal changes and trends. The masking strategy, for\nits part, evolves during follow-up to better capture pathological changes,\nensuring a more accurate assessment of disease progression. Using OPHDIAT, a\nlarge follow-up screening dataset targeting diabetic retinopathy (DR), we\nevaluated the pre-trained weights on a longitudinal task, which is to predict\nthe severity label of the next visit within 3 years based on the past time\nseries examinations. Our results demonstrated the relevancy of both time-aware\nposition embedding and masking strategies based on disease progression\nknowledge. Compared to popular baseline models and standard longitudinal\nTransformers, these simple yet effective extensions significantly enhance the\npredictive ability of deep classification models.\n","authors":["Rachid Zeghlache","Pierre-Henri Conze","Mostafa El Habib Daho","Yihao Li","Alireza Rezaei","Hugo Le Boité","Ramin Tadayoni","Pascal Massin","Béatrice Cochener","Ikram Brahim","Gwenolé Quellec","Mathieu Lamard"],"pdf_url":"https://arxiv.org/pdf/2403.16272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16271v1","updated":"2024-03-24T19:32:39Z","published":"2024-03-24T19:32:39Z","title":"Object Detectors in the Open Environment:Challenges, Solutions, and\n  Outlook","summary":"  With the emergence of foundation models, deep learning-based object detectors\nhave shown practical usability in closed set scenarios. However, for real-world\ntasks, object detectors often operate in open environments, where crucial\nfactors (\\eg, data distribution, objective) that influence model learning are\noften changing. The dynamic and intricate nature of the open environment poses\nnovel and formidable challenges to object detectors. Unfortunately, current\nresearch on object detectors in open environments lacks a comprehensive\nanalysis of their distinctive characteristics, challenges, and corresponding\nsolutions, which hinders their secure deployment in critical real-world\nscenarios. This paper aims to bridge this gap by conducting a comprehensive\nreview and analysis of object detectors in open environments. We initially\nidentified limitations of key structural components within the existing\ndetection pipeline and propose the open environment object detector challenge\nframework that includes four quadrants (\\ie, out-of-domain, out-of-category,\nrobust learning, and incremental learning) based on the dimensions of the data\n/ target changes. For each quadrant of challenges in the proposed framework, we\npresent a detailed description and systematic analysis of the overarching goals\nand core difficulties, systematically review the corresponding solutions, and\nbenchmark their performance over multiple widely adopted datasets. In addition,\nwe engage in a discussion of open problems and potential avenues for future\nresearch. This paper aims to provide a fresh, comprehensive, and systematic\nunderstanding of the challenges and solutions associated with open-environment\nobject detectors, thus catalyzing the development of more solid applications in\nreal-world scenarios.\n","authors":["Siyuan Liang","Wei Wang","Ruoyu Chen","Aishan Liu","Boxi Wu","Ee-Chien Chang","Xiaochun Cao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.16271v1.pdf","comment":"32 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.16270v1","updated":"2024-03-24T19:22:15Z","published":"2024-03-24T19:22:15Z","title":"Constricting Normal Latent Space for Anomaly Detection with Normal-only\n  Training Data","summary":"  In order to devise an anomaly detection model using only normal training\ndata, an autoencoder (AE) is typically trained to reconstruct the data. As a\nresult, the AE can extract normal representations in its latent space. During\ntest time, since AE is not trained using real anomalies, it is expected to\npoorly reconstruct the anomalous data. However, several researchers have\nobserved that it is not the case. In this work, we propose to limit the\nreconstruction capability of AE by introducing a novel latent constriction\nloss, which is added to the existing reconstruction loss. By using our method,\nno extra computational cost is added to the AE during test time. Evaluations\nusing three video anomaly detection benchmark datasets, i.e., Ped2, Avenue, and\nShanghaiTech, demonstrate the effectiveness of our method in limiting the\nreconstruction capability of AE, which leads to a better anomaly detection\nmodel.\n","authors":["Marcella Astrid","Muhammad Zaigham Zaheer","Seung-Ik Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16270v1.pdf","comment":"ICLR Workshop 2024 (PML4LRS)"},{"id":"http://arxiv.org/abs/2303.12054v3","updated":"2024-03-24T19:16:21Z","published":"2023-03-21T17:45:38Z","title":"Influencer Backdoor Attack on Semantic Segmentation","summary":"  When a small number of poisoned samples are injected into the training\ndataset of a deep neural network, the network can be induced to exhibit\nmalicious behavior during inferences, which poses potential threats to\nreal-world applications. While they have been intensively studied in\nclassification, backdoor attacks on semantic segmentation have been largely\noverlooked. Unlike classification, semantic segmentation aims to classify every\npixel within a given image. In this work, we explore backdoor attacks on\nsegmentation models to misclassify all pixels of a victim class by injecting a\nspecific trigger on non-victim pixels during inferences, which is dubbed\nInfluencer Backdoor Attack (IBA). IBA is expected to maintain the\nclassification accuracy of non-victim pixels and mislead classifications of all\nvictim pixels in every single inference and could be easily applied to\nreal-world scenes. Based on the context aggregation ability of segmentation\nmodels, we proposed a simple, yet effective, Nearest-Neighbor trigger injection\nstrategy. We also introduce an innovative Pixel Random Labeling strategy which\nmaintains optimal performance even when the trigger is placed far from the\nvictim pixels. Our extensive experiments reveal that current segmentation\nmodels do suffer from backdoor attacks, demonstrate IBA real-world\napplicability, and show that our proposed techniques can further increase\nattack performance.\n","authors":["Haoheng Lan","Jindong Gu","Philip Torr","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.12054v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16263v1","updated":"2024-03-24T18:53:57Z","published":"2024-03-24T18:53:57Z","title":"Emotion Recognition from the perspective of Activity Recognition","summary":"  Applications of an efficient emotion recognition system can be found in\nseveral domains such as medicine, driver fatigue surveillance, social robotics,\nand human-computer interaction. Appraising human emotional states, behaviors,\nand reactions displayed in real-world settings can be accomplished using latent\ncontinuous dimensions. Continuous dimensional models of human affect, such as\nthose based on valence and arousal are more accurate in describing a broad\nrange of spontaneous everyday emotions than more traditional models of discrete\nstereotypical emotion categories (e.g. happiness, surprise). Most of the prior\nwork on estimating valence and arousal considers laboratory settings and acted\ndata. But, for emotion recognition systems to be deployed and integrated into\nreal-world mobile and computing devices, we need to consider data collected in\nthe world. Action recognition is a domain of Computer Vision that involves\ncapturing complementary information on appearance from still frames and motion\nbetween frames. In this paper, we treat emotion recognition from the\nperspective of action recognition by exploring the application of deep learning\narchitectures specifically designed for action recognition, for continuous\naffect recognition. We propose a novel three-stream end-to-end deep learning\nregression pipeline with an attention mechanism, which is an ensemble design\nbased on sub-modules of multiple state-of-the-art action recognition systems.\nThe pipeline constitutes a novel data pre-processing approach with a spatial\nself-attention mechanism to extract keyframes. The optical flow of\nhigh-attention regions of the face is extracted to capture temporal context.\nAFEW-VA in-the-wild dataset has been used to conduct comparative experiments.\nQuantitative analysis shows that the proposed model outperforms multiple\nstandard baselines of both emotion recognition and action recognition models.\n","authors":["Savinay Nagendra","Prapti Panigrahi"],"pdf_url":"https://arxiv.org/pdf/2403.16263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16260v1","updated":"2024-03-24T18:43:04Z","published":"2024-03-24T18:43:04Z","title":"Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble","summary":"  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)\nfeature representation field scale in determining the efficacy of models in OOD\ndetection. Consequently, the adoption of model ensembles has emerged as a\nprominent strategy to augment this feature representation field, capitalizing\non anticipated model diversity.\n  However, our introduction of novel qualitative and quantitative model\nensemble evaluation methods, specifically Loss Basin/Barrier Visualization and\nthe Self-Coupling Index, reveals a critical drawback in existing ensemble\nmethods. We find that these methods incorporate weights that are\naffine-transformable, exhibiting limited variability and thus failing to\nachieve the desired diversity in feature representation.\n  To address this limitation, we elevate the dimensions of traditional model\nensembles, incorporating various factors such as different weight\ninitializations, data holdout, etc., into distinct supervision tasks. This\ninnovative approach, termed Multi-Comprehension (MC) Ensemble, leverages\ndiverse training tasks to generate distinct comprehensions of the data and\nlabels, thereby extending the feature representation field.\n  Our experimental results demonstrate the superior performance of the MC\nEnsemble strategy in OOD detection compared to both the naive Deep Ensemble\nmethod and a standalone model of comparable size. This underscores the\neffectiveness of our proposed approach in enhancing the model's capability to\ndetect instances outside its training distribution.\n","authors":["Chenhui Xu","Fuxun Yu","Zirui Xu","Nathan Inkawhich","Xiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16258v1","updated":"2024-03-24T18:33:16Z","published":"2024-03-24T18:33:16Z","title":"Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated\n  Synthesis","summary":"  While replacing Gaussian decoders with a conditional diffusion model enhances\nthe perceptual quality of reconstructions in neural image compression, their\nlack of inductive bias for image data restricts their ability to achieve\nstate-of-the-art perceptual levels. To address this limitation, we adopt a\nnon-isotropic diffusion model at the decoder side. This model imposes an\ninductive bias aimed at distinguishing between frequency contents, thereby\nfacilitating the generation of high-quality images. Moreover, our framework is\nequipped with a novel entropy model that accurately models the probability\ndistribution of latent representation by exploiting spatio-channel correlations\nin latent space, while accelerating the entropy decoding step. This\nchannel-wise entropy model leverages both local and global spatial contexts\nwithin each channel chunk. The global spatial context is built upon the\nTransformer, which is specifically designed for image compression tasks. The\ndesigned Transformer employs a Laplacian-shaped positional encoding, the\nlearnable parameters of which are adaptively adjusted for each channel cluster.\nOur experiments demonstrate that our proposed framework yields better\nperceptual quality compared to cutting-edge generative-based codecs, and the\nproposed entropy model contributes to notable bitrate savings.\n","authors":["Atefeh Khoshkhahtinat","Ali Zafari","Piyush M. Mehta","Nasser M. Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2403.16258v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16257v1","updated":"2024-03-24T18:33:15Z","published":"2024-03-24T18:33:15Z","title":"Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal\n  Contrastive Learning via Local Token Unlearning","summary":"  Multimodal contrastive learning has emerged as a powerful paradigm for\nbuilding high-quality features using the complementary strengths of various\ndata modalities. However, the open nature of such systems inadvertently\nincreases the possibility of backdoor attacks. These attacks subtly embed\nmalicious behaviors within the model during training, which can be activated by\nspecific triggers in the inference phase, posing significant security risks.\nDespite existing countermeasures through fine-tuning that reduce the adverse\nimpacts of such attacks, these defenses often degrade the clean accuracy and\nnecessitate the construction of extensive clean training pairs. In this paper,\nwe explore the possibility of a less-cost defense from the perspective of model\nunlearning, that is, whether the model can be made to quickly \\textbf{u}nlearn\n\\textbf{b}ackdoor \\textbf{t}hreats (UBT) by constructing a small set of\npoisoned samples. Specifically, we strengthen the backdoor shortcuts to\ndiscover suspicious samples through overfitting training prioritized by weak\nsimilarity samples. Building on the initial identification of suspicious\nsamples, we introduce an innovative token-based localized forgetting training\nregime. This technique specifically targets the poisoned aspects of the model,\napplying a focused effort to unlearn the backdoor associations and trying not\nto damage the integrity of the overall model. Experimental results show that\nour method not only ensures a minimal success rate for attacks, but also\npreserves the model's high clean accuracy.\n","authors":["Siyuan Liang","Kuanrong Liu","Jiajun Gong","Jiawei Liang","Yuan Xun","Ee-Chien Chang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.16257v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.06912v3","updated":"2024-03-24T18:10:11Z","published":"2024-03-11T17:02:11Z","title":"DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with\n  Global-Local Depth Normalization","summary":"  Radiance fields have demonstrated impressive performance in synthesizing\nnovel views from sparse input views, yet prevailing methods suffer from high\ntraining costs and slow inference speed. This paper introduces DNGaussian, a\ndepth-regularized framework based on 3D Gaussian radiance fields, offering\nreal-time and high-quality few-shot novel view synthesis at low costs. Our\nmotivation stems from the highly efficient representation and surprising\nquality of the recent 3D Gaussian Splatting, despite it will encounter a\ngeometry degradation when input views decrease. In the Gaussian radiance\nfields, we find this degradation in scene geometry primarily lined to the\npositioning of Gaussian primitives and can be mitigated by depth constraint.\nConsequently, we propose a Hard and Soft Depth Regularization to restore\naccurate scene geometry under coarse monocular depth supervision while\nmaintaining a fine-grained color appearance. To further refine detailed\ngeometry reshaping, we introduce Global-Local Depth Normalization, enhancing\nthe focus on small local depth changes. Extensive experiments on LLFF, DTU, and\nBlender datasets demonstrate that DNGaussian outperforms state-of-the-art\nmethods, achieving comparable or better results with significantly reduced\nmemory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$\nfaster rendering speed.\n","authors":["Jiahe Li","Jiawei Zhang","Xiao Bai","Jin Zheng","Xin Ning","Jun Zhou","Lin Gu"],"pdf_url":"https://arxiv.org/pdf/2403.06912v3.pdf","comment":"Accepted at CVPR 2024. Project page:\n  https://fictionarry.github.io/DNGaussian/"},{"id":"http://arxiv.org/abs/2306.12547v2","updated":"2024-03-24T18:00:57Z","published":"2023-06-21T20:21:15Z","title":"DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free\n  2D-3D Matching","summary":"  Matching 2D keypoints in an image to a sparse 3D point cloud of the scene\nwithout requiring visual descriptors has garnered increased interest due to its\nlow memory requirements, inherent privacy preservation, and reduced need for\nexpensive 3D model maintenance compared to visual descriptor-based methods.\nHowever, existing algorithms often compromise on performance, resulting in a\nsignificant deterioration compared to their descriptor-based counterparts. In\nthis paper, we introduce DGC-GNN, a novel algorithm that employs a\nglobal-to-local Graph Neural Network (GNN) that progressively exploits\ngeometric and color cues to represent keypoints, thereby improving matching\naccuracy. Our procedure encodes both Euclidean and angular relations at a\ncoarse level, forming the geometric embedding to guide the point matching. We\nevaluate DGC-GNN on both indoor and outdoor datasets, demonstrating that it not\nonly doubles the accuracy of the state-of-the-art visual descriptor-free\nalgorithm but also substantially narrows the performance gap between\ndescriptor-based and descriptor-free methods.\n","authors":["Shuzhe Wang","Juho Kannala","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2306.12547v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16246v1","updated":"2024-03-24T17:33:22Z","published":"2024-03-24T17:33:22Z","title":"Partially Blinded Unlearning: Class Unlearning for Deep Networks a\n  Bayesian Perspective","summary":"  In order to adhere to regulatory standards governing individual data privacy\nand safety, machine learning models must systematically eliminate information\nderived from specific subsets of a user's training data that can no longer be\nutilized. The emerging discipline of Machine Unlearning has arisen as a pivotal\narea of research, facilitating the process of selectively discarding\ninformation designated to specific sets or classes of data from a pre-trained\nmodel, thereby eliminating the necessity for extensive retraining from scratch.\nThe principal aim of this study is to formulate a methodology tailored for the\npurposeful elimination of information linked to a specific class of data from a\npre-trained classification network. This intentional removal is crafted to\ndegrade the model's performance specifically concerning the unlearned data\nclass while concurrently minimizing any detrimental impacts on the model's\nperformance in other classes. To achieve this goal, we frame the class\nunlearning problem from a Bayesian perspective, which yields a loss function\nthat minimizes the log-likelihood associated with the unlearned data with a\nstability regularization in parameter space. This stability regularization\nincorporates Mohalanobis distance with respect to the Fisher Information matrix\nand $l_2$ distance from the pre-trained model parameters. Our novel approach,\ntermed \\textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing\nstate-of-the-art class unlearning methods, demonstrating superior\neffectiveness. Notably, PBU achieves this efficacy without requiring awareness\nof the entire training dataset but only to the unlearned data points, marking a\ndistinctive feature of its performance.\n","authors":["Subhodip Panda","Shashwat Sourav","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2403.16246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04364v2","updated":"2024-03-24T17:22:35Z","published":"2023-12-07T15:35:42Z","title":"DemoCaricature: Democratising Caricature Generation with a Rough Sketch","summary":"  In this paper, we democratise caricature generation, empowering individuals\nto effortlessly craft personalised caricatures with just a photo and a\nconceptual sketch. Our objective is to strike a delicate balance between\nabstraction and identity, while preserving the creativity and subjectivity\ninherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing\nalongside single-image personalisation, selectively applying nuanced edits to\ncross-attention layers for a seamless merge of identity and style.\nAdditionally, we propose Random Mask Reconstruction to enhance robustness,\ndirecting the model to focus on distinctive identity and style features.\nCrucially, our aim is not to replace artists but to eliminate accessibility\nbarriers, allowing enthusiasts to engage in the artistry.\n","authors":["Dar-Yen Chen","Ayan Kumar Bhunia","Subhadeep Koley","Aneeshan Sain","Pinaki Nath Chowdhury","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2312.04364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16244v1","updated":"2024-03-24T17:21:32Z","published":"2024-03-24T17:21:32Z","title":"On the Equivalency, Substitutability, and Flexibility of Synthetic Data","summary":"  We study, from an empirical standpoint, the efficacy of synthetic data in\nreal-world scenarios. Leveraging synthetic data for training perception models\nhas become a key strategy embraced by the community due to its efficiency,\nscalability, perfect annotations, and low costs. Despite proven advantages, few\nstudies put their stress on how to efficiently generate synthetic datasets to\nsolve real-world problems and to what extent synthetic data can reduce the\neffort for real-world data collection. To answer the questions, we\nsystematically investigate several interesting properties of synthetic data --\nthe equivalency of synthetic data to real-world data, the substitutability of\nsynthetic data for real data, and the flexibility of synthetic data generators\nto close up domain gaps. Leveraging the M3Act synthetic data generator, we\nconduct experiments on DanceTrack and MOT17. Our results suggest that synthetic\ndata not only enhances model performance but also demonstrates substitutability\nfor real data, with 60% to 80% replacement without performance loss. In\naddition, our study of the impact of synthetic data distributions on downstream\nperformance reveals the importance of flexible data generators in narrowing\ndomain gaps for improved model adaptability.\n","authors":["Che-Jui Chang","Danrui Li","Seonghyeon Moon","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2403.16244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12188v2","updated":"2024-03-24T17:19:14Z","published":"2023-09-21T15:54:33Z","title":"SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on\n  Scene Graphs","summary":"  Object rearrangement is pivotal in robotic-environment interactions,\nrepresenting a significant capability in embodied AI. In this paper, we present\nSG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme\nwith a scene graph as the scene representation. Unlike previous methods that\nrely on either known goal priors or zero-shot large models, SG-Bot exemplifies\nlightweight, real-time, and user-controllable characteristics, seamlessly\nblending the consideration of commonsense knowledge with automatic generation\ncapabilities. SG-Bot employs a three-fold procedure--observation, imagination,\nand execution--to adeptly address the task. Initially, objects are discerned\nand extracted from a cluttered scene during the observation. These objects are\nfirst coarsely organized and depicted within a scene graph, guided by either\ncommonsense or user-defined criteria. Then, this scene graph subsequently\ninforms a generative model, which forms a fine-grained goal scene considering\nthe shape information from the initial scene and object semantics. Finally, for\nexecution, the initial and envisioned goal scenes are matched to formulate\nrobotic action policies. Experimental results demonstrate that SG-Bot\noutperforms competitors by a large margin.\n","authors":["Guangyao Zhai","Xiaoni Cai","Dianye Huang","Yan Di","Fabian Manhardt","Federico Tombari","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.12188v2.pdf","comment":"ICRA 2024 accepted. Project website:\n  https://sites.google.com/view/sg-bot"},{"id":"http://arxiv.org/abs/2403.14119v2","updated":"2024-03-24T17:16:53Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16242v1","updated":"2024-03-24T17:13:46Z","published":"2024-03-24T17:13:46Z","title":"Adversarially Masked Video Consistency for Unsupervised Domain\n  Adaptation","summary":"  We study the problem of unsupervised domain adaptation for egocentric videos.\nWe propose a transformer-based model to learn class-discriminative and\ndomain-invariant feature representations. It consists of two novel designs. The\nfirst module is called Generative Adversarial Domain Alignment Network with the\naim of learning domain-invariant representations. It simultaneously learns a\nmask generator and a domain-invariant encoder in an adversarial way. The\ndomain-invariant encoder is trained to minimize the distance between the source\nand target domain. The masking generator, conversely, aims at producing\nchallenging masks by maximizing the domain distance. The second is a Masked\nConsistency Learning module to learn class-discriminative representations. It\nenforces the prediction consistency between the masked target videos and their\nfull forms. To better evaluate the effectiveness of domain adaptation methods,\nwe construct a more challenging benchmark for egocentric videos, U-Ego4D. Our\nmethod achieves state-of-the-art performance on the Epic-Kitchen and the\nproposed U-Ego4D benchmark.\n","authors":["Xiaoyu Zhu","Junwei Liang","Po-Yao Huang","Alex Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2403.16242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16240v1","updated":"2024-03-24T17:12:13Z","published":"2024-03-24T17:12:13Z","title":"Low Rank Groupwise Deformations for Motion Tracking in Cardiac Cine MRI","summary":"  Diffeomorphic image registration is a commonly used method to deform one\nimage to resemble another. While warping a single image to another is useful,\nit can be advantageous to warp multiple images simultaneously, such as in\ntracking the motion of the heart across a sequence of images. In this paper,\nour objective is to propose a novel method capable of registering a group or\nsequence of images to a target image, resulting in registered images that\nappear identical and therefore have a low rank. Moreover, we aim for these\nregistered images to closely resemble the target image. Through experimental\nevidence, we will demonstrate our method's superior efficacy in producing\nlow-rank groupwise deformations compared to other state-of-the-art approaches.\n","authors":["Sean Rendell","Jinming Duan"],"pdf_url":"https://arxiv.org/pdf/2403.16240v1.pdf","comment":"A thesis submitted to the University of Birmingham for MSc Degree"},{"id":"http://arxiv.org/abs/2403.16227v1","updated":"2024-03-24T16:41:50Z","published":"2024-03-24T16:41:50Z","title":"Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for\n  Intelligent Transportation System","summary":"  Infrared and visible image fusion (IVF) plays an important role in\nintelligent transportation system (ITS). The early works predominantly focus on\nboosting the visual appeal of the fused result, and only several recent\napproaches have tried to combine the high-level vision task with IVF. However,\nthey prioritize the design of cascaded structure to seek unified suitable\nfeatures and fit different tasks. Thus, they tend to typically bias toward to\nreconstructing raw pixels without considering the significance of semantic\nfeatures. Therefore, we propose a novel prior semantic guided image fusion\nmethod based on the dual-modality strategy, improving the performance of IVF in\nITS. Specifically, to explore the independent significant semantic of each\nmodality, we first design two parallel semantic segmentation branches with a\nrefined feature adaptive-modulation (RFaM) mechanism. RFaM can perceive the\nfeatures that are semantically distinct enough in each semantic segmentation\nbranch. Then, two pilot experiments based on the two branches are conducted to\ncapture the significant prior semantic of two images, which then is applied to\nguide the fusion task in the integration of semantic segmentation branches and\nfusion branches. In addition, to aggregate both high-level semantics and\nimpressive visual effects, we further investigate the frequency response of the\nprior semantics, and propose a multi-level representation-adaptive fusion\n(MRaF) module to explicitly integrate the low-frequent prior semantic with the\nhigh-frequent details. Extensive experiments on two public datasets demonstrate\nthe superiority of our method over the state-of-the-art image fusion\napproaches, in terms of either the visual appeal or the high-level semantics.\n","authors":["Jing Li","Lu Bai","Bin Yang","Chang Li","Lingfei Ma","Lixin Cui","Edwin R. Hancock"],"pdf_url":"https://arxiv.org/pdf/2403.16227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09457v2","updated":"2024-03-24T16:37:04Z","published":"2023-10-14T00:32:11Z","title":"UCM-Net: A Lightweight and Efficient Solution for Skin Lesion\n  Segmentation using MLP and CNN","summary":"  Skin cancer is a significant public health problem, and computer-aided\ndiagnosis can help to prevent and treat it. A crucial step for computer-aided\ndiagnosis is accurately segmenting skin lesions in images, which allows for\nlesion detection, classification, and analysis. However, this task is\nchallenging due to the diverse characteristics of lesions, such as appearance,\nshape, size, color, texture, and location, as well as image quality issues like\nnoise, artifacts, and occlusions. Deep learning models have recently been\napplied to skin lesion segmentation, but they have high parameter counts and\ncomputational demands, making them unsuitable for mobile health applications.\nTo address this challenge, we propose UCM-Net, a novel, efficient, and\nlightweight solution that integrates Multi-Layer Perceptions (MLP) and\nConvolutional Neural Networks (CNN). Unlike conventional UNet architectures,\nour UCMNet-Block reduces parameter overhead and enhances UCM-Net's learning\ncapabilities, leading to robust segmentation performance. We validate UCM-Net's\ncompetitiveness through extensive experiments on PH2, isic2017 and isic2018\ndatasets. Remarkably, UCM-Net has less than 50KB parameters and less than 0.05\nGiga-Operations Per Second (GLOPs), setting a new possible standard for\nefficiency in skin lesion segmentation. The source code will be publicly\navailable.\n","authors":["Chunyu Yuan","Dongfang Zhao","Sos S. Agaian"],"pdf_url":"https://arxiv.org/pdf/2310.09457v2.pdf","comment":"17 pages, under review"},{"id":"http://arxiv.org/abs/2403.16224v1","updated":"2024-03-24T16:34:47Z","published":"2024-03-24T16:34:47Z","title":"Inverse Rendering of Glossy Objects via the Neural Plenoptic Function\n  and Radiance Fields","summary":"  Inverse rendering aims at recovering both geometry and materials of objects.\nIt provides a more compatible reconstruction for conventional rendering\nengines, compared with the neural radiance fields (NeRFs). On the other hand,\nexisting NeRF-based inverse rendering methods cannot handle glossy objects with\nlocal light interactions well, as they typically oversimplify the illumination\nas a 2D environmental map, which assumes infinite lights only. Observing the\nsuperiority of NeRFs in recovering radiance fields, we propose a novel 5D\nNeural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more\naccurate lighting-object interactions can be formulated via the rendering\nequation. We also design a material-aware cone sampling strategy to efficiently\nintegrate lights inside the BRDF lobes with the help of pre-filtered radiance\nfields. Our method has two stages: the geometry of the target object and the\npre-filtered environmental radiance fields are reconstructed in the first\nstage, and materials of the target object are estimated in the second stage\nwith the proposed NeP and material-aware cone sampling strategy. Extensive\nexperiments on the proposed real-world and synthetic datasets demonstrate that\nour method can reconstruct high-fidelity geometry/materials of challenging\nglossy objects with complex lighting interactions from nearby objects. Project\nwebpage: https://whyy.site/paper/nep\n","authors":["Haoyuan Wang","Wenbo Hu","Lei Zhu","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2403.16224v1.pdf","comment":"CVPR 2024 paper. Project webpage https://whyy.site/paper/nep"},{"id":"http://arxiv.org/abs/2403.16221v1","updated":"2024-03-24T16:29:50Z","published":"2024-03-24T16:29:50Z","title":"Exemplar-Free Class Incremental Learning via Incremental Representation","summary":"  Exemplar-Free Class Incremental Learning (efCIL) aims to continuously\nincorporate the knowledge from new classes while retaining previously learned\ninformation, without storing any old-class exemplars (i.e., samples). For this\npurpose, various efCIL methods have been proposed over the past few years,\ngenerally with elaborately constructed old pseudo-features, increasing the\ndifficulty of model development and interpretation. In contrast, we propose a\n\\textbf{simple Incremental Representation (IR) framework} for efCIL without\nconstructing old pseudo-features. IR utilizes dataset augmentation to cover a\nsuitable feature space and prevents the model from forgetting by using a single\nL2 space maintenance loss. We discard the transient classifier trained on each\none of the sequence tasks and instead replace it with a 1-near-neighbor\nclassifier for inference, ensuring the representation is incrementally updated\nduring CIL. Extensive experiments demonstrate that our proposed IR achieves\ncomparable performance while significantly preventing the model from forgetting\non CIFAR100, TinyImageNet, and ImageNetSubset datasets.\n","authors":["Libo Huang","Zhulin An","Yan Zeng","Chuanguang Yang","Xinqiang Yu","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16212v1","updated":"2024-03-24T16:11:27Z","published":"2024-03-24T16:11:27Z","title":"Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI\n  Classification in Alzheimer Diagnosis","summary":"  Exploring the application of deep learning technologies in the field of\nmedical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique\nperspective for observing and diagnosing complex neurodegenerative diseases\nsuch as Alzheimer Disease (AD). With advancements in deep learning,\nparticularly in Convolutional Neural Networks (CNNs) and the Xception network\narchitecture, we are now able to analyze and classify vast amounts of MRI data\nwith unprecedented accuracy. The progress of this technology not only enhances\nour understanding of brain structural changes but also opens up new avenues for\nmonitoring disease progression through non-invasive means and potentially\nallows for precise diagnosis in the early stages of the disease.\n  This study aims to classify MRI images using deep learning models to identify\ndifferent stages of Alzheimer Disease through a series of innovative data\nprocessing and model construction steps. Our experimental results show that the\ndeep learning framework based on the Xception model achieved a 99.6% accuracy\nrate in the multi-class MRI image classification task, demonstrating its\npotential application value in assistive diagnosis. Future research will focus\non expanding the dataset, improving model interpretability, and clinical\nvalidation to further promote the application of deep learning technology in\nthe medical field, with the hope of bringing earlier diagnosis and more\npersonalized treatment plans to Alzheimer Disease patients.\n","authors":["Shaojie Li","Haichen Qu","Xinqi Dong","Bo Dang","Hengyi Zang","Yulu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.16212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16210v1","updated":"2024-03-24T16:09:21Z","published":"2024-03-24T16:09:21Z","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One\n  Tri-Plane","summary":"  We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting.\n","authors":["Han Yan","Yang Li","Zhennan Wu","Shenzhou Chen","Weixuan Sun","Taizhang Shang","Weizhe Liu","Tian Chen","Xiaqiang Dai","Chao Ma","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.16210v1.pdf","comment":"Video: https://youtu.be/lRn-HqyCrLI"},{"id":"http://arxiv.org/abs/2403.16209v1","updated":"2024-03-24T16:08:10Z","published":"2024-03-24T16:08:10Z","title":"Image Captioning in news report scenario","summary":"  Image captioning strives to generate pertinent captions for specified images,\nsituating itself at the crossroads of Computer Vision (CV) and Natural Language\nProcessing (NLP). This endeavor is of paramount importance with far-reaching\napplications in recommendation systems, news outlets, social media, and beyond.\nParticularly within the realm of news reporting, captions are expected to\nencompass detailed information, such as the identities of celebrities captured\nin the images. However, much of the existing body of work primarily centers\naround understanding scenes and actions. In this paper, we explore the realm of\nimage captioning specifically tailored for celebrity photographs, illustrating\nits broad potential for enhancing news industry practices. This exploration\naims to augment automated news content generation, thereby facilitating a more\nnuanced dissemination of information. Our endeavor shows a broader horizon,\nenriching the narrative in news reporting through a more intuitive image\ncaptioning framework.\n","authors":["Tianrui Liu","Qi Cai","Changxin Xu","Zhanxin Zhou","Jize Xiong","Yuxin Qiao","Tsungwei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16209v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16207v1","updated":"2024-03-24T16:03:27Z","published":"2024-03-24T16:03:27Z","title":"Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing","summary":"  Deducing the 3D face from a skull is an essential but challenging task in\nforensic science and archaeology. Existing methods for automated facial\nreconstruction yield inaccurate results, suffering from the non-determinative\nnature of the problem that a skull with a sparse set of tissue depth cannot\nfully determine the skinned face. Additionally, their texture-less results\nrequire further post-processing stages to achieve a photo-realistic appearance.\nThis paper proposes an end-to-end 3D face reconstruction and exploration tool,\nproviding textured 3D faces for reference. With the help of state-of-the-art\ntext-to-image diffusion models and image-based facial reconstruction\ntechniques, we generate an initial reference 3D face, whose biological profile\naligns with the given skull. We then adapt these initial faces to meet the\nstatistical expectations of extruded anatomical landmarks on the skull through\nan optimization process. The joint statistical distribution of tissue depths is\nlearned on a small set of anatomical landmarks on the skull. To support further\nadjustment, we propose an efficient face adaptation tool to assist users in\ntuning tissue depths, either globally or at local regions, while observing\nplausible visual feedback. Experiments conducted on a real skull-face dataset\ndemonstrated the effectiveness of our proposed pipeline in terms of\nreconstruction accuracy, diversity, and stability.\n","authors":["Yongqing Liang","Congyi Zhang","Junli Zhao","Wenping Wang","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2403.16207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16205v1","updated":"2024-03-24T15:58:48Z","published":"2024-03-24T15:58:48Z","title":"Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown\n  Domains","summary":"  This paper presents an innovative framework designed to train an image\ndeblurring algorithm tailored to a specific camera device. This algorithm works\nby transforming a blurry input image, which is challenging to deblur, into\nanother blurry image that is more amenable to deblurring. The transformation\nprocess, from one blurry state to another, leverages unpaired data consisting\nof sharp and blurry images captured by the target camera device. Learning this\nblur-to-blur transformation is inherently simpler than direct blur-to-sharp\nconversion, as it primarily involves modifying blur patterns rather than the\nintricate task of reconstructing fine image details. The efficacy of the\nproposed approach has been demonstrated through comprehensive experiments on\nvarious benchmarks, where it significantly outperforms state-of-the-art methods\nboth quantitatively and qualitatively. Our code and data are available at\nhttps://zero1778.github.io/blur2blur/\n","authors":["Bang-Dang Pham","Phong Tran","Anh Tran","Cuong Pham","Rang Nguyen","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2403.16205v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16202v1","updated":"2024-03-24T15:51:17Z","published":"2024-03-24T15:51:17Z","title":"FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial\n  Temporal Network","summary":"  Biometric authentication, which utilizes contactless features, such as\nforehead patterns, has become increasingly important for identity verification\nand access management. The proposed method is based on learning a 3D\nspatio-spatial temporal convolution to create detailed pictures of forehead\npatterns. We introduce a new CNN model called the Forehead Spatio-Spatial\nTemporal Network (FH-SSTNet), which utilizes a 3D CNN architecture with triplet\nloss to capture distinguishing features. We enhance the model's discrimination\ncapability using Arcloss in the network's head. Experimentation on the Forehead\nCreases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates\nthe superior performance of FH-SSTNet compared to existing methods and\npre-trained CNNs like ResNet50, especially for forehead-based user\nverification. The results demonstrate the superior performance of FH-SSTNet for\nforehead-based user verification, confirming its effectiveness in identity\nauthentication.\n","authors":["Geetanjali Sharma","Gaurav Jaswal","Aditya Nigam","Raghavendra Ramachandra"],"pdf_url":"https://arxiv.org/pdf/2403.16202v1.pdf","comment":"6 pages, 5 Figure, IWBF conference"},{"id":"http://arxiv.org/abs/2403.16201v1","updated":"2024-03-24T15:48:29Z","published":"2024-03-24T15:48:29Z","title":"From Discrete to Continuous: Deep Fair Clustering With Transferable\n  Representations","summary":"  We consider the problem of deep fair clustering, which partitions data into\nclusters via the representations extracted by deep neural networks while hiding\nsensitive data attributes. To achieve fairness, existing methods present a\nvariety of fairness-related objective functions based on the group fairness\ncriterion. However, these works typically assume that the sensitive attributes\nare discrete and do not work for continuous sensitive variables, such as the\nproportion of the female population in an area. Besides, the potential of the\nrepresentations learned from clustering tasks to improve performance on other\ntasks is ignored by existing works. In light of these limitations, we propose a\nflexible deep fair clustering method that can handle discrete and continuous\nsensitive attributes simultaneously. Specifically, we design an information\nbottleneck style objective function to learn fair and clustering-friendly\nrepresentations. Furthermore, we explore for the first time the transferability\nof the extracted representations to other downstream tasks. Unlike existing\nworks, we impose fairness at the representation level, which could guarantee\nfairness for the transferred task regardless of clustering results. To verify\nthe effectiveness of the proposed method, we perform extensive experiments on\ndatasets with discrete and continuous sensitive attributes, demonstrating the\nadvantage of our method in comparison with state-of-the-art methods.\n","authors":["Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16198v1","updated":"2024-03-24T15:39:52Z","published":"2024-03-24T15:39:52Z","title":"Diffusion Model is a Good Pose Estimator from 3D RF-Vision","summary":"  Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs\nhuman sensing using RF signals that penetrate obstacles without revealing\nprivacy (e.g., facial information). Recently, mmWave radar has emerged as a\npromising RF-vision sensor, providing radar point clouds by processing RF\nsignals. However, the mmWave radar has a limited resolution with severe noise,\nleading to inaccurate and inconsistent human pose estimation. This work\nproposes mmDiff, a novel diffusion-based pose estimator tailored for noisy\nradar data. Our approach aims to provide reliable guidance as conditions to\ndiffusion models. Two key challenges are addressed by mmDiff: (1)\nmiss-detection of parts of human bodies, which is addressed by a module that\nisolates feature extraction from different body parts, and (2) signal\ninconsistency due to environmental interference, which is tackled by\nincorporating prior knowledge of body structure and motion. Several modules are\ndesigned to achieve these goals, whose features work as the conditions for the\nsubsequent diffusion model, eliminating the miss-detection and instability of\nHPE based on RF-vision. Extensive experiments demonstrate that mmDiff\noutperforms existing methods significantly, achieving state-of-the-art\nperformances on public datasets.\n","authors":["Junqiao Fan","Jianfei Yang","Yuecong Xu","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.16198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13356v2","updated":"2024-03-24T15:30:46Z","published":"2023-08-25T13:05:06Z","title":"CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions\n  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and\n  Classification from Ultrasound Images","summary":"  Undoubtedly breast cancer identifies itself as one of the most widespread and\nterrifying cancers across the globe. Millions of women are getting affected\neach year from it. Breast cancer remains the major one for being the reason of\nlargest number of demise of women. In the recent time of research, Medical\nImage Computing and Processing has been playing a significant role for\ndetecting and classifying breast cancers from ultrasound images and mammograms,\nalong with the celestial touch of deep neural networks. In this research, we\nfocused mostly on our rigorous implementations and iterative result analysis of\ndifferent cutting-edge modified versions of EfficientNet architectures namely\nEfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,\nnamed as CEIMVEN. We utilized transfer learning approach here for using the\npre-trained models of EfficientNet versions. We activated the hyper-parameter\ntuning procedures, added fully connected layers, discarded the unprecedented\noutliers and recorded the accuracy results from our custom modified\nEfficientNet architectures. Our deep learning model training approach was\nrelated to both identifying the cancer affected areas with region of interest\n(ROI) techniques and multiple classifications (benign, malignant and normal).\nThe approximate testing accuracies we got from the modified versions of\nEfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,\nb5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-\n99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong\npotentials of deep learning approach for the successful detection and\nclassification of breast cancers from the ultrasound images at a very early\nstage. The code for this research is available here:\nhttps://github.com/ac005sheekar/CEIMVEN-Cutting-Edge-Implementation-of-Modified-EfficientNet-V1-V2-for-BreastCancer-Detection.\n","authors":["Sheekar Banerjee","Md. Kamrul Hasan Monir"],"pdf_url":"https://arxiv.org/pdf/2308.13356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17417v2","updated":"2024-03-24T15:26:11Z","published":"2024-02-27T11:17:46Z","title":"CARZero: Cross-Attention Alignment for Radiology Zero-Shot\n  Classification","summary":"  The advancement of Zero-Shot Learning in the medical domain has been driven\nforward by using pre-trained models on large-scale image-text pairs, focusing\non image-text alignment. However, existing methods primarily rely on cosine\nsimilarity for alignment, which may not fully capture the complex relationship\nbetween medical images and reports. To address this gap, we introduce a novel\napproach called Cross-Attention Alignment for Radiology Zero-Shot\nClassification (CARZero). Our approach innovatively leverages cross-attention\nmechanisms to process image and report features, creating a Similarity\nRepresentation that more accurately reflects the intricate relationships in\nmedical semantics. This representation is then linearly projected to form an\nimage-text similarity matrix for cross-modality alignment. Additionally,\nrecognizing the pivotal role of prompt selection in zero-shot learning, CARZero\nincorporates a Large Language Model-based prompt alignment strategy. This\nstrategy standardizes diverse diagnostic expressions into a unified format for\nboth training and inference phases, overcoming the challenges of manual prompt\ndesign. Our approach is simple yet effective, demonstrating state-of-the-art\nperformance in zero-shot classification on five official chest radiograph\ndiagnostic test sets, including remarkable results on datasets with long-tail\ndistributions of rare diseases. This achievement is attributed to our new\nimage-text alignment strategy, which effectively addresses the complex\nrelationship between medical images and reports. Code and models are available\nat https://github.com/laihaoran/CARZero.\n","authors":["Haoran Lai","Qingsong Yao","Zihang Jiang","Rongsheng Wang","Zhiyang He","Xiaodong Tao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.17417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16194v1","updated":"2024-03-24T15:24:04Z","published":"2024-03-24T15:24:04Z","title":"Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised\n  Landmark Discovery","summary":"  Unsupervised landmarks discovery (ULD) for an object category is a\nchallenging computer vision problem. In pursuit of developing a robust ULD\nframework, we explore the potential of a recent paradigm of self-supervised\nlearning algorithms, known as diffusion models. Some recent works have shown\nthat these models implicitly contain important correspondence cues. Towards\nharnessing the potential of diffusion models for the ULD task, we make the\nfollowing core contributions. First, we propose a ZeroShot ULD baseline based\non simple clustering of random pixel locations with nearest neighbour matching.\nIt delivers better results than existing ULD methods. Second, motivated by the\nZeroShot performance, we develop a ULD algorithm based on diffusion features\nusing self-training and clustering which also outperforms prior methods by\nnotable margins. Third, we introduce a new proxy task based on generating\nlatent pose codes and also propose a two-stage clustering mechanism to\nfacilitate effective pseudo-labeling, resulting in a significant performance\nimprovement. Overall, our approach consistently outperforms state-of-the-art\nmethods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by\nsignificant margins.\n","authors":["Siddharth Tourani","Ahmed Alwheibi","Arif Mahmood","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2403.16194v1.pdf","comment":"Accepted in CVPR 2024"},{"id":"http://arxiv.org/abs/2112.06502v2","updated":"2024-03-24T15:12:20Z","published":"2021-12-13T09:24:45Z","title":"DGL-GAN: Discriminator Guided Learning for GAN Compression","summary":"  Generative Adversarial Networks (GANs) with high computation costs, e.g.,\nBigGAN and StyleGAN2, have achieved remarkable results in synthesizing\nhigh-resolution images from random noise. Reducing the computation cost of GANs\nwhile keeping generating photo-realistic images is a challenging field. In this\nwork, we propose a novel yet simple {\\bf D}iscriminator {\\bf G}uided {\\bf\nL}earning approach for compressing vanilla {\\bf GAN}, dubbed {\\bf DGL-GAN}.\nMotivated by the phenomenon that the teacher discriminator may contain some\nmeaningful information about both real images and fake images, we merely\ntransfer the knowledge from the teacher discriminator via the adversarial\ninteraction between the teacher discriminator and the student generator. We\napply DGL-GAN to compress the two most representative large-scale vanilla GANs,\ni.e., StyleGAN2 and BigGAN. Experiments show that DGL-GAN achieves\nstate-of-the-art (SOTA) results on both StyleGAN2 and BigGAN. Moreover, DGL-GAN\nis also effective in boosting the performance of original uncompressed GANs.\nOriginal uncompressed StyleGAN2 boosted with DGL-GAN achieves FID 2.65 on FFHQ,\nwhich achieves a new state-of-the-art performance. Code and models are\navailable at \\url{https://github.com/yuesongtian/DGL-GAN}\n","authors":["Yuesong Tian","Li Shen","Xiang Tian","Dacheng Tao","Zhifeng Li","Wei Liu","Yaowu Chen"],"pdf_url":"https://arxiv.org/pdf/2112.06502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14729v3","updated":"2024-03-24T15:11:38Z","published":"2023-10-23T09:05:18Z","title":"MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D\n  diffusion","summary":"  We introduce Multi-view Ancestral Sampling (MAS), a method for 3D motion\ngeneration, using 2D diffusion models that were trained on motions obtained\nfrom in-the-wild videos. As such, MAS opens opportunities to exciting and\ndiverse fields of motion previously under-explored as 3D data is scarce and\nhard to collect. MAS works by simultaneously denoising multiple 2D motion\nsequences representing different views of the same 3D motion. It ensures\nconsistency across all views at each diffusion step by combining the individual\ngenerations into a unified 3D sequence, and projecting it back to the original\nviews. We demonstrate MAS on 2D pose data acquired from videos depicting\nprofessional basketball maneuvers, rhythmic gymnastic performances featuring a\nball apparatus, and horse races. In each of these domains, 3D motion capture is\narduous, and yet, MAS generates diverse and realistic 3D sequences. Unlike the\nScore Distillation approach, which optimizes each sample by repeatedly applying\nsmall fixes, our method uses a sampling process that was constructed for the\ndiffusion framework. As we demonstrate, MAS avoids common issues such as\nout-of-domain sampling and mode-collapse. https://guytevet.github.io/mas-page/\n","authors":["Roy Kapon","Guy Tevet","Daniel Cohen-Or","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2310.14729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16188v1","updated":"2024-03-24T15:10:22Z","published":"2024-03-24T15:10:22Z","title":"Cross-domain Multi-modal Few-shot Object Detection via Rich Text","summary":"  Cross-modal feature extraction and integration have led to steady performance\nimprovements in few-shot learning tasks due to generating richer features.\nHowever, existing multi-modal object detection (MM-OD) methods degrade when\nfacing significant domain-shift and are sample insufficient. We hypothesize\nthat rich text information could more effectively help the model to build a\nknowledge relationship between the vision instance and its language description\nand can help mitigate domain shift. Specifically, we study the Cross-Domain\nfew-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based\nmulti-modal few-shot object detection method that utilizes rich text semantic\ninformation as an auxiliary modality to achieve domain adaptation in the\ncontext of FSOD. Our proposed network contains (i) a multi-modal feature\naggregation module that aligns the vision and language support feature\nembeddings and (ii) a rich text semantic rectify module that utilizes\nbidirectional text feature generation to reinforce multi-modal feature\nalignment and thus to enhance the model's language understanding capability. We\nevaluate our model on common standard cross-domain object detection datasets\nand demonstrate that our approach considerably outperforms existing FSOD\nmethods.\n","authors":["Zeyu Shangguan","Daniel Seita","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2403.16188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16184v1","updated":"2024-03-24T15:02:24Z","published":"2024-03-24T15:02:24Z","title":"Improving Scene Graph Generation with Relation Words' Debiasing in\n  Vision-Language Models","summary":"  Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between various\nobjects. However, this complexity and diversity in SGG also leads to\nunderrepresentation, where part of test triplets are rare or even unseen during\ntraining, resulting in imprecise predictions. To tackle this, we propose using\nthe SGG models with pretrained vision-language models (VLMs) to enhance\nrepresentation. However, due to the gap between the pretraining and SGG,\ndirectly ensembling the pretrained VLMs leads to severe biases across relation\nwords. Thus, we introduce LM Estimation to approximate the words' distribution\nunderlies in the pretraining language sets, and then use the distribution for\ndebiasing. After that, we ensemble VLMs with SGG models to enhance\nrepresentation. Considering that each model may represent better at different\nsamples, we use a certainty-aware indicator to score each sample and\ndynamically adjust the ensemble weights. Our method effectively addresses the\nwords biases, enhances SGG's representation, and achieve markable performance\nenhancements. It is training-free and integrates well with existing SGG models.\n","authors":["Yuxuan Wang","Xiaoyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16182v1","updated":"2024-03-24T15:00:44Z","published":"2024-03-24T15:00:44Z","title":"EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric\n  View of Procedural Activities in Real World","summary":"  Being able to map the activities of others into one's own point of view is\none fundamental human skill even from a very early age. Taking a step toward\nunderstanding this human ability, we introduce EgoExoLearn, a large-scale\ndataset that emulates the human demonstration following process, in which\nindividuals record egocentric videos as they execute tasks guided by\ndemonstration videos. Focusing on the potential applications in daily\nassistance and professional support, EgoExoLearn contains egocentric and\ndemonstration video data spanning 120 hours captured in daily life scenarios\nand specialized laboratories. Along with the videos we record high-quality gaze\ndata and provide detailed multimodal annotations, formulating a playground for\nmodeling the human ability to bridge asynchronous procedural actions from\ndifferent viewpoints. To this end, we present benchmarks such as cross-view\nassociation, cross-view action planning, and cross-view referenced skill\nassessment, along with detailed analysis. We expect EgoExoLearn can serve as an\nimportant resource for bridging the actions across views, thus paving the way\nfor creating AI agents capable of seamlessly learning by observing humans in\nthe real world. Code and data can be found at:\nhttps://github.com/OpenGVLab/EgoExoLearn\n","authors":["Yifei Huang","Guo Chen","Jilan Xu","Mingfang Zhang","Lijin Yang","Baoqi Pei","Hongjie Zhang","Lu Dong","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.16182v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01696v2","updated":"2024-03-24T14:50:42Z","published":"2023-12-04T07:35:02Z","title":"BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection","summary":"  Recently, the rise of query-based Transformer decoders is reshaping\ncamera-based 3D object detection. These query-based decoders are surpassing the\ntraditional dense BEV (Bird's Eye View)-based methods. However, we argue that\ndense BEV frameworks remain important due to their outstanding abilities in\ndepth estimation and object localization, depicting 3D scenes accurately and\ncomprehensively. This paper aims to address the drawbacks of the existing dense\nBEV-based 3D object detectors by introducing our proposed enhanced components,\nincluding a CRF-modulated depth estimation module enforcing object-level\nconsistencies, a long-term temporal aggregation module with extended receptive\nfields, and a two-stage object decoder combining perspective techniques with\nCRF-modulated depth embedding. These enhancements lead to a \"modernized\" dense\nBEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms\nboth BEV-based and query-based frameworks under various settings, achieving a\nstate-of-the-art result of 64.2 NDS on the nuScenes test set. Code will be\navailable at \\url{https://github.com/woxihuanjiangguo/BEVNeXt}.\n","authors":["Zhenxin Li","Shiyi Lan","Jose M. Alvarez","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2312.01696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16175v1","updated":"2024-03-24T14:35:06Z","published":"2024-03-24T14:35:06Z","title":"Enhancing MRI-Based Classification of Alzheimer's Disease with\n  Explainable 3D Hybrid Compact Convolutional Transformers","summary":"  Alzheimer's disease (AD), characterized by progressive cognitive decline and\nmemory loss, presents a formidable global health challenge, underscoring the\ncritical importance of early and precise diagnosis for timely interventions and\nenhanced patient outcomes. While MRI scans provide valuable insights into brain\nstructures, traditional analysis methods often struggle to discern intricate 3D\npatterns crucial for AD identification. Addressing this challenge, we introduce\nan alternative end-to-end deep learning model, the 3D Hybrid Compact\nConvolutional Transformers 3D (HCCT). By synergistically combining\nconvolutional neural networks (CNNs) and vision transformers (ViTs), the 3D\nHCCT adeptly captures both local features and long-range relationships within\n3D MRI scans. Extensive evaluations on prominent AD benchmark dataset, ADNI,\ndemonstrate the 3D HCCT's superior performance, surpassing state of the art CNN\nand transformer-based methods in classification accuracy. Its robust\ngeneralization capability and interpretability marks a significant stride in AD\nclassification from 3D MRI scans, promising more accurate and reliable\ndiagnoses for improved patient care and superior clinical outcomes.\n","authors":["Arindam Majee","Avisek Gupta","Sourav Raha","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2403.16175v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16172v1","updated":"2024-03-24T14:29:41Z","published":"2024-03-24T14:29:41Z","title":"Fusion of Minutia Cylinder Codes and Minutia Patch Embeddings for Latent\n  Fingerprint Recognition","summary":"  Latent fingerprints are one of the most widely used forensic evidence by law\nenforcement agencies. However, latent recognition performance is far from the\nexemplary performance of sensor fingerprint recognition due to deformations and\nartifacts within these images. In this study, we propose a fusion based local\nmatching approach towards latent fingerprint recognition. Recent latent\nrecognition studies typically relied on local descriptor generation methods, in\nwhich either handcrafted minutiae features or deep neural network features are\nextracted around a minutia of interest, in the latent recognition process.\nProposed approach would integrate these handcrafted features with a recently\nproposed deep neural network embedding features in a multi-stage fusion\napproach to significantly improve latent recognition results. Effectiveness of\nthe proposed approach has been shown on several public and private data sets.\nAs demonstrated in our experimental results, proposed method improves rank-1\nidentification accuracy by considerably for real-world datasets when compared\nto either the single usage of these features or existing state-of-the-art\nmethods in the literature.\n","authors":["Yusuf Artan","Bensu Alkan Semiz"],"pdf_url":"https://arxiv.org/pdf/2403.16172v1.pdf","comment":"9 pages,7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.16169v1","updated":"2024-03-24T14:24:13Z","published":"2024-03-24T14:24:13Z","title":"Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method","summary":"  Gaze plays a crucial role in revealing human attention and intention,\nshedding light on the cognitive processes behind human actions. The integration\nof gaze guidance with the dynamics of hand-object interactions boosts the\naccuracy of human motion prediction. However, the lack of datasets that capture\nthe intricate relationship and consistency among gaze, hand, and object\nmovements remains a substantial hurdle. In this paper, we introduce the first\nGaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task\nfor synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,\nfeatures simultaneous 3D modeling of gaze, hand, and object interactions,\ncomprising 479 sequences with an average duration of 19.1 seconds, 812\nsub-sequences, and 33 objects of various sizes. We propose a hierarchical\nframework centered on a gaze-guided hand-object interaction diffusion model,\nnamed GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions\ninto spatial-temporal features and goal pose conditions at different levels of\ninformation granularity. During the diffusion phase, two gaze-conditioned\ndiffusion models are stacked to simplify the complex synthesis of hand-object\nmotions. Here, the object motion diffusion model generates sequences of object\nmotions based on gaze conditions, while the hand motion diffusion model\nproduces hand motions based on the generated object motion. To improve\nfine-grained goal pose alignment, we introduce a Spherical Gaussian constraint\nto guide the denoising step. In the subsequent post-diffusion phase, we\noptimize the generated hand motions using contact consistency. Our extensive\nexperiments highlight the uniqueness of our dataset and the effectiveness of\nour approach.\n","authors":["Jie Tian","Lingxiao Yang","Ran Ji","Yuexin Ma","Lan Xu","Jingyi Yu","Ye Shi","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08924v2","updated":"2024-03-24T14:23:59Z","published":"2023-12-14T13:31:01Z","title":"Training-free Zero-shot Composed Image Retrieval with Local Concept\n  Reranking","summary":"  Composed image retrieval attempts to retrieve an image of interest from\ngallery images through a composed query of a reference image and its\ncorresponding modified text. It has recently attracted attention due to the\ncollaboration of information-rich images and concise language to precisely\nexpress the requirements of target images. Most current composed image\nretrieval methods follow a supervised learning approach to training on a costly\ntriplet dataset composed of a reference image, modified text, and a\ncorresponding target image. To avoid difficult to-obtain labeled triplet\ntraining data, zero-shot composed image retrieval (ZS-CIR) has been introduced,\nwhich aims to retrieve the target image by learning from image-text pairs\n(self-supervised triplets), without the need for human-labeled triplets.\nHowever, this self-supervised triplet learning approach is computationally less\neffective and less understandable as it assumes the interaction between image\nand text is conducted with implicit query embedding without explicit semantical\ninterpretation. In this work, we present a new training-free zero-shot composed\nimage retrieval method which translates the query into explicit\nhuman-understandable text. This helps improve model learning efficiency to\nenhance the generalization capacity of foundation models. Further, we introduce\na Local Concept Re-ranking (LCR) mechanism to focus on discriminative local\ninformation extracted from the modified instructions. Extensive experiments on\nfour ZS-CIR benchmarks show that our method achieves comparable performances to\nthat of the state of-the-art triplet training based methods, but significantly\noutperforms other training-free methods on the open domain datasets (CIRR,\nCIRCO and COCO), as well as the fashion domain dataset (FashionIQ).\n","authors":["Shitong Sun","Fanghua Ye","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2312.08924v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.16167v1","updated":"2024-03-24T14:21:06Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16161v1","updated":"2024-03-24T14:02:25Z","published":"2024-03-24T14:02:25Z","title":"Towards Online Real-Time Memory-based Video Inpainting Transformers","summary":"  Video inpainting tasks have seen significant improvements in recent years\nwith the rise of deep neural networks and, in particular, vision transformers.\nAlthough these models show promising reconstruction quality and temporal\nconsistency, they are still unsuitable for live videos, one of the last steps\nto make them completely convincing and usable. The main limitations are that\nthese state-of-the-art models inpaint using the whole video (offline\nprocessing) and show an insufficient frame rate. In our approach, we propose a\nframework to adapt existing inpainting transformers to these constraints by\nmemorizing and refining redundant computations while maintaining a decent\ninpainting quality. Using this framework with some of the most recent\ninpainting models, we show great online results with a consistent throughput\nabove 20 frames per second. The code and pretrained models will be made\navailable upon acceptance.\n","authors":["Guillaume Thiry","Hao Tang","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2403.16161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04784v2","updated":"2024-03-24T13:56:31Z","published":"2023-12-08T01:53:06Z","title":"Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular\n  Video","summary":"  Recent advancements in 3D avatar generation excel with multi-view supervision\nfor photorealistic models. However, monocular counterparts lag in quality\ndespite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is\na fully-differentiable pipeline that learns high-fidelity 3D human avatars from\njust a single RGB video. A pose-conditioned deformable NeRF is optimized to\nvolumetrically represent a human subject in canonical T-pose. The canonical\nrepresentation is then leveraged to efficiently associate neural textures using\n2D-3D correspondences. This enables the separation of diffused color generation\nand lighting correction branches that jointly compose an RGB prediction. The\ndesign allows to control intermediate results for human pose, body shape,\ntexture, and lighting with text prompts. An image-conditioned diffusion model\nthereby helps to animate appearance and pose of the 3D avatar to create video\nsequences with previously unseen human motion. Extensive experiments show that\nReCaLaB outperforms previous monocular approaches in terms of image quality for\nimage synthesis tasks. Moreover, natural language offers an intuitive user\ninterface for creative manipulation of 3D human avatars.\n","authors":["Yuchen Rao","Eduardo Perez Pellitero","Benjamin Busam","Yiren Zhou","Jifei Song"],"pdf_url":"https://arxiv.org/pdf/2312.04784v2.pdf","comment":"Video link: https://youtu.be/Oz83z1es2J4"},{"id":"http://arxiv.org/abs/2403.13352v2","updated":"2024-03-24T13:45:42Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Haoran Feng","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16146v1","updated":"2024-03-24T13:36:23Z","published":"2024-03-24T13:36:23Z","title":"Realtime Robust Shape Estimation of Deformable Linear Object","summary":"  Realtime shape estimation of continuum objects and manipulators is essential\nfor developing accurate planning and control paradigms. The existing methods\nthat create dense point clouds from camera images, and/or use distinguishable\nmarkers on a deformable body have limitations in realtime tracking of large\ncontinuum objects/manipulators. The physical occlusion of markers can often\ncompromise accurate shape estimation. We propose a robust method to estimate\nthe shape of linear deformable objects in realtime using scattered and\nunordered key points. By utilizing a robust probability-based labeling\nalgorithm, our approach identifies the true order of the detected key points\nand then reconstructs the shape using piecewise spline interpolation. The\napproach only relies on knowing the number of the key points and the interval\nbetween two neighboring points. We demonstrate the robustness of the method\nwhen key points are partially occluded. The proposed method is also integrated\ninto a simulation in Unity for tracking the shape of a cable with a length of\n1m and a radius of 5mm. The simulation results show that our proposed approach\nachieves an average length error of 1.07% over the continuum's centerline and\nan average cross-section error of 2.11mm. The real-world experiments of\ntracking and estimating a heavy-load cable prove that the proposed approach is\nrobust under occlusion and complex entanglement scenarios.\n","authors":["Jiaming Zhang","Zhaomeng Zhang","Yihao Liu","Yaqian Chen","Amir Kheradmand","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2403.16146v1.pdf","comment":"This paper has been accepted to IEEE ICRA 2024 as a contributed paper"},{"id":"http://arxiv.org/abs/2403.16143v1","updated":"2024-03-24T13:31:31Z","published":"2024-03-24T13:31:31Z","title":"CFAT: Unleashing TriangularWindows for Image Super-resolution","summary":"  Transformer-based models have revolutionized the field of image\nsuper-resolution (SR) by harnessing their inherent ability to capture complex\ncontextual features. The overlapping rectangular shifted window technique used\nin transformer architecture nowadays is a common practice in super-resolution\nmodels to improve the quality and robustness of image upscaling. However, it\nsuffers from distortion at the boundaries and has limited unique shifting\nmodes. To overcome these weaknesses, we propose a non-overlapping triangular\nwindow technique that synchronously works with the rectangular one to mitigate\nboundary-level distortion and allows the model to access more unique sifting\nmodes. In this paper, we propose a Composite Fusion Attention Transformer\n(CFAT) that incorporates triangular-rectangular window-based local attention\nwith a channel-based global attention technique in image super-resolution. As a\nresult, CFAT enables attention mechanisms to be activated on more image pixels\nand captures long-range, multi-scale features to improve SR performance. The\nextensive experimental results and ablation study demonstrate the effectiveness\nof CFAT in the SR domain. Our proposed model shows a significant 0.7 dB\nperformance improvement over other state-of-the-art SR architectures.\n","authors":["Abhisek Ray","Gaurav Kumar","Maheshkumar H. Kolekar"],"pdf_url":"https://arxiv.org/pdf/2403.16143v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16141v1","updated":"2024-03-24T13:27:49Z","published":"2024-03-24T13:27:49Z","title":"Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes","summary":"  Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic\nscenes often involve explicit modeling of scene dynamics. However, this\napproach faces challenges in modeling scene dynamics in urban environments,\nwhere moving objects of various categories and scales are present. In such\nsettings, it becomes crucial to effectively eliminate moving objects to\naccurately reconstruct static backgrounds. Our research introduces an\ninnovative method, termed here as Entity-NeRF, which combines the strengths of\nknowledge-based and statistical strategies. This approach utilizes entity-wise\nstatistics, leveraging entity segmentation and stationary entity classification\nthrough thing/stuff segmentation. To assess our methodology, we created an\nurban scene dataset masked with moving objects. Our comprehensive experiments\ndemonstrate that Entity-NeRF notably outperforms existing techniques in\nremoving moving objects and reconstructing static urban backgrounds, both\nquantitatively and qualitatively.\n","authors":["Takashi Otonari","Satoshi Ikehata","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2403.16141v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024), Project website:\n  https://otonari726.github.io/entitynerf/"},{"id":"http://arxiv.org/abs/2403.16131v1","updated":"2024-03-24T13:01:57Z","published":"2024-03-24T13:01:57Z","title":"Salience DETR: Enhancing Detection Transformer with Hierarchical\n  Salience Filtering Refinement","summary":"  DETR-like methods have significantly increased detection performance in an\nend-to-end manner. The mainstream two-stage frameworks of them perform dense\nself-attention and select a fraction of queries for sparse cross-attention,\nwhich is proven effective for improving performance but also introduces a heavy\ncomputational burden and high dependence on stable query selection. This paper\ndemonstrates that suboptimal two-stage selection strategies result in scale\nbias and redundancy due to the mismatch between selected queries and objects in\ntwo-stage initialization. To address these issues, we propose hierarchical\nsalience filtering refinement, which performs transformer encoding only on\nfiltered discriminative queries, for a better trade-off between computational\nefficiency and precision. The filtering process overcomes scale bias through a\nnovel scale-independent salience supervision. To compensate for the semantic\nmisalignment among queries, we introduce elaborate query refinement modules for\nstable two-stage initialization. Based on above improvements, the proposed\nSalience DETR achieves significant improvements of +4.0% AP, +0.2% AP, +4.4% AP\non three challenging task-specific detection datasets, as well as 49.2% AP on\nCOCO 2017 with less FLOPs. The code is available at\nhttps://github.com/xiuqhou/Salience-DETR.\n","authors":["Xiuquan Hou","Meiqin Liu","Senlin Zhang","Ping Wei","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16131v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09334v2","updated":"2024-03-24T13:00:54Z","published":"2024-03-14T12:22:54Z","title":"Video Editing via Factorized Diffusion Distillation","summary":"  We introduce Emu Video Edit (EVE), a model that establishes a new\nstate-of-the art in video editing without relying on any supervised video\nediting data. To develop EVE we separately train an image editing adapter and a\nvideo generation adapter, and attach both to the same text-to-image model.\nThen, to align the adapters towards video editing we introduce a new\nunsupervised distillation procedure, Factorized Diffusion Distillation. This\nprocedure distills knowledge from one or more teachers simultaneously, without\nany supervised data. We utilize this procedure to teach EVE to edit videos by\njointly distilling knowledge to (i) precisely edit each individual frame from\nthe image editing adapter, and (ii) ensure temporal consistency among the\nedited frames using the video generation adapter. Finally, to demonstrate the\npotential of our approach in unlocking other capabilities, we align additional\ncombinations of adapters\n","authors":["Uriel Singer","Amit Zohar","Yuval Kirstain","Shelly Sheynin","Adam Polyak","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2403.09334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16128v1","updated":"2024-03-24T12:55:50Z","published":"2024-03-24T12:55:50Z","title":"Enhancing Video Transformers for Action Understanding with VLM-aided\n  Training","summary":"  Owing to their ability to extract relevant spatio-temporal video embeddings,\nVision Transformers (ViTs) are currently the best performing models in video\naction understanding. However, their generalization over domains or datasets is\nsomewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated\nexceptional generalization performance, but are currently unable to process\nvideos. Consequently, they cannot extract spatio-temporal patterns that are\ncrucial for action understanding. In this paper, we propose the Four-tiered\nPrompts (FTP) framework that takes advantage of the complementary strengths of\nViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability\nbut improve the visual encodings to be more comprehensive and general by\naligning them with VLM outputs. The FTP framework adds four feature processors\nthat focus on specific aspects of human action in videos: action category,\naction components, action description, and context information. The VLMs are\nonly employed during training, and inference incurs a minimal computation cost.\nOur approach consistently yields state-of-the-art performance. For instance, we\nachieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on\nSomething-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.\n","authors":["Hui Lu","Hu Jian","Ronald Poppe","Albert Ali Salah"],"pdf_url":"https://arxiv.org/pdf/2403.16128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01099v2","updated":"2024-03-24T12:55:31Z","published":"2023-10-02T11:17:19Z","title":"HyMNet: a Multimodal Deep Learning System for Hypertension\n  Classification using Fundus Photographs and Cardiometabolic Risk Factors","summary":"  In recent years, deep learning has shown promise in predicting hypertension\n(HTN) from fundus images. However, most prior research has primarily focused on\nanalyzing a single type of data, which may not capture the full complexity of\nHTN risk. To address this limitation, this study introduces a multimodal deep\nlearning (MMDL) system, dubbed HyMNet, which combines fundus images and\ncardiometabolic risk factors, specifically age and gender, to improve\nhypertension detection capabilities. Our MMDL system uses RETFound, a\nfoundation model pre-trained on 1.6 million retinal images, for the fundus path\nand a fully connected neural network for the age and gender path. The two paths\nare jointly trained by concatenating the feature vectors from each path that\nare then fed into a fusion network. The system was trained on 5,016 retinal\nimages from 1,243 individuals collected from the Saudi Ministry of National\nGuard Health Affairs. The results show that the multimodal model that\nintegrates fundus images along with age and gender outperforms the unimodal\nsystem trained solely on fundus photographs, with an F1 score of 0.771 [0.747,\n0.796], and 0.745 [0.719, 0.772] for hypertension detection, respectively.\nAdditionally, we studied the effect underlying diabetes mellitus has on the\nmodel's predictive ability, concluding that diabetes is used as a confounding\nvariable for distinguishing hypertensive cases. Our code and model weights are\npublicly available at https://github.com/MohammedSB/HyMNet.\n","authors":["Mohammed Baharoon","Hessa Almatar","Reema Alduhayan","Tariq Aldebasi","Badr Alahmadi","Yahya Bokhari","Mohammed Alawad","Ahmed Almazroa","Abdulrhman Aljouie"],"pdf_url":"https://arxiv.org/pdf/2310.01099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12466v2","updated":"2024-03-24T12:42:25Z","published":"2024-03-19T05:50:48Z","title":"Few-shot Object Localization","summary":"  Existing object localization methods are tailored to locate a specific class\nof objects, relying on abundant labeled data for model optimization. However,\nin numerous real-world scenarios, acquiring large labeled data can be arduous,\nsignificantly constraining the broader application of localization models. To\nbridge this research gap, this paper proposes the novel task of Few-Shot Object\nLocalization (FSOL), which seeks to achieve precise localization with limited\nsamples available. This task achieves generalized object localization by\nleveraging a small number of labeled support samples to query the positional\ninformation of objects within corresponding images. To advance this research\nfield, we propose an innovative high-performance baseline model. Our model\nintegrates a dual-path feature augmentation module to enhance shape association\nand gradient differences between supports and query images, alongside a self\nquery module designed to explore the association between feature maps and query\nimages. Experimental results demonstrate a significant performance improvement\nof our approach in the FSOL task, establishing an efficient benchmark for\nfurther research. All codes and data are available at\nhttps://github.com/Ryh1218/FSOL.\n","authors":["Yunhan Ren","Bo Li","Chengyang Zhang","Yong Zhang","Baocai Yin"],"pdf_url":"https://arxiv.org/pdf/2403.12466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16124v1","updated":"2024-03-24T12:41:58Z","published":"2024-03-24T12:41:58Z","title":"Enhancing Visual Continual Learning with Language-Guided Supervision","summary":"  Continual learning (CL) aims to empower models to learn new tasks without\nforgetting previously acquired knowledge. Most prior works concentrate on the\ntechniques of architectures, replay data, regularization, \\etc. However, the\ncategory name of each class is largely neglected. Existing methods commonly\nutilize the one-hot labels and randomly initialize the classifier head. We\nargue that the scarce semantic information conveyed by the one-hot labels\nhampers the effective knowledge transfer across tasks. In this paper, we\nrevisit the role of the classifier head within the CL paradigm and replace the\nclassifier with semantic knowledge from pretrained language models (PLMs).\nSpecifically, we use PLMs to generate semantic targets for each class, which\nare frozen and serve as supervision signals during training. Such targets fully\nconsider the semantic correlation between all classes across tasks. Empirical\nstudies show that our approach mitigates forgetting by alleviating\nrepresentation drifting and facilitating knowledge transfer across tasks. The\nproposed method is simple to implement and can seamlessly be plugged into\nexisting methods with negligible adjustments. Extensive experiments based on\neleven mainstream baselines demonstrate the effectiveness and generalizability\nof our approach to various protocols. For example, under the class-incremental\nlearning setting on ImageNet-100, our method significantly improves the Top-1\naccuracy by 3.2\\% to 6.1\\% while reducing the forgetting rate by 2.6\\% to\n13.1\\%.\n","authors":["Bolin Ni","Hongbo Zhao","Chenghao Zhang","Ke Hu","Gaofeng Meng","Zhaoxiang Zhang","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.16124v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2401.05010v2","updated":"2024-03-24T12:32:06Z","published":"2024-01-10T08:56:02Z","title":"Less is More: A Closer Look at Semantic-based Few-Shot Learning","summary":"  Few-shot Learning aims to learn and distinguish new categories with a very\nlimited number of available images, presenting a significant challenge in the\nrealm of deep learning. Recent researchers have sought to leverage the\nadditional textual or linguistic information of these rare categories with a\npre-trained language model to facilitate learning, thus partially alleviating\nthe problem of insufficient supervision signals. However, the full potential of\nthe textual information and pre-trained language model have been underestimated\nin the few-shot learning till now, resulting in limited performance\nenhancements. To address this, we propose a simple but effective framework for\nfew-shot learning tasks, specifically designed to exploit the textual\ninformation and language model. In more detail, we explicitly exploit the\nzero-shot capability of the pre-trained language model with the learnable\nprompt. And we just add the visual feature with the textual feature for\ninference directly without the intricate designed fusion modules in previous\nworks. Additionally, we apply the self-ensemble and distillation to further\nenhance these components. Our extensive experiments conducted across four\nwidely used few-shot datasets demonstrate that our simple framework achieves\nimpressive results. Particularly noteworthy is its outstanding performance in\nthe 1-shot learning task, surpassing state-of-the-art methods by an average of\n3.0\\% in classification accuracy. \\footnote{We will make the source codes of\nthe proposed framework publicly available upon acceptance. }.\n","authors":["Chunpeng Zhou","Haishuai Wang","Xilu Yuan","Zhi Yu","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2401.05010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16116v1","updated":"2024-03-24T12:15:28Z","published":"2024-03-24T12:15:28Z","title":"Self-Supervised Multi-Frame Neural Scene Flow","summary":"  Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown\nremarkable adaptability in the context of large out-of-distribution autonomous\ndriving. Despite their success, the underlying reasons for their astonishing\ngeneralization capabilities remain unclear. Our research addresses this gap by\nexamining the generalization capabilities of NSFP through the lens of uniform\nstability, revealing that its performance is inversely proportional to the\nnumber of input point clouds. This finding sheds light on NSFP's effectiveness\nin handling large-scale point cloud scene flow estimation tasks. Motivated by\nsuch theoretical insights, we further explore the improvement of scene flow\nestimation by leveraging historical point clouds across multiple frames, which\ninherently increases the number of point clouds. Consequently, we propose a\nsimple and effective method for multi-frame point cloud scene flow estimation,\nalong with a theoretical evaluation of its generalization abilities. Our\nanalysis confirms that the proposed method maintains a limited generalization\nerror, suggesting that adding multiple frames to the scene flow optimization\nprocess does not detract from its generalizability. Extensive experimental\nresults on large-scale autonomous driving Waymo Open and Argoverse lidar\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.\n","authors":["Dongrui Liu","Daqi Liu","Xueqian Li","Sihao Lin","Hongwei xie","Bing Wang","Xiaojun Chang","Lei Chu"],"pdf_url":"https://arxiv.org/pdf/2403.16116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16112v1","updated":"2024-03-24T12:05:23Z","published":"2024-03-24T12:05:23Z","title":"Opportunities and challenges in the application of large artificial\n  intelligence models in radiology","summary":"  Influenced by ChatGPT, artificial intelligence (AI) large models have\nwitnessed a global upsurge in large model research and development. As people\nenjoy the convenience by this AI large model, more and more large models in\nsubdivided fields are gradually being proposed, especially large models in\nradiology imaging field. This article first introduces the development history\nof large models, technical details, workflow, working principles of multimodal\nlarge models and working principles of video generation large models. Secondly,\nwe summarize the latest research progress of AI large models in radiology\neducation, radiology report generation, applications of unimodal and multimodal\nradiology. Finally, this paper also summarizes some of the challenges of large\nAI models in radiology, with the aim of better promoting the rapid revolution\nin the field of radiography.\n","authors":["Liangrui Pan","Zhenyu Zhao","Ying Lu","Kewei Tang","Liyong Fu","Qingchun Liang","Shaoliang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.16112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18411v2","updated":"2024-03-24T12:04:11Z","published":"2024-02-28T15:31:45Z","title":"Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal\n  Transport","summary":"  Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images\nsharing the same category across diverse domains without relying on labeled\ndata. Prior approaches have typically decomposed the UCIR problem into two\ndistinct tasks: intra-domain representation learning and cross-domain feature\nalignment. However, these segregated strategies overlook the potential\nsynergies between these tasks. This paper introduces ProtoOT, a novel Optimal\nTransport formulation explicitly tailored for UCIR, which integrates\nintra-domain feature representation learning and cross-domain alignment into a\nunified framework. ProtoOT leverages the strengths of the K-means clustering\nmethod to effectively manage distribution imbalances inherent in UCIR. By\nutilizing K-means for generating initial prototypes and approximating class\nmarginal distributions, we modify the constraints in Optimal Transport\naccordingly, significantly enhancing its performance in UCIR scenarios.\nFurthermore, we incorporate contrastive learning into the ProtoOT framework to\nfurther improve representation learning. This encourages local semantic\nconsistency among features with similar semantics, while also explicitly\nenforcing separation between features and unmatched prototypes, thereby\nenhancing global discriminativeness. ProtoOT surpasses existing\nstate-of-the-art methods by a notable margin across benchmark datasets.\nNotably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%,\nand on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is\navailable at https://github.com/HCVLAB/ProtoOT.\n","authors":["Bin Li","Ye Shi","Qian Yu","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2402.18411v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.16111v1","updated":"2024-03-24T12:04:06Z","published":"2024-03-24T12:04:06Z","title":"EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing","summary":"  Current diffusion-based video editing primarily focuses on local editing\n(\\textit{e.g.,} object/background editing) or global style editing by utilizing\nvarious dense correspondences. However, these methods often fail to accurately\nedit the foreground and background simultaneously while preserving the original\nlayout. We find that the crux of the issue stems from the imprecise\ndistribution of attention weights across designated regions, including\ninaccurate text-to-attribute control and attention leakage. To tackle this\nissue, we introduce EVA, a \\textbf{zero-shot} and \\textbf{multi-attribute}\nvideo editing framework tailored for human-centric videos with complex motions.\nWe incorporate a Spatial-Temporal Layout-Guided Attention mechanism that\nleverages the intrinsic positive and negative correspondences of cross-frame\ndiffusion features. To avoid attention leakage, we utilize these\ncorrespondences to boost the attention scores of tokens within the same\nattribute across all video frames while limiting interactions between tokens of\ndifferent attributes in the self-attention layer. For precise text-to-attribute\nmanipulation, we use discrete text embeddings focused on specific layout areas\nwithin the cross-attention layer. Benefiting from the precise attention weight\ndistribution, EVA can be easily generalized to multi-object editing scenarios\nand achieves accurate identity mapping. Extensive experiments demonstrate EVA\nachieves state-of-the-art results in real-world scenarios. Full results are\nprovided at https://knightyxp.github.io/EVA/\n","authors":["Xiangpeng Yang","Linchao Zhu","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16111v1.pdf","comment":"Project page: https://knightyxp.github.io/EVA"},{"id":"http://arxiv.org/abs/2403.16095v1","updated":"2024-03-24T11:19:59Z","published":"2024-03-24T11:19:59Z","title":"CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D\n  Gaussian Field","summary":"  Recently neural radiance fields (NeRF) have been widely exploited as 3D\nrepresentations for dense simultaneous localization and mapping (SLAM). Despite\ntheir notable successes in surface modeling and novel view synthesis, existing\nNeRF-based methods are hindered by their computationally intensive and\ntime-consuming volume rendering pipeline. This paper presents an efficient\ndense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D\nGaussian field with high consistency and geometric stability. Through an\nin-depth analysis of Gaussian Splatting, we propose several techniques to\nconstruct a consistent and stable 3D Gaussian field suitable for tracking and\nmapping. Additionally, a novel depth uncertainty model is proposed to ensure\nthe selection of valuable Gaussian primitives during optimization, thereby\nimproving tracking efficiency and accuracy. Experiments on various datasets\ndemonstrate that CG-SLAM achieves superior tracking and mapping performance\nwith a notable tracking speed of up to 15 Hz. We will make our source code\npublicly available. Project page: https://zju3dv.github.io/cg-slam.\n","authors":["Jiarui Hu","Xianhao Chen","Boyin Feng","Guanglin Li","Liangjing Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16095v1.pdf","comment":"Project Page: https://zju3dv.github.io/cg-slam"},{"id":"http://arxiv.org/abs/2403.16092v1","updated":"2024-03-24T11:09:41Z","published":"2024-03-24T11:09:41Z","title":"Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap","summary":"  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different pre-training strategies.\nOur results show notable improvements in model robustness to simulated data,\neven improving real-world performance in some cases. Last, we delve into the\ncorrelation between the real-to-simulated gap and image reconstruction metrics,\nidentifying FID and LPIPS as strong indicators.\n","authors":["Carl Lindström","Georg Hess","Adam Lilja","Maryam Fatemi","Lars Hammarstrand","Christoffer Petersson","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2403.16092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03059v6","updated":"2024-03-24T10:29:46Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v6.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2402.12928v4","updated":"2024-03-24T10:06:59Z","published":"2024-02-20T11:28:50Z","title":"A Literature Review of Literature Reviews in Pattern Analysis and\n  Machine Intelligence","summary":"  By consolidating scattered knowledge, the literature review provides a\ncomprehensive understanding of the investigated topic. However, reading,\nconducting, or peer-reviewing review papers generally demands a significant\ninvestment of time and effort from researchers. To improve efficiency, this\npaper aims to provide a thorough review of reviews in the PAMI field from\ndiverse perspectives. First, this paper proposes several article-level,\nfield-normalized, and large language model-empowered bibliometric indicators to\nevaluate reviews. To facilitate this, a meta-data database dubbed RiPAMI, and a\ntopic dataset are constructed. Second, based on these indicators, the study\npresents comparative analyses of representative reviews, unveiling the\ncharacteristics of publications across various fields, periods, and journals.\nThe newly emerging AI-generated literature reviews are also appraised, and the\nobserved differences suggest that most AI-generated reviews still lag behind\nhuman-authored reviews in multiple aspects. Third, we briefly provide a\nsubjective evaluation of representative PAMI reviews and introduce a paper\nstructure-based typology of literature reviews. This typology may improve the\nclarity and effectiveness for scholars in reading and writing reviews, while\nalso serving as a guide for AI systems in generating well-organized reviews.\nFinally, this work offers insights into the current challenges of literature\nreviews and envisions future directions for their development.\n","authors":["Penghai Zhao","Xin Zhang","Ming-Ming Cheng","Jian Yang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.12928v4.pdf","comment":"IEEE version v1. [February 19, 2024] IEEE version v2 with typos\n  fixed. [February 23, 2024] IEEE version v3 with errors fixed. [February 29,\n  2024] IEEE version v4 with improved quaility. [February 29, 2024]"},{"id":"http://arxiv.org/abs/2403.16080v1","updated":"2024-03-24T10:06:40Z","published":"2024-03-24T10:06:40Z","title":"PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic\n  Human Modeling","summary":"  High-quality human reconstruction and photo-realistic rendering of a dynamic\nscene is a long-standing problem in computer vision and graphics. Despite\nconsiderable efforts invested in developing various capture systems and\nreconstruction algorithms, recent advancements still struggle with loose or\noversized clothing and overly complex poses. In part, this is due to the\nchallenges of acquiring high-quality human datasets. To facilitate the\ndevelopment of these fields, in this paper, we present PKU-DyMVHumans, a\nversatile human-centric dataset for high-fidelity reconstruction and rendering\nof dynamic human scenarios from dense multi-view videos. It comprises 8.2\nmillion frames captured by more than 56 synchronized cameras across diverse\nscenarios. These sequences comprise 32 human subjects across 45 different\nscenarios, each with a high-detailed appearance and realistic human motion.\nInspired by recent advancements in neural radiance field (NeRF)-based scene\nrepresentations, we carefully set up an off-the-shelf framework that is easy to\nprovide those state-of-the-art NeRF-based implementations and benchmark on\nPKU-DyMVHumans dataset. It is paving the way for various applications like\nfine-grained foreground/background decomposition, high-quality human\nreconstruction and photo-realistic novel view synthesis of a dynamic scene.\nExtensive studies are performed on the benchmark, demonstrating new\nobservations and challenges that emerge from using such high-fidelity dynamic\ndata. The dataset is available at: https://pku-dymvhumans.github.io.\n","authors":["Xiaoyun Zheng","Liwei Liao","Xufeng Li","Jianbo Jiao","Rongjie Wang","Feng Gao","Shiqi Wang","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13438v3","updated":"2024-03-24T10:06:25Z","published":"2024-03-18T17:38:29Z","title":"See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single\n  Image","summary":"  Humans can not only recognize and understand the world in its current state\nbut also envision future scenarios that extend beyond immediate perception. To\nresemble this profound human capacity, we introduce zero-shot task\nhallucination -- given a single RGB image of any scene comprising unknown\nenvironments and objects, our model can identify potential tasks and imagine\ntheir execution in a vivid narrative, realized as a video. We develop a modular\npipeline that progressively enhances scene decomposition, comprehension, and\nreconstruction, incorporating VLM for dynamic interaction and 3D motion\nplanning for object trajectories. Our model can discover diverse tasks, with\nthe generated task videos demonstrating realistic and compelling visual\noutcomes that are understandable by both machines and humans. Project Page:\nhttps://dannymcy.github.io/zeroshot_task_hallucination/\n","authors":["Chenyang Ma","Kai Lu","Ta-Ying Cheng","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2403.13438v3.pdf","comment":"Project Page: https://dannymcy.github.io/zeroshot_task_hallucination/"},{"id":"http://arxiv.org/abs/2310.19258v2","updated":"2024-03-24T09:32:51Z","published":"2023-10-30T04:04:02Z","title":"Improving Online Source-free Domain Adaptation for Object Detection by\n  Unsupervised Data Acquisition","summary":"  Effective object detection in mobile robots is challenged by deployment in\ndiverse and unfamiliar environments. Online Source-Free Domain Adaptation\n(O-SFDA) offers model adaptation using a stream of unlabeled data from a target\ndomain in online manner. However, not all captured frames contain information\nthat is beneficial for adaptation, particularly when there is a strong class\nimbalance. This paper introduces a novel approach to enhance O-SFDA for\nadaptive object detection in mobile robots via unsupervised data acquisition.\nOur methodology prioritizes the most informative unlabeled frames for inclusion\nin the online training process. Empirical evaluation on a real-world dataset\nreveals that our method outperforms existing state-of-the-art O-SFDA\ntechniques, demonstrating the viability of unsupervised data acquisition for\nimproving adaptive object detection in mobile robots.\n","authors":["Xiangyu Shi","Yanyuan Qiao","Qi Wu","Lingqiao Liu","Feras Dayoub"],"pdf_url":"https://arxiv.org/pdf/2310.19258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07347v2","updated":"2024-03-24T09:22:13Z","published":"2024-03-12T06:07:29Z","title":"Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic\n  Architecture","summary":"  Video Motion Magnification (VMM) aims to reveal subtle and imperceptible\nmotion information of objects in the macroscopic world. Prior methods directly\nmodel the motion field from the Eulerian perspective by Representation Learning\nthat separates shape and texture or Multi-domain Learning from phase\nfluctuations. Inspired by the frequency spectrum, we observe that the\nlow-frequency components with stable energy always possess spatial structure\nand less noise, making them suitable for modeling the subtle motion field. To\nthis end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion\nMagnification with a Multi-level Isomorphic Architecture to capture multi-level\nhigh-frequency details and a stable low-frequency structure (motion field) in\nvideo space. Since high-frequency details and subtle motions are susceptible to\ninformation degradation due to their inherent subtlety and unavoidable external\ninterference from noise, we carefully design Sparse High/Low-pass Filters to\nenhance the integrity of details and motion structures, and a Sparse Frequency\nMixer to promote seamless recoupling. Besides, we innovatively design a\ncontrastive regularization for this task to strengthen the model's ability to\ndiscriminate irrelevant features, reducing undesired motion magnification.\nExtensive experiments on both Real-world and Synthetic Datasets show that our\nFD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\\times$\nand boosts inference speed by 1.68$\\times$ than the latest method. Our code is\navailable at https://github.com/Jiafei127/FD4MM.\n","authors":["Fei Wang","Dan Guo","Kun Li","Zhun Zhong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07347v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16071v1","updated":"2024-03-24T09:18:21Z","published":"2024-03-24T09:18:21Z","title":"Landmark-Guided Cross-Speaker Lip Reading with Mutual Information\n  Regularization","summary":"  Lip reading, the process of interpreting silent speech from visual lip\nmovements, has gained rising attention for its wide range of realistic\napplications. Deep learning approaches greatly improve current lip reading\nsystems. However, lip reading in cross-speaker scenarios where the speaker\nidentity changes, poses a challenging problem due to inter-speaker variability.\nA well-trained lip reading system may perform poorly when handling a brand new\nspeaker. To learn a speaker-robust lip reading model, a key insight is to\nreduce visual variations across speakers, avoiding the model overfitting to\nspecific speakers. In this work, in view of both input visual clues and latent\nrepresentations based on a hybrid CTC/attention architecture, we propose to\nexploit the lip landmark-guided fine-grained visual clues instead of\nfrequently-used mouth-cropped images as input features, diminishing\nspeaker-specific appearance characteristics. Furthermore, a max-min mutual\ninformation regularization approach is proposed to capture speaker-insensitive\nlatent representations. Experimental evaluations on public lip reading datasets\ndemonstrate the effectiveness of the proposed approach under the intra-speaker\nand inter-speaker conditions.\n","authors":["Linzhi Wu","Xingyu Zhang","Yakun Zhang","Changyan Zheng","Tiejun Liu","Liang Xie","Ye Yan","Erwei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16071v1.pdf","comment":"To appear in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16067v1","updated":"2024-03-24T08:34:08Z","published":"2024-03-24T08:34:08Z","title":"Robust Diffusion Models for Adversarial Purification","summary":"  Diffusion models (DMs) based adversarial purification (AP) has shown to be\nthe most powerful alternative to adversarial training (AT). However, these\nmethods neglect the fact that pre-trained diffusion models themselves are not\nrobust to adversarial attacks as well. Additionally, the diffusion process can\neasily destroy semantic information and generate a high quality image but\ntotally different from the original input image after the reverse process,\nleading to degraded standard accuracy. To overcome these issues, a natural idea\nis to harness adversarial training strategy to retrain or fine-tune the\npre-trained diffusion model, which is computationally prohibitive. We propose a\nnovel robust reverse process with adversarial guidance, which is independent of\ngiven pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust\nguidance can not only ensure to generate purified examples retaining more\nsemantic content but also mitigate the accuracy-robustness trade-off of DMs for\nthe first time, which also provides DM-based AP an efficient adaptive ability\nto new attacks. Extensive experiments are conducted to demonstrate that our\nmethod achieves the state-of-the-art results and exhibits generalization\nagainst different attacks.\n","authors":["Guang Lin","Zerui Tao","Jianhai Zhang","Toshihisa Tanaka","Qibin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14137v2","updated":"2024-03-24T07:58:01Z","published":"2024-03-21T05:13:12Z","title":"SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion\n  and Inter-Class Separability in Image Classification","summary":"  To address the issues of MixUp and its variants (e.g., Manifold MixUp) in\nimage classification tasks-namely, their neglect of mixing within the same\nclass (intra-class mixup) and their inadequacy in enhancing intra-class\ncohesion through their mixing operations-we propose a novel mixup method named\nSynerMix-Intra and, building upon this, introduce a synergistic mixup solution\nnamed SynerMix. SynerMix-Intra specifically targets intra-class mixup to\nbolster intra-class cohesion, a feature not addressed by current mixup methods.\nFor each mini-batch, it leverages feature representations of unaugmented\noriginal images from each class to generate a synthesized feature\nrepresentation through random linear interpolation. All synthesized\nrepresentations are then fed into the classification and loss layers to\ncalculate an average classification loss that significantly enhances\nintra-class cohesion. Furthermore, SynerMix combines SynerMix-Intra with an\nexisting mixup approach (e.g., MixUp, Manifold MixUp), which primarily focuses\non inter-class mixup and has the benefit of enhancing inter-class separability.\nIn doing so, it integrates both inter- and intra-class mixup in a balanced way\nwhile concurrently improving intra-class cohesion and inter-class separability.\nExperimental results on six datasets show that SynerMix achieves a 0.1% to\n3.43% higher accuracy than the best of either MixUp or SynerMix-Intra alone,\naveraging a 1.16% gain. It also surpasses the top-performer of either Manifold\nMixUp or SynerMix-Intra by 0.12% to 5.16%, with an average gain of 1.11%. Given\nthat SynerMix is model-agnostic, it holds significant potential for application\nin other domains where mixup methods have shown promise, such as speech and\ntext classification. Our code is publicly available at:\nhttps://github.com/wxitxy/synermix.git.\n","authors":["Ye Xu","Ya Gao","Xiaorong Qiu","Yang Chen","Ying Ji"],"pdf_url":"https://arxiv.org/pdf/2403.14137v2.pdf","comment":"25 pages,12 figures"},{"id":"http://arxiv.org/abs/2403.16051v1","updated":"2024-03-24T07:36:38Z","published":"2024-03-24T07:36:38Z","title":"Segment Anything Model for Road Network Graph Extraction","summary":"  We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for\nextracting large-scale, vectorized road network graphs from satellite imagery.\nTo predict graph geometry, we formulate it as a dense semantic segmentation\ntask, leveraging the inherent strengths of SAM. The image encoder of SAM is\nfine-tuned to produce probability masks for roads and intersections, from which\nthe graph vertices are extracted via simple non-maximum suppression. To predict\ngraph topology, we designed a lightweight transformer-based graph neural\nnetwork, which leverages the SAM image embeddings to estimate the edge\nexistence probabilities between vertices. Our approach directly predicts the\ngraph vertices and edges for large regions without expensive and complex\npost-processing heuristics, and is capable of building complete road network\ngraphs spanning multiple square kilometers in a matter of seconds. With its\nsimple, straightforward, and minimalist design, SAM-Road achieves comparable\naccuracy with the state-of-the-art method RNGDet++, while being 40 times faster\non the City-scale dataset. We thus demonstrate the power of a foundational\nvision model when applied to a graph learning task. The code is available at\nhttps://github.com/htcr/sam_road.\n","authors":["Congrui Hetang","Haoru Xue","Cindy Le","Tianwei Yue","Wenping Wang","Yihui He"],"pdf_url":"https://arxiv.org/pdf/2403.16051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16050v1","updated":"2024-03-24T07:33:08Z","published":"2024-03-24T07:33:08Z","title":"A General and Efficient Federated Split Learning with Pre-trained Image\n  Transformers for Heterogeneous Data","summary":"  Federated Split Learning (FSL) is a promising distributed learning paradigm\nin practice, which gathers the strengths of both Federated Learning (FL) and\nSplit Learning (SL) paradigms, to ensure model privacy while diminishing the\nresource overhead of each client, especially on large transformer models in a\nresource-constrained environment, e.g., Internet of Things (IoT). However,\nalmost all works merely investigate the performance with simple neural network\nmodels in FSL. Despite the minor efforts focusing on incorporating Vision\nTransformers (ViT) as model architectures, they train ViT from scratch, thereby\nleading to enormous training overhead in each device with limited resources.\nTherefore, in this paper, we harness Pre-trained Image Transformers (PITs) as\nthe initial model, coined FES-PIT, to accelerate the training process and\nimprove model robustness. Furthermore, we propose FES-PTZO to hinder the\ngradient inversion attack, especially having the capability compatible with\nblack-box scenarios, where the gradient information is unavailable. Concretely,\nFES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO)\noptimization, which replaces the backward propagation with just one forward\nprocess. Empirically, we are the first to provide a systematic evaluation of\nFSL methods with PITs in real-world datasets, different partial device\nparticipations, and heterogeneous data splits. Our experiments verify the\neffectiveness of our algorithms.\n","authors":["Yifan Shi","Yuhui Zhang","Ziyue Huang","Xiaofeng Yang","Li Shen","Wei Chen","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16048v1","updated":"2024-03-24T07:29:04Z","published":"2024-03-24T07:29:04Z","title":"Edit3K: Universal Representation Learning for Video Editing Components","summary":"  This paper focuses on understanding the predominant video creation pipeline,\ni.e., compositional video editing with six main types of editing components,\nincluding video effects, animation, transition, filter, sticker, and text. In\ncontrast to existing visual representation learning of visual materials (i.e.,\nimages/videos), we aim to learn visual representations of editing\nactions/components that are generally applied on raw materials. We start by\nproposing the first large-scale dataset for editing components of video\ncreation, which covers about $3,094$ editing components with $618,800$ videos.\nEach video in our dataset is rendered by various image/video materials with a\nsingle editing component, which supports atomic visual understanding of\ndifferent editing components. It can also benefit several downstream tasks,\ne.g., editing component recommendation, editing component\nrecognition/retrieval, etc. Existing visual representation methods perform\npoorly because it is difficult to disentangle the visual appearance of editing\ncomponents from raw materials. To that end, we benchmark popular alternative\nsolutions and propose a novel method that learns to attend to the appearance of\nediting components regardless of raw materials. Our method achieves favorable\nresults on editing component retrieval/recognition compared to the alternative\nsolutions. A user study is also conducted to show that our representations\ncluster visually similar editing components better than other alternatives.\nFurthermore, our learned representations used to transition recommendation\ntasks achieve state-of-the-art results on the AutoTransition dataset. The code\nand dataset will be released for academic use.\n","authors":["Xin Gu","Libo Zhang","Fan Chen","Longyin Wen","Yufei Wang","Tiejian Luo","Sijie Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01024v2","updated":"2024-03-24T07:10:27Z","published":"2023-07-03T13:55:44Z","title":"SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation","summary":"  Domain adaptation (DA) has demonstrated significant promise for real-time\nnighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art\n(SOTA) DA still lacks the potential object with accurate pixel-level location\nand boundary to generate the high-quality target domain training sample. This\nkey issue constrains the transfer learning of the real-time daytime SOTA\ntrackers for challenging nighttime UAV tracking. Recently, the notable Segment\nAnything Model (SAM) has achieved a remarkable zero-shot generalization ability\nto discover abundant potential objects due to its huge data-driven training\napproach. To solve the aforementioned issue, this work proposes a novel\nSAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA.\nSpecifically, an innovative SAM-powered target domain training sample swelling\nis designed to determine enormous high-quality target domain training samples\nfrom every single raw nighttime image. This novel one-to-many generation\nsignificantly expands the high-quality target domain training sample for DA.\nComprehensive experiments on extensive nighttime UAV videos prove the\nrobustness and domain adaptability of SAM-DA for nighttime UAV tracking.\nEspecially, compared to the SOTA DA, SAM-DA can achieve better performance with\nfewer raw nighttime images, i.e., the fewer-better training. This economized\ntraining approach facilitates the quick validation and deployment of algorithms\nfor UAVs. The code is available at https://github.com/vision4robotics/SAM-DA.\n","authors":["Changhong Fu","Liangliang Yao","Haobo Zuo","Guangze Zheng","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2307.01024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16043v1","updated":"2024-03-24T07:04:08Z","published":"2024-03-24T07:04:08Z","title":"Semantic Is Enough: Only Semantic Information For NeRF Reconstruction","summary":"  Recent research that combines implicit 3D representation with semantic\ninformation, like Semantic-NeRF, has proven that NeRF model could perform\nexcellently in rendering 3D structures with semantic labels. This research aims\nto extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing\nsolely on semantic output and removing the RGB output component. We reformulate\nthe model and its training procedure to leverage only the cross-entropy loss\nbetween the model semantic output and the ground truth semantic images,\nremoving the colour data traditionally used in the original Semantic-NeRF\napproach. We then conduct a series of identical experiments using the original\nand the modified Semantic-NeRF model. Our primary objective is to obverse the\nimpact of this modification on the model performance by Semantic-NeRF, focusing\non tasks such as scene understanding, object detection, and segmentation. The\nresults offer valuable insights into the new way of rendering the scenes and\nprovide an avenue for further research and development in semantic-focused 3D\nscene understanding.\n","authors":["Ruibo Wang","Song Zhang","Ping Huang","Donghai Zhang","Wei Yan"],"pdf_url":"https://arxiv.org/pdf/2403.16043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11557v2","updated":"2024-03-24T07:01:37Z","published":"2023-12-17T09:05:47Z","title":"SAI3D: Segment Any Instance in 3D Scenes","summary":"  Advancements in 3D instance segmentation have traditionally been tethered to\nthe availability of annotated datasets, limiting their application to a narrow\nspectrum of object categories. Recent efforts have sought to harness\nvision-language models like CLIP for open-set semantic reasoning, yet these\nmethods struggle to distinguish between objects of the same categories and rely\non specific prompts that are not universally applicable. In this paper, we\nintroduce SAI3D, a novel zero-shot 3D instance segmentation approach that\nsynergistically leverages geometric priors and semantic cues derived from\nSegment Anything Model (SAM). Our method partitions a 3D scene into geometric\nprimitives, which are then progressively merged into 3D instance segmentations\nthat are consistent with the multi-view SAM masks. Moreover, we design a\nhierarchical region-growing algorithm with a dynamic thresholding mechanism,\nwhich largely improves the robustness of finegrained 3D scene parsing.Empirical\nevaluations on ScanNet, Matterport3D and the more challenging ScanNet++\ndatasets demonstrate the superiority of our approach. Notably, SAI3D\noutperforms existing open-vocabulary baselines and even surpasses\nfully-supervised methods in class-agnostic segmentation on ScanNet++. Our\nproject page is at https://yd-yin.github.io/SAI3D.\n","authors":["Yingda Yin","Yuzheng Liu","Yang Xiao","Daniel Cohen-Or","Jingwei Huang","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2312.11557v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16034v1","updated":"2024-03-24T06:30:02Z","published":"2024-03-24T06:30:02Z","title":"V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative\n  Perception","summary":"  Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled\nautonomous vehicles to share sensing information to see through occlusions,\ngreatly boosting the perception capability. However, there are no real-world\ndatasets to facilitate the real V2X cooperative perception research -- existing\ndatasets either only support Vehicle-to-Infrastructure cooperation or\nVehicle-to-Vehicle cooperation. In this paper, we propose a dataset that has a\nmixture of multiple vehicles and smart infrastructure simultaneously to\nfacilitate the V2X cooperative perception development with multi-modality\nsensing data. Our V2X-Real is collected using two connected automated vehicles\nand two smart infrastructures, which are all equipped with multi-modal sensors\nincluding LiDAR sensors and multi-view cameras. The whole dataset contains 33K\nLiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10\ncategories in very challenging urban scenarios. According to the collaboration\nmode and ego perspective, we derive four types of datasets for Vehicle-Centric,\nInfrastructure-Centric, Vehicle-to-Vehicle, and\nInfrastructure-to-Infrastructure cooperative perception. Comprehensive\nmulti-class multi-agent benchmarks of SOTA cooperative perception methods are\nprovided. The V2X-Real dataset and benchmark codes will be released.\n","authors":["Hao Xiang","Zhaoliang Zheng","Xin Xia","Runsheng Xu","Letian Gao","Zewei Zhou","Xu Han","Xinkai Ji","Mingxi Li","Zonglin Meng","Li Jin","Mingyue Lei","Zhaoyang Ma","Zihang He","Haoxuan Ma","Yunshuang Yuan","Yingqian Zhao","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16028v1","updated":"2024-03-24T06:10:22Z","published":"2024-03-24T06:10:22Z","title":"Exploring the Impact of Dataset Bias on Dataset Distillation","summary":"  Dataset Distillation (DD) is a promising technique to synthesize a smaller\ndataset that preserves essential information from the original dataset. This\nsynthetic dataset can serve as a substitute for the original large-scale one,\nand help alleviate the training workload. However, current DD methods typically\noperate under the assumption that the dataset is unbiased, overlooking\npotential bias issues within the dataset itself. To fill in this blank, we\nsystematically investigate the influence of dataset bias on DD. To the best of\nour knowledge, this is the first exploration in the DD domain. Given that there\nare no suitable biased datasets for DD, we first construct two biased datasets,\nCMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis.\nThen we utilize existing DD methods to generate synthetic datasets on CMNIST-DD\nand CCIFAR10-DD, and evaluate their performance following the standard process.\nExperiments demonstrate that biases present in the original dataset\nsignificantly impact the performance of the synthetic dataset in most cases,\nwhich highlights the necessity of identifying and mitigating biases in the\noriginal datasets during DD. Finally, we reformulate DD within the context of a\nbiased dataset. Our code along with biased datasets are available at\nhttps://github.com/yaolu-zjut/Biased-DD.\n","authors":["Yao Lu","Jianyang Gu","Xuguang Chen","Saeed Vahidian","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2403.16028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16024v1","updated":"2024-03-24T05:57:00Z","published":"2024-03-24T05:57:00Z","title":"A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA","summary":"  This paper presents a comprehensive study on the unified module for\naccelerating stable-diffusion processes, specifically focusing on the lcm-lora\nmodule. Stable-diffusion processes play a crucial role in various scientific\nand engineering domains, and their acceleration is of paramount importance for\nefficient computational performance. The standard iterative procedures for\nsolving fixed-source discrete ordinates problems often exhibit slow\nconvergence, particularly in optically thick scenarios. To address this\nchallenge, unconditionally stable diffusion-acceleration methods have been\ndeveloped, aiming to enhance the computational efficiency of transport\nequations and discrete ordinates problems. This study delves into the\ntheoretical foundations and numerical results of unconditionally stable\ndiffusion synthetic acceleration methods, providing insights into their\nstability and performance for model discrete ordinates problems. Furthermore,\nthe paper explores recent advancements in diffusion model acceleration,\nincluding on device acceleration of large diffusion models via gpu aware\noptimizations, highlighting the potential for significantly improved inference\nlatency. The results and analyses in this study provide important insights into\nstable diffusion processes and have important ramifications for the creation\nand application of acceleration methods specifically, the lcm-lora module in a\nvariety of computing environments.\n","authors":["Ayush Thakur","Rashmi Vashisth"],"pdf_url":"https://arxiv.org/pdf/2403.16024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16023v1","updated":"2024-03-24T05:55:39Z","published":"2024-03-24T05:55:39Z","title":"RPMArt: Towards Robust Perception and Manipulation for Articulated\n  Objects","summary":"  Articulated objects are commonly found in daily life. It is essential that\nrobots can exhibit robust perception and manipulation skills for articulated\nobjects in real-world robotic applications. However, existing methods for\narticulated objects insufficiently address noise in point clouds and struggle\nto bridge the gap between simulation and reality, thus limiting the practical\ndeployment in real-world scenarios. To tackle these challenges, we propose a\nframework towards Robust Perception and Manipulation for Articulated Objects\n(RPMArt), which learns to estimate the articulation parameters and manipulate\nthe articulation part from the noisy point cloud. Our primary contribution is a\nRobust Articulation Network (RoArtNet) that is able to predict both joint\nparameters and affordable points robustly by local feature learning and point\ntuple voting. Moreover, we introduce an articulation-aware classification\nscheme to enhance its ability for sim-to-real transfer. Finally, with the\nestimated affordable point and articulation joint constraint, the robot can\ngenerate robust actions to manipulate articulated objects. After learning only\nfrom synthetic data, RPMArt is able to transfer zero-shot to real-world\narticulated objects. Experimental results confirm our approach's effectiveness,\nwith our framework achieving state-of-the-art performance in both noise-added\nsimulation and real-world environments. The code and data will be open-sourced\nfor reproduction. More results are published on the project website at\nhttps://r-pmart.github.io .\n","authors":["Junbo Wang","Wenhai Liu","Qiaojun Yu","Yang You","Liu Liu","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.16023v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024), project website at\n  https://r-pmart.github.io"},{"id":"http://arxiv.org/abs/2403.16020v1","updated":"2024-03-24T05:50:00Z","published":"2024-03-24T05:50:00Z","title":"PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for\n  Faster Inference","summary":"  As deep neural networks evolve from convolutional neural networks (ConvNets)\nto advanced vision transformers (ViTs), there is an increased need to eliminate\nredundant data for faster processing without compromising accuracy. Previous\nmethods are often architecture-specific or necessitate re-training, restricting\ntheir applicability with frequent model updates. To solve this, we first\nintroduce a novel property of lightweight ConvNets: their ability to identify\nkey discriminative patch regions in images, irrespective of model's final\naccuracy or size. We demonstrate that fully-connected layers are the primary\nbottleneck for ConvNets performance, and their suppression with simple weight\nrecalibration markedly enhances discriminative patch localization performance.\nUsing this insight, we introduce PaPr, a method for substantially pruning\nredundant patches with minimal accuracy loss using lightweight ConvNets across\na variety of deep learning architectures, including ViTs, ConvNets, and hybrid\ntransformers, without any re-training. Moreover, the simple early-stage\none-step patch pruning with PaPr enhances existing patch reduction methods.\nThrough extensive testing on diverse architectures, PaPr achieves significantly\nhigher accuracy over state-of-the-art patch reduction methods with similar FLOP\ncount reduction. More specifically, PaPr reduces about 70% of redundant patches\nin videos with less than 0.8% drop in accuracy, and up to 3.7x FLOPs reduction,\nwhich is a 15% more reduction with 2.5% higher accuracy.\n","authors":["Tanvir Mahmud","Burhaneddin Yaman","Chun-Hao Liu","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2403.16020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05997v3","updated":"2024-03-24T05:46:10Z","published":"2023-01-15T02:04:02Z","title":"Exploiting Auxiliary Caption for Video Grounding","summary":"  Video grounding aims to locate a moment of interest matching the given query\nsentence from an untrimmed video. Previous works ignore the {sparsity dilemma}\nin video annotations, which fails to provide the context information between\npotential events and query sentences in the dataset. In this paper, we contend\nthat exploiting easily available captions which describe general actions, i.e.,\nauxiliary captions defined in our paper, will significantly boost the\nperformance. To this end, we propose an Auxiliary Caption Network (ACNet) for\nvideo grounding. Specifically, we first introduce dense video captioning to\ngenerate dense captions and then obtain auxiliary captions by Non-Auxiliary\nCaption Suppression (NACS). To capture the potential information in auxiliary\ncaptions, we propose Caption Guided Attention (CGA) project the semantic\nrelations between auxiliary captions and query sentences into temporal space\nand fuse them into visual representations. Considering the gap between\nauxiliary captions and ground truth, we propose Asymmetric Cross-modal\nContrastive Learning (ACCL) for constructing more negative pairs to maximize\ncross-modal mutual information. Extensive experiments on three public datasets\n(i.e., ActivityNet Captions, TACoS and ActivityNet-CG) demonstrate that our\nmethod significantly outperforms state-of-the-art methods.\n","authors":["Hongxiang Li","Meng Cao","Xuxin Cheng","Zhihong Zhu","Yaowei Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2301.05997v3.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2310.15168v3","updated":"2024-03-24T23:32:50Z","published":"2023-10-23T17:59:52Z","title":"Ghost on the Shell: An Expressive Representation of General 3D Shapes","summary":"  The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.\n","authors":["Zhen Liu","Yao Feng","Yuliang Xiu","Weiyang Liu","Liam Paull","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2310.15168v3.pdf","comment":"ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:\n  https://gshell3d.github.io/)"},{"id":"http://arxiv.org/abs/2311.01788v2","updated":"2024-03-24T18:22:12Z","published":"2023-11-03T09:01:37Z","title":"Fast ellipsoidal conformal and quasi-conformal parameterization of\n  genus-0 closed surfaces","summary":"  Surface parameterization plays a fundamental role in many science and\nengineering problems. In particular, as genus-0 closed surfaces are\ntopologically equivalent to a sphere, many spherical parameterization methods\nhave been developed over the past few decades. However, in practice, mapping a\ngenus-0 closed surface onto a sphere may result in a large distortion due to\ntheir geometric difference. In this work, we propose a new framework for\ncomputing ellipsoidal conformal and quasi-conformal parameterizations of\ngenus-0 closed surfaces, in which the target parameter domain is an ellipsoid\ninstead of a sphere. By combining simple conformal transformations with\ndifferent types of quasi-conformal mappings, we can easily achieve a large\nvariety of ellipsoidal parameterizations with their bijectivity guaranteed by\nquasi-conformal theory. Numerical experiments are presented to demonstrate the\neffectiveness of the proposed framework.\n","authors":["Gary P. T. Choi"],"pdf_url":"https://arxiv.org/pdf/2311.01788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16210v1","updated":"2024-03-24T16:09:21Z","published":"2024-03-24T16:09:21Z","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One\n  Tri-Plane","summary":"  We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting.\n","authors":["Han Yan","Yang Li","Zhennan Wu","Shenzhou Chen","Weixuan Sun","Taizhang Shang","Weizhe Liu","Tian Chen","Xiaqiang Dai","Chao Ma","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.16210v1.pdf","comment":"Video: https://youtu.be/lRn-HqyCrLI"},{"id":"http://arxiv.org/abs/2310.14729v3","updated":"2024-03-24T15:11:38Z","published":"2023-10-23T09:05:18Z","title":"MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D\n  diffusion","summary":"  We introduce Multi-view Ancestral Sampling (MAS), a method for 3D motion\ngeneration, using 2D diffusion models that were trained on motions obtained\nfrom in-the-wild videos. As such, MAS opens opportunities to exciting and\ndiverse fields of motion previously under-explored as 3D data is scarce and\nhard to collect. MAS works by simultaneously denoising multiple 2D motion\nsequences representing different views of the same 3D motion. It ensures\nconsistency across all views at each diffusion step by combining the individual\ngenerations into a unified 3D sequence, and projecting it back to the original\nviews. We demonstrate MAS on 2D pose data acquired from videos depicting\nprofessional basketball maneuvers, rhythmic gymnastic performances featuring a\nball apparatus, and horse races. In each of these domains, 3D motion capture is\narduous, and yet, MAS generates diverse and realistic 3D sequences. Unlike the\nScore Distillation approach, which optimizes each sample by repeatedly applying\nsmall fixes, our method uses a sampling process that was constructed for the\ndiffusion framework. As we demonstrate, MAS avoids common issues such as\nout-of-domain sampling and mode-collapse. https://guytevet.github.io/mas-page/\n","authors":["Roy Kapon","Guy Tevet","Daniel Cohen-Or","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2310.14729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16024v1","updated":"2024-03-24T05:57:00Z","published":"2024-03-24T05:57:00Z","title":"A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA","summary":"  This paper presents a comprehensive study on the unified module for\naccelerating stable-diffusion processes, specifically focusing on the lcm-lora\nmodule. Stable-diffusion processes play a crucial role in various scientific\nand engineering domains, and their acceleration is of paramount importance for\nefficient computational performance. The standard iterative procedures for\nsolving fixed-source discrete ordinates problems often exhibit slow\nconvergence, particularly in optically thick scenarios. To address this\nchallenge, unconditionally stable diffusion-acceleration methods have been\ndeveloped, aiming to enhance the computational efficiency of transport\nequations and discrete ordinates problems. This study delves into the\ntheoretical foundations and numerical results of unconditionally stable\ndiffusion synthetic acceleration methods, providing insights into their\nstability and performance for model discrete ordinates problems. Furthermore,\nthe paper explores recent advancements in diffusion model acceleration,\nincluding on device acceleration of large diffusion models via gpu aware\noptimizations, highlighting the potential for significantly improved inference\nlatency. The results and analyses in this study provide important insights into\nstable diffusion processes and have important ramifications for the creation\nand application of acceleration methods specifically, the lcm-lora module in a\nvariety of computing environments.\n","authors":["Ayush Thakur","Rashmi Vashisth"],"pdf_url":"https://arxiv.org/pdf/2403.16024v1.pdf","comment":null}]},"2024-03-23T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.15959v1","updated":"2024-03-23T23:36:26Z","published":"2024-03-23T23:36:26Z","title":"Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction","summary":"  Tasks where robots must cooperate with humans, such as navigating around a\ncluttered home or sorting everyday items, are challenging because they exhibit\na wide range of valid actions that lead to similar outcomes. Moreover,\nzero-shot cooperation between human-robot partners is an especially challenging\nproblem because it requires the robot to infer and adapt on the fly to a latent\nhuman intent, which could vary significantly from human to human. Recently,\ndeep learned motion prediction models have shown promising results in\npredicting human intent but are prone to being confidently incorrect. In this\nwork, we present Risk-Calibrated Interactive Planning (RCIP), which is a\nframework for measuring and calibrating risk associated with uncertain action\nselection in human-robot cooperation, with the fundamental idea that the robot\nshould ask for human clarification when the risk associated with the\nuncertainty in the human's intent cannot be controlled. RCIP builds on the\ntheory of set-valued risk calibration to provide a finite-sample statistical\nguarantee on the cumulative loss incurred by the robot while minimizing the\ncost of human clarification in complex multi-step settings. Our main insight is\nto frame the risk control problem as a sequence-level multi-hypothesis testing\nproblem, allowing efficient calibration using a low-dimensional parameter that\ncontrols a pre-trained risk-aware policy. Experiments across a variety of\nsimulated and real-world environments demonstrate RCIP's ability to predict and\nadapt to a diverse set of dynamic human intents.\n","authors":["Justin Lidard","Hang Pham","Ariel Bachman","Bryan Boateng","Anirudha Majumdar"],"pdf_url":"https://arxiv.org/pdf/2403.15959v1.pdf","comment":"Website with additional information, videos, and code:\n  https://risk-calibrated-planning.github.io/"},{"id":"http://arxiv.org/abs/2403.15941v1","updated":"2024-03-23T22:04:03Z","published":"2024-03-23T22:04:03Z","title":"Explore until Confident: Efficient Exploration for Embodied Question\n  Answering","summary":"  We consider the problem of Embodied Question Answering (EQA), which refers to\nsettings where an embodied agent such as a robot needs to actively explore an\nenvironment to gather information until it is confident about the answer to a\nquestion. In this work, we leverage the strong semantic reasoning capabilities\nof large vision-language models (VLMs) to efficiently explore and answer such\nquestions. However, there are two main challenges when using VLMs in EQA: they\ndo not have an internal memory for mapping the scene to be able to plan how to\nexplore over time, and their confidence can be miscalibrated and can cause the\nrobot to prematurely stop exploration or over-explore. We propose a method that\nfirst builds a semantic map of the scene based on depth information and via\nvisual prompting of a VLM - leveraging its vast knowledge of relevant regions\nof the scene for exploration. Next, we use conformal prediction to calibrate\nthe VLM's question answering confidence, allowing the robot to know when to\nstop exploration - leading to a more calibrated and efficient exploration\nstrategy. To test our framework in simulation, we also contribute a new EQA\ndataset with diverse, realistic human-robot scenarios and scenes built upon the\nHabitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot\nexperiments show our proposed approach improves the performance and efficiency\nover baselines that do no leverage VLM for exploration or do not calibrate its\nconfidence. Webpage with experiment videos and code:\nhttps://explore-eqa.github.io/\n","authors":["Allen Z. Ren","Jaden Clark","Anushri Dixit","Masha Itkina","Anirudha Majumdar","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2403.15941v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2309.04937v3","updated":"2024-03-23T21:09:19Z","published":"2023-09-10T05:45:36Z","title":"LONER: LiDAR Only Neural Representations for Real-Time SLAM","summary":"  This paper proposes LONER, the first real-time LiDAR SLAM algorithm that uses\na neural implicit scene representation. Existing implicit mapping methods for\nLiDAR show promising results in large-scale reconstruction, but either require\ngroundtruth poses or run slower than real-time. In contrast, LONER uses LiDAR\ndata to train an MLP to estimate a dense map in real-time, while simultaneously\nestimating the trajectory of the sensor. To achieve real-time performance, this\npaper proposes a novel information-theoretic loss function that accounts for\nthe fact that different regions of the map may be learned to varying degrees\nthroughout online training. The proposed method is evaluated qualitatively and\nquantitatively on two open-source datasets. This evaluation illustrates that\nthe proposed loss function converges faster and leads to more accurate geometry\nreconstruction than other loss functions used in depth-supervised neural\nimplicit frameworks. Finally, this paper shows that LONER estimates\ntrajectories competitively with state-of-the-art LiDAR SLAM methods, while also\nproducing dense maps competitive with existing real-time implicit mapping\nmethods that use groundtruth poses.\n","authors":["Seth Isaacson","Pou-Chun Kung","Mani Ramanagopal","Ram Vasudevan","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2309.04937v3.pdf","comment":"First two authors equally contributed. Webpage:\n  https://umautobots.github.io/loner"},{"id":"http://arxiv.org/abs/2309.08865v3","updated":"2024-03-23T20:33:45Z","published":"2023-09-16T04:01:34Z","title":"ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical\n  Information System","summary":"  Mass casualty incidents (MCIs) pose a significant challenge to emergency\nmedical services by overwhelming available resources and personnel. Effective\nvictim assessment is the key to minimizing casualties during such a crisis. We\nintroduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical\nInformation System, to aid first responders in MCI events. It leverages speech\nprocessing, natural language processing, and deep learning to help with acuity\nclassification. This is deployed on a quadruped that performs victim\nlocalization and preliminary injury severity assessment. First responders\naccess victim information through a Graphical User Interface that is updated in\nreal-time. To validate our proposed algorithmic triage protocol, we used the\nUnitree Go1 quadruped. The robot identifies humans, interacts with them, gets\nvitals and information, and assigns an acuity label. Simulations of an MCI in\nsoftware and a controlled environment outdoors were conducted. The system\nachieved a triage-level classification precision of over 74% on average and 99%\nfor the most critical victims, i.e. level 1 acuity, outperforming\nstate-of-the-art deep learning-based triage labeling systems. In this paper, we\nshowcase the potential of human-robot interaction in assisting medical\npersonnel in MCI events.\n","authors":["Revanth Krishna Senthilkumaran","Mridu Prashanth","Hrishikesh Viswanath","Sathvika Kotha","Kshitij Tiwari","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.08865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15852v4","updated":"2024-03-23T16:54:01Z","published":"2024-02-24T16:39:16Z","title":"NaVid: Video-based VLM Plans the Next Step for Vision-and-Language\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) stands as a key research problem of\nEmbodied AI, aiming at enabling agents to navigate in unseen environments\nfollowing linguistic instructions. In this field, generalization is a\nlong-standing challenge, either to out-of-distribution scenes or from Sim to\nReal. In this paper, we propose NaVid, a video-based large vision language\nmodel (VLM), to mitigate such a generalization gap. NaVid makes the first\nendeavour to showcase the capability of VLMs to achieve state-of-the-art level\nnavigation performance without any maps, odometer and depth inputs. Following\nhuman instruction, NaVid only requires an on-the-fly video stream from a\nmonocular RGB camera equipped on the robot to output the next-step action. Our\nformulation mimics how humans navigate and naturally gets rid of the problems\nintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.\nMoreover, our video-based approach can effectively encode the historical\nobservations of robots as spatio-temporal contexts for decision-making and\ninstruction following. We train NaVid with 550k navigation samples collected\nfrom VLN-CE trajectories, including action-planning and instruction-reasoning\nsamples, along with 665k large-scale web data. Extensive experiments show that\nNaVid achieves SOTA performance in simulation environments and the real world,\ndemonstrating superior cross-dataset and Sim2Real transfer. We thus believe our\nproposed VLM approach plans the next step for not only the navigation agents\nbut also this research field.\n","authors":["Jiazhao Zhang","Kunyu Wang","Rongtao Xu","Gengze Zhou","Yicong Hong","Xiaomeng Fang","Qi Wu","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15852v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15870v1","updated":"2024-03-23T15:35:54Z","published":"2024-03-23T15:35:54Z","title":"iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding","summary":"  The pathfinding problem, which aims to identify a collision-free path between\ntwo points, is crucial for many applications, such as robot navigation and\nautonomous driving. Classic methods, such as A$^*$ search, perform well on\nsmall-scale maps but face difficulties scaling up. Conversely, data-driven\napproaches can improve pathfinding efficiency but require extensive data\nlabeling and lack theoretical guarantees, making it challenging for practical\napplications. To combine the strengths of the two methods, we utilize the\nimperative learning (IL) strategy and propose a novel self-supervised\npathfinding framework, termed imperative learning-based A$^*$ (iA$^*$).\nSpecifically, iA$^*$ is a bilevel optimization process where the lower-level\noptimization is dedicated to finding the optimal path by a differentiable A$^*$\nsearch module, and the upper-level optimization narrows down the search space\nto improve efficiency via setting suitable initial values from a data-driven\nmodel. Besides, the model within the upper-level optimization is a fully\nconvolutional network, trained by the calculated loss in the lower-level\noptimization. Thus, the framework avoids extensive data labeling and can be\napplied in diverse environments. Our comprehensive experiments demonstrate that\niA$^*$ surpasses both classical and data-driven methods in pathfinding\nefficiency and shows superior robustness among different tasks, validated with\npublic datasets and simulation environments.\n","authors":["Xiangyu Chen","Fan Yang","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15857v1","updated":"2024-03-23T14:47:26Z","published":"2024-03-23T14:47:26Z","title":"Automated System-level Testing of Unmanned Aerial Systems","summary":"  Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.\n","authors":["Hassan Sartaj","Asmar Muqeet","Muhammad Zohaib Iqbal","Muhammad Uzair Khan"],"pdf_url":"https://arxiv.org/pdf/2403.15857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14552v2","updated":"2024-03-23T14:15:02Z","published":"2023-09-25T21:51:48Z","title":"Tactile Estimation of Extrinsic Contact Patch for Stable Placement","summary":"  Precise perception of contact interactions is essential for fine-grained\nmanipulation skills for robots. In this paper, we present the design of\nfeedback skills for robots that must learn to stack complex-shaped objects on\ntop of each other (see Fig.1). To design such a system, a robot should be able\nto reason about the stability of placement from very gentle contact\ninteractions. Our results demonstrate that it is possible to infer the\nstability of object placement based on tactile readings during contact\nformation between the object and its environment. In particular, we estimate\nthe contact patch between a grasped object and its environment using force and\ntactile observations to estimate the stability of the object during a contact\nformation. The contact patch could be used to estimate the stability of the\nobject upon release of the grasp. The proposed method is demonstrated in\nvarious pairs of objects that are used in a very popular board game.\n","authors":["Kei Ota","Devesh K. Jha","Krishna Murthy Jatavallabhula","Asako Kanezaki","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2309.14552v2.pdf","comment":"Accepted at ICRA2024"},{"id":"http://arxiv.org/abs/2403.15834v1","updated":"2024-03-23T13:21:09Z","published":"2024-03-23T13:21:09Z","title":"ARO: Large Language Model Supervised Robotics Text2Skill Autonomous\n  Learning","summary":"  Robotics learning highly relies on human expertise and efforts, such as\ndemonstrations, design of reward functions in reinforcement learning,\nperformance evaluation using human feedback, etc. However, reliance on human\nassistance can lead to expensive learning costs and make skill learning\ndifficult to scale. In this work, we introduce the Large Language Model\nSupervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims\nto replace human participation in the robot skill learning process with\nlarge-scale language models that incorporate reward function design and\nperformance evaluation. We provide evidence that our approach enables fully\nautonomous robot skill learning, capable of completing partial tasks without\nhuman intervention. Furthermore, we also analyze the limitations of this\napproach in task understanding and optimization stability.\n","authors":["Yiwen Chen","Yuyao Ye","Ziyi Chen","Chuheng Zhang","Marcelo H. Ang"],"pdf_url":"https://arxiv.org/pdf/2403.15834v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.15826v1","updated":"2024-03-23T12:53:51Z","published":"2024-03-23T12:53:51Z","title":"Scaling Learning based Policy Optimization for Temporal Tasks via\n  Dropout","summary":"  This paper introduces a model-based approach for training feedback\ncontrollers for an autonomous agent operating in a highly nonlinear\nenvironment. We desire the trained policy to ensure that the agent satisfies\nspecific task objectives, expressed in discrete-time Signal Temporal Logic\n(DT-STL). One advantage for reformulation of a task via formal frameworks, like\nDT-STL, is that it permits quantitative satisfaction semantics. In other words,\ngiven a trajectory and a DT-STL formula, we can compute the robustness, which\ncan be interpreted as an approximate signed distance between the trajectory and\nthe set of trajectories satisfying the formula. We utilize feedback\ncontrollers, and we assume a feed forward neural network for learning these\nfeedback controllers. We show how this learning problem is similar to training\nrecurrent neural networks (RNNs), where the number of recurrent units is\nproportional to the temporal horizon of the agent's task objectives. This poses\na challenge: RNNs are susceptible to vanishing and exploding gradients, and\nna\\\"{i}ve gradient descent-based strategies to solve long-horizon task\nobjectives thus suffer from the same problems. To tackle this challenge, we\nintroduce a novel gradient approximation algorithm based on the idea of dropout\nor gradient sampling. We show that, the existing smooth semantics for\nrobustness are inefficient regarding gradient computation when the\nspecification becomes complex. To address this challenge, we propose a new\nsmooth semantics for DT-STL that under-approximates the robustness value and\nscales well for backpropagation over a complex specification. We show that our\ncontrol synthesis methodology, can be quite helpful for stochastic gradient\ndescent to converge with less numerical issues, enabling scalable\nbackpropagation over long time horizons and trajectories over high dimensional\nstate spaces.\n","authors":["Navid Hashemi","Bardh Hoxha","Danil Prokhorov","Georgios Fainekos","Jyotirmoy Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2403.15826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14945v2","updated":"2024-03-23T12:39:14Z","published":"2023-09-26T14:00:25Z","title":"Integration of Large Language Models within Cognitive Architectures for\n  Autonomous Robots","summary":"  Symbolic reasoning systems have been used in cognitive architectures to\nprovide inference and planning capabilities. However, defining domains and\nproblems has proven difficult and prone to errors. Moreover, Large Language\nModels (LLMs) have emerged as tools to process natural language for different\ntasks. In this paper, we propose the use of LLMs to tackle these problems. This\nway, this paper proposes the integration of LLMs in the ROS 2-integrated\ncognitive architecture MERLIN2 for autonomous robots. Specifically, we present\nthe design, development and deployment of how to leverage the reasoning\ncapabilities of LLMs inside the deliberative processes of MERLIN2. As a result,\nthe deliberative system is updated from a PDDL-based planner system to a\nnatural language planning system. This proposal is evaluated quantitatively and\nqualitatively, measuring the impact of incorporating the LLMs in the cognitive\narchitecture. Results show that a classical approach achieves better\nperformance but the proposed solution provides an enhanced interaction through\nnatural language.\n","authors":["Miguel Á. González-Santamarta","Francisco J. Rodríguez-Lera","Ángel Manuel Guerrero-Higueras","Vicente Matellán-Olivera"],"pdf_url":"https://arxiv.org/pdf/2309.14945v2.pdf","comment":"8 pages, 6 figures, 2 tables, Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15813v1","updated":"2024-03-23T12:00:00Z","published":"2024-03-23T12:00:00Z","title":"Learning Early Social Maneuvers for Enhanced Social Navigation","summary":"  Socially compliant navigation is an integral part of safety features in\nHuman-Robot Interaction. Traditional approaches to mobile navigation prioritize\nphysical aspects, such as efficiency, but social behaviors gain traction as\nrobots appear more in daily life. Recent techniques to improve the social\ncompliance of navigation often rely on predefined features or reward functions,\nintroducing assumptions about social human behavior. To address this\nlimitation, we propose a novel Learning from Demonstration (LfD) framework for\nsocial navigation that exclusively utilizes raw sensory data. Additionally, the\nproposed system contains mechanisms to consider the future paths of the\nsurrounding pedestrians, acknowledging the temporal aspect of the problem. The\nfinal product is expected to reduce the anxiety of people sharing their\nenvironment with a mobile robot, helping them trust that the robot is aware of\ntheir presence and will not harm them. As the framework is currently being\ndeveloped, we outline its components, present experimental results, and discuss\nfuture work towards realizing this framework.\n","authors":["Yigit Yıldırim","Mehmet Suzer","Emre Ugur"],"pdf_url":"https://arxiv.org/pdf/2403.15813v1.pdf","comment":"Submitted to the workshop of Robot Trust for Symbiotic Societies\n  (RTSS) at ICRA 2024 on March 23, 2024"},{"id":"http://arxiv.org/abs/2403.15812v1","updated":"2024-03-23T11:50:20Z","published":"2024-03-23T11:50:20Z","title":"The Impact of Evolutionary Computation on Robotic Design: A Case Study\n  with an Underactuated Hand Exoskeleton","summary":"  Robotic exoskeletons can enhance human strength and aid people with physical\ndisabilities. However, designing them to ensure safety and optimal performance\npresents significant challenges. Developing exoskeletons should incorporate\nspecific optimization algorithms to find the best design. This study\ninvestigates the potential of Evolutionary Computation (EC) methods in robotic\ndesign optimization, with an underactuated hand exoskeleton (U-HEx) used as a\ncase study. We propose improving the performance and usability of the U-HEx\ndesign, which was initially optimized using a naive brute-force approach, by\nintegrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch\nAlgorithm. Comparative analysis revealed that EC methods consistently yield\nmore precise and optimal solutions than brute force in a significantly shorter\ntime. This allowed us to improve the optimization by increasing the number of\nvariables in the design, which was impossible with naive methods. The results\nshow significant improvements in terms of the torque magnitude the device\ntransfers to the user, enhancing its efficiency. These findings underline the\nimportance of performing proper optimization while designing exoskeletons, as\nwell as providing a significant improvement to this specific robotic design.\n","authors":["Baris Akbas","Huseyin Taner Yuksel","Aleyna Soylemez","Mazhar Eid Zyada","Mine Sarac","Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2403.15812v1.pdf","comment":"6 pages (+ref), 4 figures, IEEE International Conference on Robotics\n  and Automation (ICRA) 2024"},{"id":"http://arxiv.org/abs/2403.15805v1","updated":"2024-03-23T11:30:44Z","published":"2024-03-23T11:30:44Z","title":"AirCrab: A Hybrid Aerial-Ground Manipulator with An Active Wheel","summary":"  Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground\nmanipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF)\nmanipulator. AirCrab leverages a single point of contact with the ground to\nreduce position drift and improve manipulation accuracy. The single active\nwheel enables locomotion on narrow surfaces without adding significant weight\nto the robot. To realize accurate attitude maintenance using propellers on the\nground, we design a control allocation method for AirCrab that prioritizes\nattitude control and dynamically adjusts the thrust input to reduce energy\nconsumption. Experiments verify the effectiveness of the proposed control\nmethod and the gain in manipulation accuracy with ground contact. A series of\noperations to complete the letters 'NTU' demonstrates the capability of the\nrobot to perform challenging hybrid aerial-ground manipulation missions.\n","authors":["Muqing Cao","Jiayan Zhao","Xinhang Xu","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.15805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14447v2","updated":"2024-03-23T11:26:38Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and Spot, the quadruped robot manufactured\nby Boston Dynamics. The key-novelty is the focus on the robot's perspective,\ni.e., on the data captured by the robot's sensors. These make 3D body pose\nanalysis challenging because being close to the ground captures humans only\npartially. The scenario underlying HARPER includes 15 actions, of which 10\ninvolve physical contact between the robot and users. The Corpus contains not\nonly the recordings of the built-in stereo cameras of Spot, but also those of a\n6-camera OptiTrack system (all recordings are synchronized). This leads to\nground-truth skeletal representations with a precision lower than a millimeter.\nIn addition, the Corpus includes reproducible benchmarks on 3D Human Pose\nEstimation, Human Pose Forecasting, and Collision Prediction, all based on\npublicly available baseline approaches. This enables future HARPER users to\nrigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro","Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15798v1","updated":"2024-03-23T11:09:41Z","published":"2024-03-23T11:09:41Z","title":"Vid2Real HRI: Align video-based HRI study designs with real-world\n  settings","summary":"  HRI research using autonomous robots in real-world settings can produce\nresults with the highest ecological validity of any study modality, but many\ndifficulties limit such studies' feasibility and effectiveness. We propose\nVid2Real HRI, a research framework to maximize real-world insights offered by\nvideo-based studies. The Vid2Real HRI framework was used to design an online\nstudy using first-person videos of robots as real-world encounter surrogates.\nThe online study ($n = 385$) distinguished the within-subjects effects of four\nrobot behavioral conditions on perceived social intelligence and human\nwillingness to help the robot enter an exterior door. A real-world,\nbetween-subjects replication ($n = 26$) using two conditions confirmed the\nvalidity of the online study's findings and the sufficiency of the participant\nrecruitment target ($22$) based on a power analysis of online study results.\nThe Vid2Real HRI framework offers HRI researchers a principled way to take\nadvantage of the efficiency of video-based study modalities while generating\ndirectly transferable knowledge of real-world HRI. Code and data from the study\nare provided at https://vid2real.github.io/vid2realHRI\n","authors":["Elliott Hauser","Yao-Cheng Chan","Sadanand Modak","Joydeep Biswas","Justin Hart"],"pdf_url":"https://arxiv.org/pdf/2403.15798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15791v1","updated":"2024-03-23T10:38:59Z","published":"2024-03-23T10:38:59Z","title":"DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving\n  Environment for Real-World Performance Validation","summary":"  In this study, we introduce the DriveEnv-NeRF framework, which leverages\nNeural Radiance Fields (NeRF) to enable the validation and faithful forecasting\nof the efficacy of autonomous driving agents in a targeted real-world scene.\nStandard simulator-based rendering often fails to accurately reflect real-world\nperformance due to the sim-to-real gap, which represents the disparity between\nvirtual simulations and real-world conditions. To mitigate this gap, we propose\na workflow for building a high-fidelity simulation environment of the targeted\nreal-world scene using NeRF. This approach is capable of rendering realistic\nimages from novel viewpoints and constructing 3D meshes for emulating\ncollisions. The validation of these capabilities through the comparison of\nsuccess rates in both simulated and real environments demonstrates the benefits\nof using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the\nDriveEnv-NeRF framework can serve as a training environment for autonomous\ndriving agents under various lighting conditions. This approach enhances the\nrobustness of the agents and reduces performance degradation when deployed to\nthe target real scene, compared to agents fully trained using the standard\nsimulator rendering pipeline.\n","authors":["Mu-Yi Shen","Chia-Chi Hsu","Hao-Yu Hou","Yu-Chen Huang","Wei-Fang Sun","Chia-Che Chang","Yu-Lun Liu","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2403.15791v1.pdf","comment":"Project page: https://github.com/muyishen2040/DriveEnvNeRF"},{"id":"http://arxiv.org/abs/2403.15762v1","updated":"2024-03-23T08:26:20Z","published":"2024-03-23T08:26:20Z","title":"RicMonk: A Three-Link Brachiation Robot with Passive Grippers for\n  Energy-Efficient Brachiation","summary":"  This paper presents the design, analysis, and performance evaluation of\nRicMonk, a novel three-link brachiation robot equipped with passive hook-shaped\ngrippers. Brachiation, an agile and energy-efficient mode of locomotion\nobserved in primates, has inspired the development of RicMonk to explore\nversatile locomotion and maneuvers on ladder-like structures. The robot's\nanatomical resemblance to gibbons and the integration of a tail mechanism for\nenergy injection contribute to its unique capabilities. The paper discusses the\nuse of the Direct Collocation methodology for optimizing trajectories for the\nrobot's dynamic behaviors and stabilization of these trajectories using a\nTime-varying Linear Quadratic Regulator. With RicMonk we demonstrate\nbidirectional brachiation, and provide comparative analysis with its\npredecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the\npresence of a passive tail helps improve energy efficiency. The system design,\ncontrollers, and software implementation are publicly available on GitHub and\nthe video demonstration of the experiments can be viewed YouTube.\n","authors":["Shourie S. Grama","Mahdi Javadi","Shivesh Kumar","Hossein Zamani Boroujeni","Frank Kirchner"],"pdf_url":"https://arxiv.org/pdf/2403.15762v1.pdf","comment":"Open sourced system design, controllers, software implementation can\n  be found at https://github.com/dfki-ric-underactuated-lab/ricmonk and a video\n  demonstrating the experiments performed with RicMonk can be found at\n  https://www.youtube.com/watch?v=hOuDQI7CD8w"},{"id":"http://arxiv.org/abs/2209.01426v2","updated":"2024-03-23T07:50:26Z","published":"2022-09-03T14:00:43Z","title":"Space Filling Curves for Coverage Path Planning with Online Obstacle\n  Avoidance","summary":"  The paper presents a strategy for robotic exploration problem using\nSpace-Filling curves (SFC). The strategy plans a path that avoids unknown\nobstacles while ensuring complete coverage of the free space in region of\ninterest. The region of interest is first tessellated, and the tiles/cells are\nconnected using a SFC pattern. A robot follows the SFC to explore the entire\narea. However, obstacles can block the systematic movement of the robot. We\novercome this problem by determining an alternate path online that avoids the\nblocked cells while ensuring all the accessible cells are visited at least\nonce. The proposed strategy chooses next waypoint based on the graph\nconnectivity of the cells and the obstacle encountered so far. It is online,\nexhaustive and works in situations demanding non-uniform coverage. The\ncompleteness of the strategy is proved and its desirable properties are\ndiscussed with examples.\n","authors":["Ashay Wakode","Arpita Sinha"],"pdf_url":"https://arxiv.org/pdf/2209.01426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05022v2","updated":"2024-03-23T06:58:58Z","published":"2023-10-08T05:48:30Z","title":"Fully Spiking Neural Network for Legged Robots","summary":"  In recent years, legged robots based on deep reinforcement learning have made\nremarkable progress. Quadruped robots have demonstrated the ability to complete\nchallenging tasks in complex environments and have been deployed in real-world\nscenarios to assist humans. Simultaneously, bipedal and humanoid robots have\nachieved breakthroughs in various demanding tasks. Current reinforcement\nlearning methods can utilize diverse robot bodies and historical information to\nperform actions. However, prior research has not emphasized the speed and\nenergy consumption of network inference, as well as the biological significance\nof the neural networks themselves. Most of the networks employed are\ntraditional artificial neural networks that utilize multilayer perceptrons\n(MLP). In this paper, we successfully apply a novel Spiking Neural Network\n(SNN) to process legged robots, achieving outstanding results across a range of\nsimulated terrains. SNN holds a natural advantage over traditional neural\nnetworks in terms of inference speed and energy consumption, and their\npulse-form processing of body perception signals offers improved biological\ninterpretability. Applying more biomimetic neural networks to legged robots can\nfurther reduce the heat dissipation and structural burden caused by the high\npower consumption of neural networks. To the best of our knowledge, this is the\nfirst work to implement SNN in legged robots.\n","authors":["Xiaoyang Jiang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Jingtong Ma","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15716v1","updated":"2024-03-23T04:36:12Z","published":"2024-03-23T04:36:12Z","title":"Distributed Robust Learning based Formation Control of Mobile Robots\n  based on Bioinspired Neural Dynamics","summary":"  This paper addresses the challenges of distributed formation control in\nmultiple mobile robots, introducing a novel approach that enhances real-world\npracticability. We first introduce a distributed estimator using a variable\nstructure and cascaded design technique, eliminating the need for derivative\ninformation to improve the real time performance. Then, a kinematic tracking\ncontrol method is developed utilizing a bioinspired neural dynamic-based\napproach aimed at providing smooth control inputs and effectively resolving the\nspeed jump issue. Furthermore, to address the challenges for robots operating\nwith completely unknown dynamics and disturbances, a learning-based robust\ndynamic controller is developed. This controller provides real time parameter\nestimates while maintaining its robustness against disturbances. The overall\nstability of the proposed method is proved with rigorous mathematical analysis.\nAt last, multiple comprehensive simulation studies have shown the advantages\nand effectiveness of the proposed method.\n","authors":["Zhe Xu","Tao Yan","Simon X. Yang","S. Andrew Gadsden","Mohammad Biglarbegian"],"pdf_url":"https://arxiv.org/pdf/2403.15716v1.pdf","comment":"This paper is accepted by IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.15712v1","updated":"2024-03-23T04:18:49Z","published":"2024-03-23T04:18:49Z","title":"PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture\n  Search","summary":"  Multiple object tracking is a critical task in autonomous driving. Existing\nworks primarily focus on the heuristic design of neural networks to obtain high\naccuracy. As tracking accuracy improves, however, neural networks become\nincreasingly complex, posing challenges for their practical application in real\ndriving scenarios due to the high level of latency. In this paper, we explore\nthe use of the neural architecture search (NAS) methods to search for efficient\narchitectures for tracking, aiming for low real-time latency while maintaining\nrelatively high accuracy. Another challenge for object tracking is the\nunreliability of a single sensor, therefore, we propose a multi-modal framework\nto improve the robustness. Experiments demonstrate that our algorithm can run\non edge devices within lower latency constraints, thus greatly reducing the\ncomputational requirements for multi-modal object tracking while keeping lower\nlatency.\n","authors":["Chensheng Peng","Zhaoyu Zeng","Jinling Gao","Jundong Zhou","Masayoshi Tomizuka","Xinbing Wang","Chenghu Zhou","Nanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15712v1.pdf","comment":"IEEE Robotics and Automation Letters 2024. Code is available at\n  https://github.com/PholyPeng/PNAS-MOT"},{"id":"http://arxiv.org/abs/2309.10062v2","updated":"2024-03-23T03:50:18Z","published":"2023-09-18T18:17:56Z","title":"SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language\n  Models","summary":"  In this work, we introduce SMART-LLM, an innovative framework designed for\nembodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task\nPlanning using Large Language Models (LLMs), harnesses the power of LLMs to\nconvert high-level task instructions provided as input into a multi-robot task\nplan. It accomplishes this by executing a series of stages, including task\ndecomposition, coalition formation, and task allocation, all guided by\nprogrammatic LLM prompts within the few-shot prompting paradigm. We create a\nbenchmark dataset designed for validating the multi-robot task planning\nproblem, encompassing four distinct categories of high-level instructions that\nvary in task complexity. Our evaluation experiments span both simulation and\nreal-world scenarios, demonstrating that the proposed model can achieve\npromising results for generating multi-robot task plans. The experimental\nvideos, code, and datasets from the work can be found at\nhttps://sites.google.com/view/smart-llm/.\n","authors":["Shyam Sundar Kannan","Vishnunandan L. N. Venkatesh","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2309.10062v2.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2312.14481v2","updated":"2024-03-23T03:13:35Z","published":"2023-12-22T07:17:51Z","title":"SurgicalPart-SAM: Part-to-Whole Collaborative Prompting for Surgical\n  Instrument Segmentation","summary":"  The Segment Anything Model (SAM) exhibits promise in generic object\nsegmentation and offers potential for various applications. Existing methods\nhave applied SAM to surgical instrument segmentation (SIS) by tuning SAM-based\nframeworks with surgical data. However, they fall short in two crucial aspects:\n(1) Straightforward model tuning with instrument masks treats each instrument\nas a single entity, neglecting their complex structures and fine-grained\ndetails; and (2) Instrument category-based prompts are not flexible and\ninformative enough to describe instrument structures. To address these\nproblems, in this paper, we investigate text promptable SIS and propose\nSurgicalPart-SAM (SP-SAM), a novel SAM efficient-tuning approach that\nexplicitly integrates instrument structure knowledge with SAM's generic\nknowledge, guided by expert knowledge on instrument part compositions.\nSpecifically, we achieve this by proposing (1) Collaborative Prompts that\ndescribe instrument structures via collaborating category-level and part-level\ntexts; (2) Cross-Modal Prompt Encoder that encodes text prompts jointly with\nvisual embeddings into discriminative part-level representations; and (3)\nPart-to-Whole Adaptive Fusion and Hierarchical Decoding that adaptively fuse\nthe part-level representations into a whole for accurate instrument\nsegmentation in surgical scenarios. Built upon them, SP-SAM acquires a better\ncapability to comprehend surgical instruments in terms of both overall\nstructure and part-level details. Extensive experiments on both the EndoVis2018\nand EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with\nminimal tunable parameters. The code will be available at\nhttps://github.com/wenxi-yue/SurgicalPart-SAM.\n","authors":["Wenxi Yue","Jing Zhang","Kun Hu","Qiuxia Wu","Zongyuan Ge","Yong Xia","Jiebo Luo","Zhiyong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.14481v2.pdf","comment":"Technical Report. The source code will be released at\n  https://github.com/wenxi-yue/SurgicalPart-SAM"},{"id":"http://arxiv.org/abs/2403.15658v1","updated":"2024-03-23T00:39:05Z","published":"2024-03-23T00:39:05Z","title":"Data-Driven Predictive Control for Robust Exoskeleton Locomotion","summary":"  Exoskeleton locomotion must be robust while being adaptive to different users\nwith and without payloads. To address these challenges, this work introduces a\ndata-driven predictive control (DDPC) framework to synthesize walking gaits for\nlower-body exoskeletons, employing Hankel matrices and a state transition\nmatrix for its data-driven model. The proposed approach leverages DDPC through\na multi-layer architecture. At the top layer, DDPC serves as a planner\nemploying Hankel matrices and a state transition matrix to generate a\ndata-driven model that can learn and adapt to varying users and payloads. At\nthe lower layer, our method incorporates inverse kinematics and passivity-based\ncontrol to map the planned trajectory from DDPC into the full-order states of\nthe lower-body exoskeleton. We validate the effectiveness of this approach\nthrough numerical simulations and hardware experiments conducted on the\nAtalante lower-body exoskeleton with different payloads. Moreover, we conducted\na comparative analysis against the model predictive control (MPC) framework\nbased on the reduced-order linear inverted pendulum (LIP) model. Through this\ncomparison, the paper demonstrates that DDPC enables robust bipedal walking at\nvarious velocities while accounting for model uncertainties and unknown\nperturbations.\n","authors":["Kejun Li","Jeeseop Kim","Xiaobin Xiong","Kaveh Akbari Hamed","Yisong Yue","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2403.15658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18847v2","updated":"2024-03-23T00:28:21Z","published":"2023-10-28T23:25:19Z","title":"Bird's Eye View Based Pretrained World model for Visual Navigation","summary":"  Sim2Real transfer has gained popularity because it helps transfer from\ninexpensive simulators to real world. This paper presents a novel system that\nfuses components in a traditional World Model into a robust system, trained\nentirely within a simulator, that Zero-Shot transfers to the real world. To\nfacilitate transfer, we use an intermediary representation that is based on\n\\textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a\nsimulator by first learning to translate from complex \\textit{First-Person View\n(FPV)} based RGB images to BEV representations, then learning to navigate using\nthose representations. Later, when tested in the real world, the robot uses the\nperception model that translates FPV-based RGB images to embeddings that were\nlearned by the FPV to BEV translator and that can be used by the downstream\npolicy. The incorporation of state-checking modules using \\textit{Anchor\nimages} and Mixture Density LSTM not only interpolates uncertain and missing\nobservations but also enhances the robustness of the model in the real-world.\nWe trained the model using data from a Differential drive robot in the CARLA\nsimulator. Our methodology's effectiveness is shown through the deployment of\ntrained models onto a real-world Differential drive robot. Lastly we release a\ncomprehensive codebase, dataset and models for training and deployment\n(\\url{https://sites.google.com/view/value-explicit-pretraining}).\n","authors":["Kiran Lekkala","Chen Liu","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2310.18847v2.pdf","comment":"Under Review at the IROS 2024; Accepted at NeurIPS 2023, Robot\n  Learning Workshop"},{"id":"http://arxiv.org/abs/2403.15717v1","updated":"2024-03-23T04:44:55Z","published":"2024-03-23T04:44:55Z","title":"Ev-Edge: Efficient Execution of Event-based Vision Algorithms on\n  Commodity Edge Platforms","summary":"  Event cameras have emerged as a promising sensing modality for autonomous\nnavigation systems, owing to their high temporal resolution, high dynamic range\nand negligible motion blur. To process the asynchronous temporal event streams\nfrom such sensors, recent research has shown that a mix of Artificial Neural\nNetworks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN\nalgorithms are necessary to achieve high accuracies across a range of\nperception tasks. However, we observe that executing such workloads on\ncommodity edge platforms which feature heterogeneous processing elements such\nas CPUs, GPUs and neural accelerators results in inferior performance. This is\ndue to the mismatch between the irregular nature of event streams and diverse\ncharacteristics of algorithms on the one hand and the underlying hardware\nplatform on the other. We propose Ev-Edge, a framework that contains three key\noptimizations to boost the performance of event-based vision systems on edge\nplatforms: (1) An Event2Sparse Frame converter directly transforms raw event\nstreams into sparse frames, enabling the use of sparse libraries with minimal\nencoding overheads (2) A Dynamic Sparse Frame Aggregator merges sparse frames\nat runtime by trading off the temporal granularity of events and computational\ndemand thereby improving hardware utilization (3) A Network Mapper maps\nconcurrently executing tasks to different processing elements while also\nselecting layer precision by considering both compute and communication\noverheads. On several state-of-art networks for a range of autonomous\nnavigation tasks, Ev-Edge achieves 1.28x-2.05x improvements in latency and\n1.23x-2.15x in energy over an all-GPU implementation on the NVIDIA Jetson\nXavier AGX platform for single-task execution scenarios. Ev-Edge also achieves\n1.43x-1.81x latency improvements over round-robin scheduling methods in\nmulti-task execution scenarios.\n","authors":["Shrihari Sridharan","Surya Selvam","Kaushik Roy","Anand Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2403.15717v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2403.15902v1","updated":"2024-03-23T18:12:39Z","published":"2024-03-23T18:12:39Z","title":"Utilizing Motion Matching with Deep Reinforcement Learning for Target\n  Location Tasks","summary":"  We present an approach using deep reinforcement learning (DRL) to directly\ngenerate motion matching queries for long-term tasks, particularly targeting\nthe reaching of specific locations. By integrating motion matching and DRL, our\nmethod demonstrates the rapid learning of policies for target location tasks\nwithin minutes on a standard desktop, employing a simple reward design.\nAdditionally, we propose a unique hit reward and obstacle curriculum scheme to\nenhance policy learning in environments with moving obstacles.\n","authors":["Jeongmin Lee","Taesoo Kwon","Hyunju Shin","Yoonsang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.15902v1.pdf","comment":"Eurographics 2024 Short Papers"},{"id":"http://arxiv.org/abs/2110.10494v2","updated":"2024-03-23T14:52:51Z","published":"2021-10-20T11:16:00Z","title":"Deep Point Cloud Normal Estimation via Triplet Learning","summary":"  Normal estimation on 3D point clouds is a fundamental problem in 3D vision\nand graphics. Current methods often show limited accuracy in predicting normals\nat sharp features (e.g., edges and corners) and less robustness to noise. In\nthis paper, we propose a novel normal estimation method for point clouds. It\nconsists of two phases: (a) feature encoding which learns representations of\nlocal patches, and (b) normal estimation that takes the learned representation\nas input and regresses the normal vector. We are motivated that local patches\non isotropic and anisotropic surfaces have similar or distinct normals, and\nthat separable features or representations can be learned to facilitate normal\nestimation. To realise this, we first construct triplets of local patches on 3D\npoint cloud data, and design a triplet network with a triplet loss for feature\nencoding. We then design a simple network with several MLPs and a loss function\nto regress the normal vector. Despite having a smaller network size compared to\nmost other methods, experimental results show that our method preserves sharp\nfeatures and achieves better normal estimation results on CAD-like shapes.\n","authors":["Weijia Wang","Xuequan Lu","Dasith de Silva Edirimuni","Xiao Liu","Antonio Robles-Kelly"],"pdf_url":"https://arxiv.org/pdf/2110.10494v2.pdf","comment":"Accepted by ICME 2022. Supplementary material available at\n  https://ieeexplore.ieee.org/document/9859844/media#media"},{"id":"http://arxiv.org/abs/2311.17050v2","updated":"2024-03-23T02:22:04Z","published":"2023-11-28T18:56:01Z","title":"Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using\n  Diffusion Models","summary":"  We present Surf-D, a novel method for generating high-quality 3D shapes as\nSurfaces with arbitrary topologies using Diffusion models. Previous methods\nexplored shape generation with different representations and they suffer from\nlimited topologies and poor geometry details. To generate high-quality surfaces\nof arbitrary topologies, we use the Unsigned Distance Field (UDF) as our\nsurface representation to accommodate arbitrary topologies. Furthermore, we\npropose a new pipeline that employs a point-based AutoEncoder to learn a\ncompact and continuous latent space for accurately encoding UDF and support\nhigh-resolution mesh extraction. We further show that our new pipeline\nsignificantly outperforms the prior approaches to learning the distance fields,\nsuch as the grid-based AutoEncoder, which is not scalable and incapable of\nlearning accurate UDF. In addition, we adopt a curriculum learning strategy to\nefficiently embed various surfaces. With the pretrained shape latent space, we\nemploy a latent diffusion model to acquire the distribution of various shapes.\nExtensive experiments are presented on using Surf-D for unconditional\ngeneration, category conditional generation, image conditional generation, and\ntext-to-shape tasks. The experiments demonstrate the superior performance of\nSurf-D in shape generation across multiple modalities as conditions. Visit our\nproject page at https://yzmblog.github.io/projects/SurfD/.\n","authors":["Zhengming Yu","Zhiyang Dou","Xiaoxiao Long","Cheng Lin","Zekun Li","Yuan Liu","Norman Müller","Taku Komura","Marc Habermann","Christian Theobalt","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17050v2.pdf","comment":"Project Page: https://yzmblog.github.io/projects/SurfD/"}]},"2024-03-26T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17917v1","updated":"2024-03-26T17:54:05Z","published":"2024-03-26T17:54:05Z","title":"Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes","summary":"  This paper presents two algorithms for multi-agent dynamic coverage in\nspatiotemporal environments, where the coverage algorithms are informed by the\nmethod of data assimilation. In particular, we show that by considering the\ninformation assimilation algorithm, here a Numerical Gaussian Process Kalman\nFilter, the influence of measurements taken at one position on the uncertainty\nof the estimate at another location can be computed. We use this relationship\nto propose new coverage algorithms. Furthermore, we show that the controllers\nnaturally extend to the multi-agent context, allowing for a distributed-control\ncentral-information paradigm for multi-agent coverage. Finally, we demonstrate\nthe algorithms through a realistic simulation of a team of UAVs collecting wind\ndata over a region in Austria.\n","authors":["Devansh R. Agrawal","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2403.17917v1.pdf","comment":"8 pages, 2 figures, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03773v3","updated":"2024-03-26T17:17:39Z","published":"2023-04-04T21:49:02Z","title":"Safe Explicable Planning","summary":"  Human expectations arise from their understanding of others and the world. In\nthe context of human-AI interaction, this understanding may not align with\nreality, leading to the AI agent failing to meet expectations and compromising\nteam performance. Explicable planning, introduced as a method to bridge this\ngap, aims to reconcile human expectations with the agent's optimal behavior,\nfacilitating interpretable decision-making. However, an unresolved critical\nissue is ensuring safety in explicable planning, as it could result in\nexplicable behaviors that are unsafe. To address this, we propose Safe\nExplicable Planning (SEP), which extends the prior work to support the\nspecification of a safety bound. The goal of SEP is to find behaviors that\nalign with human expectations while adhering to the specified safety criterion.\nOur approach generalizes the consideration of multiple objectives stemming from\nmultiple models rather than a single model, yielding a Pareto set of safe\nexplicable policies. We present both an exact method, guaranteeing finding the\nPareto set, and a more efficient greedy method that finds one of the policies\nin the Pareto set. Additionally, we offer approximate solutions based on state\naggregation to improve scalability. We provide formal proofs that validate the\ndesired theoretical properties of these methods. Evaluation through simulations\nand physical robot experiments confirms the effectiveness of our approach for\nsafe explicable planning.\n","authors":["Akkamahadevi Hanni","Andrew Boateng","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.03773v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17849v1","updated":"2024-03-26T16:38:12Z","published":"2024-03-26T16:38:12Z","title":"Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial\n  Vehicles","summary":"  Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple\nagents from respective start to goal locations such that no paths conflict. We\naddress the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles\nwhich are subject to location-dependent noise restrictions. We solve this\nproblem by searching a constraint tree for which the subproblem at each node is\na set of shortest path problems subject to the noise and fuel constraints and\nconflict zone avoidance. A labeling algorithm is presented to solve this\nsubproblem, including the conflict zones which are treated as dynamic\nobstacles. We present the experimental results of the algorithms for various\ngraph sizes and number of agents.\n","authors":["Drew Scott","Satyanarayana G. Manyam","David W. Casbeer","Manish Kumar","Isaac E. Weintraub"],"pdf_url":"https://arxiv.org/pdf/2403.17849v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2309.02937v2","updated":"2024-03-26T15:51:49Z","published":"2023-09-06T12:04:24Z","title":"Resilient source seeking with robot swarms","summary":"  We present a solution for locating the source, or maximum, of an unknown\nscalar field using a swarm of mobile robots. Unlike relying on the traditional\ngradient information, the swarm determines an ascending direction to approach\nthe source with arbitrary precision. The ascending direction is calculated from\nmeasurements of the field strength at the robot locations and their relative\npositions concerning the centroid. Rather than focusing on individual robots,\nwe focus the analysis on the density of robots per unit area to guarantee a\nmore resilient swarm, i.e., the functionality remains even if individuals go\nmissing or are misplaced during the mission. We reinforce the robustness of the\nalgorithm by providing sufficient conditions for the swarm shape so that the\nascending direction is almost parallel to the gradient. The swarm can respond\nto an unexpected environment by morphing its shape and exploiting the existence\nof multiple ascending directions. Finally, we validate our approach numerically\nwith hundreds of robots. The fact that a large number of robots always\ncalculate an ascending direction compensates for the loss of individuals and\nmitigates issues arising from the actuator and sensor noises.\n","authors":["Antonio Acuaviva","Jesus Bautista","Weijia Yao","Juan Jimenez","Hector Garcia de Marina"],"pdf_url":"https://arxiv.org/pdf/2309.02937v2.pdf","comment":"7 pages, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2403.17788v1","updated":"2024-03-26T15:20:56Z","published":"2024-03-26T15:20:56Z","title":"System Calibration of a Field Phenotyping Robot with Multiple\n  High-Precision Profile Laser Scanners","summary":"  The creation of precise and high-resolution crop point clouds in agricultural\nfields has become a key challenge for high-throughput phenotyping applications.\nThis work implements a novel calibration method to calibrate the laser scanning\nsystem of an agricultural field robot consisting of two industrial-grade laser\nscanners used for high-precise 3D crop point cloud creation. The calibration\nmethod optimizes the transformation between the scanner origins and the robot\npose by minimizing 3D point omnivariances within the point cloud. Moreover, we\npresent a novel factor graph-based pose estimation method that fuses total\nstation prism measurements with IMU and GNSS heading information for\nhigh-precise pose determination during calibration. The root-mean-square error\nof the distances to a georeferenced ground truth point cloud results in 0.8 cm\nafter parameter optimization. Furthermore, our results show the importance of a\nreference point cloud in the calibration method needed to estimate the vertical\ntranslation of the calibration. Challenges arise due to non-static parameters\nwhile the robot moves, indicated by systematic deviations to a ground truth\nterrestrial laser scan.\n","authors":["Felix Esser","Gereon Tombrink","Andre Cornelißen","Lasse Klingbeil","Heiner Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2403.17788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17779v1","updated":"2024-03-26T15:12:46Z","published":"2024-03-26T15:12:46Z","title":"Optical Flow Based Detection and Tracking of Moving Objects for\n  Autonomous Vehicles","summary":"  Accurate velocity estimation of surrounding moving objects and their\ntrajectories are critical elements of perception systems in\nAutomated/Autonomous Vehicles (AVs) with a direct impact on their safety. These\nare non-trivial problems due to the diverse types and sizes of such objects and\ntheir dynamic and random behaviour. Recent point cloud based solutions often\nuse Iterative Closest Point (ICP) techniques, which are known to have certain\nlimitations. For example, their computational costs are high due to their\niterative nature, and their estimation error often deteriorates as the relative\nvelocities of the target objects increase (>2 m/sec). Motivated by such\nshortcomings, this paper first proposes a novel Detection and Tracking of\nMoving Objects (DATMO) for AVs based on an optical flow technique, which is\nproven to be computationally efficient and highly accurate for such problems.\n\\textcolor{black}{This is achieved by representing the driving scenario as a\nvector field and applying vector calculus theories to ensure spatiotemporal\ncontinuity.} We also report the results of a comprehensive performance\nevaluation of the proposed DATMO technique, carried out in this study using\nsynthetic and real-world data. The results of this study demonstrate the\nsuperiority of the proposed technique, compared to the DATMO techniques in the\nliterature, in terms of estimation accuracy and processing time in a wide range\nof relative velocities of moving objects. Finally, we evaluate and discuss the\nsensitivity of the estimation error of the proposed DATMO technique to various\nsystem and environmental parameters, as well as the relative velocities of the\nmoving objects.\n","authors":["MReza Alipour Sormoli","Mehrdad Dianati","Sajjad Mozaffari","Roger woodman"],"pdf_url":"https://arxiv.org/pdf/2403.17779v1.pdf","comment":"This manuscript has been accepted as a regular paper in Transactions\n  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)"},{"id":"http://arxiv.org/abs/2403.17774v1","updated":"2024-03-26T15:07:27Z","published":"2024-03-26T15:07:27Z","title":"LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous\n  Navigation in Agriculture Fields","summary":"  Autonomous navigation is crucial for various robotics applications in\nagriculture. However, many existing methods depend on RTK-GPS systems, which\nare expensive and susceptible to poor signal coverage. This paper introduces a\nstate-of-the-art LiDAR-based navigation system that can achieve over-canopy\nautonomous navigation in row-crop fields, even when the canopy fully blocks the\ninterrow spacing. Our crop row detection algorithm can detect crop rows across\ndiverse scenarios, encompassing various crop types, growth stages, weed\npresence, and discontinuities within the crop rows. Without utilizing the\nglobal localization of the robot, our navigation system can perform autonomous\nnavigation in these challenging scenarios, detect the end of the crop rows, and\nnavigate to the next crop row autonomously, providing a crop-agnostic approach\nto navigate the whole row-crop field. This navigation system has undergone\ntests in various simulated agricultural fields, achieving an average of\n$2.98cm$ autonomous driving accuracy without human intervention on the custom\nAmiga robot. In addition, the qualitative results of our crop row detection\nalgorithm from the actual soybean fields validate our LiDAR-based crop row\ndetection algorithm's potential for practical agricultural applications.\n","authors":["Ruiji Liu","Francisco Yandun","George Kantor"],"pdf_url":"https://arxiv.org/pdf/2403.17774v1.pdf","comment":"7 pages, 9 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2311.15030v2","updated":"2024-03-26T14:33:51Z","published":"2023-11-25T13:51:14Z","title":"Tuning-free Quasi-stiffness Control Framework of a Powered Transfemoral\n  Prosthesis for Task-adaptive Walking","summary":"  Impedance-based control represents a prevalent strategy in the development of\npowered transfemoral prostheses. However, creating a task-adaptive, tuning-free\ncontroller that effectively generalizes across diverse locomotion modes and\nterrain conditions continues to be a significant challenge. This letter\nproposes a tuning-free and task-adaptive quasi-stiffness control framework for\npowered prostheses that generalizes across various walking tasks, including the\ntorque-angle relationship reconstruction part and the quasi-stiffness\ncontroller design part. A Gaussian Process Regression (GPR) model is introduced\nto predict the target features of the human joint angle and torque in a new\ntask. Subsequently, a Kernelized Movement Primitives (KMP) is employed to\nreconstruct the torque-angle relationship of the new task from multiple human\nreference trajectories and estimated target features. Based on the torque-angle\nrelationship of the new task, a quasi-stiffness control approach is designed\nfor a powered prosthesis. Finally, the proposed framework is validated through\npractical examples, including varying speeds and inclines walking tasks.\nNotably, the proposed framework not only aligns with but frequently surpasses\nthe performance of a benchmark finite state machine impedance controller\n(FSMIC) without necessitating manual impedance tuning and has the potential to\nexpand to variable walking tasks in daily life for the transfemoral amputees.\n","authors":["Teng Ma","Shucong Yin","Zhimin Hou","Binxin Huang","Haoyong Yu","Chenglong Fu"],"pdf_url":"https://arxiv.org/pdf/2311.15030v2.pdf","comment":"8 pages, 10 figures. This work has been submitted to the IEEE-RAL for\n  possible publication"},{"id":"http://arxiv.org/abs/2311.01885v2","updated":"2024-03-26T12:59:44Z","published":"2023-11-03T12:54:05Z","title":"Domain Randomization via Entropy Maximization","summary":"  Varying dynamics parameters in simulation is a popular Domain Randomization\n(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\nNevertheless, DR heavily hinges on the choice of the sampling distribution of\nthe dynamics parameters, since high variability is crucial to regularize the\nagent's behavior but notoriously leads to overly conservative policies when\nrandomizing excessively. In this paper, we propose a novel approach to address\nsim-to-real transfer, which automatically shapes dynamics distributions during\ntraining in simulation without requiring real-world data. We introduce DOmain\nRAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\nproblem that directly maximizes the entropy of the training distribution while\nretaining generalization capabilities. In achieving this, DORAEMON gradually\nincreases the diversity of sampled dynamics parameters as long as the\nprobability of success of the current policy is sufficiently high. We\nempirically validate the consistent benefits of DORAEMON in obtaining highly\nadaptive and generalizable policies, i.e. solving the task at hand across the\nwidest range of dynamics parameters, as opposed to representative baselines\nfrom the DR literature. Notably, we also demonstrate the Sim2Real applicability\nof DORAEMON through its successful zero-shot transfer in a robotic manipulation\nsetup under unknown real-world parameters.\n","authors":["Gabriele Tiboni","Pascal Klink","Jan Peters","Tatiana Tommasi","Carlo D'Eramo","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2311.01885v2.pdf","comment":"Published as a conference paper at ICLR 2024. Project website at\n  https://gabrieletiboni.github.io/doraemon/"},{"id":"http://arxiv.org/abs/2403.17667v1","updated":"2024-03-26T12:57:05Z","published":"2024-03-26T12:57:05Z","title":"Learning Goal-Directed Object Pushing in Cluttered Scenes with\n  Location-Based Attention","summary":"  Non-prehensile planar pushing is a challenging task due to its underactuated\nnature with hybrid-dynamics, where a robot needs to reason about an object's\nlong-term behaviour and contact-switching, while being robust to contact\nuncertainty. The presence of clutter in the environment further complicates\nthis task, introducing the need to include more sophisticated spatial analysis\nto avoid collisions. Building upon prior work on reinforcement learning (RL)\nwith multimodal categorical exploration for planar pushing, in this paper we\nincorporate location-based attention to enable robust navigation through\nclutter. Unlike previous RL literature addressing this obstacle avoidance\npushing task, our framework requires no predefined global paths and considers\nthe target orientation of the manipulated object. Our results demonstrate that\nthe learned policies successfully navigate through a wide range of complex\nobstacle configurations, including dynamic obstacles, with smooth motions,\nachieving the desired target object pose. We also validate the transferability\nof the learned policies to robotic hardware using the KUKA iiwa robot arm.\n","authors":["Nils Dengler","Juan Del Aguila Ferrandis","João Moura","Sethu Vijayakumar","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.17667v1.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)"},{"id":"http://arxiv.org/abs/2402.03246v5","updated":"2024-03-26T12:35:03Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17633v1","updated":"2024-03-26T12:08:14Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01598v3","updated":"2024-03-26T11:54:40Z","published":"2023-06-02T15:09:19Z","title":"Towards Source-free Domain Adaptive Semantic Segmentation via\n  Importance-aware and Prototype-contrast Learning","summary":"  Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.\n","authors":["Yihong Cao","Hui Zhang","Xiao Lu","Zheng Xiao","Kailun Yang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01598v3.pdf","comment":"Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\n  source code is publicly available at\n  https://github.com/yihong-97/Source-free-IAPC"},{"id":"http://arxiv.org/abs/2403.17622v1","updated":"2024-03-26T11:51:58Z","published":"2024-03-26T11:51:58Z","title":"Online Tree Reconstruction and Forest Inventory on a Mobile Robotic\n  System","summary":"  Terrestrial laser scanning (TLS) is the standard technique used to create\naccurate point clouds for digital forest inventories. However, the measurement\nprocess is demanding, requiring up to two days per hectare for data collection,\nsignificant data storage, as well as resource-heavy post-processing of 3D data.\nIn this work, we present a real-time mapping and analysis system that enables\nonline generation of forest inventories using mobile laser scanners that can be\nmounted e.g. on mobile robots. Given incrementally created and locally accurate\nsubmaps-data payloads-our approach extracts tree candidates using a custom,\nVoronoi-inspired clustering algorithm. Tree candidates are reconstructed using\nan adapted Hough algorithm, which enables robust modeling of the tree stem.\nFurther, we explicitly incorporate the incremental nature of the data\ncollection by consistently updating the database using a pose graph LiDAR SLAM\nsystem. This enables us to refine our estimates of the tree traits if an area\nis revisited later during a mission. We demonstrate competitive accuracy to TLS\nor manual measurements using laser scanners that we mounted on backpacks or\nmobile robots operating in conifer, broad-leaf and mixed forests. Our results\nachieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm\n(averaged across these sequences)-with no post-processing required after the\nmission is complete.\n","authors":["Leonard Freißmuth","Matias Mattamala","Nived Chebrolu","Simon Schaefer","Stefan Leutenegger","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.17622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02021v5","updated":"2024-03-26T11:41:36Z","published":"2022-09-05T15:41:13Z","title":"When Robotics Meets Wireless Communications: An Introductory Tutorial","summary":"  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles\n(UAVs) within the research community, industry, and society is growing fast.\nMany of these agents are nowadays equipped with communication systems that are,\nin some cases, essential to successfully achieve certain tasks. In this\ncontext, we have begun to witness the development of a new interdisciplinary\nresearch field at the intersection of robotics and communications. This\nresearch field has been boosted by the intention of integrating UAVs within the\n5G and 6G communication networks. This research will undoubtedly lead to many\nimportant applications in the near future. Nevertheless, one of the main\nobstacles to the development of this research area is that most researchers\naddress these problems by oversimplifying either the robotics or the\ncommunications aspect. This impedes the ability of reaching the full potential\nof this new interdisciplinary research area. In this tutorial, we present some\nof the modelling tools necessary to address problems involving both robotics\nand communication from an interdisciplinary perspective. As an illustrative\nexample of such problems, we focus in this tutorial on the issue of\ncommunication-aware trajectory planning.\n","authors":["Daniel Bonilla Licea","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2209.02021v5.pdf","comment":"35 pages, 192 references"},{"id":"http://arxiv.org/abs/2403.17606v1","updated":"2024-03-26T11:38:19Z","published":"2024-03-26T11:38:19Z","title":"Interactive Identification of Granular Materials using Force\n  Measurements","summary":"  The ability to identify granular materials facilitates the emergence of\nvarious new applications in robotics, ranging from cooking at home to truck\nloading at mining sites. However, granular material identification remains a\nchallenging and underexplored area. In this work, we present a novel\ninteractive material identification framework that enables robots to identify a\nwide range of granular materials using only a force-torque sensor for\nperception. Our framework, comprising interactive exploration, feature\nextraction, and classification stages, prioritizes simplicity and transparency\nfor seamless integration into various manipulation pipelines. We evaluate the\nproposed approach through extensive experiments with a real-world dataset\ncomprising 11 granular materials, which we also make publicly available.\nAdditionally, we conducted a comprehensive qualitative analysis of the dataset\nto offer deeper insights into its nature, aiding future development. Our\nresults show that the proposed method is capable of accurately identifying a\nwide range of granular materials solely relying on force measurements obtained\nfrom direct interaction with the materials. Code and dataset are available at:\nhttps://irobotics.aalto.fi/indentify_granular/.\n","authors":["Samuli Hynninen","Tran Nguyen Le","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.17606v1.pdf","comment":"Submitted to 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17565v1","updated":"2024-03-26T10:19:04Z","published":"2024-03-26T10:19:04Z","title":"Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control\n  via Spectral Method Model","summary":"  In this work, we present a model-based optimal boundary control design for an\naerial robotic system composed of a quadrotor carrying a flexible cable. The\nwhole system is modeled by partial differential equations (PDEs) combined with\nboundary conditions described by ordinary differential equations (ODEs). The\nproper orthogonal decomposition (POD) method is adopted to project the original\ninfinite-dimensional system on a subspace spanned by orthogonal basis\nfunctions. Based on the reduced order model, nonlinear model predictive control\n(NMPC) is implemented online to realize shape trajectory tracking of the\nflexible cable in an optimal predictive fashion. The proposed reduced modeling\nand optimal control paradigms are numerically verified against an accurate\nhigh-dimensional FDM-based model in different scenarios and the controller's\nsuperior performance is shown compared to an optimally tuned PID controller.\n","authors":["Yaolei Shen","Chiara Gabellieri","Antonio Franchi"],"pdf_url":"https://arxiv.org/pdf/2403.17565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17551v1","updated":"2024-03-26T09:58:27Z","published":"2024-03-26T09:58:27Z","title":"Time-Optimal Flight with Safety Constraints and Data-driven Dynamics","summary":"  Time-optimal quadrotor flight is an extremely challenging problem due to the\nlimited control authority encountered at the limit of handling. Model\nPredictive Contouring Control (MPCC) has emerged as a leading model-based\napproach for time optimization problems such as drone racing. However, the\nstandard MPCC formulation used in quadrotor racing introduces the notion of the\ngates directly in the cost function, creating a multi-objective optimization\nthat continuously trades off between maximizing progress and tracking the path\naccurately. This paper introduces three key components that enhance the MPCC\napproach for drone racing. First and foremost, we provide safety guarantees in\nthe form of a constraint and terminal set. The safety set is designed as a\nspatial constraint which prevents gate collisions while allowing for\ntime-optimization only in the cost function. Second, we augment the existing\nfirst principles dynamics with a residual term that captures complex\naerodynamic effects and thrust forces learned directly from real world data.\nThird, we use Trust Region Bayesian Optimization (TuRBO), a state of the art\nglobal Bayesian Optimization algorithm, to tune the hyperparameters of the MPC\ncontroller given a sparse reward based on lap time minimization. The proposed\napproach achieves similar lap times to the best state-of-the-art RL and\noutperforms the best time-optimal controller while satisfying constraints. In\nboth simulation and real-world, our approach consistently prevents gate crashes\nwith 100\\% success rate, while pushing the quadrotor to its physical limit\nreaching speeds of more than 80km/h.\n","authors":["Maria Krinner","Angel Romero","Leonard Bauersfeld","Melanie Zeilinger","Andrea Carron","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.17551v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2110.07953v2","updated":"2024-03-26T09:52:36Z","published":"2021-10-15T09:12:44Z","title":"Attention-based Estimation and Prediction of Human Intent to augment\n  Haptic Glove aided Control of Robotic Hand","summary":"  The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH)\nexecuting in-hand manipulation of certain objects of interest. The high\ndimensional motion signals in HG and RH possess intrinsic variability of\nkinematics resulting in difficulty to establish a direct mapping of the motion\nsignals from HG onto the RH. An estimation mechanism is proposed to quantify\nthe motion signal acquired from the human controller in relation to the\nintended goal pose of the object being held by the robotic hand. A control\nalgorithm is presented to transform the synthesized intent at the RH and allow\nrelocation of the object to the expected goal pose. The lag in synthesis of the\nintent in the presence of communication delay leads to a requirement of\npredicting the estimated intent. We leverage an attention-based convolutional\nneural network encoder to predict the trajectory of intent for a certain\nlookahead to compensate for the delays. The proposed methodology is evaluated\nacross objects of different shapes, mass, and materials. We present a\ncomparative performance of the estimation and prediction mechanisms on\n5G-driven real-world robotic setup against benchmark methodologies. The\ntest-MSE in prediction of human intent is reported to yield ~ 97.3 -98.7%\nimprovement of accuracy in comparison to LSTM-based benchmark\n","authors":["Muneeb Ahmed","Rajesh Kumar","Qaim Abbas","Brejesh Lall","Arzad A. Kherani","Sudipto Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2110.07953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17531v1","updated":"2024-03-26T09:36:26Z","published":"2024-03-26T09:36:26Z","title":"Design and Preliminary Evaluation of a Torso Stabiliser for Individuals\n  with Spinal Cord Injury","summary":"  Spinal cord injuries (SCIs) generally result in sensory and mobility\nimpairments, with torso instability being particularly debilitating. Existing\ntorso stabilisers are often rigid and restrictive. This paper presents an early\ninvestigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso\nstabiliser inspired by devices such as centrifugal clutches and seat-belt\nmechanisms. Firstly, the paper presents a motion-capture (MoCap) and\nOpenSim-based kinematic analysis of the cable-based system to understand\nrequisite device characteristics. The simulated evaluation resulted in the\ncable-based device to require 55-60cm of unrestricted travel, and to lock at a\nthreshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is\nintroduced. The proposed mechanical device is transparent during activities of\ndaily living, and transitions to compliant blocking when incipient fall is\ndetected. Prototype behaviour was then validated using a MoCap-based kinematic\nanalysis to verify non-restrictive movement, reliable transition to blocking,\nand compliance of the blocking.\n","authors":["Rejin John Varghese","Man-Yan Tong","Isabella Szczech","Peter Bryan","Dario Farina","Etienne Burdet"],"pdf_url":"https://arxiv.org/pdf/2403.17531v1.pdf","comment":"4 pages, 4 figures, 10 references. Submitted to IEEE EMBC 2024\n  conference"},{"id":"http://arxiv.org/abs/2403.16291v2","updated":"2024-03-26T09:23:07Z","published":"2024-03-24T20:43:29Z","title":"Guessing human intentions to avoid dangerous situations in caregiving\n  robots","summary":"  For robots to interact socially, they must interpret human intentions and\nanticipate their potential outcomes accurately. This is particularly important\nfor social robots designed for human care, which may face potentially dangerous\nsituations for people, such as unseen obstacles in their way, that should be\navoided. This paper explores the Artificial Theory of Mind (ATM) approach to\ninferring and interpreting human intentions. We propose an algorithm that\ndetects risky situations for humans, selecting a robot action that removes the\ndanger in real time. We use the simulation-based approach to ATM and adopt the\n'like-me' policy to assign intentions and actions to people. Using this\nstrategy, the robot can detect and act with a high rate of success under\ntime-constrained situations. The algorithm has been implemented as part of an\nexisting robotics cognitive architecture and tested in simulation scenarios.\nThree experiments have been conducted to test the implementation's robustness,\nprecision and real-time response, including a simulated scenario, a\nhuman-in-the-loop hybrid configuration and a real-world scenario.\n","authors":["Noé Zapata","Gerardo Pérez","Lucas Bonilla","Pedro Núñez","Pilar Bachiller","Pablo Bustos"],"pdf_url":"https://arxiv.org/pdf/2403.16291v2.pdf","comment":"8 pages, 6 figures. Submitted to IROS2024. For associated mpeg file\n  see https://youtu.be/87UEB8P97KY"},{"id":"http://arxiv.org/abs/2011.07529v3","updated":"2024-03-26T08:54:31Z","published":"2020-11-15T13:40:21Z","title":"Full Attitude Intelligent Controller Design of a Heliquad under Complete\n  Failure of an Actuator","summary":"  In this paper, we design a reliable Heliquad and develop an intelligent\ncontroller to handle one actuators complete failure. Heliquad is a multi-copter\nsimilar to Quadcopter, with four actuators diagonally symmetric from the\ncenter. Each actuator has two control inputs; the first input changes the\npropeller blades collective pitch (also called variable pitch), and the other\ninput changes the rotation speed. For reliable operation and high torque\ncharacteristic requirement for yaw control, a cambered airfoil is used to\ndesign propeller blades. A neural network-based control allocation is designed\nto provide complete control authority even under a complete loss of one\nactuator. Nonlinear quaternion based outer loop position control, with\nproportional-derivative inner loop for attitude control and neural\nnetwork-based control allocation is used in controller design. The proposed\ncontroller and Heliquad designs performance is evaluated using a\nsoftware-in-loop simulation to track the position reference command under\nfailure. The results clearly indicate that the Heliquad with an intelligent\ncontroller provides necessary tracking performance even under a complete loss\nof one actuator.\n","authors":["Eeshan Kulkarni","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2011.07529v3.pdf","comment":"7 pages, For video go to\n  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN"},{"id":"http://arxiv.org/abs/2403.11384v3","updated":"2024-03-26T08:40:12Z","published":"2024-03-18T00:22:30Z","title":"Towards Massive Interaction with Generalist Robotics: A Systematic\n  Review of XR-enabled Remote Human-Robot Interaction Systems","summary":"  The rising interest of generalist robots seek to create robots with\nversatility to handle multiple tasks in a variety of environments, and human\nwill interact with such robots through immersive interfaces. In the context of\nhuman-robot interaction (HRI), this survey provides an exhaustive review of the\napplications of extended reality (XR) technologies in the field of remote HRI.\nWe developed a systematic search strategy based on the PRISMA methodology. From\nthe initial 2,561 articles selected, 100 research papers that met our inclusion\ncriteria were included. We categorized and summarized the domain in detail,\ndelving into XR technologies, including augmented reality (AR), virtual reality\n(VR), and mixed reality (MR), and their applications in facilitating intuitive\nand effective remote control and interaction with robotic systems. The survey\nhighlights existing articles on the application of XR technologies, user\nexperience enhancement, and various interaction designs for XR in remote HRI,\nproviding insights into current trends and future directions. We also\nidentified potential gaps and opportunities for future research to improve\nremote HRI systems through XR technology to guide and inform future XR and\nrobotics research.\n","authors":["Xian Wang","Luyao Shen","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11384v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02444v2","updated":"2024-03-26T08:13:01Z","published":"2023-04-05T14:02:53Z","title":"Autonomous Hook-Based Grasping and Transportation with Quadcopters","summary":"  Payload grasping and transportation with quadcopters is an active research\narea that has rapidly developed over the last decade. To grasp a payload\nwithout human interaction, most state-of-the-art approaches apply robotic arms\nthat are attached to the quadcopter body. However, due to the large weight and\npower consumption of these aerial manipulators, their agility and flight time\nare limited. This paper proposes a motion control and planning method for\ntransportation with a lightweight, passive manipulator structure that consists\nof a hook attached to a quadrotor using a 1 DoF revolute joint. To perform\npayload grasping, transportation, and release, first, time-optimal reference\ntrajectories are designed through specific waypoints to ensure the fast and\nreliable execution of the tasks. Then, a two-stage motion control approach is\ndeveloped based on a robust geometric controller for precise and reliable\nreference tracking and a linear--quadratic payload regulator for rapid setpoint\nstabilization of the payload swing. Furthermore, stability of the closed-loop\nsystem is mathematically proven to give safety guarantee for its operation. The\nproposed control architecture and design are evaluated in a high-fidelity\nphysical simulator, and also in real flight experiments, using a custom-made\nquadrotor--hook manipulator platform.\n","authors":["Péter Antal","Tamás Péni","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2304.02444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07658v3","updated":"2024-03-26T08:01:29Z","published":"2024-01-15T13:00:35Z","title":"Robustness Evaluation of Localization Techniques for Autonomous Racing","summary":"  This work introduces SynPF, an MCL-based algorithm tailored for high-speed\nracing environments. Benchmarked against Cartographer, a state-of-the-art\npose-graph SLAM algorithm, SynPF leverages synergies from previous\nparticle-filtering methods and synthesizes them for the high-performance racing\ndomain. Our extensive in-field evaluations reveal that while Cartographer\nexcels under nominal conditions, it struggles when subjected to wheel-slip, a\ncommon phenomenon in a racing scenario due to varying grip levels and\naggressive driving behaviour. Conversely, SynPF demonstrates robustness in\nthese challenging conditions and a low-latency computation time of 1.25 ms on\non-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled\nautonomous racing vehicle, this work not only highlights the vulnerabilities of\nexisting algorithms in high-speed scenarios, tested up until 7.6 m/s, but also\nemphasizes the potential of SynPF as a viable alternative, especially in\ndeteriorating odometry conditions.\n","authors":["Tian Yi Lim","Edoardo Ghignone","Nicolas Baumann","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2401.07658v3.pdf","comment":"Accepted at the Design, Automation and Test in Europe Conference 2024\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2403.17459v1","updated":"2024-03-26T07:47:52Z","published":"2024-03-26T07:47:52Z","title":"High-Power, Flexible, Robust Hand: Development of Musculoskeletal Hand\n  Using Machined Springs and Realization of Self-Weight Supporting Motion with\n  Humanoid","summary":"  Human can not only support their body during standing or walking, but also\nsupport them by hand, so that they can dangle a bar and others. But most\nhumanoid robots support their body only in the foot and they use their hand\njust to manipulate objects because their hands are too weak to support their\nbody. Strong hands are supposed to enable humanoid robots to act in much\nbroader scene. Therefore, we developed new life-size five-fingered hand that\ncan support the body of life-size humanoid robot. It is tendon-driven and\nunderactuated hand and actuators in forearms produce large gripping force. This\nhand has flexible joints using machined springs, which can be designed\nintegrally with the attachment. Thus, it has both structural strength and\nimpact resistance in spite of small size. As other characteristics, this hand\nhas force sensors to measure external force and the fingers can be flexed along\nobjects though the number of actuators to flex fingers is less than that of\nfingers. We installed the developed hand on musculoskeletal humanoid \"Kengoro\"\nand achieved two self-weight supporting motions: push-up motion and dangling\nmotion.\n","authors":["Shogo Makino","Kento Kawaharazuka","Masaya Kawamura","Yuki Asano","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.17459v1.pdf","comment":"accepted at IROS2017"},{"id":"http://arxiv.org/abs/2403.17452v1","updated":"2024-03-26T07:33:53Z","published":"2024-03-26T07:33:53Z","title":"Five-fingered Hand with Wide Range of Thumb Using Combination of\n  Machined Springs and Variable Stiffness Joints","summary":"  Human hands can not only grasp objects of various shape and size and\nmanipulate them in hands but also exert such a large gripping force that they\ncan support the body in the situations such as dangling a bar and climbing a\nladder. On the other hand, it is difficult for most robot hands to manage both.\nTherefore in this paper we developed the hand which can grasp various objects\nand exert large gripping force. To develop such hand, we focused on the thumb\nCM joint with wide range of motion and the MP joints of four fingers with the\nDOF of abduction and adduction. Based on the hand with large gripping force and\nflexibility using machined spring, we applied above mentioned joint mechanism\nto the hand. The thumb CM joint has wide range of motion because of the\ncombination of three machined springs and MP joints of four fingers have\nvariable rigidity mechanism instead of driving each joint independently in\norder to move joint in limited space and by limited actuators. Using the\ndeveloped hand, we achieved the grasping of various objects, supporting a large\nload and several motions with an arm.\n","authors":["Shogo Makino","Kento Kawaharazuka","Ayaka Fujii","Masaya Kawamura","Tasuku Makabe","Moritaka Onitsuka","Yuki Asano","Kei Okada","Koji Kawasaki","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.17452v1.pdf","comment":"accepted at IROS2018"},{"id":"http://arxiv.org/abs/2403.17448v1","updated":"2024-03-26T07:26:27Z","published":"2024-03-26T07:26:27Z","title":"Adaptive Line-Of-Sight guidance law based on vector fields path\n  following for underactuated unmanned surface vehicle","summary":"  The focus of this paper is to develop a methodology that enables an unmanned\nsurface vehicle (USV) to efficiently track a planned path. The introduction of\na vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate\ntrajectory tracking and minimizing the overshoot response time during USV\ntracking of curved paths improves the overall line-of-sight (LOS) guidance\nmethod. These improvements contribute to faster convergence to the desired\npath, reduce oscillations, and can mitigate the effects of persistent external\ndisturbances. It is shown that the proposed guidance law exhibits k-exponential\nstability when converging to the desired path consisting of straight and curved\nlines. The results in the paper show that the proposed method effectively\nimproves the accuracy of the USV tracking the desired path while ensuring the\nsafety of the USV work.\n","authors":["Jie Qi","Ronghua Wang","Nailong Wu","Yuxin Fan","Jigang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17441v1","updated":"2024-03-26T07:19:06Z","published":"2024-03-26T07:19:06Z","title":"Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke\n  Conditions","summary":"  Robust odometry estimation in perceptually degraded environments represents a\nkey challenge in the field of robotics. In this paper, we propose a LiDAR-radar\nfusion method for robust odometry for adverse environment with LiDAR\ndegeneracy. By comparing the LiDAR point cloud with the radar static point\ncloud obtained through preprocessing module, it is possible to identify\ninstances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the\neffectiveness of our method in challenging conditions such as dense smoke,\nshowcasing its ability to reliably estimate odometry and identify/remove\ndynamic points prone to LiDAR degeneracy.\n","authors":["Chiyun Noh","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11011v2","updated":"2024-03-26T07:03:18Z","published":"2023-09-20T02:26:41Z","title":"OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for\n  Autonomous Driving","summary":"  Visual Odometry (VO) plays a pivotal role in autonomous systems, with a\nprincipal challenge being the lack of depth information in camera images. This\npaper introduces OCC-VO, a novel framework that capitalizes on recent advances\nin deep learning to transform 2D camera images into 3D semantic occupancy,\nthereby circumventing the traditional need for concurrent estimation of ego\nposes and landmark locations. Within this framework, we utilize the TPV-Former\nto convert surround view cameras' images into 3D semantic occupancy. Addressing\nthe challenges presented by this transformation, we have specifically tailored\na pose estimation and mapping algorithm that incorporates Semantic Label\nFilter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for\nmaintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes\nnot only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement\nin trajectory accuracy against ORB-SLAM3, but also emphasize our ability to\nconstruct a comprehensive map. Our implementation is open-sourced and available\nat: https://github.com/USTCLH/OCC-VO.\n","authors":["Heng Li","Yifan Duan","Xinran Zhang","Haiyi Liu","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11011v2.pdf","comment":"7pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.01557v2","updated":"2024-03-26T06:50:43Z","published":"2023-08-03T06:36:21Z","title":"Motion Planning Diffusion: Learning and Planning of Robot Motions with\n  Diffusion Models","summary":"  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n","authors":["Joao Carvalho","An T. Le","Mark Baierl","Dorothea Koert","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2308.01557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17417v1","updated":"2024-03-26T06:14:58Z","published":"2024-03-26T06:14:58Z","title":"Cyclic pursuit formation control for arbitrary desired shapes","summary":"  A multi-agent system comprises numerous agents that autonomously make\ndecisions to collectively accomplish tasks, drawing significant attention for\ntheir wide-ranging applications. Within this context, formation control emerges\nas a prominent task, wherein agents collaboratively shape and maneuver while\npreserving formation integrity. Our focus centers on cyclic pursuit, a method\nfacilitating the formation of circles, ellipses, and figure-eights under the\nassumption that agents can only perceive the relative positions of those\npreceding them. However, this method's scope has been restricted to these\nspecific shapes, leaving the feasibility of forming other shapes uncertain. In\nresponse, our study proposes a novel method based on cyclic pursuit capable of\nforming a broader array of shapes, enabling agents to individually shape while\npursuing preceding agents, thereby extending the repertoire of achievable\nformations. We present two scenarios concerning the information available to\nagents and devise formation control methods tailored to each scenario. Through\nextensive simulations, we demonstrate the efficacy of our proposed method in\nforming multiple shapes, including those represented as Fourier series, thereby\nunderscoring the versatility and effectiveness of our approach.\n","authors":["Anna Fujioka","Masaki Ogura","Naoki Wakamiya"],"pdf_url":"https://arxiv.org/pdf/2403.17417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17392v1","updated":"2024-03-26T05:23:12Z","published":"2024-03-26T05:23:12Z","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in\n  unknown obstructed soft terrain","summary":"  Navigating multi-robot systems in complex terrains has always been a\nchallenging task. This is due to the inherent limitations of traditional robots\nin collision avoidance, adaptation to unknown environments, and sustained\nenergy efficiency. In order to overcome these limitations, this research\nproposes a solution by integrating living insects with miniature electronic\ncontrollers to enable robotic-like programmable control, and proposing a novel\ncontrol algorithm for swarming. Although these creatures, called cyborg\ninsects, have the ability to instinctively avoid collisions with neighbors and\nobstacles while adapting to complex terrains, there is a lack of literature on\nthe control of multi-cyborg systems. This research gap is due to the difficulty\nin coordinating the movements of a cyborg system under the presence of insects'\ninherent individual variability in their reactions to control input. In\nresponse to this issue, we propose a novel swarm navigation algorithm\naddressing these challenges. The effectiveness of the algorithm is demonstrated\nthrough an experimental validation in which a cyborg swarm was successfully\nnavigated through an unknown sandy field with obstacles and hills. This\nresearch contributes to the domain of swarm robotics and showcases the\npotential of integrating biological organisms with robotics and control theory\nto create more intelligent autonomous systems with real-world applications.\n","authors":["Yang Bai","Phuoc Thanh Tran Ngoc","Huu Duoc Nguyen","Duc Long Le","Quang Huy Ha","Kazuki Kai","Yu Xiang See To","Yaosheng Deng","Jie Song","Naoki Wakamiya","Hirotaka Sato","Masaki Ogura"],"pdf_url":"https://arxiv.org/pdf/2403.17392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17367v1","updated":"2024-03-26T04:05:01Z","published":"2024-03-26T04:05:01Z","title":"RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment","summary":"  Combining the mobility of legged robots with the manipulation skills of arms\nhas the potential to significantly expand the operational range and enhance the\ncapabilities of robotic systems in performing various mobile manipulation\ntasks. Existing approaches are confined to imprecise six degrees of freedom\n(DoF) manipulation and possess a limited arm workspace. In this paper, we\npropose a novel framework, RoboDuet, which employs two collaborative policies\nto realize locomotion and manipulation simultaneously, achieving whole-body\ncontrol through interactions between each other. Surprisingly, going beyond the\nlarge-range pose tracking, we find that the two-policy framework may enable\ncross-embodiment deployment such as using different quadrupedal robots or other\narms. Our experiments demonstrate that the policies trained through RoboDuet\ncan accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot\nexchange of legged robots, and can be deployed in the real world to perform\nvarious mobile manipulation tasks. Our project page with demo videos is at\nhttps://locomanip-duet.github.io .\n","authors":["Guoping Pan","Qingwei Ben","Zhecheng Yuan","Guangqi Jiang","Yandong Ji","Jiangmiao Pang","Houde Liu","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17353v1","updated":"2024-03-26T03:32:45Z","published":"2024-03-26T03:32:45Z","title":"Multi-Objective Trajectory Planning with Dual-Encoder","summary":"  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'\nperformance in dynamic tasks. Traditional methods rely on solving complex\nnonlinear programming problems, bringing significant delays in generating\noptimized trajectories. In this paper, we propose a two-stage approach to\naccelerate time-jerk optimal trajectory planning. Firstly, we introduce a\ndual-encoder based transformer model to establish a good preliminary\ntrajectory. This trajectory is subsequently refined through sequential\nquadratic programming to improve its optimality and robustness. Our approach\noutperforms the state-of-the-art by up to 79.72\\% in reducing trajectory\nplanning time. Compared with existing methods, our method shrinks the\noptimality gap with the objective function value decreasing by up to 29.9\\%.\n","authors":["Beibei Zhang","Tian Xiang","Chentao Mao","Yuhua Zheng","Shuai Li","Haoyi Niu","Xiangming Xi","Wenyuan Bai","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17353v1.pdf","comment":"6 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2403.17347v1","updated":"2024-03-26T03:15:14Z","published":"2024-03-26T03:15:14Z","title":"Unified Path and Gait Planning for Safe Bipedal Robot Navigation","summary":"  Safe path and gait planning are essential for bipedal robots to navigate\ncomplex real-world environments. The prevailing approaches often plan the path\nand gait separately in a hierarchical fashion, potentially resulting in unsafe\nmovements due to neglecting the physical constraints of walking robots. A\nsafety-critical path must not only avoid obstacles but also ensure that the\nrobot's gaits are subject to its dynamic and kinematic constraints. This work\npresents a novel approach that unifies path planning and gait planning via a\nModel Predictive Control (MPC) using the Linear Inverted Pendulum (LIP) model\nrepresenting bipedal locomotion. This approach considers environmental\nconstraints, such as obstacles, and the robot's kinematics and dynamics\nconstraints. By using discrete-time Control Barrier Functions for obstacle\navoidance, our approach generates the next foot landing position, ensuring\nrobust walking gaits and a safe navigation path within clustered environments.\nWe validated our proposed approach in simulation using a Digit robot in 20\nrandomly created environments. The results demonstrate improved performance in\nterms of safety and robustness when compared to hierarchical path and gait\nplanning frameworks.\n","authors":["Chengyang Peng","Victor Paredes","Ayonga Hereid"],"pdf_url":"https://arxiv.org/pdf/2403.17347v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.17320v1","updated":"2024-03-26T02:02:35Z","published":"2024-03-26T02:02:35Z","title":"Leveraging Symmetry in RL-based Legged Locomotion Control","summary":"  Model-free reinforcement learning is a promising approach for autonomously\nsolving challenging robotics control problems, but faces exploration difficulty\nwithout information of the robot's kinematics and dynamics morphology. The\nunder-exploration of multiple modalities with symmetric states leads to\nbehaviors that are often unnatural and sub-optimal. This issue becomes\nparticularly pronounced in the context of robotic systems with morphological\nsymmetries, such as legged robots for which the resulting asymmetric and\naperiodic behaviors compromise performance, robustness, and transferability to\nreal hardware. To mitigate this challenge, we can leverage symmetry to guide\nand improve the exploration in policy learning via equivariance/invariance\nconstraints. In this paper, we investigate the efficacy of two approaches to\nincorporate symmetry: modifying the network architectures to be strictly\nequivariant/invariant, and leveraging data augmentation to approximate\nequivariant/invariant actor-critics. We implement the methods on challenging\nloco-manipulation and bipedal locomotion tasks and compare with an\nunconstrained baseline. We find that the strictly equivariant policy\nconsistently outperforms other methods in sample efficiency and task\nperformance in simulation. In addition, symmetry-incorporated approaches\nexhibit better gait quality, higher robustness and can be deployed zero-shot in\nreal-world experiments.\n","authors":["Zhi Su","Xiaoyu Huang","Daniel Ordoñez-Apraez","Yunfei Li","Zhongyu Li","Qiayuan Liao","Giulio Turrisi","Massimiliano Pontil","Claudio Semini","Yi Wu","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2403.17320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04225v2","updated":"2024-03-26T00:37:21Z","published":"2023-03-07T20:34:33Z","title":"Feeling Optimistic? Ambiguity Attitudes for Online Decision Making","summary":"  Due to the complexity of many decision making problems, tree search\nalgorithms often have inadequate information to produce accurate transition\nmodels. Robust methods, designed to make safe decisions when faced with these\nuncertainties, often overlook the impact expressions of uncertainty have on how\nthe decision is made. This work introduces the Ambiguity Attitude Graph Search\n(AAGS), advocating for more precise representation of ambiguities (uncertainty\nfrom a set of plausible models) in decision making. Additionally, AAGS allows\nusers to adjust their ambiguity attitude (or preference), promoting exploration\nand improving users' ability to control how an agent should respond when faced\nwith a set of valid alternatives. Simulation in a dynamic sailing environment\nshows how highly stochastic environments can lead robust methods to fail.\nResults further demonstrate how adjusting ambiguity attitudes better fulfills\nobjectives while mitigating this failure mode of robust approaches. Because\nthis approach is a generalization of the robust framework, these results\nfurther demonstrate how algorithms focused on ambiguity have applicability\nbeyond safety-critical systems.\n","authors":["Jared J. Beard","R. Michael Butts","Yu Gu"],"pdf_url":"https://arxiv.org/pdf/2303.04225v2.pdf","comment":"6 pages, 5 figures, 2 algorithms. Submitted to the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems in Abu Dhabi, UAE\n  (Oct 14-18, 2024)"},{"id":"http://arxiv.org/abs/2403.17288v1","updated":"2024-03-26T00:35:06Z","published":"2024-03-26T00:35:06Z","title":"Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms","summary":"  The formation trajectory planning using complete graphs to model\ncollaborative constraints becomes computationally intractable as the number of\ndrones increases due to the curse of dimensionality. To tackle this issue, this\npaper presents a sparse graph construction method for formation planning to\nrealize better efficiency-performance trade-off. Firstly, a sparsification\nmechanism for complete graphs is designed to ensure the global rigidity of\nsparsified graphs, which is a necessary condition for uniquely corresponding to\na geometric shape. Secondly, a good sparse graph is constructed to preserve the\nmain structural feature of complete graphs sufficiently. Since the graph-based\nformation constraint is described by Laplacian matrix, the sparse graph\nconstruction problem is equivalent to submatrix selection, which has\ncombinatorial time complexity and needs a scoring metric. Via comparative\nsimulations, the Max-Trace matrix-revealing metric shows the promising\nperformance. The sparse graph is integrated into the formation planning.\nSimulation results with 72 drones in complex environments demonstrate that when\npreserving 30\\% connection edges, our method has comparative formation error\nand recovery performance w.r.t. complete graphs. Meanwhile, the planning\nefficiency is improved by approximate an order of magnitude. Benchmark\ncomparisons and ablation studies are conducted to fully validate the merits of\nour method.\n","authors":["Yuan Zhou","Lun Quan","Chao Xu","Guangtong Xu","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18149v1","updated":"2024-03-26T23:17:05Z","published":"2024-03-26T23:17:05Z","title":"Code Generation for Conic Model-Predictive Control on Microcontrollers\n  with TinyMPC","summary":"  Conic constraints appear in many important control applications like legged\nlocomotion, robotic manipulation, and autonomous rocket landing. However,\ncurrent solvers for conic optimization problems have relatively heavy\ncomputational demands in terms of both floating-point operations and memory\nfootprint, making them impractical for use on small embedded devices. We extend\nTinyMPC, an open-source, high-speed solver targeting low-power embedded control\napplications, to handle second-order cone constraints. We also present\ncode-generation software to enable deployment of TinyMPC on a variety of\nmicrocontrollers. We benchmark our generated code against state-of-the-art\nembedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed\nincrease over ECOS while consuming less memory. Finally, we demonstrate\nTinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained\nquadrotor with fast dynamics. TinyMPC and its code-generation tools are\npublicly available at https://tinympc.org.\n","authors":["Sam Schoedel","Khai Nguyen","Elakhya Nedumaran","Brian Plancher","Zachary Manchester"],"pdf_url":"https://arxiv.org/pdf/2403.18149v1.pdf","comment":"Submitted to CDC, 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18145v1","updated":"2024-03-26T23:10:41Z","published":"2024-03-26T23:10:41Z","title":"A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution","summary":"  One area of research in multi-agent path finding is to determine how\nreplanning can be efficiently achieved in the case of agents being delayed\nduring execution. One option is to reschedule the passing order of agents,\ni.e., the sequence in which agents visit the same location. In response, we\npropose Switchable-Edge Search (SES), an A*-style algorithm designed to find\noptimal passing orders. We prove the optimality of SES and evaluate its\nefficiency via simulations. The best variant of SES takes less than 1 second\nfor small- and medium-sized problems and runs up to 4 times faster than\nbaselines for large-sized problems.\n","authors":["Ying Feng","Adittyo Paul","Zhe Chen","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2403.18145v1.pdf","comment":"ICAPS 2024"},{"id":"http://arxiv.org/abs/2403.16967v2","updated":"2024-03-26T22:00:27Z","published":"2024-03-25T17:26:08Z","title":"Visual Whole-Body Control for Legged Loco-Manipulation","summary":"  We study the problem of mobile manipulation using legged robots equipped with\nan arm, namely legged loco-manipulation. The robot legs, while usually utilized\nfor mobility, offer an opportunity to amplify the manipulation capabilities by\nconducting whole-body control. That is, the robot can control the legs and the\narm at the same time to extend its workspace. We propose a framework that can\nconduct the whole-body control autonomously with visual observations. Our\napproach, namely Visual Whole-Body Control(VBC), is composed of a low-level\npolicy using all degrees of freedom to track the end-effector manipulator\nposition and a high-level policy proposing the end-effector position based on\nvisual inputs. We train both levels of policies in simulation and perform\nSim2Real transfer for real robot deployment. We perform extensive experiments\nand show significant improvements over baselines in picking up diverse objects\nin different configurations (heights, locations, orientations) and\nenvironments. Project page: https://wholebody-b1.github.io\n","authors":["Minghuan Liu","Zixuan Chen","Xuxin Cheng","Yandong Ji","Ruihan Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16967v2.pdf","comment":"The first two authors contribute equally. Project page:\n  https://wholebody-b1.github.io"},{"id":"http://arxiv.org/abs/2311.02749v3","updated":"2024-03-26T21:42:34Z","published":"2023-11-05T19:59:36Z","title":"Fast Point Cloud to Mesh Reconstruction for Deformable Object Tracking","summary":"  The world around us is full of soft objects we perceive and deform with\ndexterous hand movements. For a robotic hand to control soft objects, it has to\nacquire online state feedback of the deforming object. While RGB-D cameras can\ncollect occluded point clouds at a rate of 30Hz, this does not represent a\ncontinuously trackable object surface. Hence, in this work, we developed a\nmethod that takes as input a template mesh which is the mesh of an object in\nits non-deformed state and a deformed point cloud of the same object, and then\nshapes the template mesh such that it matches the deformed point cloud. The\nreconstruction of meshes from point clouds has long been studied in the field\nof Computer graphics under 3D reconstruction and 4D reconstruction, however,\nboth lack the speed and generalizability needed for robotics applications. Our\nmodel is designed using a point cloud auto-encoder and a Real-NVP architecture.\nOur trained model can perform mesh reconstruction and tracking at a rate of\n58Hz on a template mesh of 3000 vertices and a deformed point cloud of 5000\npoints and is generalizable to the deformations of six different object\ncategories which are assumed to be made of soft material in our experiments\n(scissors, hammer, foam brick, cleanser bottle, orange, and dice). The object\nmeshes are taken from the YCB benchmark dataset. An instance of a downstream\napplication can be the control algorithm for a robotic hand that requires\nonline feedback from the state of the manipulated object which would allow\nonline grasp adaptation in a closed-loop manner. Furthermore, the tracking\ncapacity of our method can help in the system identification of deforming\nobjects in a marker-free approach. In future work, we will extend our trained\nmodel to generalize beyond six object categories and additionally to real-world\ndeforming point clouds.\n","authors":["Elham Amin Mansour","Hehui Zheng","Robert K. Katzschmann"],"pdf_url":"https://arxiv.org/pdf/2311.02749v3.pdf","comment":"8 pages with appendix,16 figures"},{"id":"http://arxiv.org/abs/2302.08463v3","updated":"2024-03-26T21:33:23Z","published":"2023-02-16T18:14:13Z","title":"Dynamic Grasping with a Learned Meta-Controller","summary":"  Grasping moving objects is a challenging task that requires multiple\nsubmodules such as object pose predictor, arm motion planner, etc. Each\nsubmodule operates under its own set of meta-parameters. For example, how far\nthe pose predictor should look into the future (i.e., look-ahead time) and the\nmaximum amount of time the motion planner can spend planning a motion (i.e.,\ntime budget). Many previous works assign fixed values to these parameters;\nhowever, at different moments within a single episode of dynamic grasping, the\noptimal values should vary depending on the current scene. In this work, we\npropose a dynamic grasping pipeline with a meta-controller that controls the\nlook-ahead time and time budget dynamically. We learn the meta-controller\nthrough reinforcement learning with a sparse reward. Our experiments show the\nmeta-controller improves the grasping success rate (up to 28% in the most\ncluttered environment) and reduces grasping time, compared to the strongest\nbaseline. Our meta-controller learns to reason about the reachable workspace\nand maintain the predicted pose within the reachable region. In addition, it\nassigns a small but sufficient time budget for the motion planner. Our method\ncan handle different objects, trajectories, and obstacles. Despite being\ntrained only with 3-6 random cuboidal obstacles, our meta-controller\ngeneralizes well to 7-9 obstacles and more realistic out-of-domain household\nsetups with unseen obstacle shapes.\n","authors":["Yinsen Jia","Jingxi Xu","Dinesh Jayaraman","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2302.08463v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.02126v2","updated":"2024-03-26T21:20:57Z","published":"2023-12-04T18:53:24Z","title":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM","summary":"  Dense simultaneous localization and mapping (SLAM) is crucial for robotics\nand augmented reality applications. However, current methods are often hampered\nby the non-volumetric or implicit way they represent a scene. This work\nintroduces SplaTAM, an approach that, for the first time, leverages explicit\nvolumetric representations, i.e., 3D Gaussians, to enable high-fidelity\nreconstruction from a single unposed RGB-D camera, surpassing the capabilities\nof existing methods. SplaTAM employs a simple online tracking and mapping\nsystem tailored to the underlying Gaussian representation. It utilizes a\nsilhouette mask to elegantly capture the presence of scene density. This\ncombination enables several benefits over prior representations, including fast\nrendering and dense optimization, quickly determining if areas have been\npreviously mapped, and structured map expansion by adding more Gaussians.\nExtensive experiments show that SplaTAM achieves up to 2x superior performance\nin camera pose estimation, map construction, and novel-view synthesis over\nexisting methods, paving the way for more immersive high-fidelity SLAM\napplications.\n","authors":["Nikhil Keetha","Jay Karhade","Krishna Murthy Jatavallabhula","Gengshan Yang","Sebastian Scherer","Deva Ramanan","Jonathon Luiten"],"pdf_url":"https://arxiv.org/pdf/2312.02126v2.pdf","comment":"CVPR 2024. Website: https://spla-tam.github.io/"},{"id":"http://arxiv.org/abs/2403.18096v1","updated":"2024-03-26T20:41:35Z","published":"2024-03-26T20:41:35Z","title":"Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot\n  Interaction","summary":"  Although mobile robots have on-board sensors to perform navigation, their\nefficiency in completing paths can be enhanced by planning to avoid human\ninteraction. Infrastructure cameras can capture human activity continuously for\nthe purpose of compiling activity analytics to choose efficient times and\nroutes. We describe a cascade temporal filtering method to efficiently extract\nshort- and long-term activity in two time dimensions, isochronal and\nchronological, for use in global path planning and local navigation\nrespectively. The temporal filter has application either independently, or, if\nobject recognition is also required, it can be used as a pre-filter to perform\nactivity-gating of the more computationally expensive neural network\nprocessing. For a testbed 32-camera network, we show how this hybrid approach\ncan achieve over 8 times improvement in frames per second throughput and 6.5\ntimes reduction of system power use. We also show how the cost map of static\nobjects in the ROS robot software development framework is augmented with\ndynamic regions determined from the temporal filter.\n","authors":["Lawrence O'Gorman"],"pdf_url":"https://arxiv.org/pdf/2403.18096v1.pdf","comment":"15 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18066v1","updated":"2024-03-26T19:35:22Z","published":"2024-03-26T19:35:22Z","title":"Path Integral Control with Rollout Clustering and Dynamic Obstacles","summary":"  Model Predictive Path Integral (MPPI) control has proven to be a powerful\ntool for the control of uncertain systems (such as systems subject to\ndisturbances and systems with unmodeled dynamics). One important limitation of\nthe baseline MPPI algorithm is that it does not utilize simulated trajectories\nto their fullest extent. For one, it assumes that the average of all\ntrajectories weighted by their performance index will be a safe trajectory. In\nthis paper, multiple examples are shown where the previous assumption does not\nhold, and a trajectory clustering technique is presented that reduces the\nchances of the weighted average crossing in an unsafe region. Secondly, MPPI\ndoes not account for dynamic obstacles, so the authors put forward a novel cost\nfunction that accounts for dynamic obstacles without adding significant\ncomputation time to the overall algorithm. The novel contributions proposed in\nthis paper were evaluated with extensive simulations to demonstrate\nimprovements upon the state-of-the-art MPPI techniques.\n","authors":["Steven Patrick","Efstathios Bakolas"],"pdf_url":"https://arxiv.org/pdf/2403.18066v1.pdf","comment":"8 pages, 5 figures, extended version of ACC 2024 submission"},{"id":"http://arxiv.org/abs/2403.18062v1","updated":"2024-03-26T19:26:53Z","published":"2024-03-26T19:26:53Z","title":"ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models\n  through Geometric Decomposition","summary":"  Task-oriented grasping of unfamiliar objects is a necessary skill for robots\nin dynamic in-home environments. Inspired by the human capability to grasp such\nobjects through intuition about their shape and structure, we present a novel\nzero-shot task-oriented grasping method leveraging a geometric decomposition of\nthe target object into simple, convex shapes that we represent in a graph\nstructure, including geometric attributes and spatial relationships. Our\napproach employs minimal essential information - the object's name and the\nintended task - to facilitate zero-shot task-oriented grasping. We utilize the\ncommonsense reasoning capabilities of large language models to dynamically\nassign semantic meaning to each decomposed part and subsequently reason over\nthe utility of each part for the intended task. Through extensive experiments\non a real-world robotics platform, we demonstrate that our grasping approach's\ndecomposition and reasoning pipeline is capable of selecting the correct part\nin 92% of the cases and successfully grasping the object in 82% of the tasks we\nevaluate. Additional videos, experiments, code, and data are available on our\nproject website: https://shapegrasp.github.io/.\n","authors":["Samuel Li","Sarthak Bhagat","Joseph Campbell","Yaqi Xie","Woojun Kim","Katia Sycara","Simon Stepputtis"],"pdf_url":"https://arxiv.org/pdf/2403.18062v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.08344v2","updated":"2024-03-26T19:25:53Z","published":"2023-12-13T18:28:09Z","title":"FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects","summary":"  We present FoundationPose, a unified foundation model for 6D object pose\nestimation and tracking, supporting both model-based and model-free setups. Our\napproach can be instantly applied at test-time to a novel object without\nfine-tuning, as long as its CAD model is given, or a small number of reference\nimages are captured. We bridge the gap between these two setups with a neural\nimplicit representation that allows for effective novel view synthesis, keeping\nthe downstream pose estimation modules invariant under the same unified\nframework. Strong generalizability is achieved via large-scale synthetic\ntraining, aided by a large language model (LLM), a novel transformer-based\narchitecture, and contrastive learning formulation. Extensive evaluation on\nmultiple public datasets involving challenging scenarios and objects indicate\nour unified approach outperforms existing methods specialized for each task by\na large margin. In addition, it even achieves comparable results to\ninstance-level methods despite the reduced assumptions. Project page:\nhttps://nvlabs.github.io/FoundationPose/\n","authors":["Bowen Wen","Wei Yang","Jan Kautz","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2312.08344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18041v1","updated":"2024-03-26T18:52:50Z","published":"2024-03-26T18:52:50Z","title":"Learning Piecewise Residuals of Control Barrier Functions for Safety of\n  Switching Systems using Multi-Output Gaussian Processes","summary":"  Control barrier functions (CBFs) have recently been introduced as a\nsystematic tool to ensure safety by establishing set invariance. When combined\nwith a control Lyapunov function (CLF), they form a safety-critical control\nmechanism. However, the effectiveness of CBFs and CLFs is closely tied to the\nsystem model. In practice, model uncertainty can jeopardize safety and\nstability guarantees and may lead to undesirable performance. In this paper, we\ndevelop a safe learning-based control strategy for switching systems in the\nface of uncertainty. We focus on the case that a nominal model is available for\na true underlying switching system. This uncertainty results in piecewise\nresiduals for each switching surface, impacting the CLF and CBF constraints. We\nintroduce a batch multi-output Gaussian process (MOGP) framework to approximate\nthese piecewise residuals, thereby mitigating the adverse effects of\nuncertainty. A particular structure of the covariance function enables us to\nconvert the MOGP-based chance constraints CLF and CBF into second-order cone\nconstraints, which leads to a convex optimization. We analyze the feasibility\nof the resulting optimization and provide the necessary and sufficient\nconditions for feasibility. The effectiveness of the proposed strategy is\nvalidated through a simulation of a switching adaptive cruise control system.\n","authors":["Mohammad Aali","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18041v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.09573"},{"id":"http://arxiv.org/abs/2403.18033v1","updated":"2024-03-26T18:39:38Z","published":"2024-03-26T18:39:38Z","title":"SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation","summary":"  The increase in non-biodegradable waste is a worldwide concern. Recycling\nfacilities play a crucial role, but their automation is hindered by the complex\ncharacteristics of waste recycling lines like clutter or object deformation. In\naddition, the lack of publicly available labeled data for these environments\nmakes developing robust perception systems challenging. Our work explores the\nbenefits of multimodal perception for object segmentation in real waste\nmanagement scenarios. First, we present SpectralWaste, the first dataset\ncollected from an operational plastic waste sorting facility that provides\nsynchronized hyperspectral and conventional RGB images. This dataset contains\nlabels for several categories of objects that commonly appear in sorting plants\nand need to be detected and separated from the main trash flow for several\nreasons, such as security in the management line or reuse. Additionally, we\npropose a pipeline employing different object segmentation architectures and\nevaluate the alternatives on our dataset, conducting an extensive analysis for\nboth multimodal and unimodal alternatives. Our evaluation pays special\nattention to efficiency and suitability for real-time processing and\ndemonstrates how HSI can bring a boost to RGB-only perception in these\nrealistic industrial settings without much computational overhead.\n","authors":["Sara Casao","Fernando Peña","Alberto Sabater","Rosa Castillón","Darío Suárez","Eduardo Montijano","Ana C. Murillo"],"pdf_url":"https://arxiv.org/pdf/2403.18033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18021v1","updated":"2024-03-26T18:13:20Z","published":"2024-03-26T18:13:20Z","title":"A Study on the Use of Simulation in Synthesizing Path-Following Control\n  Policies for Autonomous Ground Robots","summary":"  We report results obtained and insights gained while answering the following\nquestion: how effective is it to use a simulator to establish path following\ncontrol policies for an autonomous ground robot? While the quality of the\nsimulator conditions the answer to this question, we found that for the\nsimulation platform used herein, producing four control policies for path\nplanning was straightforward once a digital twin of the controlled robot was\navailable. The control policies established in simulation and subsequently\ndemonstrated in the real world are PID control, MPC, and two neural network\n(NN) based controllers. Training the two NN controllers via imitation learning\nwas accomplished expeditiously using seven simple maneuvers: follow three\ncircles clockwise, follow the same circles counter-clockwise, and drive\nstraight. A test randomization process that employs random micro-simulations is\nused to rank the ``goodness'' of the four control policies. The policy ranking\nnoted in simulation correlates well with the ranking observed when the control\npolicies were tested in the real world. The simulation platform used is\npublicly available and BSD3-released as open source; a public Docker image is\navailable for reproducibility studies. It contains a dynamics engine, a sensor\nsimulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in\nthe simulator and the real world experiments.\n","authors":["Harry Zhang","Stefan Caldararu","Aaron Young","Alexis Ruiz","Huzaifa Unjhawala","Ishaan Mahajan","Sriram Ashokkumar","Nevindu Batagoda","Zhenhao Zhou","Luning Bakke","Dan Negrut"],"pdf_url":"https://arxiv.org/pdf/2403.18021v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.18015v1","updated":"2024-03-26T18:04:10Z","published":"2024-03-26T18:04:10Z","title":"A Constructive Method for Designing Safe Multirate Controllers for\n  Differentially-Flat Systems","summary":"  We present a multi-rate control architecture that leverages fundamental\nproperties of differential flatness to synthesize controllers for\nsafety-critical nonlinear dynamical systems. We propose a two-layer\narchitecture, where the high-level generates reference trajectories using a\nlinear Model Predictive Controller, and the low-level tracks this reference\nusing a feedback controller. The novelty lies in how we couple these layers, to\nachieve formal guarantees on recursive feasibility of the MPC problem, and\nsafety of the nonlinear system. Furthermore, using differential flatness, we\nprovide a constructive means to synthesize the multi-rate controller, thereby\nremoving the need to search for suitable Lyapunov or barrier functions, or to\napproximately linearize/discretize nonlinear dynamics. We show the synthesized\ncontroller is a convex optimization problem, making it amenable to real-time\nimplementations. The method is demonstrated experimentally on a ground rover\nand a quadruped robotic system.\n","authors":["Devansh R. Agrawal","Hardik Parwana","Ryan K. Cosner","Ugo Rosolia","Aaron D. Ames","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2403.18015v1.pdf","comment":"6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.17937v1","updated":"2024-03-26T17:59:58Z","published":"2024-03-26T17:59:58Z","title":"Efficient Video Object Segmentation via Modulated Cross-Attention Memory","summary":"  Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.\n","authors":["Abdelrahman Shaker","Syed Talal Wasim","Martin Danelljan","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17936v1","updated":"2024-03-26T17:59:52Z","published":"2024-03-26T17:59:52Z","title":"ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture\n  Synthesis","summary":"  Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.\n","authors":["Muhammad Hamza Mughal","Rishabh Dabral","Ikhsanul Habibie","Lucia Donatelli","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2403.17936v1.pdf","comment":"CVPR 2024. Project Page:\n  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/"},{"id":"http://arxiv.org/abs/2403.17935v1","updated":"2024-03-26T17:59:24Z","published":"2024-03-26T17:59:24Z","title":"OmniVid: A Generative Framework for Universal Video Understanding","summary":"  The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.\n","authors":["Junke Wang","Dongdong Chen","Chong Luo","Bo He","Lu Yuan","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.17935v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17934v1","updated":"2024-03-26T17:59:23Z","published":"2024-03-26T17:59:23Z","title":"AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation","summary":"  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.\n","authors":["Qingping Sun","Yanjun Wang","Ailing Zeng","Wanqi Yin","Chen Wei","Wenjia Wang","Haiyi Mei","Chi Sing Leung","Ziwei Liu","Lei Yang","Zhongang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.17934v1.pdf","comment":"Homepage: https://ttxskk.github.io/AiOS/"},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17931v1","updated":"2024-03-26T17:58:22Z","published":"2024-03-26T17:58:22Z","title":"Track Everything Everywhere Fast and Robustly","summary":"  We propose a novel test-time optimization approach for efficiently and\nrobustly tracking any pixel at any time in a video. The latest state-of-the-art\noptimization-based tracking technique, OmniMotion, requires a prohibitively\nlong optimization time, rendering it impractical for downstream applications.\nOmniMotion is sensitive to the choice of random seeds, leading to unstable\nconvergence. To improve efficiency and robustness, we introduce a novel\ninvertible deformation network, CaDeX++, which factorizes the function\nrepresentation into a local spatial-temporal feature grid and enhances the\nexpressivity of the coupling blocks with non-linear functions. While CaDeX++\nincorporates a stronger geometric bias within its architectural design, it also\ntakes advantage of the inductive bias provided by the vision foundation models.\nOur system utilizes monocular depth estimation to represent scene geometry and\nenhances the objective by incorporating DINOv2 long-term semantics to regulate\nthe optimization process. Our experiments demonstrate a substantial improvement\nin training speed (more than \\textbf{10 times} faster), robustness, and\naccuracy in tracking over the SoTA optimization-based method OmniMotion.\n","authors":["Yunzhou Song","Jiahui Lei","Ziyun Wang","Lingjie Liu","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2403.17931v1.pdf","comment":"project page: https://timsong412.github.io/FastOmniTrack/"},{"id":"http://arxiv.org/abs/2403.17929v1","updated":"2024-03-26T17:58:07Z","published":"2024-03-26T17:58:07Z","title":"Towards Explaining Hypercomplex Neural Networks","summary":"  Hypercomplex neural networks are gaining increasing interest in the deep\nlearning community. The attention directed towards hypercomplex models\noriginates from several aspects, spanning from purely theoretical and\nmathematical characteristics to the practical advantage of lightweight models\nover conventional networks, and their unique properties to capture both global\nand local relations. In particular, a branch of these architectures,\nparameterized hypercomplex neural networks (PHNNs), has also gained popularity\ndue to their versatility across a multitude of application domains.\nNonetheless, only few attempts have been made to explain or interpret their\nintricacies. In this paper, we propose inherently interpretable PHNNs and\nquaternion-like networks, thus without the need for any post-hoc method. To\nachieve this, we define a type of cosine-similarity transform within the\nparameterized hypercomplex domain. This PHB-cos transform induces weight\nalignment with relevant input features and allows to reduce the model into a\nsingle linear transform, rendering it directly interpretable. In this work, we\nstart to draw insights into how this unique branch of neural models operates.\nWe observe that hypercomplex networks exhibit a tendency to concentrate on the\nshape around the main object of interest, in addition to the shape of the\nobject itself. We provide a thorough analysis, studying single neurons of\ndifferent layers and comparing them against how real-valued networks learn. The\ncode of the paper is available at https://github.com/ispamm/HxAI.\n","authors":["Eleonora Lopez","Eleonora Grassucci","Debora Capriotti","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.17929v1.pdf","comment":"The paper has been accepted at IEEE WCCI 2024"},{"id":"http://arxiv.org/abs/2403.17926v1","updated":"2024-03-26T17:57:20Z","published":"2024-03-26T17:57:20Z","title":"FastCAR: Fast Classification And Regression Multi-Task Learning via Task\n  Consolidation for Modelling a Continuous Property Variable of Object Classes","summary":"  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)\nfor a classification and a regression task, despite task heterogeneity with\nonly subtle correlation. It addresses object classification and continuous\nproperty variable regression, a crucial use case in science and engineering.\nFastCAR involves a labeling transformation approach that can be used with a\nsingle-task regression network architecture. FastCAR outperforms traditional\nMTL model families, parametrized in the landscape of architecture and loss\nweighting schemes, when learning of both tasks are collectively considered\n(classification accuracy of 99.54%, regression mean absolute percentage error\nof 2.3%). The experiments performed used an Advanced Steel Property dataset\ncontributed by us. The dataset comprises 4536 images of 224x224 pixels,\nannotated with object classes and hardness properties that take continuous\nvalues. With the labeling transformation and single-task regression network\narchitecture, FastCAR achieves reduced latency and time efficiency.\n","authors":["Anoop Kini","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17924v1","updated":"2024-03-26T17:57:05Z","published":"2024-03-26T17:57:05Z","title":"AID: Attention Interpolation of Text-to-Image Diffusion","summary":"  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n","authors":["Qiyuan He","Jinghao Wang","Ziwei Liu","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17920v1","updated":"2024-03-26T17:55:11Z","published":"2024-03-26T17:55:11Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","summary":"  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n","authors":["Sherwin Bahmani","Xian Liu","Yifan Wang","Ivan Skorokhodov","Victor Rong","Ziwei Liu","Xihui Liu","Jeong Joon Park","Sergey Tulyakov","Gordon Wetzstein","Andrea Tagliasacchi","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2403.17920v1.pdf","comment":"Project Page: https://sherwinbahmani.github.io/tc4d"},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17915v1","updated":"2024-03-26T17:52:23Z","published":"2024-03-26T17:52:23Z","title":"Leveraging Near-Field Lighting for Monocular Depth Estimation from\n  Endoscopy Videos","summary":"  Monocular depth estimation in endoscopy videos can enable assistive and\nrobotic surgery to obtain better coverage of the organ and detection of various\nhealth issues. Despite promising progress on mainstream, natural image depth\nestimation, techniques perform poorly on endoscopy images due to a lack of\nstrong geometric features and challenging illumination effects. In this paper,\nwe utilize the photometric cues, i.e., the light emitted from an endoscope and\nreflected by the surface, to improve monocular depth estimation. We first\ncreate two novel loss functions with supervised and self-supervised variants\nthat utilize a per-pixel shading representation. We then propose a novel depth\nrefinement network (PPSNet) that leverages the same per-pixel shading\nrepresentation. Finally, we introduce teacher-student transfer learning to\nproduce better depth maps from both synthetic data with supervision and\nclinical data with self-supervision. We achieve state-of-the-art results on the\nC3VD dataset while estimating high-quality depth maps from clinical data. Our\ncode, pre-trained models, and supplementary materials can be found on our\nproject page: https://ppsnet.github.io/\n","authors":["Akshay Paruchuri","Samuel Ehrenstein","Shuxian Wang","Inbar Fried","Stephen M. Pizer","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2403.17915v1.pdf","comment":"26 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.17909v1","updated":"2024-03-26T17:46:25Z","published":"2024-03-26T17:46:25Z","title":"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing\n  Change Detection","summary":"  Deep learning has shown remarkable success in remote sensing change detection\n(CD), aiming to identify semantic change regions between co-registered\nsatellite image pairs acquired at distinct time stamps. However, existing\nconvolutional neural network and transformer-based frameworks often struggle to\naccurately segment semantic change regions. Moreover, transformers-based\nmethods with standard self-attention suffer from quadratic computational\ncomplexity with respect to the image resolution, making them less practical for\nCD tasks with limited training data. To address these issues, we propose an\nefficient change detection framework, ELGC-Net, which leverages rich contextual\ninformation to precisely estimate change regions while reducing the model size.\nOur ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The\nfocus of our design is the introduction of an Efficient Local-Global Context\nAggregator module within the encoder, capturing enhanced global context and\nlocal spatial information through a novel pooled-transpose (PT) attention and\ndepthwise convolution, respectively. The PT attention employs pooling\noperations for robust feature extraction and minimizes computational cost with\ntransposed attention. Extensive experiments on three challenging CD datasets\ndemonstrate that ELGC-Net outperforms existing methods. Compared to the recent\ntransformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in\nintersection over union metric on the LEVIR-CD dataset, while significantly\nreducing trainable parameters. Our proposed ELGC-Net sets a new\nstate-of-the-art performance in remote sensing change detection benchmarks.\nFinally, we also introduce ELGC-Net-LW, a lighter variant with significantly\nreduced computational complexity, suitable for resource-constrained settings,\nwhile achieving comparable performance. Project url\nhttps://github.com/techmn/elgcnet.\n","authors":["Mubashir Noman","Mustansar Fiaz","Hisham Cholakkal","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17909v1.pdf","comment":"accepted at IEEE TGRS"},{"id":"http://arxiv.org/abs/2403.17905v1","updated":"2024-03-26T17:45:06Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Chen Yiwei","Tang Chao","Aghabiglou Amir","Chu Chung San","Wiaux Yves"],"pdf_url":"https://arxiv.org/pdf/2403.17905v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2307.16897v2","updated":"2024-03-26T17:40:47Z","published":"2023-07-31T17:59:48Z","title":"DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields","summary":"  Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of dynamic 3D scenes. However, their capabilities lag behind those\noffered by conventional representations such as 2D videos because of\nalgorithmic challenges and the lack of large-scale multi-view real-world\ndatasets. We address the dataset limitation with DiVa-360, a real-world 360\ndynamic visual dataset that contains synchronized high-resolution and\nlong-duration multi-view video sequences of table-scale scenes captured using a\ncustomized low-cost system with 53 cameras. It contains 21 object-centric\nsequences categorized by different motion types, 25 intricate hand-object\ninteraction sequences, and 8 long-duration sequences for a total of 17.4 M\nimage frames. In addition, we provide foreground-background segmentation masks,\nsynchronized audio, and text descriptions. We benchmark the state-of-the-art\ndynamic neural field methods on DiVa-360 and provide insights about existing\nmethods and future challenges on long-duration neural field capture.\n","authors":["Cheng-You Lu","Peisen Zhou","Angela Xing","Chandradeep Pokhariya","Arnab Dey","Ishaan Shah","Rugved Mavidipalli","Dylan Hu","Andrew Comport","Kefan Chen","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2307.16897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17898v1","updated":"2024-03-26T17:39:36Z","published":"2024-03-26T17:39:36Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n","authors":["Kerui Ren","Lihan Jiang","Tao Lu","Mulin Yu","Linning Xu","Zhangkai Ni","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.17898v1.pdf","comment":"Project page: https://city-super.github.io/octree-gs/"},{"id":"http://arxiv.org/abs/2403.17893v1","updated":"2024-03-26T17:29:26Z","published":"2024-03-26T17:29:26Z","title":"A Survey on 3D Egocentric Human Pose Estimation","summary":"  Egocentric human pose estimation aims to estimate human body poses and\ndevelop body representations from a first-person camera perspective. It has\ngained vast popularity in recent years because of its wide range of\napplications in sectors like XR-technologies, human-computer interaction, and\nfitness tracking. However, to the best of our knowledge, there is no systematic\nliterature review based on the proposed solutions regarding egocentric 3D human\npose estimation. To that end, the aim of this survey paper is to provide an\nextensive overview of the current state of egocentric pose estimation research.\nIn this paper, we categorize and discuss the popular datasets and the different\npose estimation models, highlighting the strengths and weaknesses of different\nmethods by comparative analysis. This survey can be a valuable resource for\nboth researchers and practitioners in the field, offering insights into key\nconcepts and cutting-edge solutions in egocentric pose estimation, its\nwide-ranging applications, as well as the open problems with future scope.\n","authors":["Md Mushfiqur Azam","Kevin Desai"],"pdf_url":"https://arxiv.org/pdf/2403.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17888v1","updated":"2024-03-26T17:21:24Z","published":"2024-03-26T17:21:24Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","summary":"  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.\n","authors":["Binbin Huang","Zehao Yu","Anpei Chen","Andreas Geiger","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17888v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.17884v1","updated":"2024-03-26T17:16:04Z","published":"2024-03-26T17:16:04Z","title":"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using\n  Sentinel Data","summary":"  Utilizing satellite imagery for wildfire detection presents substantial\npotential for practical applications. To advance the development of machine\nlearning algorithms in this domain, our study introduces the \\textit{Sen2Fire}\ndataset--a challenging satellite remote sensing dataset tailored for wildfire\ndetection. This dataset is curated from Sentinel-2 multi-spectral data and\nSentinel-5P aerosol product, comprising a total of 2466 image patches. Each\npatch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive\nsensitivities of various wavebands to wildfire responses, our research focuses\non optimizing wildfire detection by evaluating different wavebands and\nemploying a combination of spectral indices, such as normalized burn ratio\n(NBR) and normalized difference vegetation index (NDVI). The results suggest\nthat, in contrast to using all bands for wildfire detection, selecting specific\nband combinations yields superior performance. Additionally, our study\nunderscores the positive impact of integrating Sentinel-5 aerosol data for\nwildfire detection. The code and dataset are available online\n(https://zenodo.org/records/10881058).\n","authors":["Yonghao Xu","Amanda Berg","Leif Haglund"],"pdf_url":"https://arxiv.org/pdf/2403.17884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02640v3","updated":"2024-03-26T17:14:14Z","published":"2024-03-05T04:08:19Z","title":"HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic\n  Intersection and Vehicle-Infrastructure Cooperative","summary":"  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous\nDriving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one\nof the important research area. Due to the complexity of traffic conditions\nsuch as blind spots and occlusion, it greatly limits the perception\ncapabilities of single-view roadside sensing systems. To further enhance the\naccuracy of roadside perception and provide better information to the vehicle\nside, in this paper, we constructed holographic intersections with various\nlayouts to build a large-scale multi-sensor holographic vehicle-infrastructure\ncooperation dataset, called HoloVIC. Our dataset includes 3 different types of\nsensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the\ndifferent intersections. Each intersection is equipped with 6-18 sensors to\ncapture synchronous data. While autonomous vehicles pass through these\nintersections for collecting VIC data. HoloVIC contains in total on 100k+\nsynchronous frames from different sensors. Additionally, we annotated 3D\nbounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs\nof the same objects across different devices and consecutive frames in\nsequence. Based on HoloVIC, we formulated four tasks to facilitate the\ndevelopment of related research. We also provide benchmarks for these tasks.\n","authors":["Cong Ma","Lei Qiao","Chengkai Zhu","Kai Liu","Zelong Kong","Qing Li","Xueqi Zhou","Yuheng Kan","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2403.02640v3.pdf","comment":"Accept to CVPR 2024, Benchmark Website: https://holovic.net"},{"id":"http://arxiv.org/abs/2403.17883v1","updated":"2024-03-26T17:13:17Z","published":"2024-03-26T17:13:17Z","title":"Superior and Pragmatic Talking Face Generation with Teacher-Student\n  Framework","summary":"  Talking face generation technology creates talking videos from arbitrary\nappearance and motion signal, with the \"arbitrary\" offering ease of use but\nalso introducing challenges in practical applications. Existing methods work\nwell with standard inputs but suffer serious performance degradation with\nintricate real-world ones. Moreover, efficiency is also an important concern in\ndeployment. To comprehensively address these issues, we introduce SuperFace, a\nteacher-student framework that balances quality, robustness, cost and\neditability. We first propose a simple but effective teacher model capable of\nhandling inputs of varying qualities to generate high-quality results. Building\non this, we devise an efficient distillation strategy to acquire an\nidentity-specific student model that maintains quality with significantly\nreduced computational load. Our experiments validate that SuperFace offers a\nmore comprehensive solution than existing methods for the four mentioned\nobjectives, especially in reducing FLOPs by 99\\% with the student model.\nSuperFace can be driven by both video and audio and allows for localized facial\nattributes editing.\n","authors":["Chao Liang","Jianwen Jiang","Tianyun Zhong","Gaojie Lin","Zhengkun Rong","Jiaqi Yang","Yongming Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17881v1","updated":"2024-03-26T17:12:34Z","published":"2024-03-26T17:12:34Z","title":"Deepfake Generation and Detection: A Benchmark and Survey","summary":"  In addition to the advancements in deepfake generation, corresponding\ndetection technologies need to continuously evolve to regulate the potential\nmisuse of deepfakes, such as for privacy invasion and phishing attacks. This\nsurvey comprehensively reviews the latest developments in deepfake generation\nand detection, summarizing and analyzing the current state of the art in this\nrapidly evolving field. We first unify task definitions, comprehensively\nintroduce datasets and metrics, and discuss the development of generation and\ndetection technology frameworks. Then, we discuss the development of several\nrelated sub-fields and focus on researching four mainstream deepfake fields:\npopular face swap, face reenactment, talking face generation, and facial\nattribute editing, as well as foreign detection. Subsequently, we\ncomprehensively benchmark representative methods on popular datasets for each\nfield, fully evaluating the latest and influential works published in top\nconferences/journals. Finally, we analyze the challenges and future research\ndirections of the discussed fields. We closely follow the latest developments\nin https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.\n","authors":["Gan Pei","Jiangning Zhang","Menghan Hu","Guangtao Zhai","Chengjie Wang","Zhenyu Zhang","Jian Yang","Chunhua Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17879v1","updated":"2024-03-26T17:11:51Z","published":"2024-03-26T17:11:51Z","title":"Low-Latency Neural Stereo Streaming","summary":"  The rise of new video modalities like virtual reality or autonomous driving\nhas increased the demand for efficient multi-view video compression methods,\nboth in terms of rate-distortion (R-D) performance and in terms of delay and\nruntime. While most recent stereo video compression approaches have shown\npromising performance, they compress left and right views sequentially, leading\nto poor parallelization and runtime performance. This work presents Low-Latency\nneural codec for Stereo video Streaming (LLSS), a novel parallel stereo video\ncoding method designed for fast and efficient low-latency stereo video\nstreaming. Instead of using a sequential cross-view motion compensation like\nexisting methods, LLSS introduces a bidirectional feature shifting module to\ndirectly exploit mutual information among views and encode them effectively\nwith a joint cross-view prior model for entropy coding. Thanks to this design,\nLLSS processes left and right views in parallel, minimizing latency; all while\nsubstantially improving R-D performance compared to both existing neural and\nconventional codecs.\n","authors":["Qiqi Hou","Farzad Farhadzadeh","Amir Said","Guillaume Sautiere","Hoang Le"],"pdf_url":"https://arxiv.org/pdf/2403.17879v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17869v1","updated":"2024-03-26T16:57:33Z","published":"2024-03-26T16:57:33Z","title":"To Supervise or Not to Supervise: Understanding and Addressing the Key\n  Challenges of 3D Transfer Learning","summary":"  Transfer learning has long been a key factor in the advancement of many\nfields including 2D image analysis. Unfortunately, its applicability in 3D data\nprocessing has been relatively limited. While several approaches for 3D\ntransfer learning have been proposed in recent literature, with contrastive\nlearning gaining particular prominence, most existing methods in this domain\nhave only been studied and evaluated in limited scenarios. Most importantly,\nthere is currently a lack of principled understanding of both when and why 3D\ntransfer learning methods are applicable. Remarkably, even the applicability of\nstandard supervised pre-training is poorly understood. In this work, we conduct\nthe first in-depth quantitative and qualitative investigation of supervised and\ncontrastive pre-training strategies and their utility in downstream 3D tasks.\nWe demonstrate that layer-wise analysis of learned features provides\nsignificant insight into the downstream utility of trained networks. Informed\nby this analysis, we propose a simple geometric regularization strategy, which\nimproves the transferability of supervised pre-training. Our work thus sheds\nlight onto both the specific challenges of 3D transfer learning, as well as\nstrategies to overcome them.\n","authors":["Souhail Hadgi","Lei Li","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2403.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2401.06003v2","updated":"2024-03-26T16:30:20Z","published":"2024-01-11T16:06:36Z","title":"TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering","summary":"  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/\n","authors":["Linus Franke","Darius Rückert","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2401.06003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17839v1","updated":"2024-03-26T16:27:37Z","published":"2024-03-26T16:27:37Z","title":"ReMamber: Referring Image Segmentation with Mamba Twister","summary":"  Referring Image Segmentation (RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve the\nstate-of-the-art on three challenging benchmarks. Moreover, we conduct thorough\nanalyses of ReMamber and discuss other fusion designs using Mamba. These\nprovide valuable perspectives for future research.\n","authors":["Yuhuan Yang","Chaofan Ma","Jiangchao Yao","Zhun Zhong","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17834v1","updated":"2024-03-26T16:19:56Z","published":"2024-03-26T16:19:56Z","title":"A foundation model utilizing chest CT volumes and radiology reports for\n  supervised-level zero-shot detection of abnormalities","summary":"  A major challenge in computational research in 3D medical imaging is the lack\nof comprehensive datasets. Addressing this issue, our study introduces CT-RATE,\nthe first 3D medical imaging dataset that pairs images with textual reports.\nCT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188\nthrough various reconstructions, from 21,304 unique patients, along with\ncorresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,\na CT-focused contrastive language-image pre-training framework. As a versatile,\nself-supervised model, CT-CLIP is designed for broad application and does not\nrequire task-specific training. Remarkably, CT-CLIP outperforms\nstate-of-the-art, fully supervised methods in multi-abnormality detection\nacross all key metrics, thus eliminating the need for manual annotation. We\nalso demonstrate its utility in case retrieval, whether using imagery or\ntextual queries, thereby advancing knowledge dissemination. The open-source\nrelease of CT-RATE and CT-CLIP marks a significant advancement in medical AI,\nenhancing 3D imaging analysis and fostering innovation in healthcare.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Furkan Almas","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Irem Dogan","Muhammed Furkan Dasdelen","Bastian Wittmann","Enis Simsar","Mehmet Simsar","Emine Bensu Erdemir","Abdullah Alanbay","Anjany Sekuboyina","Berkan Lafci","Mehmet K. Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17830v1","updated":"2024-03-26T16:10:21Z","published":"2024-03-26T16:10:21Z","title":"Assessment of Multimodal Large Language Models in Alignment with Human\n  Values","summary":"  Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.\n","authors":["Zhelun Shi","Zhipin Wang","Hongxing Fan","Zaibin Zhang","Lijun Li","Yongting Zhang","Zhenfei Yin","Lu Sheng","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2403.17830v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.02692"},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2403.17823v1","updated":"2024-03-26T16:04:19Z","published":"2024-03-26T16:04:19Z","title":"Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders","summary":"  Self-supervised pre-training of image encoders is omnipresent in the\nliterature, particularly following the introduction of Masked autoencoders\n(MAE). Current efforts attempt to learn object-centric representations from\nmotion in videos. In particular, SiamMAE recently introduced a Siamese network,\ntraining a shared-weight encoder from two frames of a video with a high\nasymmetric masking ratio (95%). In this work, we propose CropMAE, an\nalternative approach to the Siamese pre-training introduced by SiamMAE. Our\nmethod specifically differs by exclusively considering pairs of cropped images\nsourced from the same image but cropped differently, deviating from the\nconventional pairs of frames extracted from a video. CropMAE therefore\nalleviates the need for video datasets, while maintaining competitive\nperformances and drastically reducing pre-training time. Furthermore, we\ndemonstrate that CropMAE learns similar object-centric representations without\nexplicit motion, showing that current self-supervised learning methods do not\nlearn objects from motion, but rather thanks to the Siamese architecture.\nFinally, CropMAE achieves the highest masking ratio to date (98.5%), enabling\nthe reconstruction of images using only two visible patches. Our code is\navailable at https://github.com/alexandre-eymael/CropMAE.\n","authors":["Alexandre Eymaël","Renaud Vandeghen","Anthony Cioppa","Silvio Giancola","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2403.17823v1.pdf","comment":"19 pages, 6 figures, 3 tables, 1 page of supplementary material"},{"id":"http://arxiv.org/abs/2403.17822v1","updated":"2024-03-26T16:00:31Z","published":"2024-03-26T16:00:31Z","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","summary":"  3D Gaussian splatting, a novel differentiable rendering technique, has\nachieved state-of-the-art novel view synthesis results with high rendering\nspeeds and relatively low training times. However, its performance on scenes\ncommonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. We extend 3D Gaussian splatting with depth and\nnormal cues to tackle challenging indoor datasets and showcase techniques for\nefficient mesh extraction, an important downstream application. Specifically,\nwe regularize the optimization procedure with depth information, enforce local\nsmoothness of nearby Gaussians, and use the geometry of the 3D Gaussians\nsupervised by normal cues to achieve better alignment with the true scene\ngeometry. We improve depth estimation and novel view synthesis results over\nbaselines and show how this simple yet effective regularization technique can\nbe used to directly extract meshes from the Gaussian representation yielding\nmore physically accurate reconstructions on indoor scenes. Our code will be\nreleased in https://github.com/maturk/dn-splatter.\n","authors":["Matias Turkulainen","Xuqian Ren","Iaroslav Melekhov","Otto Seiskari","Esa Rahtu","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2403.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00454v3","updated":"2024-03-26T15:41:17Z","published":"2023-09-30T18:13:41Z","title":"SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time\n  Echocardiograms with Self- and Weakly-Supervised Learning","summary":"  Echocardiography has become an indispensable clinical imaging modality for\ngeneral heart health assessment. From calculating biomarkers such as ejection\nfraction to the probability of a patient's heart failure, accurate segmentation\nof the heart structures allows doctors to assess the heart's condition and\ndevise treatments with greater precision and accuracy. However, achieving\naccurate and reliable left ventricle segmentation is time-consuming and\nchallenging due to different reasons. Hence, clinicians often rely on\nsegmenting the left ventricular (LV) in two specific echocardiogram frames to\nmake a diagnosis. This limited coverage in manual LV segmentation poses a\nchallenge for developing automatic LV segmentation with high temporal\nconsistency, as the resulting dataset is typically annotated sparsely. In\nresponse to this challenge, this work introduces SimLVSeg, a novel paradigm\nthat enables video-based networks for consistent LV segmentation from sparsely\nannotated echocardiogram videos. SimLVSeg consists of self-supervised\npre-training with temporal masking, followed by weakly supervised learning\ntailored for LV segmentation from sparse annotations. We demonstrate how\nSimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%\n(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset\n(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two\ntypes of video segmentation networks: 2D super image and 3D segmentation. To\nshow the effectiveness of our approach, we provide extensive ablation studies,\nincluding pre-training settings and various deep learning backbones. We further\nconduct an out-of-distribution test to showcase SimLVSeg's generalizability on\nunseen distribution (CAMUS dataset). The code is publicly available at\nhttps://github.com/fadamsyah/SimLVSeg.\n","authors":["Fadillah Maani","Asim Ukaye","Nada Saadi","Numan Saeed","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2310.00454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08639v2","updated":"2024-03-26T15:40:20Z","published":"2024-03-13T15:51:23Z","title":"HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction","summary":"  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n","authors":["Yi Zhou","Hui Zhang","Jiaqian Yu","Yifan Yang","Sangil Jung","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.08639v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17801v1","updated":"2024-03-26T15:40:05Z","published":"2024-03-26T15:40:05Z","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","summary":"  We present a method for reconstructing 3D shape of arbitrary Lambertian\nobjects based on measurements by miniature, energy-efficient, low-cost\nsingle-photon cameras. These cameras, operating as time resolved image sensors,\nilluminate the scene with a very fast pulse of diffuse light and record the\nshape of that pulse as it returns back from the scene at a high temporal\nresolution. We propose to model this image formation process, account for its\nnon-idealities, and adapt neural rendering to reconstruct 3D geometry from a\nset of spatially distributed sensors with known poses. We show that our\napproach can successfully recover complex 3D shapes from simulated data. We\nfurther demonstrate 3D object reconstruction from real-world captures,\nutilizing measurements from a commodity proximity sensor. Our work draws a\nconnection between image-based modeling and active range scanning and is a step\ntowards 3D vision with single-photon cameras.\n","authors":["Fangzhou Mu","Carter Sifferman","Sacha Jungerman","Yiquan Li","Mark Han","Michael Gleicher","Mohit Gupta","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17787v1","updated":"2024-03-26T15:20:49Z","published":"2024-03-26T15:20:49Z","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","summary":"  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2403.17787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17782v1","updated":"2024-03-26T15:15:15Z","published":"2024-03-26T15:15:15Z","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","summary":"  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n","authors":["Chenjian Gao","Boyan Jiang","Xinghui Li","Yingpeng Zhang","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17782v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12225v2","updated":"2024-03-26T15:06:00Z","published":"2024-02-19T15:33:09Z","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and\n  Scalability","summary":"  Auto-regressive models have achieved impressive results in 2D image\ngeneration by modeling joint distributions in grid space. In this paper, we\nextend auto-regressive models to 3D domains, and seek a stronger ability of 3D\nshape generation by improving auto-regressive models at capacity and\nscalability simultaneously. Firstly, we leverage an ensemble of publicly\navailable 3D datasets to facilitate the training of large-scale models. It\nconsists of a comprehensive collection of approximately 900,000 objects, with\nmultiple properties of meshes, points, voxels, rendered images, and text\ncaptions. This diverse labeled dataset, termed Objaverse-Mix, empowers our\nmodel to learn from a wide range of object variations. However, directly\napplying 3D auto-regression encounters critical challenges of high\ncomputational demands on volumetric grids and ambiguous auto-regressive order\nalong grid dimensions, resulting in inferior quality of 3D shapes. To this end,\nwe then present a novel framework Argus3D in terms of capacity. Concretely, our\napproach introduces discrete representation learning based on a latent vector\ninstead of volumetric grids, which not only reduces computational costs but\nalso preserves essential geometric details by learning the joint distributions\nin a more tractable order. The capacity of conditional generation can thus be\nrealized by simply concatenating various conditioning inputs to the latent\nvector, such as point clouds, categories, images, and texts. In addition,\nthanks to the simplicity of our model architecture, we naturally scale up our\napproach to a larger model with an impressive 3.6 billion parameters, further\nenhancing the quality of versatile 3D generation. Extensive experiments on four\ngeneration tasks demonstrate that Argus3D can synthesize diverse and faithful\nshapes across multiple categories, achieving remarkable performance.\n","authors":["Xuelin Qian","Yu Wang","Simian Luo","Yinda Zhang","Ying Tai","Zhenyu Zhang","Chengjie Wang","Xiangyang Xue","Bo Zhao","Tiejun Huang","Yunsheng Wu","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2402.12225v2.pdf","comment":"Project page: https://argus-3d.github.io/ . Datasets:\n  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:\n  substantial text overlap with arXiv:2303.14700"},{"id":"http://arxiv.org/abs/2403.17770v1","updated":"2024-03-26T14:59:11Z","published":"2024-03-26T14:59:11Z","title":"CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node\n  Segmentation","summary":"  Despite the significant success achieved by deep learning methods in medical\nimage segmentation, researchers still struggle in the computer-aided diagnosis\nof abdominal lymph nodes due to the complex abdominal environment, small and\nindistinguishable lesions, and limited annotated data. To address these\nproblems, we present a pipeline that integrates the conditional diffusion model\nfor lymph node generation and the nnU-Net model for lymph node segmentation to\nimprove the segmentation performance of abdominal lymph nodes through\nsynthesizing a diversity of realistic abdominal lymph node data. We propose\nLN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph\nnode (LN) generation. LN-DDPM utilizes lymph node masks and anatomical\nstructure masks as model conditions. These conditions work in two conditioning\nmechanisms: global structure conditioning and local detail conditioning, to\ndistinguish between lymph nodes and their surroundings and better capture lymph\nnode characteristics. The obtained paired abdominal lymph node images and masks\nare used for the downstream segmentation task. Experimental results on the\nabdominal lymph node datasets demonstrate that LN-DDPM outperforms other\ngenerative methods in the abdominal lymph node image synthesis and better\nassists the downstream abdominal lymph node segmentation task.\n","authors":["Yongrui Yu","Hanyu Chen","Zitian Zhang","Qiong Xiao","Wenhui Lei","Linrui Dai","Yu Fu","Hui Tan","Guan Wang","Peng Gao","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17057v2","updated":"2024-03-26T14:54:04Z","published":"2023-11-28T18:59:52Z","title":"ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person\n  Interactions","summary":"  Current approaches for 3D human motion synthesis generate high-quality\nanimations of digital humans performing a wide variety of actions and gestures.\nHowever, a notable technological gap exists in addressing the complex dynamics\nof multi-human interactions within this paradigm. In this work, we present\nReMoS, a denoising diffusion-based model that synthesizes full-body reactive\nmotion of a person in a two-person interaction scenario. Assuming the motion of\none person is given, we employ a combined spatio-temporal cross-attention\nmechanism to synthesize the reactive body and hand motion of the second person,\nthereby completing the interactions between the two. We demonstrate ReMoS\nacross challenging two-person scenarios such as pair-dancing, Ninjutsu,\nkickboxing, and acrobatics, where one person's movements have complex and\ndiverse influences on the other. We also contribute the ReMoCap dataset for\ntwo-person interactions containing full-body and finger motions. We evaluate\nReMoS through multiple quantitative metrics, qualitative visualizations, and a\nuser study, and also indicate usability in interactive motion editing\napplications.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2311.17057v2.pdf","comment":"17 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.17765v1","updated":"2024-03-26T14:53:24Z","published":"2024-03-26T14:53:24Z","title":"MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash\n  Representations","summary":"  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing\nmultiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM\neffectively tracks camera positions and incrementally builds a scalable\nmulti-map representation for both small and large indoor environments. It\ndynamically allocates sub-maps for newly observed local regions, enabling\nconstraint-free mapping without prior scene information. Unlike traditional\ngrid-based methods, we use three orthogonal axis-aligned planes for\nhash-encoding scene properties, significantly reducing hash collisions and the\nnumber of trainable parameters. This hybrid approach not only speeds up\nconvergence but also enhances the fidelity of surface reconstruction.\nFurthermore, our optimization strategy concurrently optimizes all sub-maps\nintersecting with the current camera frustum, ensuring global consistency.\nExtensive testing on both real-world and synthetic datasets has shown that\nMUTE-SLAM delivers state-of-the-art surface reconstruction quality and\ncompetitive tracking performance across diverse indoor settings. The code will\nbe made public upon acceptance of the paper.\n","authors":["Yifan Yan","Ruomin He","Zhenghua Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15585v2","updated":"2024-03-26T14:51:57Z","published":"2024-03-22T19:19:51Z","title":"MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis","summary":"  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces \\textbf{MedPromptX}, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX\n","authors":["Mai A. Shaaban","Adnan Khan","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.15585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17761v1","updated":"2024-03-26T14:51:53Z","published":"2024-03-26T14:51:53Z","title":"Makeup Prior Models for 3D Facial Makeup Estimation and Applications","summary":"  In this work, we introduce two types of makeup prior models to extend\nexisting 3D face prior models: PCA-based and StyleGAN2-based priors. The\nPCA-based prior model is a linear model that is easy to construct and is\ncomputationally efficient. However, it retains only low-frequency information.\nConversely, the StyleGAN2-based model can represent high-frequency information\nwith relatively higher computational cost than the PCA-based model. Although\nthere is a trade-off between the two models, both are applicable to 3D facial\nmakeup estimation and related applications. By leveraging makeup prior models\nand designing a makeup consistency module, we effectively address the\nchallenges that previous methods faced in robustly estimating makeup,\nparticularly in the context of handling self-occluded faces. In experiments, we\ndemonstrate that our approach reduces computational costs by several orders of\nmagnitude, achieving speeds up to 180 times faster. In addition, by improving\nthe accuracy of the estimated makeup, we confirm that our methods are highly\nadvantageous for various 3D facial makeup applications such as 3D makeup face\nreconstruction, user-friendly makeup editing, makeup transfer, and\ninterpolation.\n","authors":["Xingchao Yang","Takafumi Taketomi","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2403.17761v1.pdf","comment":"CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page"},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.17755v1","updated":"2024-03-26T14:44:51Z","published":"2024-03-26T14:44:51Z","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data\n  Copyright Protection","summary":"  In the realm of healthcare, the challenges of copyright protection and\nunauthorized third-party misuse are increasingly significant. Traditional\nmethods for data copyright protection are applied prior to data distribution,\nimplying that models trained on these data become uncontrollable. This paper\nintroduces a novel approach, named DataCook, designed to safeguard the\ncopyright of healthcare data during the deployment phase. DataCook operates by\n\"cooking\" the raw data before distribution, enabling the development of models\nthat perform normally on this processed data. However, during the deployment\nphase, the original test data must be also \"cooked\" through DataCook to ensure\nnormal model performance. This process grants copyright holders control over\nauthorization during the deployment phase. The mechanism behind DataCook is by\ncrafting anti-adversarial examples (AntiAdv), which are designed to enhance\nmodel confidence, as opposed to standard adversarial examples (Adv) that aim to\nconfuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,\nensuring that the data processed by DataCook remains easily understandable. We\nconducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D\ndata and the high-resolution variants. The outcomes indicate that DataCook\neffectively meets its objectives, preventing models trained on AntiAdv from\nanalyzing unauthorized data effectively, without compromising the validity and\naccuracy of the data in legitimate scenarios. Code and data are available at\nhttps://github.com/MedMNIST/DataCook.\n","authors":["Sihan Shang","Jiancheng Yang","Zhenglong Sun","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2403.17755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06247v2","updated":"2024-03-26T14:42:21Z","published":"2024-03-10T16:11:17Z","title":"Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation","summary":"  We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.\n","authors":["Mingyu Lee","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06247v2.pdf","comment":"18 pages, Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17749v1","updated":"2024-03-26T14:40:17Z","published":"2024-03-26T14:40:17Z","title":"Multi-Task Dense Prediction via Mixture of Low-Rank Experts","summary":"  Previous multi-task dense prediction methods based on the Mixture of Experts\n(MoE) have received great performance but they neglect the importance of\nexplicitly modeling the global relations among all tasks. In this paper, we\npresent a novel decoder-focused method for multi-task dense prediction, called\nMixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,\nMLoRE adds a generic convolution path to the original MoE structure, where each\ntask feature can go through this path for explicit parameter sharing.\nFurthermore, to control the parameters and computational cost brought by the\nincrease in the number of experts, we take inspiration from LoRA and propose to\nleverage the low-rank format of a vanilla convolution in the expert network.\nSince the low-rank experts have fewer parameters and can be dynamically\nparameterized into the generic convolution, the parameters and computational\ncost do not change much with the increase of experts. Benefiting from this\ndesign, we increase the number of experts and its reception field to enlarge\nthe representation capacity, facilitating multiple dense tasks learning in a\nunified network. Extensive experiments on the PASCAL-Context and NYUD-v2\nbenchmarks show that our MLoRE achieves superior performance compared to\nprevious state-of-the-art methods on all metrics. Our code is available at\nhttps://github.com/YuqiYang213/MLoRE.\n","authors":["Yuqi Yang","Peng-Tao Jiang","Qibin Hou","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.17749v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08270v2","updated":"2024-03-26T14:39:43Z","published":"2024-03-13T05:46:36Z","title":"Identity-aware Dual-constraint Network for Cloth-Changing Person\n  Re-identification","summary":"  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify\nthe target person in more realistic surveillance scenarios, where pedestrians\nusually change their clothing. Despite great progress, limited cloth-changing\ntraining samples in existing CC-ReID datasets still prevent the model from\nadequately learning cloth-irrelevant features. In addition, due to the absence\nof explicit supervision to keep the model constantly focused on\ncloth-irrelevant areas, existing methods are still hampered by the disruption\nof clothing variations. To solve the above issues, we propose an Identity-aware\nDual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the\nmodel extract cloth-irrelevant clues, we propose a Clothes Diversity\nAugmentation (CDA), which generates more realistic cloth-changing samples by\nenriching the clothing color while preserving the texture. In addition, a\nMulti-scale Constraint Block (MCB) is designed, which extracts fine-grained\nidentity-related features and effectively transfers cloth-irrelevant knowledge.\nMoreover, a Counterfactual-guided Attention Module (CAM) is presented, which\nlearns cloth-irrelevant features from channel and space dimensions and utilizes\nthe counterfactual intervention for supervising the attention map to highlight\nidentity-related regions. Finally, a Semantic Alignment Constraint (SAC) is\ndesigned to facilitate high-level semantic feature interaction. Comprehensive\nexperiments on four CC-ReID datasets indicate that our method outperforms prior\nstate-of-the-art approaches.\n","authors":["Peini Guo","Mengyuan Liu","Hong Liu","Ruijia Fan","Guoquan Wang","Bin He"],"pdf_url":"https://arxiv.org/pdf/2403.08270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17734v1","updated":"2024-03-26T14:21:49Z","published":"2024-03-26T14:21:49Z","title":"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation\n  scans using Linked Denoising Diffusion Probabilistic Models","summary":"  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging\nand radiotherapy is hindered by the limited availability of large imaging data\nrepositories. With recent research and improvements in denoising diffusion\nprobabilistic models (DDPM), high quality synthetic medical scans are now\npossible. Despite this, there is currently no way of generating multiple\nrelated images, such as a corresponding ground truth which can be used to train\nmodels, so synthetic scans are often manually annotated before use. This\nresearch introduces a novel architecture that is able to generate multiple,\nrelated PET-CT-tumour mask pairs using paired networks and conditional\nencoders. Our approach includes innovative, time step-controlled mechanisms and\na `noise-seeding' strategy to improve DDPM sampling consistency. While our\nmodel requires a modified perceptual loss function to ensure accurate feature\nalignment we show generation of clearly aligned synthetic images and\nimprovement in segmentation accuracy with generated images.\n","authors":["Rowan Bradbury","Katherine A. Vallis","Bartlomiej W. Papiez"],"pdf_url":"https://arxiv.org/pdf/2403.17734v1.pdf","comment":"to be published in IEEE International Symposium on Biomedical Imaging\n  2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17464v2","updated":"2024-03-26T14:15:25Z","published":"2024-02-27T12:42:06Z","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","summary":"  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly.\n","authors":["Bi'an Du","Xiang Gao","Wei Hu","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17725v1","updated":"2024-03-26T14:13:44Z","published":"2024-03-26T14:13:44Z","title":"Deep Learning for Segmentation of Cracks in High-Resolution Images of\n  Steel Bridges","summary":"  Automating the current bridge visual inspection practices using drones and\nimage processing techniques is a prominent way to make these inspections more\neffective, robust, and less expensive. In this paper, we investigate the\ndevelopment of a novel deep-learning method for the detection of fatigue cracks\nin high-resolution images of steel bridges. First, we present a novel and\nchallenging dataset comprising of images of cracks in steel bridges. Secondly,\nwe integrate the ConvNext neural network with a previous state- of-the-art\nencoder-decoder network for crack segmentation. We study and report, the\neffects of the use of background patches on the network performance when\napplied to high-resolution images of cracks in steel bridges. Finally, we\nintroduce a loss function that allows the use of more background patches for\nthe training process, which yields a significant reduction in false positive\nrates.\n","authors":["Andrii Kompanets","Gautam Pai","Remco Duits","Davide Leonetti","Bert Snijder"],"pdf_url":"https://arxiv.org/pdf/2403.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17712v1","updated":"2024-03-26T13:58:47Z","published":"2024-03-26T13:58:47Z","title":"Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A\n  New Benchmark","summary":"  The widespread use of various chemical gases in industrial processes\nnecessitates effective measures to prevent their leakage during transportation\nand storage, given their high toxicity. Thermal infrared-based computer vision\ndetection techniques provide a straightforward approach to identify gas leakage\nareas. However, the development of high-quality algorithms has been challenging\ndue to the low texture in thermal images and the lack of open-source datasets.\nIn this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),\nwhich employs an RGB-assisted two-stream network architecture to integrate\ntexture information from RGB images and gas area information from thermal\nimages. Additionally, to facilitate the research of invisible gas detection, we\nintroduce Gas-DB, an extensive open-source gas detection database including\nabout 1.3K well-annotated RGB-thermal images with eight variant collection\nscenes. Experimental results demonstrate that our method successfully leverages\nthe advantages of both modalities, achieving state-of-the-art (SOTA)\nperformance among RGB-thermal methods, surpassing single-stream SOTA models in\nterms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,\nand 4.88%, respectively. The code and data will be made available soon.\n","authors":["Jue Wang","Yuxiang Lin","Qi Zhao","Dong Luo","Shuaibao Chen","Wei Chen","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15094v2","updated":"2024-03-26T13:57:26Z","published":"2023-05-24T12:22:23Z","title":"InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree\n  Neural Radiance Fields","summary":"  We propose InNeRF360, an automatic system that accurately removes\ntext-specified objects from 360-degree Neural Radiance Fields (NeRF). The\nchallenge is to effectively remove objects while inpainting perceptually\nconsistent content for the missing regions, which is particularly demanding for\nexisting NeRF models due to their implicit volumetric representation. Moreover,\nunbounded scenes are more prone to floater artifacts in the inpainted region\nthan frontal-facing scenes, as the change of object appearance and background\nacross views is more sensitive to inaccurate segmentations and inconsistent\ninpainting. With a trained NeRF and a text description, our method efficiently\nremoves specified objects and inpaints visually consistent content without\nartifacts. We apply depth-space warping to enforce consistency across multiview\ntext-encoded segmentations, and then refine the inpainted NeRF model using\nperceptual priors and 3D diffusion-based geometric priors to ensure visual\nplausibility. Through extensive experiments in segmentation and inpainting on\n360-degree and frontal-facing NeRFs, we show that our approach is effective and\nenhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.\n","authors":["Dongqing Wang","Tong Zhang","Alaa Abboud","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2305.15094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17709v1","updated":"2024-03-26T13:56:34Z","published":"2024-03-26T13:56:34Z","title":"Groupwise Query Specialization and Quality-Aware Multi-Assignment for\n  Transformer-based Visual Relationship Detection","summary":"  Visual Relationship Detection (VRD) has seen significant advancements with\nTransformer-based architectures recently. However, we identify two key\nlimitations in a conventional label assignment for training Transformer-based\nVRD models, which is a process of mapping a ground-truth (GT) to a prediction.\nUnder the conventional assignment, an unspecialized query is trained since a\nquery is expected to detect every relation, which makes it difficult for a\nquery to specialize in specific relations. Furthermore, a query is also\ninsufficiently trained since a GT is assigned only to a single prediction,\ntherefore near-correct or even correct predictions are suppressed by being\nassigned no relation as a GT. To address these issues, we propose Groupwise\nQuery Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise\nQuery Specialization trains a specialized query by dividing queries and\nrelations into disjoint groups and directing a query in a specific query group\nsolely toward relations in the corresponding relation group. Quality-Aware\nMulti-Assignment further facilitates the training by assigning a GT to multiple\npredictions that are significantly close to a GT in terms of a subject, an\nobject, and the relation in between. Experimental results and analyses show\nthat SpeaQ effectively trains specialized queries, which better utilize the\ncapacity of a model, resulting in consistent performance gains with zero\nadditional inference cost across multiple VRD models and benchmarks. Code is\navailable at https://github.com/mlvlab/SpeaQ.\n","authors":["Jongha Kim","Jihwan Park","Jinyoung Park","Jinyoung Kim","Sehyung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17709v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.16014v2","updated":"2024-03-26T13:55:40Z","published":"2023-12-26T11:49:23Z","title":"Passive Non-Line-of-Sight Imaging with Light Transport Modulation","summary":"  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.\n","authors":["Jiarui Zhang","Ruixu Geng","Xiaolong Du","Yan Chen","Houqiang Li","Yang Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2403.17702v1","updated":"2024-03-26T13:40:52Z","published":"2024-03-26T13:40:52Z","title":"The Solution for the CVPR 2023 1st foundation model challenge-Track2","summary":"  In this paper, we propose a solution for cross-modal transportation\nretrieval. Due to the cross-domain problem of traffic images, we divide the\nproblem into two sub-tasks of pedestrian retrieval and vehicle retrieval\nthrough a simple strategy. In pedestrian retrieval tasks, we use IRRA as the\nbase model and specifically design an Attribute Classification to mine the\nknowledge implied by attribute labels. More importantly, We use the strategy of\nInclusion Relation Matching to make the image-text pairs with inclusion\nrelation have similar representation in the feature space. For the vehicle\nretrieval task, we use BLIP as the base model. Since aligning the color\nattributes of vehicles is challenging, we introduce attribute-based object\ndetection techniques to add color patch blocks to vehicle images for color data\naugmentation. This serves as strong prior information, helping the model\nperform the image-text alignment. At the same time, we incorporate labeled\nattributes into the image-text alignment loss to learn fine-grained alignment\nand prevent similar images and texts from being incorrectly separated. Our\napproach ranked first in the final B-board test with a score of 70.9.\n","authors":["Haonan Xu","Yurui Huang","Sishun Pan","Zhihao Guan","Yi Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17694v1","updated":"2024-03-26T13:35:02Z","published":"2024-03-26T13:35:02Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","summary":"  In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait\n","authors":["Huawei Wei","Zejun Yang","Zhisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2311.16081v2","updated":"2024-03-26T13:32:06Z","published":"2023-11-27T18:52:09Z","title":"ViT-Lens: Towards Omni-modal Representations","summary":"  Aiming to advance AI agents, large foundation models significantly improve\nreasoning and instruction execution, yet the current focus on vision and\nlanguage neglects the potential of perceiving diverse modalities in open-world\nenvironments. However, the success of data-driven vision and language models is\ncostly or even infeasible to be reproduced for rare modalities. In this paper,\nwe present ViT-Lens-2 that facilitates efficient omni-modal representation\nlearning by perceiving novel modalities with a pretrained ViT and aligning them\nto a pre-defined space. Specifically, the modality-specific lens is tuned to\nproject any-modal signals to an intermediate embedding space, which are then\nprocessed by a strong ViT with pre-trained visual knowledge. The encoded\nrepresentations are optimized toward aligning with the modal-independent space,\npre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified\nsolution for representation learning of increasing modalities with two\nappealing advantages: (i) Unlocking the great potential of pretrained ViTs to\nnovel modalities effectively with efficient data regime; (ii) Enabling emergent\ndownstream capabilities through modality alignment and shared ViT parameters.\nWe tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,\ntactile and EEG, and set new state-of-the-art results across various\nunderstanding tasks, such as zero-shot classification. By seamlessly\nintegrating ViT-Lens-2 into Multimodal Foundation Models, we enable\nAny-modality to Text and Image Generation in a zero-shot manner. Code and\nmodels are available at https://github.com/TencentARC/ViT-Lens.\n","authors":["Weixian Lei","Yixiao Ge","Kun Yi","Jianfeng Zhang","Difei Gao","Dylan Sun","Yuying Ge","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2311.16081v2.pdf","comment":"This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.11708v3","updated":"2024-03-26T13:21:52Z","published":"2024-03-18T12:12:45Z","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n","authors":["Kaijie Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11708v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2304.10417v3","updated":"2024-03-26T13:16:02Z","published":"2023-04-20T16:01:55Z","title":"SINC: Spatial Composition of 3D Human Motions for Simultaneous Action\n  Generation","summary":"  Our goal is to synthesize 3D human motions given textual inputs describing\nsimultaneous actions, for example 'waving hand' while 'walking' at the same\ntime. We refer to generating such simultaneous movements as performing 'spatial\ncompositions'. In contrast to temporal compositions that seek to transition\nfrom one action to another, spatial compositing requires understanding which\nbody parts are involved in which action, to be able to move them\nsimultaneously. Motivated by the observation that the correspondence between\nactions and body parts is encoded in powerful language models, we extract this\nknowledge by prompting GPT-3 with text such as \"what are the body parts\ninvolved in the action <action name>?\", while also providing the parts list and\nfew-shot examples. Given this action-part mapping, we combine body parts from\ntwo motions together and establish the first automated method to spatially\ncompose two actions. However, training data with compositional actions is\nalways limited by the combinatorics. Hence, we further create synthetic data\nwith this approach, and use it to train a new state-of-the-art text-to-motion\ngeneration model, called SINC (\"SImultaneous actioN Compositions for 3D human\nmotions\"). In our experiments, that training with such GPT-guided synthetic\ndata improves spatial composition generation over baselines. Our code is\npublicly available at https://sinc.is.tue.mpg.de/.\n","authors":["Nikos Athanasiou","Mathis Petrovich","Michael J. Black","Gül Varol"],"pdf_url":"https://arxiv.org/pdf/2304.10417v3.pdf","comment":"Teaser Fixed"},{"id":"http://arxiv.org/abs/2403.14135v2","updated":"2024-03-26T13:15:12Z","published":"2024-03-21T05:10:26Z","title":"Powerful Lossy Compression for Noisy Images","summary":"  Image compression and denoising represent fundamental challenges in image\nprocessing with many real-world applications. To address practical demands,\ncurrent solutions can be categorized into two main strategies: 1) sequential\nmethod; and 2) joint method. However, sequential methods have the disadvantage\nof error accumulation as there is information loss between multiple individual\nmodels. Recently, the academic community began to make some attempts to tackle\nthis problem through end-to-end joint methods. Most of them ignore that\ndifferent regions of noisy images have different characteristics. To solve\nthese problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware\njoint solution exploits local and non-local features for image compression and\ndenoising simultaneously. We design an end-to-end trainable network, which\nincludes the main encoder branch, the guidance branch, and the signal-to-noise\nratio~(SNR) aware branch. We conducted extensive experiments on both synthetic\nand real-world datasets, demonstrating that our joint solution outperforms\nexisting state-of-the-art methods.\n","authors":["Shilv Cai","Xiaoguo Liang","Shuning Cao","Luxin Yan","Sheng Zhong","Liqun Chen","Xu Zou"],"pdf_url":"https://arxiv.org/pdf/2403.14135v2.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2308.10185v2","updated":"2024-03-26T13:11:07Z","published":"2023-08-20T07:26:51Z","title":"ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights","summary":"  Though the success of CLIP-based training recipes in vision-language models,\ntheir scalability to more modalities (e.g., 3D, audio, etc.) is limited to\nlarge-scale data, which is expensive or even inapplicable for rare modalities.\nIn this paper, we present ViT-Lens that facilitates efficient omni-modal\nrepresentation learning by perceiving novel modalities with a pretrained ViT\nand aligning to a pre-defined space. Specifically, the modality-specific lens\nis tuned to project multimodal signals to the shared embedding space, which are\nthen processed by a strong ViT that carries pre-trained image knowledge. The\nencoded multimodal representations are optimized toward aligning with the\nmodal-independent space, pre-defined by off-the-shelf foundation models. A\nwell-trained lens with a ViT backbone has the potential to serve as one of\nthese foundation models, supervising the learning of subsequent modalities.\nViT-Lens provides a unified solution for representation learning of increasing\nmodalities with two appealing benefits: (i) Exploiting the pretrained ViT\nacross tasks and domains effectively with efficient data regime; (ii) Emergent\ndownstream capabilities of novel modalities are demonstrated due to the\nmodality alignment space. We evaluate ViT-Lens in the context of 3D as an\ninitial verification. In zero-shot 3D classification, ViT-Lens achieves\nsubstantial improvements over previous state-of-the-art, showing 52.0% accuracy\non Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,\nwe enable zero-shot 3D question-answering by simply integrating the trained 3D\nlens into the InstructBLIP model without any adaptation. We will release the\nresults of ViT-Lens on more modalities in the near future.\n","authors":["Weixian Lei","Yixiao Ge","Jianfeng Zhang","Dylan Sun","Kun Yi","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2308.10185v2.pdf","comment":"19 pages, 4 figures and 9 tables"},{"id":"http://arxiv.org/abs/2403.17678v1","updated":"2024-03-26T13:05:49Z","published":"2024-03-26T13:05:49Z","title":"Hierarchical Light Transformer Ensembles for Multimodal Trajectory\n  Forecasting","summary":"  Accurate trajectory forecasting is crucial for the performance of various\nsystems, such as advanced driver-assistance systems and self-driving vehicles.\nThese forecasts allow to anticipate events leading to collisions and,\ntherefore, to mitigate them. Deep Neural Networks have excelled in motion\nforecasting, but issues like overconfidence and uncertainty quantification\npersist. Deep Ensembles address these concerns, yet applying them to multimodal\ndistributions remains challenging. In this paper, we propose a novel approach\nnamed Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently\ntraining an ensemble of Transformer architectures using a novel hierarchical\nloss function. HLT-Ens leverages grouped fully connected layers, inspired by\ngrouped convolution techniques, to capture multimodal distributions,\neffectively. Through extensive experimentation, we demonstrate that HLT-Ens\nachieves state-of-the-art performance levels, offering a promising avenue for\nimproving trajectory forecasting techniques.\n","authors":["Adrien Lafage","Mathieu Barbier","Gianni Franchi","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2403.17678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17672v1","updated":"2024-03-26T13:02:38Z","published":"2024-03-26T13:02:38Z","title":"Predicting Perceived Gloss: Do Weak Labels Suffice?","summary":"  Estimating perceptual attributes of materials directly from images is a\nchallenging task due to their complex, not fully-understood interactions with\nexternal factors, such as geometry and lighting. Supervised deep learning\nmodels have recently been shown to outperform traditional approaches, but rely\non large datasets of human-annotated images for accurate perception\npredictions. Obtaining reliable annotations is a costly endeavor, aggravated by\nthe limited ability of these models to generalise to different aspects of\nappearance. In this work, we show how a much smaller set of human annotations\n(\"strong labels\") can be effectively augmented with automatically derived \"weak\nlabels\" in the context of learning a low-dimensional image-computable gloss\nmetric. We evaluate three alternative weak labels for predicting human gloss\nperception from limited annotated data. Incorporating weak labels enhances our\ngloss prediction beyond the current state of the art. Moreover, it enables a\nsubstantial reduction in human annotation costs without sacrificing accuracy,\nwhether working with rendered images or real photographs.\n","authors":["Julia Guerrero-Viu","J. Daniel Subias","Ana Serrano","Katherine R. Storrs","Roland W. Fleming","Belen Masia","Diego Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2403.17672v1.pdf","comment":"Computer Graphics Forum (Eurographics 2024)"},{"id":"http://arxiv.org/abs/2310.01819v3","updated":"2024-03-26T12:59:39Z","published":"2023-10-03T06:16:38Z","title":"TP2O: Creative Text Pair-to-Object Generation using Balance\n  Swap-Sampling","summary":"  Generating creative combinatorial objects from two seemingly unrelated object\ntexts is a challenging task in text-to-image synthesis, often hindered by a\nfocus on emulating existing data distributions. In this paper, we develop a\nstraightforward yet highly effective method, called \\textbf{balance\nswap-sampling}. First, we propose a swapping mechanism that generates a novel\ncombinatorial object image set by randomly exchanging intrinsic elements of two\ntext embeddings through a cutting-edge diffusion model. Second, we introduce a\nbalance swapping region to efficiently sample a small subset from the newly\ngenerated image set by balancing CLIP distances between the new images and\ntheir original generations, increasing the likelihood of accepting the\nhigh-quality combinations. Last, we employ a segmentation method to compare\nCLIP distances among the segmented components, ultimately selecting the most\npromising object from the sampled subset. Extensive experiments demonstrate\nthat our approach outperforms recent SOTA T2I methods. Surprisingly, our\nresults even rival those of human artists, such as frog-broccoli.\n","authors":["Jun Li","Zedong Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2310.01819v3.pdf","comment":"Project page: https://tp2o.github.io/anon/"},{"id":"http://arxiv.org/abs/2312.00869v2","updated":"2024-03-26T12:56:55Z","published":"2023-12-01T19:00:17Z","title":"Segment and Caption Anything","summary":"  We propose a method to efficiently equip the Segment Anything Model (SAM)\nwith the ability to generate regional captions. SAM presents strong\ngeneralizability to segment anything while is short for semantic understanding.\nBy introducing a lightweight query-based feature mixer, we align the\nregion-specific features with the embedding space of language models for later\ncaption generation. As the number of trainable parameters is small (typically\nin the order of tens of millions), it costs less computation, less memory\nusage, and less communication bandwidth, resulting in both fast and scalable\ntraining. To address the scarcity problem of regional caption data, we propose\nto first pre-train our model on objection detection and segmentation tasks. We\ncall this step weak supervision pretraining since the pre-training data only\ncontains category names instead of full-sentence descriptions. The weak\nsupervision pretraining allows us to leverage many publicly available object\ndetection and segmentation datasets. We conduct extensive experiments to\ndemonstrate the superiority of our method and validate each design choice. This\nwork serves as a stepping stone towards scaling up regional captioning data and\nsheds light on exploring efficient ways to augment SAM with regional semantics.\nThe project page, along with the associated code, can be accessed via\nhttps://xk-huang.github.io/segment-caption-anything/.\n","authors":["Xiaoke Huang","Jianfeng Wang","Yansong Tang","Zheng Zhang","Han Hu","Jiwen Lu","Lijuan Wang","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2312.00869v2.pdf","comment":"The project page, along with the associated code, can be accessed via\n  https://xk-huang.github.io/segment-caption-anything/; Update author\n  information; Accepted by CVPR 24"},{"id":"http://arxiv.org/abs/2403.17664v1","updated":"2024-03-26T12:53:10Z","published":"2024-03-26T12:53:10Z","title":"DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with\n  Space-sensitive Customization and Semantic Preservation","summary":"  Facial Appearance Editing (FAE) aims to modify physical attributes, such as\npose, expression and lighting, of human facial images while preserving\nattributes like identity and background, showing great importance in\nphotograph. In spite of the great progress in this area, current researches\ngenerally meet three challenges: low generation fidelity, poor attribute\npreservation, and inefficient inference. To overcome above challenges, this\npaper presents DiffFAE, a one-stage and highly-efficient diffusion-based\nframework tailored for high-fidelity FAE. For high-fidelity query attributes\ntransfer, we adopt Space-sensitive Physical Customization (SPC), which ensures\nthe fidelity and generalization ability by utilizing rendering texture derived\nfrom 3D Morphable Model (3DMM). In order to preserve source attributes, we\nintroduce the Region-responsive Semantic Composition (RSC). This module is\nguided to learn decoupled source-regarding features, thereby better preserving\nthe identity and alleviating artifacts from non-facial attributes such as hair,\nclothes, and background. We further introduce a consistency regularization for\nour pipeline to enhance editing controllability by leveraging prior knowledge\nin the attention matrices of diffusion model. Extensive experiments demonstrate\nthe superiority of DiffFAE over existing methods, achieving state-of-the-art\nperformance in facial appearance editing.\n","authors":["Qilin Wang","Jiangning Zhang","Chengming Xu","Weijian Cao","Ying Tai","Yue Han","Yanhao Ge","Hong Gu","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.17664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14149v4","updated":"2024-03-26T12:47:12Z","published":"2023-12-21T18:59:06Z","title":"TagAlign: Improving Vision-Language Alignment with Multi-Tag\n  Classification","summary":"  The crux of learning vision-language models is to extract semantically\naligned information from visual and linguistic data. Existing attempts usually\nface the problem of coarse alignment, e.g., the vision encoder struggles in\nlocalizing an attribute-specified object. In this work, we propose an\nembarrassingly simple approach to better align image and text features with no\nneed of additional data formats other than image-text pairs. Concretely, given\nan image and its paired text, we manage to parse objects (e.g., cat) and\nattributes (e.g., black) from the description, which are highly likely to exist\nin the image. It is noteworthy that the parsing pipeline is fully automatic and\nthus enjoys good scalability. With these parsed semantics as supervision\nsignals, we can complement the commonly used image-text contrastive loss with\nthe multi-tag classification loss. Extensive experimental results on a broad\nsuite of semantic segmentation datasets substantiate the average 5.2\\%\nimprovement of our framework over existing alternatives. Furthermore, the\nvisualization results indicate that attribute supervision makes vision-language\nmodels accurately localize attribute-specified objects. Project page can be\nfound at https://qinying-liu.github.io/Tag-Align.\n","authors":["Qinying Liu","Wei Wu","Kecheng Zheng","Zhan Tong","Jiawei Liu","Yu Liu","Wei Chen","Zilei Wang","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2312.14149v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03246v5","updated":"2024-03-26T12:35:03Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17651v1","updated":"2024-03-26T12:31:58Z","published":"2024-03-26T12:31:58Z","title":"Exploring Dynamic Transformer for Efficient Object Tracking","summary":"  The speed-precision trade-off is a critical problem for visual object\ntracking which usually requires low latency and deployment on constrained\nresources. Existing solutions for efficient tracking mainly focus on adopting\nlight-weight backbones or modules, which nevertheless come at the cost of a\nsacrifice in precision. In this paper, inspired by dynamic network routing, we\npropose DyTrack, a dynamic transformer framework for efficient tracking.\nReal-world tracking scenarios exhibit diverse levels of complexity. We argue\nthat a simple network is sufficient for easy frames in video sequences, while\nmore computation could be assigned to difficult ones. DyTrack automatically\nlearns to configure proper reasoning routes for various inputs, gaining better\nutilization of the available computational budget. Thus, it can achieve higher\nperformance with the same running speed. We formulate instance-specific\ntracking as a sequential decision problem and attach terminating branches to\nintermediate layers of the entire model. Especially, to fully utilize the\ncomputations, we introduce the feature recycling mechanism to reuse the outputs\nof predecessors. Furthermore, a target-aware self-distillation strategy is\ndesigned to enhance the discriminating capabilities of early predictions by\neffectively mimicking the representation pattern of the deep model. Extensive\nexperiments on multiple benchmarks demonstrate that DyTrack achieves promising\nspeed-precision trade-offs with only a single model. For instance, DyTrack\nobtains 64.9% AUC on LaSOT with a speed of 256 fps.\n","authors":["Jiawen Zhu","Xin Chen","Haiwen Diao","Shuai Li","Jun-Yan He","Chenyang Li","Bin Luo","Dong Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.17651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02109v2","updated":"2024-03-26T12:28:02Z","published":"2023-12-04T18:39:00Z","title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder\n  and Explicit Adaptation","summary":"  This work introduces ArtAdapter, a transformative text-to-image (T2I) style\ntransfer framework that transcends traditional limitations of color,\nbrushstrokes, and object shape, capturing high-level style elements such as\ncomposition and distinctive artistic expression. The integration of a\nmulti-level style encoder with our proposed explicit adaptation mechanism\nenables ArtAdapter to achieve unprecedented fidelity in style transfer,\nensuring close alignment with textual descriptions. Additionally, the\nincorporation of an Auxiliary Content Adapter (ACA) effectively separates\ncontent from style, alleviating the borrowing of content from style references.\nMoreover, our novel fast finetuning approach could further enhance zero-shot\nstyle representation while mitigating the risk of overfitting. Comprehensive\nevaluations confirm that ArtAdapter surpasses current state-of-the-art methods.\n","authors":["Dar-Yen Chen","Hamish Tennent","Ching-Wen Hsu"],"pdf_url":"https://arxiv.org/pdf/2312.02109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17639v1","updated":"2024-03-26T12:21:47Z","published":"2024-03-26T12:21:47Z","title":"High-Resolution Image Translation Model Based on Grayscale Redefinition","summary":"  Image-to-image translation is a technique that focuses on transferring images\nfrom one domain to another while maintaining the essential content\nrepresentations. In recent years, image-to-image translation has gained\nsignificant attention and achieved remarkable advancements due to its diverse\napplications in computer vision and image processing tasks. In this work, we\npropose an innovative method for image translation between different domains.\nFor high-resolution image translation tasks, we use a grayscale adjustment\nmethod to achieve pixel-level translation. For other tasks, we utilize the\nPix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and\nimproved loss to enhance the image translation performance. On the other hand,\nto tackle the issue of sparse training data, we adopt model weight\ninitialization from other task to optimize the performance of the current task.\n","authors":["Xixian Wu","Dian Chao","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17638v1","updated":"2024-03-26T12:17:46Z","published":"2024-03-26T12:17:46Z","title":"Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with\n  Relative Geometric Consistency","summary":"  We propose a voxel-based optimization framework, ReVoRF, for few-shot\nradiance fields that strategically address the unreliability in pseudo novel\nview synthesis. Our method pivots on the insight that relative depth\nrelationships within neighboring regions are more reliable than the absolute\ncolor values in disoccluded areas. Consequently, we devise a bilateral\ngeometric consistency loss that carefully navigates the trade-off between color\nfidelity and geometric accuracy in the context of depth consistency for\nuncertain regions. Moreover, we present a reliability-guided learning strategy\nto discern and utilize the variable quality across synthesized views,\ncomplemented by a reliability-aware voxel smoothing algorithm that smoothens\nthe transition between reliable and unreliable data patches. Our approach\nallows for a more nuanced use of all available data, promoting enhanced\nlearning from regions previously considered unsuitable for high-quality\nreconstruction. Extensive experiments across diverse datasets reveal that our\napproach attains significant gains in efficiency and accuracy, delivering\nrendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\%\nimprovement in PSNR over existing few-shot methods. Code is available at\nhttps://github.com/HKCLynn/ReVoRF.\n","authors":["Yingjie Xu","Bangzhen Liu","Hao Tang","Bailin Deng","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2403.17638v1.pdf","comment":"CVPR 2024 final version"},{"id":"http://arxiv.org/abs/2403.15010v2","updated":"2024-03-26T12:16:14Z","published":"2024-03-22T07:47:13Z","title":"Clean-image Backdoor Attacks","summary":"  To gather a significant quantity of annotated training data for\nhigh-performance image classification models, numerous companies opt to enlist\nthird-party providers to label their unlabeled data. This practice is widely\nregarded as secure, even in cases where some annotated errors occur, as the\nimpact of these minor inaccuracies on the final performance of the models is\nnegligible and existing backdoor attacks require attacker's ability to poison\nthe training images. Nevertheless, in this paper, we propose clean-image\nbackdoor attacks which uncover that backdoors can still be injected via a\nfraction of incorrect labels without modifying the training images.\nSpecifically, in our attacks, the attacker first seeks a trigger feature to\ndivide the training images into two parts: those with the feature and those\nwithout it. Subsequently, the attacker falsifies the labels of the former part\nto a backdoor class. The backdoor will be finally implanted into the target\nmodel after it is trained on the poisoned data. During the inference phase, the\nattacker can activate the backdoor in two ways: slightly modifying the input\nimage to obtain the trigger feature, or taking an image that naturally has the\ntrigger feature as input. We conduct extensive experiments to demonstrate the\neffectiveness and practicality of our attacks. According to the experimental\nresults, we conclude that our attacks seriously jeopardize the fairness and\nrobustness of image classification models, and it is necessary to be vigilant\nabout the incorrect labels in outsourced labeling.\n","authors":["Dazhong Rong","Guoyao Yu","Shuheng Shen","Xinyi Fu","Peng Qian","Jianhai Chen","Qinming He","Xing Fu","Weiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06683v2","updated":"2024-03-26T12:10:13Z","published":"2024-03-11T12:57:51Z","title":"Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency","summary":"  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n","authors":["Charlie Budd","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17633v1","updated":"2024-03-26T12:08:14Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17631v1","updated":"2024-03-26T12:08:04Z","published":"2024-03-26T12:08:04Z","title":"AniArtAvatar: Animatable 3D Art Avatar from a Single Image","summary":"  We present a novel approach for generating animatable 3D-aware art avatars\nfrom a single image, with controllable facial expressions, head poses, and\nshoulder movements. Unlike previous reenactment methods, our approach utilizes\na view-conditioned 2D diffusion model to synthesize multi-view images from a\nsingle art portrait with a neutral expression. With the generated colors and\nnormals, we synthesize a static avatar using an SDF-based neural surface. For\navatar animation, we extract control points, transfer the motion with these\npoints, and deform the implicit canonical space. Firstly, we render the front\nimage of the avatar, extract the 2D landmarks, and project them to the 3D space\nusing a trained SDF network. We extract 3D driving landmarks using 3DMM and\ntransfer the motion to the avatar landmarks. To animate the avatar pose, we\nmanually set the body height and bound the head and torso of an avatar with two\ncages. The head and torso can be animated by transforming the two cages. Our\napproach is a one-shot pipeline that can be applied to various styles.\nExperiments demonstrate that our method can generate high-quality 3D art\navatars with desired control over different motions.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.17631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01598v3","updated":"2024-03-26T11:54:40Z","published":"2023-06-02T15:09:19Z","title":"Towards Source-free Domain Adaptive Semantic Segmentation via\n  Importance-aware and Prototype-contrast Learning","summary":"  Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.\n","authors":["Yihong Cao","Hui Zhang","Xiao Lu","Zheng Xiao","Kailun Yang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01598v3.pdf","comment":"Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\n  source code is publicly available at\n  https://github.com/yihong-97/Source-free-IAPC"},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17615v1","updated":"2024-03-26T11:48:37Z","published":"2024-03-26T11:48:37Z","title":"Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles\n  from 3D Cell Painting Images","summary":"  Despite their black-box nature, deep learning models are extensively used in\nimage-based drug discovery to extract feature vectors from single cells in\nmicroscopy images. To better understand how these networks perform\nrepresentation learning, we employ visual explainability techniques (e.g.,\nGrad-CAM). Our analyses reveal several mechanisms by which supervised models\ncheat, exploiting biologically irrelevant pixels when extracting morphological\nfeatures from images, such as noise in the background. This raises doubts\nregarding the fidelity of learned single-cell representations and their\nrelevance when investigating downstream biological questions. To address this\nmisalignment between researcher expectations and machine behavior, we introduce\nGrad-CAMO, a novel single-cell interpretability score for supervised feature\nextractors. Grad-CAMO measures the proportion of a model's attention that is\nconcentrated on the cell of interest versus the background. This metric can be\nassessed per-cell or averaged across a validation set, offering a tool to audit\nindividual features vectors or guide the improved design of deep learning\narchitectures. Importantly, Grad-CAMO seamlessly integrates into existing\nworkflows, requiring no dataset or model modifications, and is compatible with\nboth 2D and 3D Cell Painting data. Additional results are available at\nhttps://github.com/eigenvivek/Grad-CAMO.\n","authors":["Vivek Gopalakrishnan","Jingzhe Ma","Zhiyong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.17615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17610v1","updated":"2024-03-26T11:43:05Z","published":"2024-03-26T11:43:05Z","title":"MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","summary":"  Foot contact is an important cue not only for human motion capture but also\nfor motion understanding and physically plausible motion generation. However,\nmost of the foot-contact annotations in existing datasets are estimated by\npurely visual matching and distance thresholding, which results in low accuracy\nand coarse granularity. Even though existing multimodal datasets\nsynergistically capture plantar pressure (foot contact) and visual signals,\nthey are specifically designed for small-range and slow motion such as Taiji\nQuan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal\ndataset with large-range and fast human motion, as well as accurate and dense\nfoot-contact annotation. To fill this gap, we propose a Multimodal MoCap\nDataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate\nand dense plantar pressure signals synchronized with RGBD observations, which\nis especially useful for both plausible shape estimation, robust pose fitting\nwithout foot drifting, and accurate global translation tracking. To validate\nthe dataset, we propose an RGBD-P SMPL fitting method and also a\nmonocular-video-based baseline framework, VP-MoCap, for human motion capture.\nExperiments demonstrate that our RGBD-P SMPL Fitting results significantly\noutperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA\nmethods in foot-contact and global translation estimation accuracy. We believe\nthe configuration of the dataset and the baseline frameworks will stimulate the\nresearch in this direction and also provide a good reference for MoCap\napplications in various domains. Project page:\nhttps://haolyuan.github.io/MMVP-Dataset/.\n","authors":["He Zhang","Shenghao Ren","Haolei Yuan","Jianhui Zhao","Fan Li","Shuangpeng Sun","Zhenghao Liang","Tao Yu","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17610v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04701v3","updated":"2024-03-26T11:26:17Z","published":"2024-03-07T17:48:48Z","title":"ObjectCompose: Evaluating Resilience of Vision-Based Models on\n  Object-to-Background Compositional Changes","summary":"  Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiment to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.git\n","authors":["Hashmat Shadab Malik","Muhammad Huzaifa","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15905v2","updated":"2024-03-26T11:11:49Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v2.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2403.07576v2","updated":"2024-03-26T10:55:51Z","published":"2024-03-12T12:05:43Z","title":"FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine\n  Tuning in High-resolution Medical Image Classification","summary":"  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to\ntransfer pre-trained models to downstream tasks, avoiding the high cost of\nupdating entire large-scale pre-trained models (LPMs). In this work, we present\nFine-grained Prompt Tuning (FPT), a novel PEFT method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPEFT methods, especially in high-resolution contexts. To achieve this, we first\nfreeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2311.13385v3","updated":"2024-03-26T10:21:46Z","published":"2023-11-22T13:27:36Z","title":"SegVol: Universal and Interactive Volumetric Medical Image Segmentation","summary":"  Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. Extensive experiments on 10 internal validation tasks and 18 external\nvalidation tasks verify that SegVol outperforms the state of the art by a large\nmargin. Through its capacity to provide precise volumetric segmentation across\nvarious anatomical categories, SegVol has the potential to accelerate\nadvancements in medical imaging diagnosis and facilitate treatment\noptimization. The model and code are publicly available at:\nhttps://github.com/BAAI-DCAI/SegVol.\n","authors":["Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.13385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03611v2","updated":"2024-03-26T10:13:11Z","published":"2023-12-06T16:55:53Z","title":"DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions","summary":"  Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.\n","authors":["Yunhan Yang","Yukun Huang","Xiaoyang Wu","Yuan-Chen Guo","Song-Hai Zhang","Hengshuang Zhao","Tong He","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03611v2.pdf","comment":"Project Page: https://yhyang-myron.github.io/DreamComposer/"},{"id":"http://arxiv.org/abs/2312.08879v2","updated":"2024-03-26T10:04:11Z","published":"2023-12-12T11:00:39Z","title":"Regularizing Self-supervised 3D Scene Flows with Surface Awareness and\n  Cyclic Consistency","summary":"  Learning without supervision how to predict 3D scene flows from point clouds\nis essential to many perception systems. We propose a novel learning framework\nfor this task which improves the necessary regularization. Relying on the\nassumption that scene elements are mostly rigid, current smoothness losses are\nbuilt on the definition of ``rigid clusters\" in the input point clouds. The\ndefinition of these clusters is challenging and has a significant impact on the\nquality of predicted flows. We introduce two new consistency losses that\nenlarge clusters while preventing them from spreading over distinct objects. In\nparticular, we enforce \\emph{temporal} consistency with a forward-backward\ncyclic loss and \\emph{spatial} consistency by considering surface orientation\nsimilarity in addition to spatial proximity. The proposed losses are\nmodel-independent and can thus be used in a plug-and-play fashion to\nsignificantly improve the performance of existing models, as demonstrated on\ntwo most widely used architectures. We also showcase the effectiveness and\ngeneralization capability of our framework on four standard sensor-unique\ndriving datasets, achieving state-of-the-art performance in 3D scene flow\nestimation. Our codes are available on https://github.com/ctu-vras/sac-flow.\n","authors":["Patrik Vacek","David Hurych","Karel Zimmermann","Patrick Perez","Tomas Svoboda"],"pdf_url":"https://arxiv.org/pdf/2312.08879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.17549v1","updated":"2024-03-26T09:55:49Z","published":"2024-03-26T09:55:49Z","title":"Practical Applications of Advanced Cloud Services and Generative AI\n  Systems in Medical Image Analysis","summary":"  The medical field is one of the important fields in the application of\nartificial intelligence technology. With the explosive growth and\ndiversification of medical data, as well as the continuous improvement of\nmedical needs and challenges, artificial intelligence technology is playing an\nincreasingly important role in the medical field. Artificial intelligence\ntechnologies represented by computer vision, natural language processing, and\nmachine learning have been widely penetrated into diverse scenarios such as\nmedical imaging, health management, medical information, and drug research and\ndevelopment, and have become an important driving force for improving the level\nand quality of medical services.The article explores the transformative\npotential of generative AI in medical imaging, emphasizing its ability to\ngenerate syntheticACM-2 data, enhance images, aid in anomaly detection, and\nfacilitate image-to-image translation. Despite challenges like model\ncomplexity, the applications of generative models in healthcare, including\nMed-PaLM 2 technology, show promising results. By addressing limitations in\ndataset size and diversity, these models contribute to more accurate diagnoses\nand improved patient outcomes. However, ethical considerations and\ncollaboration among stakeholders are essential for responsible implementation.\nThrough experiments leveraging GANs to augment brain tumor MRI datasets, the\nstudy demonstrates how generative AI can enhance image quality and diversity,\nultimately advancing medical diagnostics and patient care.\n","authors":["Jingyu Xu","Binbin Wu","Jiaxin Huang","Yulu Gong","Yifan Zhang","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17545v1","updated":"2024-03-26T09:49:35Z","published":"2024-03-26T09:49:35Z","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions","summary":"  Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.\n","authors":["Shun Inadumi","Seiya Kawano","Akishige Yuguchi","Yasutomo Kawanishi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.17545v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17541v1","updated":"2024-03-26T09:44:34Z","published":"2024-03-26T09:44:34Z","title":"WordRobe: Text-Guided Generation of Textured 3D Garments","summary":"  In this paper, we tackle a new and challenging problem of text-driven\ngeneration of 3D garments with high-quality textures. We propose \"WordRobe\", a\nnovel framework for the generation of unposed & textured 3D garment meshes from\nuser-friendly text prompts. We achieve this by first learning a latent\nrepresentation of 3D garments using a novel coarse-to-fine training strategy\nand a loss for latent disentanglement, promoting better latent interpolation.\nSubsequently, we align the garment latent space to the CLIP embedding space in\na weakly supervised manner, enabling text-driven 3D garment generation and\nediting. For appearance modeling, we leverage the zero-shot generation\ncapability of ControlNet to synthesize view-consistent texture maps in a single\nfeed-forward inference step, thereby drastically decreasing the generation time\nas compared to existing methods. We demonstrate superior performance over\ncurrent SOTAs for learning 3D garment latent space, garment interpolation, and\ntext-driven texture synthesis, supported by quantitative evaluation and\nqualitative user study. The unposed 3D garment meshes generated using WordRobe\ncan be directly fed to standard cloth simulation & animation pipelines without\nany post-processing.\n","authors":["Astitva Srivastava","Pranav Manu","Amit Raj","Varun Jampani","Avinash Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.17541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17537v1","updated":"2024-03-26T09:42:28Z","published":"2024-03-26T09:42:28Z","title":"NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using\n  Heuristics-Guided Segmentation","summary":"  Neural Radiance Field (NeRF) has been widely recognized for its excellence in\nnovel view synthesis and 3D scene reconstruction. However, their effectiveness\nis inherently tied to the assumption of static scenes, rendering them\nsusceptible to undesirable artifacts when confronted with transient distractors\nsuch as moving objects or shadows. In this work, we propose a novel paradigm,\nnamely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances\nthe separation of static scenes from transient distractors by harmoniously\ncombining the strengths of hand-crafted heuristics and state-of-the-art\nsegmentation models, thus significantly transcending the limitations of\nprevious solutions. Furthermore, we delve into the meticulous design of\nheuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based\nheuristics and color residual heuristics, catering to a diverse range of\ntexture profiles. Extensive experiments demonstrate the superiority and\nrobustness of our method in mitigating transient distractors for NeRFs trained\nin non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.\n","authors":["Jiahao Chen","Yipeng Qin","Lingjie Liu","Jiangbo Lu","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17537v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2403.17530v1","updated":"2024-03-26T09:36:20Z","published":"2024-03-26T09:36:20Z","title":"Boosting Few-Shot Learning with Disentangled Self-Supervised Learning\n  and Meta-Learning for Medical Image Classification","summary":"  Background and objective: Employing deep learning models in critical domains\nsuch as medical imaging poses challenges associated with the limited\navailability of training data. We present a strategy for improving the\nperformance and generalization capabilities of models trained in low-data\nregimes. Methods: The proposed method starts with a pre-training phase, where\nfeatures learned in a self-supervised learning setting are disentangled to\nimprove the robustness of the representations for downstream tasks. We then\nintroduce a meta-fine-tuning step, leveraging related classes between\nmeta-training and meta-testing phases but varying the granularity level. This\napproach aims to enhance the model's generalization capabilities by exposing it\nto more challenging classification tasks during meta-training and evaluating it\non easier tasks but holding greater clinical relevance during meta-testing. We\ndemonstrate the effectiveness of the proposed approach through a series of\nexperiments exploring several backbones, as well as diverse pre-training and\nfine-tuning schemes, on two distinct medical tasks, i.e., classification of\nprostate cancer aggressiveness from MRI data and classification of breast\ncancer malignity from microscopic images. Results: Our results indicate that\nthe proposed approach consistently yields superior performance w.r.t. ablation\nexperiments, maintaining competitiveness even when a distribution shift between\ntraining and evaluation data occurs. Conclusion: Extensive experiments\ndemonstrate the effectiveness and wide applicability of the proposed approach.\nWe hope that this work will add another solution to the arsenal of addressing\nlearning issues in data-scarce imaging domains.\n","authors":["Eva Pachetti","Sotirios A. Tsaftaris","Sara Colantonio"],"pdf_url":"https://arxiv.org/pdf/2403.17530v1.pdf","comment":"20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024"},{"id":"http://arxiv.org/abs/2207.12730v2","updated":"2024-03-26T09:35:03Z","published":"2022-07-26T08:34:17Z","title":"P2ANet: A Dataset and Benchmark for Dense Action Detection from Table\n  Tennis Match Broadcasting Videos","summary":"  While deep learning has been widely used for video analytics, such as video\nclassification and action detection, dense action detection with fast-moving\nsubjects from sports videos is still challenging. In this work, we release yet\nanother sports video benchmark \\TheName{} for \\emph{\\underline{P}}ing\n\\emph{\\underline{P}}ong-\\emph{\\underline{A}}ction detection, which consists of\n2,721 video clips collected from the broadcasting videos of professional table\ntennis matches in World Table Tennis Championships and Olympiads. We work with\na crew of table tennis professionals and referees on a specially designed\nannotation toolbox to obtain fine-grained action labels (in 14 classes) for\nevery ping-pong action that appeared in the dataset, and formulate two sets of\naction detection problems -- \\emph{action localization} and \\emph{action\nrecognition}. We evaluate a number of commonly-seen action recognition (e.g.,\nTSM, TSN, Video SwinTransformer, and Slowfast) and action localization models\n(e.g., BSN, BSN++, BMN, TCANet), using \\TheName{} for both problems, under\nvarious settings. These models can only achieve 48\\% area under the AR-AN curve\nfor localization and 82\\% top-one accuracy for recognition since the ping-pong\nactions are dense with fast-moving subjects but broadcasting videos are with\nonly 25 FPS. The results confirm that \\TheName{} is still a challenging task\nand can be used as a special benchmark for dense action detection from videos.\n","authors":["Jiang Bian","Xuhong Li","Tao Wang","Qingzhong Wang","Jun Huang","Chen Liu","Jun Zhao","Feixiang Lu","Dejing Dou","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2207.12730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12378v2","updated":"2024-03-26T09:31:28Z","published":"2023-09-21T11:47:01Z","title":"Unsupervised Semantic Segmentation Through Depth-Guided Feature\n  Correlation and Sampling","summary":"  Traditionally, training neural networks to perform semantic segmentation\nrequired expensive human-made annotations. But more recently, advances in the\nfield of unsupervised learning have made significant progress on this issue and\ntowards closing the gap to supervised algorithms. To achieve this, semantic\nknowledge is distilled by learning to correlate randomly sampled features from\nimages across an entire dataset. In this work, we build upon these advances by\nincorporating information about the structure of the scene into the training\nprocess through the use of depth information. We achieve this by (1) learning\ndepth-feature correlation by spatially correlate the feature maps with the\ndepth maps to induce knowledge about the structure of the scene and (2)\nimplementing farthest-point sampling to more effectively select relevant\nfeatures by utilizing 3D sampling techniques on depth information of the scene.\nFinally, we demonstrate the effectiveness of our technical contributions\nthrough extensive experimentation and present significant improvements in\nperformance across multiple benchmark datasets.\n","authors":["Leon Sick","Dominik Engel","Pedro Hermosilla","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2309.12378v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17525v1","updated":"2024-03-26T09:26:12Z","published":"2024-03-26T09:26:12Z","title":"Equipping Sketch Patches with Context-Aware Positional Encoding for\n  Graphic Sketch Representation","summary":"  The drawing order of a sketch records how it is created stroke-by-stroke by a\nhuman being. For graphic sketch representation learning, recent studies have\ninjected sketch drawing orders into graph edge construction by linking each\npatch to another in accordance to a temporal-based nearest neighboring\nstrategy. However, such constructed graph edges may be unreliable, since a\nsketch could have variants of drawings. In this paper, we propose a\nvariant-drawing-protected method by equipping sketch patches with context-aware\npositional encoding (PE) to make better use of drawing orders for learning\ngraphic sketch representation. Instead of injecting sketch drawings into graph\nedges, we embed these sequential information into graph nodes only. More\nspecifically, each patch embedding is equipped with a sinusoidal absolute PE to\nhighlight the sequential position in the drawing order. And its neighboring\npatches, ranked by the values of self-attention scores between patch\nembeddings, are equipped with learnable relative PEs to restore the contextual\npositions within a neighborhood. During message aggregation via graph\nconvolutional networks, a node receives both semantic contents from patch\nembeddings and contextual patterns from PEs by its neighbors, arriving at\ndrawing-order-enhanced sketch representations. Experimental results indicate\nthat our method significantly improves sketch healing and controllable sketch\nsynthesis.\n","authors":["Sicong Zang","Zhijun Fang"],"pdf_url":"https://arxiv.org/pdf/2403.17525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17520v1","updated":"2024-03-26T09:22:37Z","published":"2024-03-26T09:22:37Z","title":"Boosting Adversarial Training via Fisher-Rao Norm-based Regularization","summary":"  Adversarial training is extensively utilized to improve the adversarial\nrobustness of deep neural networks. Yet, mitigating the degradation of standard\ngeneralization performance in adversarial-trained models remains an open\nproblem. This paper attempts to resolve this issue through the lens of model\ncomplexity. First, We leverage the Fisher-Rao norm, a geometrically invariant\nmetric for model complexity, to establish the non-trivial bounds of the\nCross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer\nPerceptron. Then we generalize a complexity-related variable, which is\nsensitive to the changes in model width and the trade-off factors in\nadversarial training. Moreover, intensive empirical evidence validates that\nthis variable highly correlates with the generalization gap of Cross-Entropy\nloss between adversarial-trained and standard-trained models, especially during\nthe initial and final phases of the training process. Building upon this\nobservation, we propose a novel regularization framework, called Logit-Oriented\nAdversarial Training (LOAT), which can mitigate the trade-off between\nrobustness and accuracy while imposing only a negligible increase in\ncomputational overhead. Our extensive experiments demonstrate that the proposed\nregularization strategy can boost the performance of the prevalent adversarial\ntraining algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,\nacross various network architectures. Our code will be available at\nhttps://github.com/TrustAI/LOAT.\n","authors":["Xiangyu Yin","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2403.17520v1.pdf","comment":"This paper has been accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2312.04529v2","updated":"2024-03-26T09:21:29Z","published":"2023-12-07T18:50:00Z","title":"Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of\n  Illumination and Reflectance","summary":"  Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the attenuated frequency spectrum of an illumination\njointly with the reflectance of an object of known geometry from a single\nimage. Our key idea is to solve this blind inverse problem in the reflectance\nmap, an appearance representation invariant to the underlying geometry, by\nlearning to reverse the image formation with a novel diffusion model which we\nrefer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed\nreflectance map converted and completed from the single input image, DRMNet\ngenerates a reflectance map corresponding to a perfect mirror sphere while\njointly estimating the reflectance. The forward process can be understood as\ngradually filtering a natural illumination with lower and lower frequency\nreflectance and additive Gaussian noise. DRMNet learns to invert this process\nwith two subnetworks, IllNet and RefNet, which work in concert towards this\njoint estimation. The network is trained on an extensive synthetic dataset and\nis demonstrated to generalize to real images, showing state-of-the-art accuracy\non established datasets.\n","authors":["Yuto Enyo","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04529v2.pdf","comment":"to be published in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17512v1","updated":"2024-03-26T09:13:06Z","published":"2024-03-26T09:13:06Z","title":"Random-coupled Neural Network","summary":"  Improving the efficiency of current neural networks and modeling them in\nbiological neural systems have become popular research directions in recent\nyears. Pulse-coupled neural network (PCNN) is a well applicated model for\nimitating the computation characteristics of the human brain in computer vision\nand neural network fields. However, differences between the PCNN and biological\nneural systems remain: limited neural connection, high computational cost, and\nlack of stochastic property. In this study, random-coupled neural network\n(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic\ncomputing via a random inactivation process. This process randomly closes some\nneural connections in the RCNN model, realized by the random inactivation\nweight matrix of link input. This releases the computational burden of PCNN,\nmaking it affordable to achieve vast neural connections. Furthermore, the image\nand video processing mechanisms of RCNN are researched. It encodes constant\nstimuli as periodic spike trains and periodic stimuli as chaotic spike trains,\nthe same as biological neural information encoding characteristics. Finally,\nthe RCNN is applicated to image segmentation, fusion, and pulse shape\ndiscrimination subtasks. It is demonstrated to be robust, efficient, and highly\nanti-noised, with outstanding performance in all applications mentioned above.\n","authors":["Haoran Liu","Mingzhe Liu","Peng Li","Jiahui Wu","Xin Jiang","Zhuo Zuo","Bingqi Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13660v2","updated":"2024-03-26T09:09:15Z","published":"2024-03-20T15:08:57Z","title":"ProMamba: Prompt-Mamba for polyp segmentation","summary":"  Detecting polyps through colonoscopy is an important task in medical image\nsegmentation, which provides significant assistance and reference value for\nclinical surgery. However, accurate segmentation of polyps is a challenging\ntask due to two main reasons. Firstly, polyps exhibit various shapes and\ncolors. Secondly, the boundaries between polyps and their normal surroundings\nare often unclear. Additionally, significant differences between different\ndatasets lead to limited generalization capabilities of existing methods. To\naddress these issues, we propose a segmentation model based on Prompt-Mamba,\nwhich incorporates the latest Vision-Mamba and prompt technologies. Compared to\nprevious models trained on the same dataset, our model not only maintains high\nsegmentation accuracy on the validation part of the same dataset but also\ndemonstrates superior accuracy on unseen datasets, exhibiting excellent\ngeneralization capabilities. Notably, we are the first to apply the\nVision-Mamba architecture to polyp segmentation and the first to utilize prompt\ntechnology in a polyp segmentation model. Our model efficiently accomplishes\nsegmentation tasks, surpassing previous state-of-the-art methods by an average\nof 5% across six datasets. Furthermore, we have developed multiple versions of\nour model with scaled parameter counts, achieving better performance than\nprevious models even with fewer parameters. Our code and trained weights will\nbe released soon.\n","authors":["Jianhao Xie","Ruofan Liao","Ziang Zhang","Sida Yi","Yuesheng Zhu","Guibo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13660v2.pdf","comment":"10 pages, 2 figures,3 tabels"},{"id":"http://arxiv.org/abs/2403.17503v1","updated":"2024-03-26T09:04:18Z","published":"2024-03-26T09:04:18Z","title":"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) under an exemplar-free constraint has\npresented a significant challenge. Existing methods adhering to this constraint\nare prone to catastrophic forgetting, far more so than replay-based techniques\nthat retain access to past samples. In this paper, to solve the exemplar-free\nCIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The\nDS-AL contains a main stream offering an analytical (i.e., closed-form) linear\nsolution, and a compensation stream improving the inherent under-fitting\nlimitation due to adopting linear mapping. The main stream redefines the CIL\nproblem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an\nequivalence between the CIL and its joint-learning counterpart. The\ncompensation stream is governed by a Dual-Activation Compensation (DAC) module.\nThis module re-activates the embedding with a different activation function\nfrom the main stream one, and seeks fitting compensation by projecting the\nembedding to the null space of the main stream's linear mapping. Empirical\nresults demonstrate that the DS-AL, despite being an exemplar-free technique,\ndelivers performance comparable with or better than that of replay-based\nmethods across various datasets, including CIFAR-100, ImageNet-100 and\nImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to\nexecute CIL in a phase-invariant manner. This is evidenced by a\nnever-before-seen 500-phase CIL ImageNet task, which performs on a level\nidentical to a 5-phase one. Our codes are available at\nhttps://github.com/ZHUANGHP/Analytic-continual-learning.\n","authors":["Huiping Zhuang","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17503v1.pdf","comment":"Accepted in AAAI 2024"},{"id":"http://arxiv.org/abs/2403.17502v1","updated":"2024-03-26T09:03:40Z","published":"2024-03-26T09:03:40Z","title":"SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational\n  Autoencoder","summary":"  The data bottleneck has emerged as a fundamental challenge in learning based\nimage restoration methods. Researchers have attempted to generate synthesized\ntraining data using paired or unpaired samples to address this challenge. This\nstudy proposes SeNM-VAE, a semi-supervised noise modeling method that leverages\nboth paired and unpaired datasets to generate realistic degraded data. Our\napproach is based on modeling the conditional distribution of degraded and\nclean images with a specially designed graphical model. Under the variational\ninference framework, we develop an objective function for handling both paired\nand unpaired data. We employ our method to generate paired training samples for\nreal-world image denoising and super-resolution tasks. Our approach excels in\nthe quality of synthetic degraded images compared to other unpaired and paired\nnoise modeling methods. Furthermore, our approach demonstrates remarkable\nperformance in downstream image restoration tasks, even with limited paired\ndata. With more paired data, our method achieves the best performance on the\nSIDD dataset.\n","authors":["Dihan Zheng","Yihang Zou","Xiaowen Zhang","Chenglong Bao"],"pdf_url":"https://arxiv.org/pdf/2403.17502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17497v1","updated":"2024-03-26T08:58:28Z","published":"2024-03-26T08:58:28Z","title":"Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies","summary":"  In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.\n","authors":["Philipp Sadler","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2403.17497v1.pdf","comment":"9 pages, Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.05370v2","updated":"2024-03-26T08:54:49Z","published":"2023-10-09T02:59:21Z","title":"SocialCircle: Learning the Angle-based Social Interaction Representation\n  for Pedestrian Trajectory Prediction","summary":"  Analyzing and forecasting trajectories of agents like pedestrians and cars in\ncomplex scenes has become more and more significant in many intelligent systems\nand applications. The diversity and uncertainty in socially interactive\nbehaviors among a rich variety of agents make this task more challenging than\nother deterministic computer vision tasks. Researchers have made a lot of\nefforts to quantify the effects of these interactions on future trajectories\nthrough different mathematical models and network structures, but this problem\nhas not been well solved. Inspired by marine animals that localize the\npositions of their companions underwater through echoes, we build a new\nanglebased trainable social interaction representation, named SocialCircle, for\ncontinuously reflecting the context of social interactions at different angular\norientations relative to the target agent. We validate the effect of the\nproposed SocialCircle by training it along with several newly released\ntrajectory prediction models, and experiments show that the SocialCircle not\nonly quantitatively improves the prediction performance, but also qualitatively\nhelps better simulate social interactions when forecasting pedestrian\ntrajectories in a way that is consistent with human intuitions.\n","authors":["Conghao Wong","Beihao Xia","Ziqian Zou","Yulong Wang","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2310.05370v2.pdf","comment":"CVPR 2024 accepted"},{"id":"http://arxiv.org/abs/2403.17496v1","updated":"2024-03-26T08:53:25Z","published":"2024-03-26T08:53:25Z","title":"Dr.Hair: Reconstructing Scalp-Connected Hair Strands without\n  Pre-training via Differentiable Rendering of Line Segments","summary":"  In the film and gaming industries, achieving a realistic hair appearance\ntypically involves the use of strands originating from the scalp. However,\nreconstructing these strands from observed surface images of hair presents\nsignificant challenges. The difficulty in acquiring Ground Truth (GT) data has\nled state-of-the-art learning-based methods to rely on pre-training with\nmanually prepared synthetic CG data. This process is not only labor-intensive\nand costly but also introduces complications due to the domain gap when\ncompared to real-world data. In this study, we propose an optimization-based\napproach that eliminates the need for pre-training. Our method represents hair\nstrands as line segments growing from the scalp and optimizes them using a\nnovel differentiable rendering algorithm. To robustly optimize a substantial\nnumber of slender explicit geometries, we introduce 3D orientation estimation\nutilizing global optimization, strand initialization based on Laplace's\nequation, and reparameterization that leverages geometric connectivity and\nspatial proximity. Unlike existing optimization-based methods, our method is\ncapable of reconstructing internal hair flow in an absolute direction. Our\nmethod exhibits robust and accurate inverse rendering, surpassing the quality\nof existing methods and significantly improving processing speed.\n","authors":["Yusuke Takimoto","Hikari Takehara","Hiroyuki Sato","Zihao Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17496v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13039v2","updated":"2024-03-26T08:52:05Z","published":"2024-03-19T16:21:47Z","title":"Emotic Masked Autoencoder with Attention Fusion for Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) is a critical task within computer vision\nwith diverse applications across various domains. Addressing the challenge of\nlimited FER datasets, which hampers the generalization capability of expression\nrecognition models, is imperative for enhancing performance. Our paper presents\nan innovative approach integrating the MAE-Face self-supervised learning (SSL)\nmethod and Fusion Attention mechanism for expression classification,\nparticularly showcased in the 6th Affective Behavior 32 pages harvmac; added\nreferences for section 5Analysis in-the-wild (ABAW) competition. Additionally,\nwe propose preprocessing techniques to emphasize essential facial features,\nthereby enhancing model performance on both training and validation sets,\nnotably demonstrated on the Aff-wild2 dataset.\n","authors":["Bach Nguyen-Xuan","Thien Nguyen-Hoang","Nhu Tai-Do"],"pdf_url":"https://arxiv.org/pdf/2403.13039v2.pdf","comment":"6 pages; added references for section 1; corrected typo for email\n  author"},{"id":"http://arxiv.org/abs/2403.13653v2","updated":"2024-03-26T08:45:09Z","published":"2024-03-20T14:58:40Z","title":"Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction","summary":"  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n","authors":["Florian Strohm","Mihai Bâce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17726v2","updated":"2024-03-26T08:38:52Z","published":"2024-02-27T17:58:09Z","title":"VRP-SAM: SAM with Visual Reference Prompt","summary":"  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that\nempowers the Segment Anything Model (SAM) to utilize annotated reference images\nas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM\ncan utilize annotated reference images to comprehend specific objects and\nperform segmentation of specific objects in target image. It is note that the\nVRP encoder can support a variety of annotation formats for reference images,\nincluding \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.\nVRP-SAM achieves a breakthrough within the SAM framework by extending its\nversatility and applicability while preserving SAM's inherent strengths, thus\nenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,\nthe VRP encoder adopts a meta-learning strategy. To validate the effectiveness\nof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO\ndatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual\nreference segmentation with minimal learnable parameters. Furthermore, VRP-SAM\ndemonstrates strong generalization capabilities, allowing it to perform\nsegmentation of unseen objects and enabling cross-domain segmentation. The\nsource code and models will be available at\n\\url{https://github.com/syp2ysy/VRP-SAM}\n","authors":["Yanpeng Sun","Jiahui Chen","Shan Zhang","Xinyu Zhang","Qiang Chen","Gang Zhang","Errui Ding","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2402.17726v2.pdf","comment":"Accepted by CVPR 2024; The camera-ready version"},{"id":"http://arxiv.org/abs/2403.13972v2","updated":"2024-03-26T08:34:16Z","published":"2024-03-20T20:47:53Z","title":"SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing","summary":"  We propose Semantic Facial Feature Control (SeFFeC) - a novel method for\nfine-grained face shape editing. Our method enables the manipulation of\nhuman-understandable, semantic face features, such as nose length or mouth\nwidth, which are defined by different groups of facial landmarks. In contrast\nto existing methods, the use of facial landmarks enables precise measurement of\nthe facial features, which then enables training SeFFeC without any manually\nannotated labels. SeFFeC consists of a transformer-based encoder network that\ntakes a latent vector of a pre-trained generative model and a facial feature\nembedding as input, and learns to modify the latent vector to perform the\ndesired face edit operation. To ensure that the desired feature measurement is\nchanged towards the target value without altering uncorrelated features, we\nintroduced a novel semantic face feature loss. Qualitative and quantitative\nresults show that SeFFeC enables precise and fine-grained control of 23 facial\nfeatures, some of which could not previously be controlled by other methods,\nwithout requiring manual annotations. Unlike existing methods, SeFFeC also\nprovides deterministic control over the exact values of the facial features and\nmore localised and disentangled face edits.\n","authors":["Florian Strohm","Mihai Bâce","Markus Kaltenecker","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17477v1","updated":"2024-03-26T08:13:02Z","published":"2024-03-26T08:13:02Z","title":"DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on\n  360° Images","summary":"  We present DiffGaze, a novel method for generating realistic and diverse\ncontinuous human gaze sequences on 360{\\deg} images based on a conditional\nscore-based denoising diffusion model. Generating human gaze on 360{\\deg}\nimages is important for various human-computer interaction and computer\ngraphics applications, e.g. for creating large-scale eye tracking datasets or\nfor realistic animation of virtual humans. However, existing methods are\nlimited to predicting discrete fixation sequences or aggregated saliency maps,\nthereby neglecting crucial parts of natural gaze behaviour. Our method uses\nfeatures extracted from 360{\\deg} images as condition and uses two transformers\nto model the temporal and spatial dependencies of continuous human gaze. We\nevaluate DiffGaze on two 360{\\deg} image benchmarks for gaze sequence\ngeneration as well as scanpath prediction and saliency prediction. Our\nevaluations show that DiffGaze outperforms state-of-the-art methods on all\ntasks on both benchmarks. We also report a 21-participant user study showing\nthat our method generates gaze sequences that are indistinguishable from real\nhuman sequences.\n","authors":["Chuhan Jiao","Yao Wang","Guanhua Zhang","Mihai Bâce","Zhiming Hu","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12036v3","updated":"2024-03-26T08:09:43Z","published":"2022-11-22T06:19:17Z","title":"Dual Prototype Attention for Unsupervised Video Object Segmentation","summary":"  Unsupervised video object segmentation (VOS) aims to detect and segment the\nmost salient object in videos. The primary techniques used in unsupervised VOS\nare 1) the collaboration of appearance and motion information; and 2) temporal\nfusion between different frames. This paper proposes two novel prototype-based\nattention mechanisms, inter-modality attention (IMA) and inter-frame attention\n(IFA), to incorporate these techniques via dense propagation across different\nmodalities and frames. IMA densely integrates context information from\ndifferent modalities based on a mutual refinement. IFA injects global context\nof a video to the query frame, enabling a full utilization of useful properties\nfrom multiple frames. Experimental results on public benchmark datasets\ndemonstrate that our proposed approach outperforms all existing methods by a\nsubstantial margin. The proposed two components are also thoroughly validated\nvia ablative study.\n","authors":["Suhwan Cho","Minhyeok Lee","Seunghoon Lee","Dogyoon Lee","Heeseung Choi","Ig-Jae Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2211.12036v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.09974v2","updated":"2024-03-26T08:04:00Z","published":"2023-11-16T15:47:49Z","title":"From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning","summary":"  In recent years, self-supervised contrastive learning has emerged as a\ndistinguished paradigm in the artificial intelligence landscape. It facilitates\nunsupervised feature learning through contrastive delineations at the instance\nlevel. However, crafting an effective self-supervised paradigm remains a\npivotal challenge within this field. This paper delves into two crucial factors\nimpacting self-supervised contrastive learning-bach size and pretext tasks, and\nfrom a data processing standpoint, proposes an adaptive technique of batch\nfusion. The proposed method, via dimensionality reduction and reconstruction of\nbatch data, enables formerly isolated individual data to partake in intra-batch\ncommunication through the Embedding Layer. Moreover, it adaptively amplifies\nthe self-supervised feature encoding capability as the training progresses. We\nconducted a linear classification test of this method based on the classic\ncontrastive learning framework on ImageNet-1k. The empirical findings\nillustrate that our approach achieves state-of-the-art performance under\nequitable comparisons. Benefiting from its \"plug-and-play\" characteristics, we\nfurther explored other contrastive learning methods. On the ImageNet-100,\ncompared to the original performance, the top1 has seen a maximum increase of\n1.25%. We suggest that the proposed method may contribute to the advancement of\ndata-driven self-supervised learning research, bringing a fresh perspective to\nthis community.\n","authors":["Jiansong Zhang","Linlin Shen","Peizhong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09974v2.pdf","comment":"14 pages, 2 figures, the code of this paper will be released soon"},{"id":"http://arxiv.org/abs/2311.16926v4","updated":"2024-03-26T07:55:24Z","published":"2023-11-28T16:31:27Z","title":"LLaFS: When Large Language Models Meet Few-Shot Segmentation","summary":"  This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks.\n","authors":["Lanyun Zhu","Tianrun Chen","Deyi Ji","Jieping Ye","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16926v4.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.17465v1","updated":"2024-03-26T07:55:16Z","published":"2024-03-26T07:55:16Z","title":"LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated\n  Image Detection","summary":"  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times.\n","authors":["Yunpeng Luo","Junlong Du","Ke Yan","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2403.17465v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17460v1","updated":"2024-03-26T07:48:49Z","published":"2024-03-26T07:48:49Z","title":"Building Bridges across Spatial and Temporal Resolutions:\n  Reference-Based Super-Resolution via Change Priors and Conditional Diffusion\n  Model","summary":"  Reference-based super-resolution (RefSR) has the potential to build bridges\nacross spatial and temporal resolutions of remote sensing images. However,\nexisting RefSR methods are limited by the faithfulness of content\nreconstruction and the effectiveness of texture transfer in large scaling\nfactors. Conditional diffusion models have opened up new opportunities for\ngenerating realistic high-resolution images, but effectively utilizing\nreference images within these models remains an area for further exploration.\nFurthermore, content fidelity is difficult to guarantee in areas without\nrelevant reference information. To solve these issues, we propose a\nchange-aware diffusion model named Ref-Diff for RefSR, using the land cover\nchange priors to guide the denoising process explicitly. Specifically, we\ninject the priors into the denoising model to improve the utilization of\nreference information in unchanged areas and regulate the reconstruction of\nsemantically relevant content in changed areas. With this powerful guidance, we\ndecouple the semantics-guided denoising and reference texture-guided denoising\nprocesses to improve the model performance. Extensive experiments demonstrate\nthe superior effectiveness and robustness of the proposed method compared with\nstate-of-the-art RefSR methods in both quantitative and qualitative\nevaluations. The code and data are available at\nhttps://github.com/dongrunmin/RefDiff.\n","authors":["Runmin Dong","Shuai Yuan","Bin Luo","Mengxuan Chen","Jinxiao Zhang","Lixian Zhang","Weijia Li","Juepeng Zheng","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2403.17460v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.14027v2","updated":"2024-03-26T07:47:20Z","published":"2024-03-20T22:52:34Z","title":"EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship\n  Detection through Edge-Cloud Collaboration","summary":"  Detecting marine objects inshore presents challenges owing to algorithmic\nintricacies and complexities in system deployment. We propose a\ndifficulty-aware edge-cloud collaborative sensing system that splits the task\ninto object localization and fine-grained classification. Objects are\nclassified either at the edge or within the cloud, based on their estimated\ndifficulty. The framework comprises a low-power device-tailored front-end model\nfor object localization, classification, and difficulty estimation, along with\na transformer-graph convolutional network-based back-end model for fine-grained\nclassification. Our system demonstrates superior performance (mAP@0.5 +4.3%})\non widely used marine object detection datasets, significantly reducing both\ndata transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the\nsystem level. We validate the proposed system across various embedded system\nplatforms and in real-world scenarios involving drone deployment.\n","authors":["Wenjun Huang","Hanning Chen","Yang Ni","Arghavan Rezvani","Sanggeon Yun","Sungheon Jeon","Eric Pedley","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2403.14027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03180v5","updated":"2024-03-26T07:44:45Z","published":"2021-06-06T17:01:13Z","title":"Vision Transformers with Hierarchical Attention","summary":"  This paper tackles the high computational/space complexity associated with\nMulti-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,\nwe propose Hierarchical MHSA (H-MHSA), a novel approach that computes\nself-attention in a hierarchical fashion. Specifically, we first divide the\ninput image into patches as commonly done, and each patch is viewed as a token.\nThen, the proposed H-MHSA learns token relationships within local patches,\nserving as local relationship modeling. Then, the small patches are merged into\nlarger ones, and H-MHSA models the global dependencies for the small number of\nthe merged tokens. At last, the local and global attentive features are\naggregated to obtain features with powerful representation capacity. Since we\nonly calculate attention for a limited number of tokens at each step, the\ncomputational load is reduced dramatically. Hence, H-MHSA can efficiently model\nglobal relationships among tokens without sacrificing fine-grained information.\nWith the H-MHSA module incorporated, we build a family of\nHierarchical-Attention-based Transformer Networks, namely HAT-Net. To\ndemonstrate the superiority of HAT-Net in scene understanding, we conduct\nextensive experiments on fundamental vision tasks, including image\nclassification, semantic segmentation, object detection, and instance\nsegmentation. Therefore, HAT-Net provides a new perspective for vision\ntransformers. Code and pretrained models are available at\nhttps://github.com/yun-liu/HAT-Net.\n","authors":["Yun Liu","Yu-Huan Wu","Guolei Sun","Le Zhang","Ajad Chhatkuli","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2106.03180v5.pdf","comment":"Machine Intelligence Research (MIR), DOI: 10.1007/s11633-024-1393-8"},{"id":"http://arxiv.org/abs/2308.07728v5","updated":"2024-03-26T07:43:08Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17447v1","updated":"2024-03-26T07:26:00Z","published":"2024-03-26T07:26:00Z","title":"Chain of Compression: A Systematic Approach to Combinationally Compress\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have achieved significant popularity,\nbut their computational and memory intensity poses challenges for\nresource-constrained computing systems, particularly with the prerequisite of\nreal-time performance. To release this burden, model compression has become an\nimportant research focus. Many approaches like quantization, pruning, early\nexit, and knowledge distillation have demonstrated the effect of reducing\nredundancy in neural networks. Upon closer examination, it becomes apparent\nthat each approach capitalizes on its unique features to compress the neural\nnetwork, and they can also exhibit complementary behavior when combined. To\nexplore the interactions and reap the benefits from the complementary features,\nwe propose the Chain of Compression, which works on the combinational sequence\nto apply these common techniques to compress the neural network. Validated on\nthe image-based regression and classification networks across different data\nsets, our proposed Chain of Compression can significantly compress the\ncomputation cost by 100-1000 times with ignorable accuracy loss compared with\nthe baseline model.\n","authors":["Yingtao Shen","Minqing Sun","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2403.17447v1.pdf","comment":"10 pages, 15 figures"},{"id":"http://arxiv.org/abs/2306.07632v3","updated":"2024-03-26T07:00:27Z","published":"2023-06-13T09:02:57Z","title":"NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated\n  Rendering","summary":"  This paper presents a method, namely NeuS-PIR, for recovering relightable\nneural surfaces using pre-integrated rendering from multi-view images or video.\nUnlike methods based on NeRF and discrete meshes, our method utilizes implicit\nneural surface representation to reconstruct high-quality geometry, which\nfacilitates the factorization of the radiance field into two components: a\nspatially varying material field and an all-frequency lighting representation.\nThis factorization, jointly optimized using an adapted differentiable\npre-integrated rendering framework with material encoding regularization, in\nturn addresses the ambiguity of geometry reconstruction and leads to better\ndisentanglement and refinement of each scene property. Additionally, we\nintroduced a method to distil indirect illumination fields from the learned\nrepresentations, further recovering the complex illumination effect like\ninter-reflection. Consequently, our method enables advanced applications such\nas relighting, which can be seamlessly integrated with modern graphics engines.\nQualitative and quantitative experiments have shown that NeuS-PIR outperforms\nexisting methods across various tasks on both synthetic and real datasets.\nSource code is available at https://github.com/Sheldonmao/NeuSPIR\n","authors":["Shi Mao","Chenming Wu","Zhelun Shen","Yifan Wang","Dayan Wu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17432v1","updated":"2024-03-26T06:57:50Z","published":"2024-03-26T06:57:50Z","title":"Integrating Mamba Sequence Model and Hierarchical Upsampling Network for\n  Accurate Semantic Segmentation of Multiple Sclerosis Legion","summary":"  Integrating components from convolutional neural networks and state space\nmodels in medical image segmentation presents a compelling approach to enhance\naccuracy and efficiency. We introduce Mamba HUNet, a novel architecture\ntailored for robust and efficient segmentation tasks. Leveraging strengths from\nMamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet),\nMamba HUNet combines convolutional neural networks local feature extraction\npower with state space models long range dependency modeling capabilities. We\nfirst converted HUNet into a lighter version, maintaining performance parity\nand then integrated this lighter HUNet into Mamba HUNet, further enhancing its\nefficiency. The architecture partitions input grayscale images into patches,\ntransforming them into 1D sequences for processing efficiency akin to Vision\nTransformers and Mamba models. Through Visual State Space blocks and patch\nmerging layers, hierarchical features are extracted while preserving spatial\ninformation. Experimental results on publicly available Magnetic Resonance\nImaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate\nMamba HUNet's effectiveness across diverse segmentation tasks. The model's\nrobustness and flexibility underscore its potential in handling complex\nanatomical structures. These findings establish Mamba HUNet as a promising\nsolution in advancing medical image segmentation, with implications for\nimproving clinical decision making processes.\n","authors":["Kazi Shahriar Sanjid","Md. Tanzim Hossain","Md. Shakib Shahariar Junayed","Dr. Mohammad Monir Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.17432v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.17423v1","updated":"2024-03-26T06:40:03Z","published":"2024-03-26T06:40:03Z","title":"Test-time Adaptation Meets Image Enhancement: Improving Accuracy via\n  Uncertainty-aware Logit Switching","summary":"  Deep neural networks have achieved remarkable success in a variety of\ncomputer vision applications. However, there is a problem of degrading accuracy\nwhen the data distribution shifts between training and testing. As a solution\nof this problem, Test-time Adaptation~(TTA) has been well studied because of\nits practicality. Although TTA methods increase accuracy under distribution\nshift by updating the model at test time, using high-uncertainty predictions is\nknown to degrade accuracy. Since the input image is the root of the\ndistribution shift, we incorporate a new perspective on enhancing the input\nimage into TTA methods to reduce the prediction's uncertainty. We hypothesize\nthat enhancing the input image reduces prediction's uncertainty and increase\nthe accuracy of TTA methods. On the basis of our hypothesis, we propose a novel\nmethod: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the\nclassification model is combined with the image enhancement model that\ntransforms input images into recognition-friendly ones, and these models are\nupdated by existing TTA methods. Furthermore, we found that the prediction from\nthe enhanced image does not always have lower uncertainty than the prediction\nfrom the original image. Thus, we propose logit switching, which compares the\nuncertainty measure of these predictions and outputs the lower one. In our\nexperiments, we evaluate TECA with various TTA methods and show that TECA\nreduces prediction's uncertainty and increases accuracy of TTA methods despite\nhaving no hyperparameters and little parameter overhead.\n","authors":["Shohei Enomoto","Naoya Hasegawa","Kazuki Adachi","Taku Sasaki","Shin'ya Yamaguchi","Satoshi Suzuki","Takeharu Eda"],"pdf_url":"https://arxiv.org/pdf/2403.17423v1.pdf","comment":"Accepted to IJCNN2024"},{"id":"http://arxiv.org/abs/2403.16169v2","updated":"2024-03-26T06:39:30Z","published":"2024-03-24T14:24:13Z","title":"Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method","summary":"  Gaze plays a crucial role in revealing human attention and intention,\nshedding light on the cognitive processes behind human actions. The integration\nof gaze guidance with the dynamics of hand-object interactions boosts the\naccuracy of human motion prediction. However, the lack of datasets that capture\nthe intricate relationship and consistency among gaze, hand, and object\nmovements remains a substantial hurdle. In this paper, we introduce the first\nGaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task\nfor synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,\nfeatures simultaneous 3D modeling of gaze, hand, and object interactions,\ncomprising 479 sequences with an average duration of 19.1 seconds, 812\nsub-sequences, and 33 objects of various sizes. We propose a hierarchical\nframework centered on a gaze-guided hand-object interaction diffusion model,\nnamed GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions\ninto spatial-temporal features and goal pose conditions at different levels of\ninformation granularity. During the diffusion phase, two gaze-conditioned\ndiffusion models are stacked to simplify the complex synthesis of hand-object\nmotions. Here, the object motion diffusion model generates sequences of object\nmotions based on gaze conditions, while the hand motion diffusion model\nproduces hand motions based on the generated object motion. To improve\nfine-grained goal pose alignment, we introduce a Spherical Gaussian constraint\nto guide the denoising step. In the subsequent post-diffusion phase, we\noptimize the generated hand motions using contact consistency. Our extensive\nexperiments highlight the uniqueness of our dataset and the effectiveness of\nour approach.\n","authors":["Jie Tian","Lingxiao Yang","Ran Ji","Yuexin Ma","Lan Xu","Jingyi Yu","Ye Shi","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17422v1","updated":"2024-03-26T06:35:55Z","published":"2024-03-26T06:35:55Z","title":"InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse\n  Diffusion","summary":"  We present InterHandGen, a novel framework that learns the generative prior\nof two-hand interaction. Sampling from our model yields plausible and diverse\ntwo-hand shapes in close interaction with or without an object. Our prior can\nbe incorporated into any optimization or learning methods to reduce ambiguity\nin an ill-posed setup. Our key observation is that directly modeling the joint\ndistribution of multiple instances imposes high learning complexity due to its\ncombinatorial nature. Thus, we propose to decompose the modeling of joint\ndistribution into the modeling of factored unconditional and conditional single\ninstance distribution. In particular, we introduce a diffusion model that\nlearns the single-hand distribution unconditional and conditional to another\nhand via conditioning dropout. For sampling, we combine anti-penetration and\nclassifier-free guidance to enable plausible generation. Furthermore, we\nestablish the rigorous evaluation protocol of two-hand synthesis, where our\nmethod significantly outperforms baseline generative models in terms of\nplausibility and diversity. We also demonstrate that our diffusion prior can\nboost the performance of two-hand reconstruction from monocular in-the-wild\nimages, achieving new state-of-the-art accuracy.\n","authors":["Jihyun Lee","Shunsuke Saito","Giljoo Nam","Minhyuk Sung","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17422v1.pdf","comment":"Accepted to CVPR 2024, project page:\n  https://jyunlee.github.io/projects/interhandgen/"},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.09551v2","updated":"2024-03-26T06:05:17Z","published":"2023-12-15T06:04:42Z","title":"Learning-based Axial Video Motion Magnification","summary":"  Video motion magnification amplifies invisible small motions to be\nperceptible, which provides humans with a spatially dense and holistic\nunderstanding of small motions in the scene of interest. This is based on the\npremise that magnifying small motions enhances the legibility of motions. In\nthe real world, however, vibrating objects often possess convoluted systems\nthat have complex natural frequencies, modes, and directions. Existing motion\nmagnification often fails to improve legibility since the intricate motions\nstill retain complex characteristics even after being magnified, which may\ndistract us from analyzing them. In this work, we focus on improving legibility\nby proposing a new concept, axial motion magnification, which magnifies\ndecomposed motions along the user-specified direction. Axial motion\nmagnification can be applied to various applications where motions of specific\naxes are critical, by providing simplified and easily readable motion\ninformation. To achieve this, we propose a novel Motion Separation Module that\nenables to disentangle and magnify the motion representation along axes of\ninterest. Furthermore, we build a new synthetic training dataset for the axial\nmotion magnification task. Our proposed method improves the legibility of\nresulting motions along certain axes by adding a new feature: user\ncontrollability. Axial motion magnification is a more generalized concept;\nthus, our method can be directly adapted to the generic motion magnification\nand achieves favorable performance against competing methods.\n","authors":["Kwon Byung-Ki","Oh Hyun-Bin","Kim Jun-Seong","Hyunwoo Ha","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2312.09551v2.pdf","comment":"main paper: 12 pages, supplementary: 10 pages, 20 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.17409v1","updated":"2024-03-26T06:04:50Z","published":"2024-03-26T06:04:50Z","title":"Neural Clustering based Visual Representation Learning","summary":"  We investigate a fundamental aspect of machine vision: the measurement of\nfeatures, by revisiting clustering, one of the most classic approaches in\nmachine learning and data analysis. Existing visual feature extractors,\nincluding ConvNets, ViTs, and MLPs, represent an image as rectangular regions.\nThough prevalent, such a grid-style paradigm is built upon engineering practice\nand lacks explicit modeling of data distribution. In this work, we propose\nfeature extraction with clustering (FEC), a conceptually elegant yet\nsurprisingly ad-hoc interpretable neural clustering framework, which views\nfeature extraction as a process of selecting representatives from data and thus\nautomatically captures the underlying data distribution. Given an image, FEC\nalternates between grouping pixels into individual clusters to abstract\nrepresentatives and updating the deep features of pixels with current\nrepresentatives. Such an iterative working mechanism is implemented in the form\nof several neural layers and the final representatives can be used for\ndownstream tasks. The cluster assignments across layers, which can be viewed\nand inspected by humans, make the forward process of FEC fully transparent and\nempower it with promising ad-hoc interpretability. Extensive experiments on\nvarious visual recognition models and tasks verify the effectiveness,\ngenerality, and interpretability of FEC. We expect this work will provoke a\nrethink of the current de facto grid-style paradigm.\n","authors":["Guikun Chen","Xia Li","Yi Yang","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17409v1.pdf","comment":"CVPR 2024. Code: https://github.com/guikunchen/FEC/"},{"id":"http://arxiv.org/abs/2403.17390v1","updated":"2024-03-26T05:19:15Z","published":"2024-03-26T05:19:15Z","title":"SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter","summary":"  SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework,\nwhich designed specifically for point cloud data. Leveraging the\ncharacteristics of non-coincidence and weak correlation of target objects in\npoint cloud, we adopt a strategy of retaining only the truth-determining pseudo\nlabels and trimming the other fuzzy labels with points, instead of pursuing a\nbalance between the quantity and quality of pseudo labels. Besides, we notice\nthat changing the filter will make the model meet different distributed\ntargets, which is beneficial to break the training bottleneck. Two mechanism\nare introduced to achieve above ideas: strict threshold and filter switching.\nThe experiments are conducted to analyze the effectiveness of above approaches\nand their impact on the overall performance of the system. Evaluating on the\nKITTI dataset, SSF3D exhibits superior performance compared to the current\nstate-of-the-art methods. The code will be released here.\n","authors":["Songbur Wong"],"pdf_url":"https://arxiv.org/pdf/2403.17390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17387v1","updated":"2024-03-26T05:12:18Z","published":"2024-03-26T05:12:18Z","title":"Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object\n  Detection","summary":"  We delve into pseudo-labeling for semi-supervised monocular 3D object\ndetection (SSM3OD) and discover two primary issues: a misalignment between the\nprediction quality of 3D and 2D attributes and the tendency of depth\nsupervision derived from pseudo-labels to be noisy, leading to significant\noptimization conflicts with other reliable forms of supervision. We introduce a\nnovel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach\nfeatures a Decoupled Pseudo-label Generation (DPG) module, designed to\nefficiently generate pseudo-labels by separately processing 2D and 3D\nattributes. This module incorporates a unique homography-based method for\nidentifying dependable pseudo-labels in BEV space, specifically for 3D\nattributes. Additionally, we present a DepthGradient Projection (DGP) module to\nmitigate optimization conflicts caused by noisy depth supervision of\npseudo-labels, effectively decoupling the depth gradient and removing\nconflicting gradients. This dual decoupling strategy-at both the pseudo-label\ngeneration and gradient levels-significantly improves the utilization of\npseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark\ndemonstrate the superiority of our method over existing approaches.\n","authors":["Jiacheng Zhang","Jiaming Li","Xiangru Lin","Wei Zhang","Xiao Tan","Junyu Han","Errui Ding","Jingdong Wang","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17387v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2403.17377v1","updated":"2024-03-26T04:49:11Z","published":"2024-03-26T04:49:11Z","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","summary":"  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n","authors":["Donghoon Ahn","Hyoungwon Cho","Jaewon Min","Wooseok Jang","Jungwoo Kim","SeonHwa Kim","Hyun Hee Park","Kyong Hwan Jin","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17377v1.pdf","comment":"Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance"},{"id":"http://arxiv.org/abs/2403.17373v1","updated":"2024-03-26T04:27:56Z","published":"2024-03-26T04:27:56Z","title":"AIDE: An Automatic Data Engine for Object Detection in Autonomous\n  Driving","summary":"  Autonomous vehicle (AV) systems rely on robust perception models as a\ncornerstone of safety assurance. However, objects encountered on the road\nexhibit a long-tailed distribution, with rare or unseen categories posing\nchallenges to a deployed perception model. This necessitates an expensive\nprocess of continuously curating and annotating data with significant human\neffort. We propose to leverage recent advances in vision-language and large\nlanguage models to design an Automatic Data Engine (AIDE) that automatically\nidentifies issues, efficiently curates data, improves the model through\nauto-labeling, and verifies the model through generation of diverse scenarios.\nThis process operates iteratively, allowing for continuous self-improvement of\nthe model. We further establish a benchmark for open-world detection on AV\ndatasets to comprehensively evaluate various learning paradigms, demonstrating\nour method's superior performance at a reduced cost.\n","authors":["Mingfu Liang","Jong-Chyi Su","Samuel Schulter","Sparsh Garg","Shiyu Zhao","Ying Wu","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2403.17373v1.pdf","comment":"Accepted by CVPR-2024"},{"id":"http://arxiv.org/abs/2403.07636v2","updated":"2024-03-26T04:26:21Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.0% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v2.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2403.10518v2","updated":"2024-03-26T04:24:13Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v2.pdf","comment":"Accepted by CVPR2024, Project page:\n  https://li-ronghui.github.io/lodge"},{"id":"http://arxiv.org/abs/2403.16209v2","updated":"2024-03-26T04:22:02Z","published":"2024-03-24T16:08:10Z","title":"Image Captioning in news report scenario","summary":"  Image captioning strives to generate pertinent captions for specified images,\nsituating itself at the crossroads of Computer Vision (CV) and Natural Language\nProcessing (NLP). This endeavor is of paramount importance with far-reaching\napplications in recommendation systems, news outlets, social media, and beyond.\nParticularly within the realm of news reporting, captions are expected to\nencompass detailed information, such as the identities of celebrities captured\nin the images. However, much of the existing body of work primarily centers\naround understanding scenes and actions. In this paper, we explore the realm of\nimage captioning specifically tailored for celebrity photographs, illustrating\nits broad potential for enhancing news industry practices. This exploration\naims to augment automated news content generation, thereby facilitating a more\nnuanced dissemination of information. Our endeavor shows a broader horizon,\nenriching the narrative in news reporting through a more intuitive image\ncaptioning framework.\n","authors":["Tianrui Liu","Qi Cai","Changxin Xu","Bo Hong","Jize Xiong","Yuxin Qiao","Tsungwei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16209v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.12342v3","updated":"2024-03-26T04:17:42Z","published":"2023-11-21T04:28:12Z","title":"LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis","summary":"  Recent text-to-image diffusion models have reached an unprecedented level in\ngenerating high-quality images. However, their exclusive reliance on textual\nprompts often falls short in precise control of image compositions. In this\npaper, we propose LoCo, a training-free approach for layout-to-image Synthesis\nthat excels in producing high-quality images aligned with both textual prompts\nand layout instructions. Specifically, we introduce a Localized Attention\nConstraint (LAC), leveraging semantic affinity between pixels in self-attention\nmaps to create precise representations of desired objects and effectively\nensure the accurate placement of objects in designated regions. We further\npropose a Padding Token Constraint (PTC) to leverage the semantic information\nembedded in previously neglected padding tokens, improving the consistency\nbetween object appearance and layout instructions. LoCo seamlessly integrates\ninto existing text-to-image and layout-to-image models, enhancing their\nperformance in spatial control and addressing semantic failures observed in\nprior methods. Extensive experiments showcase the superiority of our approach,\nsurpassing existing state-of-the-art training-free layout-to-image methods both\nqualitatively and quantitatively across multiple benchmarks.\n","authors":["Peiang Zhao","Han Li","Ruiyang Jin","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.12342v3.pdf","comment":"Demo: https://huggingface.co/spaces/Pusheen/LoCo; Project page:\n  https://momopusheen.github.io/LoCo/"},{"id":"http://arxiv.org/abs/2309.07322v2","updated":"2024-03-26T04:17:40Z","published":"2023-09-13T21:21:50Z","title":"$\\texttt{NePhi}$: Neural Deformation Fields for Approximately\n  Diffeomorphic Medical Image Registration","summary":"  This work proposes NePhi, a generalizable neural deformation model which\nresults in approximately diffeomorphic transformations. In contrast to the\npredominant voxel-based transformation fields used in learning-based\nregistration approaches, NePhi represents deformations functionally, leading to\ngreat flexibility within the design space of memory consumption during training\nand inference, inference time, registration accuracy, as well as transformation\nregularity. Specifically, NePhi 1) requires less memory compared to voxel-based\nlearning approaches, 2) improves inference speed by predicting latent codes,\ncompared to current existing neural deformation based registration approaches\nthat \\emph{only} rely on optimization, 3) improves accuracy via instance\noptimization, and 4) shows excellent deformation regularity which is highly\ndesirable for medical image registration. We demonstrate the performance of\nNePhi on a 2D synthetic dataset as well as for real 3D lung registration. Our\nresults show that NePhi can match the accuracy of voxel-based representations\nin a single-resolution registration setting. For multi-resolution registration,\nour method matches the accuracy of current SOTA learning-based registration\napproaches with instance optimization while reducing memory requirements by a\nfactor of five.\n","authors":["Lin Tian","Hastings Greer","Raúl San José Estépar","Soumyadip Sengupta","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2309.07322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19330v2","updated":"2024-03-26T04:15:53Z","published":"2024-02-29T16:33:12Z","title":"A Novel Approach to Industrial Defect Generation through Blended Latent\n  Diffusion Model with Online Adaptation","summary":"  Effectively addressing the challenge of industrial Anomaly Detection (AD)\nnecessitates an ample supply of defective samples, a constraint often hindered\nby their scarcity in industrial contexts. This paper introduces a novel\nalgorithm designed to augment defective samples, thereby enhancing AD\nperformance. The proposed method tailors the blended latent diffusion model for\ndefect sample generation, employing a diffusion model to generate defective\nsamples in the latent space. A feature editing process, controlled by a\n``trimap\" mask and text prompts, refines the generated samples. The image\ngeneration inference process is structured into three stages: a free diffusion\nstage, an editing diffusion stage, and an online decoder adaptation stage. This\nsophisticated inference strategy yields high-quality synthetic defective\nsamples with diverse pattern variations, leading to significantly improved AD\naccuracies based on the augmented training set. Specifically, on the widely\nrecognized MVTec AD dataset, the proposed method elevates the state-of-the-art\n(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD\nmetrics AP, IAP, and IAP90, respectively. The implementation code of this work\ncan be found at the GitHub repository\nhttps://github.com/GrandpaXun242/AdaBLDM.git\n","authors":["Hanxi Li","Zhengxun Zhang","Hao Chen","Lin Wu","Bo Li","Deyin Liu","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.19330v2.pdf","comment":"13 pages,7 figures"},{"id":"http://arxiv.org/abs/2403.15931v2","updated":"2024-03-26T04:15:02Z","published":"2024-03-23T20:30:28Z","title":"X-Portrait: Expressive Portrait Animation with Hierarchical Motion\n  Attention","summary":"  We propose X-Portrait, an innovative conditional diffusion model tailored for\ngenerating expressive and temporally coherent portrait animation. Specifically,\ngiven a single portrait as appearance reference, we aim to animate it with\nmotion derived from a driving video, capturing both highly dynamic and subtle\nfacial expressions along with wide-range head movements. As its core, we\nleverage the generative prior of a pre-trained diffusion model as the rendering\nbackbone, while achieve fine-grained head pose and expression control with\nnovel controlling signals within the framework of ControlNet. In contrast to\nconventional coarse explicit controls such as facial landmarks, our motion\ncontrol module is learned to interpret the dynamics directly from the original\ndriving RGB inputs. The motion accuracy is further enhanced with a patch-based\nlocal control module that effectively enhance the motion attention to\nsmall-scale nuances like eyeball positions. Notably, to mitigate the identity\nleakage from the driving signals, we train our motion control modules with\nscaling-augmented cross-identity images, ensuring maximized disentanglement\nfrom the appearance reference modules. Experimental results demonstrate the\nuniversal effectiveness of X-Portrait across a diverse range of facial\nportraits and expressive driving sequences, and showcase its proficiency in\ngenerating captivating portrait animations with consistently maintained\nidentity characteristics.\n","authors":["You Xie","Hongyi Xu","Guoxian Song","Chao Wang","Yichun Shi","Linjie Luo"],"pdf_url":"https://arxiv.org/pdf/2403.15931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17369v1","updated":"2024-03-26T04:09:08Z","published":"2024-03-26T04:09:08Z","title":"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual\n  Prompt Tuning","summary":"  Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source\ndomains to unlabeled target domains. When adapting to adverse scenes, existing\nUDA methods fail to perform well due to the lack of instructions, leading their\nmodels to overlook discrepancies within all adverse scenes. To tackle this, we\npropose CoDA which instructs models to distinguish, focus, and learn from these\ndiscrepancies at scene and image levels. Specifically, CoDA consists of a\nChain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning\n(SAVPT) mechanism. CoD focuses on scene-level instructions to divide all\nadverse scenes into easy and hard scenes, guiding models to adapt from source\nto easy domains with easy scene images, and then to hard domains with hard\nscene images, thereby laying a solid foundation for whole adaptations. Building\nupon this foundation, we employ SAVPT to dive into more detailed image-level\ninstructions to boost performance. SAVPT features a novel metric Severity that\ndivides all adverse scene images into low-severity and high-severity images.\nThen Severity directs visual prompts and adapters, instructing models to\nconcentrate on unified severity features instead of scene-specific features,\nwithout adding complexity to the model architecture. CoDA achieves SOTA\nperformances on widely-used benchmarks under all adverse scenes. Notably, CoDA\noutperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and\nFoggy Zurich benchmarks, respectively. Our code is available at\nhttps://github.com/Cuzyoung/CoDA\n","authors":["Ziyang Gong","Fuhao Li","Yupeng Deng","Deblina Bhattacharjee","Xiangwei Zhu","Zhenming Ji"],"pdf_url":"https://arxiv.org/pdf/2403.17369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16536v2","updated":"2024-03-26T03:56:34Z","published":"2024-03-25T08:26:42Z","title":"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate\n  Spatiotemporal Forecasting","summary":"  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded\nunparalleled results in predicting temporal and spatial dynamics. However,\nmodeling extensive global information remains a formidable challenge; CNNs are\nlimited by their narrow receptive fields, and ViTs struggle with the intensive\ncomputational demands of their attention mechanisms. The emergence of recent\nMamba-based architectures has been met with enthusiasm for their exceptional\nlong-sequence modeling capabilities, surpassing established vision models in\nefficiency and accuracy, which motivates us to develop an innovative\narchitecture tailored for spatiotemporal forecasting. In this paper, we propose\nthe VMRNN cell, a new recurrent unit that integrates the strengths of Vision\nMamba blocks with LSTM. We construct a network centered on VMRNN cells to\ntackle spatiotemporal prediction tasks effectively. Our extensive evaluations\nshow that our proposed approach secures competitive results on a variety of\ntasks while maintaining a smaller model size. Our code is available at\nhttps://github.com/yyyujintang/VMRNN-PyTorch.\n","authors":["Yujin Tang","Peijie Dong","Zhenheng Tang","Xiaowen Chu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2403.16536v2.pdf","comment":"11 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.17360v1","updated":"2024-03-26T03:53:00Z","published":"2024-03-26T03:53:00Z","title":"Activity-Biometrics: Person Identification from Daily Activities","summary":"  In this work, we study a novel problem which focuses on person identification\nwhile performing daily activities. Learning biometric features from RGB videos\nis challenging due to spatio-temporal complexity and presence of appearance\nbiases such as clothing color and background. We propose ABNet, a novel\nframework which leverages disentanglement of biometric and non-biometric\nfeatures to perform effective person identification from daily activities.\nABNet relies on a bias-less teacher to learn biometric features from RGB videos\nand explicitly disentangle non-biometric features with the help of biometric\ndistortion. In addition, ABNet also exploits activity prior for biometrics\nwhich is enabled by joint biometric and activity learning. We perform\ncomprehensive evaluation of the proposed approach across five different\ndatasets which are derived from existing activity recognition benchmarks.\nFurthermore, we extensively compare ABNet with existing works in person\nidentification and demonstrate its effectiveness for activity-based biometrics\nacross all five datasets. The code and dataset can be accessed at:\n\\url{https://github.com/sacrcv/Activity-Biometrics/}\n","authors":["Shehreen Azad","Yogesh Singh Rawat"],"pdf_url":"https://arxiv.org/pdf/2403.17360v1.pdf","comment":"CVPR 2024 Main conference"},{"id":"http://arxiv.org/abs/2312.06734v2","updated":"2024-03-26T03:52:48Z","published":"2023-12-11T11:26:32Z","title":"DiffCast: A Unified Framework via Residual Diffusion for Precipitation\n  Nowcasting","summary":"  Precipitation nowcasting is an important spatio-temporal prediction task to\npredict the radar echoes sequences based on current observations, which can\nserve both meteorological science and smart city applications. Due to the\nchaotic evolution nature of the precipitation systems, it is a very challenging\nproblem. Previous studies address the problem either from the perspectives of\ndeterministic modeling or probabilistic modeling. However, their predictions\nsuffer from the blurry, high-value echoes fading away and position inaccurate\nissues. The root reason of these issues is that the chaotic evolutionary\nprecipitation systems are not appropriately modeled. Inspired by the nature of\nthe systems, we propose to decompose and model them from the perspective of\nglobal deterministic motion and local stochastic variations with residual\nmechanism. A unified and flexible framework that can equip any type of\nspatio-temporal models is proposed based on residual diffusion, which\neffectively tackles the shortcomings of previous methods. Extensive\nexperimental results on four publicly available radar datasets demonstrate the\neffectiveness and superiority of the proposed framework, compared to\nstate-of-the-art techniques. Our code is publicly available at\nhttps://github.com/DeminYu98/DiffCast.\n","authors":["Demin Yu","Xutao Li","Yunming Ye","Baoquan Zhang","Chuyao Luo","Kuai Dai","Rui Wang","Xunlai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.06734v2.pdf","comment":"CVPR 2024; https://github.com/DeminYu98/DiffCast"},{"id":"http://arxiv.org/abs/2303.02490v2","updated":"2024-03-26T03:41:26Z","published":"2023-03-04T20:08:57Z","title":"Diffusion Models Generate Images Like Painters: an Analytical Theory of\n  Outline First, Details Later","summary":"  How do diffusion generative models convert pure noise into meaningful images?\nIn a variety of pretrained diffusion models (including conditional latent space\nmodels like Stable Diffusion), we observe that the reverse diffusion process\nthat underlies image generation has the following properties: (i) individual\ntrajectories tend to be low-dimensional and resemble 2D `rotations'; (ii)\nhigh-variance scene features like layout tend to emerge earlier, while\nlow-variance details tend to emerge later; and (iii) early perturbations tend\nto have a greater impact on image content than later perturbations. To\nunderstand these phenomena, we derive and study a closed-form solution to the\nprobability flow ODE for a Gaussian distribution, which shows that the reverse\ndiffusion state rotates towards a gradually-specified target on the image\nmanifold. It also shows that generation involves first committing to an\noutline, and then to finer and finer details. We find that this solution\naccurately describes the initial phase of image generation for pretrained\nmodels, and can in principle be used to make image generation more efficient by\nskipping reverse diffusion steps. Finally, we use our solution to characterize\nthe image manifold in Stable Diffusion. Our viewpoint reveals an unexpected\nsimilarity between generation by GANs and diffusion and provides a conceptual\nlink between diffusion and image retrieval.\n","authors":["Binxu Wang","John J. Vastola"],"pdf_url":"https://arxiv.org/pdf/2303.02490v2.pdf","comment":"44 pages, 28 figures. A briefer version was presented at NeurIPS23\n  Workshop on Diffusion Models [arXiv:2311.10892]"},{"id":"http://arxiv.org/abs/1902.00615v3","updated":"2024-03-26T03:40:54Z","published":"2019-02-02T01:52:53Z","title":"Confidence-Triggered Detection: Accelerating Real-time\n  Tracking-by-detection Systems","summary":"  Real-time object tracking necessitates a delicate balance between speed and\naccuracy, a challenge exacerbated by the computational demands of deep learning\nmethods. In this paper, we propose Confidence-Triggered Detection (CTD), an\ninnovative approach that strategically bypasses object detection for frames\nclosely resembling intermediate states, leveraging tracker confidence scores.\nCTD not only enhances tracking speed but also preserves accuracy, surpassing\nexisting tracking algorithms. Through extensive evaluation across various\ntracker confidence thresholds, we identify an optimal trade-off between\ntracking speed and accuracy, providing crucial insights for parameter\nfine-tuning and enhancing CTD's practicality in real-world scenarios. Our\nexperiments across diverse detection models underscore the robustness and\nversatility of the CTD framework, demonstrating its potential to enable\nreal-time tracking in resource-constrained environments.\n","authors":["Zhicheng Ding","Zhixin Lai","Siyang Li","Edward Wong"],"pdf_url":"https://arxiv.org/pdf/1902.00615v3.pdf","comment":"9 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.17346v1","updated":"2024-03-26T03:10:45Z","published":"2024-03-26T03:10:45Z","title":"TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos","summary":"  We propose TRAM, a two-stage method to reconstruct a human's global\ntrajectory and motion from in-the-wild videos. TRAM robustifies SLAM to recover\nthe camera motion in the presence of dynamic humans and uses the scene\nbackground to derive the motion scale. Using the recovered camera as a\nmetric-scale reference frame, we introduce a video transformer model (VIMO) to\nregress the kinematic body motion of a human. By composing the two motions, we\nachieve accurate recovery of 3D humans in the world space, reducing global\nmotion errors by 60% from prior work. https://yufu-wang.github.io/tram4d/\n","authors":["Yufu Wang","Ziyun Wang","Lingjie Liu","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2403.17346v1.pdf","comment":"The project website: https://yufu-wang.github.io/tram4d/"},{"id":"http://arxiv.org/abs/2303.15230v2","updated":"2024-03-26T03:07:56Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings. The code will be available at\nhttps://github.com/bighuang624/Troika.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Min Zhang","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17343v1","updated":"2024-03-26T03:05:20Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Anna Hovakimyan","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17342v1","updated":"2024-03-26T03:03:50Z","published":"2024-03-26T03:03:50Z","title":"The Solution for the ICCV 2023 1st Scientific Figure Captioning\n  Challenge","summary":"  In this paper, we propose a solution for improving the quality of captions\ngenerated for figures in papers. We adopt the approach of summarizing the\ntextual content in the paper to generate image captions. Throughout our study,\nwe encounter discrepancies in the OCR information provided in the official\ndataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR\ninformation from all images. Moreover, we observe that certain textual content\nin the official paper pertains to images that are not relevant for captioning,\nthereby introducing noise during caption generation. To mitigate this issue, we\nleverage LLaMA to extract image-specific information by querying the textual\ncontent based on image mentions, effectively filtering out extraneous\ninformation. Additionally, we recognize a discrepancy between the primary use\nof maximum likelihood estimation during text generation and the evaluation\nmetrics such as ROUGE employed to assess the quality of generated captions. To\nbridge this gap, we integrate the BRIO model framework, enabling a more\ncoherent alignment between the generation and evaluation processes. Our\napproach ranked first in the final test with a score of 4.49.\n","authors":["Dian Chao","Xin Song","Shupeng Zhong","Boyuan Wang","Xiangyu Wu","Chen Zhu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14518v2","updated":"2024-03-26T02:45:29Z","published":"2023-12-22T08:31:11Z","title":"Joint Learning Neuronal Skeleton and Brain Circuit Topology with\n  Permutation Invariant Encoders for Neuron Classification","summary":"  Determining the types of neurons within a nervous system plays a significant\nrole in the analysis of brain connectomics and the investigation of\nneurological diseases. However, the efficiency of utilizing anatomical,\nphysiological, or molecular characteristics of neurons is relatively low and\ncostly. With the advancements in electron microscopy imaging and analysis\ntechniques for brain tissue, we are able to obtain whole-brain connectome\nconsisting neuronal high-resolution morphology and connectivity information.\nHowever, few models are built based on such data for automated neuron\nclassification. In this paper, we propose NeuNet, a framework that combines\nmorphological information of neurons obtained from skeleton and topological\ninformation between neurons obtained from neural circuit. Specifically, NeuNet\nconsists of three components, namely Skeleton Encoder, Connectome Encoder, and\nReadout Layer. Skeleton Encoder integrates the local information of neurons in\na bottom-up manner, with a one-dimensional convolution in neural skeleton's\npoint data; Connectome Encoder uses a graph neural network to capture the\ntopological information of neural circuit; finally, Readout Layer fuses the\nabove two information and outputs classification results. We reprocess and\nrelease two new datasets for neuron classification task from volume electron\nmicroscopy(VEM) images of human brain cortex and Drosophila brain. Experiments\non these two datasets demonstrated the effectiveness of our model with accuracy\nof 0.9169 and 0.9363, respectively. Code and data are available at:\nhttps://github.com/WHUminghui/NeuNet.\n","authors":["Minghui Liao","Guojia Wan","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2312.14518v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2403.02981v2","updated":"2024-03-26T02:39:15Z","published":"2024-03-05T13:59:21Z","title":"Doubly Abductive Counterfactual Inference for Text-based Image Editing","summary":"  We study text-based image editing (TBIE) of a single image by counterfactual\ninference because it is an elegant formulation to precisely address the\nrequirement: the edited image should retain the fidelity of the original one.\nThrough the lens of the formulation, we find that the crux of TBIE is that\nexisting techniques hardly achieve a good trade-off between editability and\nfidelity, mainly due to the overfitting of the single-image fine-tuning. To\nthis end, we propose a Doubly Abductive Counterfactual inference framework\n(DAC). We first parameterize an exogenous variable as a UNet LoRA, whose\nabduction can encode all the image details. Second, we abduct another exogenous\nvariable parameterized by a text encoder LoRA, which recovers the lost\neditability caused by the overfitted first abduction. Thanks to the second\nabduction, which exclusively encodes the visual transition from post-edit to\npre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit\nback to post-edit, thereby accomplishing the edit. Through extensive\nexperiments, our DAC achieves a good trade-off between editability and\nfidelity. Thus, we can support a wide spectrum of user editing intents,\nincluding addition, removal, manipulation, replacement, style transfer, and\nfacial change, which are extensively validated in both qualitative and\nquantitative evaluations. Codes are in https://github.com/xuesong39/DAC.\n","authors":["Xue Song","Jiequan Cui","Hanwang Zhang","Jingjing Chen","Richang Hong","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.02981v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17334v1","updated":"2024-03-26T02:34:48Z","published":"2024-03-26T02:34:48Z","title":"OVER-NAV: Elevating Iterative Vision-and-Language Navigation with\n  Open-Vocabulary Detection and StructurEd Representation","summary":"  Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce\na more meaningful and practical paradigm of VLN by maintaining the agent's\nmemory across tours of scenes. Although the long-term memory aligns better with\nthe persistent nature of the VLN task, it poses more challenges on how to\nutilize the highly unstructured navigation memory with extremely sparse\nsupervision. Towards this end, we propose OVER-NAV, which aims to go over and\nbeyond the current arts of IVLN techniques. In particular, we propose to\nincorporate LLMs and open-vocabulary detectors to distill key information and\nestablish correspondence between multi-modal signals. Such a mechanism\nintroduces reliable cross-modal supervision and enables on-the-fly\ngeneralization to unseen scenes without the need of extra annotation and\nre-training. To fully exploit the interpreted navigation data, we further\nintroduce a structured representation, coded Omnigraph, to effectively\nintegrate multi-modal information along the tour. Accompanied with a novel\nomnigraph fusion mechanism, OVER-NAV is able to extract the most relevant\nknowledge from omnigraph for a more accurate navigating action. In addition,\nOVER-NAV seamlessly supports both discrete and continuous environments under a\nunified framework. We demonstrate the superiority of OVER-NAV in extensive\nexperiments.\n","authors":["Ganlong Zhao","Guanbin Li","Weikai Chen","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17334v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17332v1","updated":"2024-03-26T02:32:52Z","published":"2024-03-26T02:32:52Z","title":"Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI -\n  Integrating Grey and White Matter Information","summary":"  Thresholding of networks has long posed a challenge in brain connectivity\nanalysis. Weighted networks are typically binarized using threshold measures to\nfacilitate network analysis. Previous studies on MRI-based brain networks have\npredominantly utilized density or sparsity-based thresholding techniques,\noptimized within specific ranges derived from network metrics such as path\nlength, clustering coefficient, and small-world index. Thus, determination of a\nsingle threshold value for facilitating comparative analysis of networks\nremains elusive. To address this, our study introduces Mutual K-Nearest\nNeighbor (MKNN)-based thresholding for brain network analysis. Here, nearest\nneighbor selection is based on the highest correlation between features of\nbrain regions. Construction of brain networks was accomplished by computing\nPearson correlations between grey matter volume and white matter volume for\neach pair of brain regions. Structural MRI data from 180 Parkinsons patients\nand 70 controls from the NIMHANS, India were analyzed. Subtypes within\nParkinsons disease were identified based on grey and white matter volume\natrophy using source-based morphometric decomposition. The loading coefficients\nwere correlated with clinical features to discern clinical relationship with\nthe deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51,\nintermediate type), Subtype B (N = 57, mild-severe type with mild motor\nsymptoms), and Subtype AB (N = 36, most-severe type with predominance in motor\nimpairment). Subtype-specific weighted matrices were binarized using MKNN-based\nthresholding for brain network analysis. Permutation tests on network metrics\nof resulting bipartite graphs demonstrated significant group differences in\nbetweenness centrality and participation coefficient. The identified hubs were\nspecific to each subtype, with some hubs conserved across different subtypes.\n","authors":["Tanmayee Samantaray","Jitender Saini","Pramod Kumar Pal","Bithiah Grace Jaganathan","Vijaya V Saradhi","Gupta CN"],"pdf_url":"https://arxiv.org/pdf/2403.17332v1.pdf","comment":"31 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.17330v1","updated":"2024-03-26T02:28:49Z","published":"2024-03-26T02:28:49Z","title":"Staircase Localization for Autonomous Exploration in Urban Environments","summary":"  A staircase localization method is proposed for robots to explore urban\nenvironments autonomously. The proposed method employs a modular design in the\nform of a cascade pipeline consisting of three modules of stair detection, line\nsegment detection, and stair localization modules. The stair detection module\nutilizes an object detection algorithm based on deep learning to generate a\nregion of interest (ROI). From the ROI, line segment features are extracted\nusing a deep line segment detection algorithm. The extracted line segments are\nused to localize a staircase in terms of position, orientation, and stair\ndirection. The stair detection and localization are performed only with a\nsingle RGB-D camera. Each component of the proposed pipeline does not need to\nbe designed particularly for staircases, which makes it easy to maintain the\nwhole pipeline and replace each component with state-of-the-art deep learning\ndetection techniques. The results of real-world experiments show that the\nproposed method can perform accurate stair detection and localization during\nautonomous exploration for various structured and unstructured upstairs and\ndownstairs with shadows, dirt, and occlusions by artificial and natural\nobjects.\n","authors":["Jinrae Kim","Sunggoo Jung","Sung-Kyun Kim","Youdan Kim","Ali-akbar Agha-mohammadi"],"pdf_url":"https://arxiv.org/pdf/2403.17330v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.16080v2","updated":"2024-03-26T02:25:58Z","published":"2024-03-24T10:06:40Z","title":"PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic\n  Human Modeling","summary":"  High-quality human reconstruction and photo-realistic rendering of a dynamic\nscene is a long-standing problem in computer vision and graphics. Despite\nconsiderable efforts invested in developing various capture systems and\nreconstruction algorithms, recent advancements still struggle with loose or\noversized clothing and overly complex poses. In part, this is due to the\nchallenges of acquiring high-quality human datasets. To facilitate the\ndevelopment of these fields, in this paper, we present PKU-DyMVHumans, a\nversatile human-centric dataset for high-fidelity reconstruction and rendering\nof dynamic human scenarios from dense multi-view videos. It comprises 8.2\nmillion frames captured by more than 56 synchronized cameras across diverse\nscenarios. These sequences comprise 32 human subjects across 45 different\nscenarios, each with a high-detailed appearance and realistic human motion.\nInspired by recent advancements in neural radiance field (NeRF)-based scene\nrepresentations, we carefully set up an off-the-shelf framework that is easy to\nprovide those state-of-the-art NeRF-based implementations and benchmark on\nPKU-DyMVHumans dataset. It is paving the way for various applications like\nfine-grained foreground/background decomposition, high-quality human\nreconstruction and photo-realistic novel view synthesis of a dynamic scene.\nExtensive studies are performed on the benchmark, demonstrating new\nobservations and challenges that emerge from using such high-fidelity dynamic\ndata. The dataset is available at: https://pku-dymvhumans.github.io.\n","authors":["Xiaoyun Zheng","Liwei Liao","Xufeng Li","Jianbo Jiao","Rongjie Wang","Feng Gao","Shiqi Wang","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16080v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17327v1","updated":"2024-03-26T02:21:36Z","published":"2024-03-26T02:21:36Z","title":"Accuracy enhancement method for speech emotion recognition from\n  spectrogram using temporal frequency correlation and positional information\n  learning through knowledge transfer","summary":"  In this paper, we propose a method to improve the accuracy of speech emotion\nrecognition (SER) by using vision transformer (ViT) to attend to the\ncorrelation of frequency (y-axis) with time (x-axis) in spectrogram and\ntransferring positional information between ViT through knowledge transfer. The\nproposed method has the following originality i) We use vertically segmented\npatches of log-Mel spectrogram to analyze the correlation of frequencies over\ntime. This type of patch allows us to correlate the most relevant frequencies\nfor a particular emotion with the time they were uttered. ii) We propose the\nuse of image coordinate encoding, an absolute positional encoding suitable for\nViT. By normalizing the x, y coordinates of the image to -1 to 1 and\nconcatenating them to the image, we can effectively provide valid absolute\npositional information for ViT. iii) Through feature map matching, the locality\nand location information of the teacher network is effectively transmitted to\nthe student network. Teacher network is a ViT that contains locality of\nconvolutional stem and absolute position information through image coordinate\nencoding, and student network is a structure that lacks positional encoding in\nthe basic ViT structure. In feature map matching stage, we train through the\nmean absolute error (L1 loss) to minimize the difference between the feature\nmaps of the two networks. To validate the proposed method, three emotion\ndatasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into\nlog-Mel spectrograms for comparison experiments. The experimental results show\nthat the proposed method significantly outperforms the state-of-the-art methods\nin terms of weighted accuracy while requiring significantly fewer floating\npoint operations (FLOPs). Overall, the proposed method offers an promising\nsolution for SER by providing improved efficiency and performance.\n","authors":["Jeong-Yoon Kim","Seung-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10474v3","updated":"2024-03-26T01:11:52Z","published":"2023-05-17T17:59:16Z","title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models","summary":"  Despite tremendous progress in generating high-quality images using diffusion\nmodels, synthesizing a sequence of animated frames that are both photorealistic\nand temporally coherent is still in its infancy. While off-the-shelf\nbillion-scale datasets for image generation are available, collecting similar\nvideo data of the same scale is still challenging. Also, training a video\ndiffusion model is computationally much more expensive than its image\ncounterpart. In this work, we explore finetuning a pretrained image diffusion\nmodel with video data as a practical solution for the video synthesis task. We\nfind that naively extending the image noise prior to video noise prior in video\ndiffusion leads to sub-optimal performance. Our carefully designed video noise\nprior leads to substantially better performance. Extensive experimental\nvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attains\nSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It\nalso achieves SOTA video generation quality on the small-scale UCF-101\nbenchmark with a $10\\times$ smaller model using significantly less computation\nthan the prior art.\n","authors":["Songwei Ge","Seungjun Nah","Guilin Liu","Tyler Poon","Andrew Tao","Bryan Catanzaro","David Jacobs","Jia-Bin Huang","Ming-Yu Liu","Yogesh Balaji"],"pdf_url":"https://arxiv.org/pdf/2305.10474v3.pdf","comment":"ICCV 2023. Project webpage:\n  https://research.nvidia.com/labs/dir/pyoco"},{"id":"http://arxiv.org/abs/2403.17301v1","updated":"2024-03-26T01:06:47Z","published":"2024-03-26T01:06:47Z","title":"Physical 3D Adversarial Attacks against Monocular Depth Estimation in\n  Autonomous Driving","summary":"  Deep learning-based monocular depth estimation (MDE), extensively applied in\nautonomous driving, is known to be vulnerable to adversarial attacks. Previous\nphysical attacks against MDE models rely on 2D adversarial patches, so they\nonly affect a small, localized region in the MDE map but fail under various\nviewpoints. To address these limitations, we propose 3D Depth Fool\n(3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.\n3D$^2$Fool is specifically optimized to generate 3D adversarial textures\nagnostic to model types of vehicles and to have improved robustness in bad\nweather conditions, such as rain and fog. Experimental results validate the\nsuperior performance of our 3D$^2$Fool across various scenarios, including\nvehicles, MDE models, weather conditions, and viewpoints. Real-world\nexperiments with printed 3D textures on physical vehicle models further\ndemonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.\n","authors":["Junhao Zheng","Chenhao Lin","Jiahao Sun","Zhengyu Zhao","Qian Li","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.17301v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17293v1","updated":"2024-03-26T00:41:54Z","published":"2024-03-26T00:41:54Z","title":"Tracing and segmentation of molecular patterns in 3-dimensional\n  cryo-et/em density maps through algorithmic image processing and deep\n  learning-based techniques","summary":"  Understanding the structures of biological macromolecules is highly important\nas they are closely associated with cellular functionalities. Comprehending the\nprecise organization actin filaments is crucial because they form the dynamic\ncytoskeleton, which offers structural support to cells and connects the cell's\ninterior with its surroundings. However, determining the precise organization\nof actin filaments is challenging due to the poor quality of cryo-electron\ntomography (cryo-ET) images, which suffer from low signal-to-noise (SNR) ratios\nand the presence of missing wedge, as well as diverse shape characteristics of\nactin filaments. To address these formidable challenges, the primary component\nof this dissertation focuses on developing sophisticated computational\ntechniques for tracing actin filaments. In particular, three novel\nmethodologies have been developed: i) BundleTrac, for tracing bundle-like actin\nfilaments found in Stereocilium, ii) Spaghetti Tracer, for tracing filaments\nthat move individually with loosely cohesive movements, and iii) Struwwel\nTracer, for tracing randomly orientated actin filaments in the actin network.\nThe second component of the dissertation introduces a convolutional neural\nnetwork (CNN) based segmentation model to determine the location of protein\nsecondary structures, such as helices and beta-sheets, in medium-resolution\n(5-10 Angstrom) 3-dimensional cryo-electron microscopy (cryo-EM) images. This\nmethodology later evolved into a tool named DeepSSETracer. The final component\nof the dissertation presents a novel algorithm, cylindrical fit measure, to\nestimate image structure match at helix regions in medium-resolution cryo-EM\nimages. Overall, my dissertation has made significant contributions to\naddressing critical research challenges in structural biology by introducing\nvarious computational methods and tools.\n","authors":["Salim Sazzed"],"pdf_url":"https://arxiv.org/pdf/2403.17293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18158v1","updated":"2024-03-26T23:47:17Z","published":"2024-03-26T23:47:17Z","title":"The Effects of Short Video-Sharing Services on Video Copy Detection","summary":"  The short video-sharing services that allow users to post 10-30 second videos\n(e.g., YouTube Shorts and TikTok) have attracted a lot of attention in recent\nyears. However, conventional video copy detection (VCD) methods mainly focus on\ngeneral video-sharing services (e.g., YouTube and Bilibili), and the effects of\nshort video-sharing services on video copy detection are still unclear.\nConsidering that illegally copied videos in short video-sharing services have\nservice-distinctive characteristics, especially in those time lengths, the pros\nand cons of VCD in those services are required to be analyzed. In this paper,\nwe examine the effects of short video-sharing services on VCD by constructing a\ndataset that has short video-sharing service characteristics. Our novel dataset\nis automatically constructed from the publicly available dataset to have\nreference videos and fixed short-time-length query videos, and such automation\nprocedures assure the reproducibility and data privacy preservation of this\npaper. From the experimental results focusing on segment-level and video-level\nsituations, we can see that three effects: \"Segment-level VCD in short\nvideo-sharing services is more difficult than those in general video-sharing\nservices\", \"Video-level VCD in short video-sharing services is easier than\nthose in general video-sharing services\", \"The video alignment component mainly\nsuppress the detection performance in short video-sharing services\".\n","authors":["Rintaro Yanagi","Yamato Okamoto","Shuhei Yokoo","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18151v1","updated":"2024-03-26T23:32:29Z","published":"2024-03-26T23:32:29Z","title":"Automated Report Generation for Lung Cytological Images Using a CNN\n  Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study","summary":"  Cytology plays a crucial role in lung cancer diagnosis. Pulmonary cytology\ninvolves cell morphological characterization in the specimen and reporting the\ncorresponding findings, which are extremely burdensome tasks. In this study, we\npropose a report-generation technique for lung cytology images. In total, 71\nbenign and 135 malignant pulmonary cytology specimens were collected. Patch\nimages were extracted from the captured specimen images, and the findings were\nassigned to each image as a dataset for report generation. The proposed method\nconsists of a vision model and a text decoder. In the former, a convolutional\nneural network (CNN) is used to classify a given image as benign or malignant,\nand the features related to the image are extracted from the intermediate\nlayer. Independent text decoders for benign and malignant cells are prepared\nfor text generation, and the text decoder switches according to the CNN\nclassification results. The text decoder is configured using a Transformer that\nuses the features obtained from the CNN for report generation. Based on the\nevaluation results, the sensitivity and specificity were 100% and 96.4%,\nrespectively, for automated benign and malignant case classification, and the\nsaliency map indicated characteristic benign and malignant areas. The grammar\nand style of the generated texts were confirmed as correct and in better\nagreement with gold standard compared to existing LLM-based image-captioning\nmethods and single-text-decoder ablation model. These results indicate that the\nproposed method is useful for pulmonary cytology classification and reporting.\n","authors":["Atsushi Teramoto","Ayano Michiba","Yuka Kiriyama","Tetsuya Tsukamoto","Kazuyoshi Imaizumi","Hiroshi Fujita"],"pdf_url":"https://arxiv.org/pdf/2403.18151v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.16335v2","updated":"2024-03-26T23:29:49Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v2.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"},{"id":"http://arxiv.org/abs/2308.02396v2","updated":"2024-03-26T23:17:24Z","published":"2023-07-24T17:09:40Z","title":"HOOD: Real-Time Human Presence and Out-of-Distribution Detection Using\n  FMCW Radar","summary":"  Detecting human presence indoors with millimeter-wave frequency-modulated\ncontinuous-wave (FMCW) radar faces challenges from both moving and stationary\nclutter. This work proposes a robust and real-time capable human presence and\nout-of-distribution (OOD) detection method using 60 GHz short-range FMCW radar.\nHOOD solves the human presence and OOD detection problems simultaneously in a\nsingle pipeline. Our solution relies on a reconstruction-based architecture and\nworks with radar macro and micro range-Doppler images (RDIs). HOOD aims to\naccurately detect the presence of humans in the presence or absence of moving\nand stationary disturbers. Since HOOD is also an OOD detector, it aims to\ndetect moving or stationary clutters as OOD in humans' absence and predicts the\ncurrent scene's output as \"no presence.\" HOOD performs well in diverse\nscenarios, demonstrating its effectiveness across different human activities\nand situations. On our dataset collected with a 60 GHz short-range FMCW radar,\nwe achieve an average AUROC of 94.36%. Additionally, our extensive evaluations\nand experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD\ndetection methods in terms of common OOD detection metrics. Importantly, HOOD\nalso perfectly fits on Raspberry Pi 3B+ with an ARM Cortex-A53 CPU, which\nshowcases its versatility across different hardware environments. Videos of our\nhuman presence detection experiments are available at:\nhttps://muskahya.github.io/HOOD\n","authors":["Sabri Mustafa Kahya","Muhammet Sami Yavuz","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2308.02396v2.pdf","comment":"10 pages, 2 figures, project page: https://muskahya.github.io/HOOD"},{"id":"http://arxiv.org/abs/2403.18144v1","updated":"2024-03-26T23:05:24Z","published":"2024-03-26T23:05:24Z","title":"Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from\n  Federated Learning","summary":"  Federated learning is a decentralized learning paradigm introduced to\npreserve privacy of client data. Despite this, prior work has shown that an\nattacker at the server can still reconstruct the private training data using\nonly the client updates. These attacks are known as data reconstruction attacks\nand fall into two major categories: gradient inversion (GI) and linear layer\nleakage attacks (LLL). However, despite demonstrating the effectiveness of\nthese attacks in breaching privacy, prior work has not investigated the\nusefulness of the reconstructed data for downstream tasks. In this work, we\nexplore data reconstruction attacks through the lens of training and improving\nmodels with leaked data. We demonstrate the effectiveness of both GI and LLL\nattacks in maliciously training models using the leaked data more accurately\nthan a benign federated learning strategy. Counter-intuitively, this bump in\ntraining quality can occur despite limited reconstruction quality or a small\ntotal number of leaked images. Finally, we show the limitations of these\nattacks for downstream training, individually for GI attacks and for LLL\nattacks.\n","authors":["Joshua C. Zhao","Ahaan Dabholkar","Atul Sharma","Saurabh Bagchi"],"pdf_url":"https://arxiv.org/pdf/2403.18144v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2403.18139v1","updated":"2024-03-26T22:50:36Z","published":"2024-03-26T22:50:36Z","title":"Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion\n  Probabilistic Model","summary":"  Anatomically guided PET reconstruction using MRI information has been shown\nto have the potential to improve PET image quality. However, these improvements\nare limited to PET scans with paired MRI information. In this work we employed\na diffusion probabilistic model (DPM) to infer T1-weighted-MRI (deep-MRI)\nimages from FDG-PET brain images. We then use the DPM-generated T1w-MRI to\nguide the PET reconstruction. The model was trained with brain FDG scans, and\ntested in datasets containing multiple levels of counts. Deep-MRI images\nappeared somewhat degraded than the acquired MRI images. Regarding PET image\nquality, volume of interest analysis in different brain regions showed that\nboth PET reconstructed images using the acquired and the deep-MRI images\nimproved image quality compared to OSEM. Same conclusions were found analysing\nthe decimated datasets. A subjective evaluation performed by two physicians\nconfirmed that OSEM scored consistently worse than the MRI-guided PET images\nand no significant differences were observed between the MRI-guided PET images.\nThis proof of concept shows that it is possible to infer DPM-based MRI imagery\nto guide the PET reconstruction, enabling the possibility of changing\nreconstruction parameters such as the strength of the prior on anatomically\nguided PET reconstruction in the absence of MRI.\n","authors":["Weijie Gan","Huidong Xie","Carl von Gall","Günther Platsch","Michael T. Jurkiewicz","Andrea Andrade","Udunna C. Anazodo","Ulugbek S. Kamilov","Hongyu An","Jorge Cabello"],"pdf_url":"https://arxiv.org/pdf/2403.18139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01973v3","updated":"2024-03-26T22:46:10Z","published":"2023-04-04T17:31:15Z","title":"ERM++: An Improved Baseline for Domain Generalization","summary":"  Domain Generalization (DG) measures a classifier's ability to generalize to\nnew distributions of data it was not trained on. Recent work has shown that a\nhyperparameter-tuned Empirical Risk Minimization (ERM) training procedure, that\nis simply minimizing the empirical risk on the source domains, can outperform\nmost existing DG methods. ERM has achieved such strong results while only\ntuning hyper-parameters such as learning rate, weight decay, batch size, and\ndropout. However there are additional hyperparameters which further limit\noverfitting and catastrophic forgetting. We therefore focus on tuning\npreviously untuned hyper-parameters, including training amount, initialization,\nand additional regularizers. We call the resulting stronger baseline ERM++.\nERM++ improves the performance of DG by over 5% compared to prior ERM baselines\non a standard benchmark of 5 datasets with a ResNet-50 and over 15% with a\nViT-B/16, and outperforms all SOTA methods on DomainBed with both\narchitectures. We also explore the relationship between DG performance and\nsimilarity to pre-training data, and find that similarity to pre-training data\ndistributions is an important driver of performance, but that ERM++ with\nstronger initializations can deliver strong performance even on dissimilar\ndatasets.Code is released at https://github.com/piotr-teterwak/erm_plusplus.\n","authors":["Piotr Teterwak","Kuniaki Saito","Theodoros Tsiligkaridis","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2304.01973v3.pdf","comment":"An improved baseline for Domain Generalization"},{"id":"http://arxiv.org/abs/2403.13680v2","updated":"2024-03-26T22:45:20Z","published":"2024-03-20T15:38:53Z","title":"Step-Calibrated Diffusion for Biomedical Optical Image Restoration","summary":"  High-quality, high-resolution medical imaging is essential for clinical care.\nRaman-based biomedical optical imaging uses non-ionizing infrared radiation to\nevaluate human tissues in real time and is used for early cancer detection,\nbrain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,\noptical imaging is vulnerable to image degradation due to laser scattering and\nabsorption, which can result in diagnostic errors and misguided treatment.\nRestoration of optical images is a challenging computer vision task because the\nsources of image degradation are multi-factorial, stochastic, and\ntissue-dependent, preventing a straightforward method to obtain paired\nlow-quality/high-quality data. Here, we present Restorative Step-Calibrated\nDiffusion (RSCD), an unpaired image restoration method that views the image\nrestoration problem as completing the finishing steps of a diffusion-based\nimage generation task. RSCD uses a step calibrator model to dynamically\ndetermine the severity of image degradation and the number of steps required to\ncomplete the reverse diffusion process for image restoration. RSCD outperforms\nother widely used unpaired image restoration methods on both image quality and\nperceptual evaluation metrics for restoring optical images. Medical imaging\nexperts consistently prefer images restored using RSCD in blinded comparison\nexperiments and report minimal to no hallucinations. Finally, we show that RSCD\nimproves performance on downstream clinical imaging tasks, including automated\nbrain tumor diagnosis and deep tissue imaging. Our code is available at\nhttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.\n","authors":["Yiwei Lyu","Sung Jik Cha","Cheng Jiang","Asadur Chowdury","Xinhai Hou","Edward Harake","Akhil Kondepudi","Christian Freudiger","Honglak Lee","Todd C. Hollon"],"pdf_url":"https://arxiv.org/pdf/2403.13680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18134v1","updated":"2024-03-26T22:31:05Z","published":"2024-03-26T22:31:05Z","title":"Integrative Graph-Transformer Framework for Histopathology Whole Slide\n  Image Representation and Classification","summary":"  In digital pathology, the multiple instance learning (MIL) strategy is widely\nused in the weakly supervised histopathology whole slide image (WSI)\nclassification task where giga-pixel WSIs are only labeled at the slide level.\nHowever, existing attention-based MIL approaches often overlook contextual\ninformation and intrinsic spatial relationships between neighboring tissue\ntiles, while graph-based MIL frameworks have limited power to recognize the\nlong-range dependencies. In this paper, we introduce the integrative\ngraph-transformer framework that simultaneously captures the context-aware\nrelational features and global WSI representations through a novel Graph\nTransformer Integration (GTI) block. Specifically, each GTI block consists of a\nGraph Convolutional Network (GCN) layer modeling neighboring relations at the\nlocal instance level and an efficient global attention model capturing\ncomprehensive global information from extensive feature embeddings. Extensive\nexperiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and\nBRIGHT, demonstrate the superiority of our approach over current\nstate-of-the-art MIL methods, achieving an improvement of 1.0% to 2.6% in\naccuracy and 0.7%-1.6% in AUROC.\n","authors":["Zhan Shi","Jingwei Zhang","Jun Kong","Fusheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18132v1","updated":"2024-03-26T22:26:39Z","published":"2024-03-26T22:26:39Z","title":"Recommendation of data-free class-incremental learning algorithms by\n  simulating future data","summary":"  Class-incremental learning deals with sequential data streams composed of\nbatches of classes. Various algorithms have been proposed to address the\nchallenging case where samples from past classes cannot be stored. However,\nselecting an appropriate algorithm for a user-defined setting is an open\nproblem, as the relative performance of these algorithms depends on the\nincremental settings. To solve this problem, we introduce an algorithm\nrecommendation method that simulates the future data stream. Given an initial\nset of classes, it leverages generative models to simulate future classes from\nthe same visual domain. We evaluate recent algorithms on the simulated stream\nand recommend the one which performs best in the user-defined incremental\nsetting. We illustrate the effectiveness of our method on three large datasets\nusing six algorithms and six incremental settings. Our method outperforms\ncompetitive baselines, and performance is close to that of an oracle choosing\nthe best algorithm in each setting. This work contributes to facilitate the\npractical deployment of incremental learning.\n","authors":["Eva Feillet","Adrian Popescu","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2403.18132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16967v2","updated":"2024-03-26T22:00:27Z","published":"2024-03-25T17:26:08Z","title":"Visual Whole-Body Control for Legged Loco-Manipulation","summary":"  We study the problem of mobile manipulation using legged robots equipped with\nan arm, namely legged loco-manipulation. The robot legs, while usually utilized\nfor mobility, offer an opportunity to amplify the manipulation capabilities by\nconducting whole-body control. That is, the robot can control the legs and the\narm at the same time to extend its workspace. We propose a framework that can\nconduct the whole-body control autonomously with visual observations. Our\napproach, namely Visual Whole-Body Control(VBC), is composed of a low-level\npolicy using all degrees of freedom to track the end-effector manipulator\nposition and a high-level policy proposing the end-effector position based on\nvisual inputs. We train both levels of policies in simulation and perform\nSim2Real transfer for real robot deployment. We perform extensive experiments\nand show significant improvements over baselines in picking up diverse objects\nin different configurations (heights, locations, orientations) and\nenvironments. Project page: https://wholebody-b1.github.io\n","authors":["Minghuan Liu","Zixuan Chen","Xuxin Cheng","Yandong Ji","Ruihan Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16967v2.pdf","comment":"The first two authors contribute equally. Project page:\n  https://wholebody-b1.github.io"},{"id":"http://arxiv.org/abs/2312.01629v2","updated":"2024-03-26T21:58:28Z","published":"2023-12-04T05:13:59Z","title":"CLAMP: Contrastive LAnguage Model Prompt-tuning","summary":"  Large language models (LLMs) have emerged as powerful general-purpose\ninterfaces for many machine learning problems. Recent work has adapted LLMs to\ngenerative visual tasks like image captioning, visual question answering, and\nvisual chat, using a relatively small amount of instruction-tuning data. In\nthis paper, we explore whether modern LLMs can also be adapted to classifying\nan image into a set of categories. First, we evaluate multimodal LLMs that are\ntuned for generative tasks on zero-shot image classification and find that\ntheir performance is far below that of specialized models like CLIP. We then\npropose an approach for light fine-tuning of LLMs using the same contrastive\nimage-caption matching objective as CLIP. Our results show that LLMs can,\nindeed, achieve good image classification performance when adapted this way.\nOur approach beats state-of-the-art mLLMs by 13% and slightly outperforms\ncontrastive learning with a custom text model, while also retaining the LLM's\ngenerative abilities. LLM initialization appears to particularly help\nclassification in domains under-represented in the visual pre-training data.\n","authors":["Piotr Teterwak","Ximeng Sun","Bryan A. Plummer","Kate Saenko","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2312.01629v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2403.17888v1","updated":"2024-03-26T17:21:24Z","published":"2024-03-26T17:21:24Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","summary":"  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.\n","authors":["Binbin Huang","Zehao Yu","Anpei Chen","Andreas Geiger","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17888v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2401.06003v2","updated":"2024-03-26T16:30:20Z","published":"2024-01-11T16:06:36Z","title":"TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering","summary":"  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/\n","authors":["Linus Franke","Darius Rückert","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2401.06003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2403.17782v1","updated":"2024-03-26T15:15:15Z","published":"2024-03-26T15:15:15Z","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","summary":"  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n","authors":["Chenjian Gao","Boyan Jiang","Xinghui Li","Yingpeng Zhang","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17782v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.17761v1","updated":"2024-03-26T14:51:53Z","published":"2024-03-26T14:51:53Z","title":"Makeup Prior Models for 3D Facial Makeup Estimation and Applications","summary":"  In this work, we introduce two types of makeup prior models to extend\nexisting 3D face prior models: PCA-based and StyleGAN2-based priors. The\nPCA-based prior model is a linear model that is easy to construct and is\ncomputationally efficient. However, it retains only low-frequency information.\nConversely, the StyleGAN2-based model can represent high-frequency information\nwith relatively higher computational cost than the PCA-based model. Although\nthere is a trade-off between the two models, both are applicable to 3D facial\nmakeup estimation and related applications. By leveraging makeup prior models\nand designing a makeup consistency module, we effectively address the\nchallenges that previous methods faced in robustly estimating makeup,\nparticularly in the context of handling self-occluded faces. In experiments, we\ndemonstrate that our approach reduces computational costs by several orders of\nmagnitude, achieving speeds up to 180 times faster. In addition, by improving\nthe accuracy of the estimated makeup, we confirm that our methods are highly\nadvantageous for various 3D facial makeup applications such as 3D makeup face\nreconstruction, user-friendly makeup editing, makeup transfer, and\ninterpolation.\n","authors":["Xingchao Yang","Takafumi Taketomi","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2403.17761v1.pdf","comment":"CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page"},{"id":"http://arxiv.org/abs/2403.17694v1","updated":"2024-03-26T13:35:02Z","published":"2024-03-26T13:35:02Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","summary":"  In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait\n","authors":["Huawei Wei","Zejun Yang","Zhisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17672v1","updated":"2024-03-26T13:02:38Z","published":"2024-03-26T13:02:38Z","title":"Predicting Perceived Gloss: Do Weak Labels Suffice?","summary":"  Estimating perceptual attributes of materials directly from images is a\nchallenging task due to their complex, not fully-understood interactions with\nexternal factors, such as geometry and lighting. Supervised deep learning\nmodels have recently been shown to outperform traditional approaches, but rely\non large datasets of human-annotated images for accurate perception\npredictions. Obtaining reliable annotations is a costly endeavor, aggravated by\nthe limited ability of these models to generalise to different aspects of\nappearance. In this work, we show how a much smaller set of human annotations\n(\"strong labels\") can be effectively augmented with automatically derived \"weak\nlabels\" in the context of learning a low-dimensional image-computable gloss\nmetric. We evaluate three alternative weak labels for predicting human gloss\nperception from limited annotated data. Incorporating weak labels enhances our\ngloss prediction beyond the current state of the art. Moreover, it enables a\nsubstantial reduction in human annotation costs without sacrificing accuracy,\nwhether working with rendered images or real photographs.\n","authors":["Julia Guerrero-Viu","J. Daniel Subias","Ana Serrano","Katherine R. Storrs","Roland W. Fleming","Belen Masia","Diego Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2403.17672v1.pdf","comment":"Computer Graphics Forum (Eurographics 2024)"},{"id":"http://arxiv.org/abs/2403.17541v1","updated":"2024-03-26T09:44:34Z","published":"2024-03-26T09:44:34Z","title":"WordRobe: Text-Guided Generation of Textured 3D Garments","summary":"  In this paper, we tackle a new and challenging problem of text-driven\ngeneration of 3D garments with high-quality textures. We propose \"WordRobe\", a\nnovel framework for the generation of unposed & textured 3D garment meshes from\nuser-friendly text prompts. We achieve this by first learning a latent\nrepresentation of 3D garments using a novel coarse-to-fine training strategy\nand a loss for latent disentanglement, promoting better latent interpolation.\nSubsequently, we align the garment latent space to the CLIP embedding space in\na weakly supervised manner, enabling text-driven 3D garment generation and\nediting. For appearance modeling, we leverage the zero-shot generation\ncapability of ControlNet to synthesize view-consistent texture maps in a single\nfeed-forward inference step, thereby drastically decreasing the generation time\nas compared to existing methods. We demonstrate superior performance over\ncurrent SOTAs for learning 3D garment latent space, garment interpolation, and\ntext-driven texture synthesis, supported by quantitative evaluation and\nqualitative user study. The unposed 3D garment meshes generated using WordRobe\ncan be directly fed to standard cloth simulation & animation pipelines without\nany post-processing.\n","authors":["Astitva Srivastava","Pranav Manu","Amit Raj","Varun Jampani","Avinash Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.17541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17496v1","updated":"2024-03-26T08:53:25Z","published":"2024-03-26T08:53:25Z","title":"Dr.Hair: Reconstructing Scalp-Connected Hair Strands without\n  Pre-training via Differentiable Rendering of Line Segments","summary":"  In the film and gaming industries, achieving a realistic hair appearance\ntypically involves the use of strands originating from the scalp. However,\nreconstructing these strands from observed surface images of hair presents\nsignificant challenges. The difficulty in acquiring Ground Truth (GT) data has\nled state-of-the-art learning-based methods to rely on pre-training with\nmanually prepared synthetic CG data. This process is not only labor-intensive\nand costly but also introduces complications due to the domain gap when\ncompared to real-world data. In this study, we propose an optimization-based\napproach that eliminates the need for pre-training. Our method represents hair\nstrands as line segments growing from the scalp and optimizes them using a\nnovel differentiable rendering algorithm. To robustly optimize a substantial\nnumber of slender explicit geometries, we introduce 3D orientation estimation\nutilizing global optimization, strand initialization based on Laplace's\nequation, and reparameterization that leverages geometric connectivity and\nspatial proximity. Unlike existing optimization-based methods, our method is\ncapable of reconstructing internal hair flow in an absolute direction. Our\nmethod exhibits robust and accurate inverse rendering, surpassing the quality\nof existing methods and significantly improving processing speed.\n","authors":["Yusuke Takimoto","Hikari Takehara","Hiroyuki Sato","Zihao Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17496v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2306.07632v3","updated":"2024-03-26T07:00:27Z","published":"2023-06-13T09:02:57Z","title":"NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated\n  Rendering","summary":"  This paper presents a method, namely NeuS-PIR, for recovering relightable\nneural surfaces using pre-integrated rendering from multi-view images or video.\nUnlike methods based on NeRF and discrete meshes, our method utilizes implicit\nneural surface representation to reconstruct high-quality geometry, which\nfacilitates the factorization of the radiance field into two components: a\nspatially varying material field and an all-frequency lighting representation.\nThis factorization, jointly optimized using an adapted differentiable\npre-integrated rendering framework with material encoding regularization, in\nturn addresses the ambiguity of geometry reconstruction and leads to better\ndisentanglement and refinement of each scene property. Additionally, we\nintroduced a method to distil indirect illumination fields from the learned\nrepresentations, further recovering the complex illumination effect like\ninter-reflection. Consequently, our method enables advanced applications such\nas relighting, which can be seamlessly integrated with modern graphics engines.\nQualitative and quantitative experiments have shown that NeuS-PIR outperforms\nexisting methods across various tasks on both synthetic and real datasets.\nSource code is available at https://github.com/Sheldonmao/NeuSPIR\n","authors":["Shi Mao","Chenming Wu","Zhelun Shen","Yifan Wang","Dayan Wu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10518v2","updated":"2024-03-26T04:24:13Z","published":"2024-03-15T17:59:33Z","title":"Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation\n  Guided by the Characteristic Dance Primitives","summary":"  We propose Lodge, a network capable of generating extremely long dance\nsequences conditioned on given music. We design Lodge as a two-stage coarse to\nfine diffusion architecture, and propose the characteristic dance primitives\nthat possess significant expressiveness as intermediate representations between\ntwo diffusion models. The first stage is global diffusion, which focuses on\ncomprehending the coarse-level music-dance correlation and production\ncharacteristic dance primitives. In contrast, the second-stage is the local\ndiffusion, which parallelly generates detailed motion sequences under the\nguidance of the dance primitives and choreographic rules. In addition, we\npropose a Foot Refine Block to optimize the contact between the feet and the\nground, enhancing the physical realism of the motion. Our approach can\nparallelly generate dance sequences of extremely long length, striking a\nbalance between global choreographic patterns and local motion quality and\nexpressiveness. Extensive experiments validate the efficacy of our method.\n","authors":["Ronghui Li","YuXiang Zhang","Yachao Zhang","Hongwen Zhang","Jie Guo","Yan Zhang","Yebin Liu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.10518v2.pdf","comment":"Accepted by CVPR2024, Project page:\n  https://li-ronghui.github.io/lodge"},{"id":"http://arxiv.org/abs/2403.17371v1","updated":"2024-03-26T04:11:47Z","published":"2024-03-26T04:11:47Z","title":"Robust Containment Queries over Collections of Rational Parametric\n  Curves via Generalized Winding Numbers","summary":"  Point containment queries for regions bound by watertight geometric surfaces,\ni.e. closed and without self-intersections, can be evaluated straightforwardly\nwith a number of well-studied algorithms. However, when such assumptions on\ndomain geometry are not met, these methods are theoretically unfounded at best\nand practically unusable at worst. More robust classification schemes utilize\ngeneralized winding numbers, a mathematical construction that is indifferent to\nimperfections in the often human-defined geometric model. We extend this\nmethodology to more general curved shapes, defining a robust containment query\nfor regions whose boundary elements are defined by a collection of rational\nparametric curves. In doing so, we devise an algorithm that is stable and\naccurate at arbitrary points in space, circumventing the typical difficulties\nfor queries that are arbitrarily close or coincident with the model. This is\ndone by reducing the generalized winding number problem to an integer winding\nnumber problem, which is solved by approximating each curve with a polyline\nthat provably has the same winding number at the point of interest. We\ndemonstrate the improvements in computational complexity granted by this method\nover conventional techniques, as well as the robustness induced by its\napplication\n","authors":["Jacob Spainhour","David Gunderman","Kenneth Weiss"],"pdf_url":"https://arxiv.org/pdf/2403.17371v1.pdf","comment":"14 pages, 18 figures"},{"id":"http://arxiv.org/abs/2303.02490v2","updated":"2024-03-26T03:41:26Z","published":"2023-03-04T20:08:57Z","title":"Diffusion Models Generate Images Like Painters: an Analytical Theory of\n  Outline First, Details Later","summary":"  How do diffusion generative models convert pure noise into meaningful images?\nIn a variety of pretrained diffusion models (including conditional latent space\nmodels like Stable Diffusion), we observe that the reverse diffusion process\nthat underlies image generation has the following properties: (i) individual\ntrajectories tend to be low-dimensional and resemble 2D `rotations'; (ii)\nhigh-variance scene features like layout tend to emerge earlier, while\nlow-variance details tend to emerge later; and (iii) early perturbations tend\nto have a greater impact on image content than later perturbations. To\nunderstand these phenomena, we derive and study a closed-form solution to the\nprobability flow ODE for a Gaussian distribution, which shows that the reverse\ndiffusion state rotates towards a gradually-specified target on the image\nmanifold. It also shows that generation involves first committing to an\noutline, and then to finer and finer details. We find that this solution\naccurately describes the initial phase of image generation for pretrained\nmodels, and can in principle be used to make image generation more efficient by\nskipping reverse diffusion steps. Finally, we use our solution to characterize\nthe image manifold in Stable Diffusion. Our viewpoint reveals an unexpected\nsimilarity between generation by GANs and diffusion and provides a conceptual\nlink between diffusion and image retrieval.\n","authors":["Binxu Wang","John J. Vastola"],"pdf_url":"https://arxiv.org/pdf/2303.02490v2.pdf","comment":"44 pages, 28 figures. A briefer version was presented at NeurIPS23\n  Workshop on Diffusion Models [arXiv:2311.10892]"},{"id":"http://arxiv.org/abs/2305.10474v3","updated":"2024-03-26T01:11:52Z","published":"2023-05-17T17:59:16Z","title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models","summary":"  Despite tremendous progress in generating high-quality images using diffusion\nmodels, synthesizing a sequence of animated frames that are both photorealistic\nand temporally coherent is still in its infancy. While off-the-shelf\nbillion-scale datasets for image generation are available, collecting similar\nvideo data of the same scale is still challenging. Also, training a video\ndiffusion model is computationally much more expensive than its image\ncounterpart. In this work, we explore finetuning a pretrained image diffusion\nmodel with video data as a practical solution for the video synthesis task. We\nfind that naively extending the image noise prior to video noise prior in video\ndiffusion leads to sub-optimal performance. Our carefully designed video noise\nprior leads to substantially better performance. Extensive experimental\nvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attains\nSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It\nalso achieves SOTA video generation quality on the small-scale UCF-101\nbenchmark with a $10\\times$ smaller model using significantly less computation\nthan the prior art.\n","authors":["Songwei Ge","Seungjun Nah","Guilin Liu","Tyler Poon","Andrew Tao","Bryan Catanzaro","David Jacobs","Jia-Bin Huang","Ming-Yu Liu","Yogesh Balaji"],"pdf_url":"https://arxiv.org/pdf/2305.10474v3.pdf","comment":"ICCV 2023. Project webpage:\n  https://research.nvidia.com/labs/dir/pyoco"},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time-consuming. Therefore, the challenging task of reconstructing\nvisually accurate HDR images from their Low Dynamic Range (LDR) counterparts is\ngaining attention in the vision research community. A major challenge in this\nresearch problem is the lack of datasets, which capture diverse scene\nconditions (e.g., lighting, shadows, weather, locations, landscapes, objects,\nhumans, buildings) and various image features (e.g., color, contrast,\nsaturation, hue, luminance, brightness, radiance). To address this gap, in this\npaper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic\nHDR images sampled from the GTA-V video game. We perform thorough evaluation of\nthe proposed dataset, which demonstrates significant qualitative and\nquantitative improvements of the state-of-the-art HDR image reconstruction\nmethods. Furthermore, we demonstrate the effectiveness of the proposed dataset\nand its impact on additional computer vision tasks including 3D human pose\nestimation, human body part segmentation, and holistic scene segmentation. The\ndataset, data collection pipeline, and evaluation code are available at:\nhttps://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"}]},"2024-03-27T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.18778v1","updated":"2024-03-27T17:26:42Z","published":"2024-03-27T17:26:42Z","title":"3P-LLM: Probabilistic Path Planning using Large Language Model for\n  Autonomous Robot Navigation","summary":"  Much worldly semantic knowledge can be encoded in large language models\n(LLMs). Such information could be of great use to robots that want to carry out\nhigh-level, temporally extended commands stated in natural language. However,\nthe lack of real-world experience that language models have is a key limitation\nthat makes it challenging to use them for decision-making inside a particular\nembodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo\nchatbot by OpenAI) for robotic path planning. The shortcomings of conventional\napproaches to managing complex environments and developing trustworthy plans\nfor shifting environmental conditions serve as the driving force behind the\nresearch. Due to the sophisticated natural language processing abilities of\nLLM, the capacity to provide effective and adaptive path-planning algorithms in\nreal-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is\nwell suited for path planning in robotics. In numerous simulated scenarios, the\nresearch compares the performance of GPT-3.5-turbo with that of\nstate-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*.\nWe observed that GPT-3.5-turbo is able to provide real-time path planning\nfeedback to the robot and outperforms its counterparts. This paper establishes\nthe foundation for LLM-powered path planning for robotic systems.\n","authors":["Ehsan Latif"],"pdf_url":"https://arxiv.org/pdf/2403.18778v1.pdf","comment":"Exploratory Study"},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2403.18764v1","updated":"2024-03-27T17:02:21Z","published":"2024-03-27T17:02:21Z","title":"Temporal Logic Formalisation of ISO 34502 Critical Scenarios: Modular\n  Construction with the RSS Safety Distance","summary":"  As the development of autonomous vehicles progresses, efficient safety\nassurance methods become increasingly necessary. Safety assurance methods such\nas monitoring and scenario-based testing call for formalisation of driving\nscenarios. In this paper, we develop a temporal-logic formalisation of an\nimportant class of critical scenarios in the ISO standard 34502. We use signal\ntemporal logic (STL) as a logical formalism. Our formalisation has two main\nfeatures: 1) modular composition of logical formulas for systematic and\ncomprehensive formalisation (following the compositional methodology of ISO\n34502); 2) use of the RSS distance for defining danger. We find our\nformalisation comes with few parameters to tune thanks to the RSS distance. We\nexperimentally evaluated our formalisation; using its results, we discuss the\nvalidity of our formalisation and its stability with respect to the choice of\nsome parameter values.\n","authors":["Jesse Reimann","Nico Mansion","James Haydon","Benjamin Bray","Agnishom Chattopadhyay","Sota Sato","Masaki Waga","Étienne André","Ichiro Hasuo","Naoki Ueda","Yosuke Yokoyama"],"pdf_url":"https://arxiv.org/pdf/2403.18764v1.pdf","comment":"12 pages, 4 figures, 5 tables. Accepted to SAC 2024"},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2403.18760v1","updated":"2024-03-27T16:58:20Z","published":"2024-03-27T16:58:20Z","title":"MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task\n  Planning with Open-Source Large Language Model","summary":"  In the realm of data-driven AI technology, the application of open-source\nlarge language models (LLMs) in robotic task planning represents a significant\nmilestone. Recent robotic task planning methods based on open-source LLMs\ntypically leverage vast task planning datasets to enhance models' planning\nabilities. While these methods show promise, they struggle with complex\nlong-horizon tasks, which require comprehending more context and generating\nlonger action sequences. This paper addresses this limitation by proposing\nMLDT, theMulti-Level Decomposition Task planning method. This method\ninnovatively decomposes tasks at the goal-level, task-level, and action-level\nto mitigate the challenge of complex long-horizon tasks. In order to enhance\nopen-source LLMs' planning abilities, we introduce a goal-sensitive corpus\ngeneration method to create high-quality training data and conduct instruction\ntuning on the generated corpus. Since the complexity of the existing datasets\nis not high enough, we construct a more challenging dataset, LongTasks, to\nspecifically evaluate planning ability on complex long-horizon tasks. We\nevaluate our method using various LLMs on four datasets in VirtualHome. Our\nresults demonstrate a significant performance enhancement in robotic task\nplanning, showcasing MLDT's effectiveness in overcoming the limitations of\nexisting methods based on open-source LLMs as well as its practicality in\ncomplex, real-world scenarios.\n","authors":["Yike Wu","Jiatao Zhang","Nan Hu","LanLing Tang","Guilin Qi","Jun Shao","Jie Ren","Wei Song"],"pdf_url":"https://arxiv.org/pdf/2403.18760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18721v1","updated":"2024-03-27T16:11:49Z","published":"2024-03-27T16:11:49Z","title":"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics\n  Lab Investigations","summary":"  Robot systems in education can leverage Large language models' (LLMs) natural\nlanguage understanding capabilities to provide assistance and facilitate\nlearning. This paper proposes a multimodal interactive robot (PhysicsAssistant)\nbuilt on YOLOv8 object detection, cameras, speech recognition, and chatbot\nusing LLM to provide assistance to students' physics labs. We conduct a user\nstudy on ten 8th-grade students to empirically evaluate the performance of\nPhysicsAssistant with a human expert. The Expert rates the assistants'\nresponses to student queries on a 0-4 scale based on Bloom's taxonomy to\nprovide educational support. We have compared the performance of\nPhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human\nexpert rating of both systems for factual understanding is the same. However,\nthe rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2\nand 2.6, respectively) is significantly higher than PhysicsAssistant (p <\n0.05). However, the response time of GPT-4 is significantly higher than\nPhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively\nlower response quality of PhysicsAssistant than GPT-4, it has shown potential\nfor being used as a real-time lab assistant to provide timely responses and can\noffload teachers' labor to assist with repetitive tasks. To the best of our\nknowledge, this is the first attempt to build such an interactive multimodal\nrobotic assistant for K-12 science (physics) education.\n","authors":["Ehsan Latif","Ramviyas Parasuraman","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.18721v1.pdf","comment":"Submitted to IEEE RO-MAN"},{"id":"http://arxiv.org/abs/2403.18695v1","updated":"2024-03-27T15:44:25Z","published":"2024-03-27T15:44:25Z","title":"An Efficient Risk-aware Branch MPC for Automated Driving that is Robust\n  to Uncertain Vehicle Behaviors","summary":"  One of the critical challenges in automated driving is ensuring safety of\nautomated vehicles despite the unknown behavior of the other vehicles. Although\nmotion prediction modules are able to generate a probability distribution\nassociated with various behavior modes, their probabilistic estimates are often\ninaccurate, thus leading to a possibly unsafe trajectory. To overcome this\nchallenge, we propose a risk-aware motion planning framework that appropriately\naccounts for the ambiguity in the estimated probability distribution. We\nformulate the risk-aware motion planning problem as a min-max optimization\nproblem and develop an efficient iterative method by incorporating a\nregularization term in the probability update step. Via extensive numerical\nstudies, we validate the convergence of our method and demonstrate its\nadvantages compared to the state-of-the-art approaches.\n","authors":["Luyao Zhang","George Pantazis","Shaohang Han","Sergio Grammatico"],"pdf_url":"https://arxiv.org/pdf/2403.18695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18692v1","updated":"2024-03-27T15:42:01Z","published":"2024-03-27T15:42:01Z","title":"Teaching Introductory HRI: UChicago Course \"Human-Robot Interaction:\n  Research and Practice\"","summary":"  In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction:\nResearch and Practice as a hands-on introduction to human-robot interaction\n(HRI) research for both undergraduate and graduate students at the University\nof Chicago. Since 2020, I have taught and refined this course each academic\nyear. Human-Robot Interaction: Research and Practice focuses on the core\nconcepts and cutting-edge research in the field of human-robot interaction\n(HRI), covering topics that include: nonverbal robot behavior, verbal robot\nbehavior, social dynamics, norms & ethics, collaboration & learning, group\ninteractions, applications, and future challenges of HRI. Course meetings\ninvolve students in the class leading discussions about cutting-edge\npeer-reviewed research HRI publications. Students also participate in a\nquarter-long collaborative research project, where they pursue an HRI research\nquestion that often involves conducing their own human-subjects research study\nwhere they recruit human subjects to interact with a robot. In this paper, I\ndetail the structure of the course and its learning goals as well as my\nreflections and student feedback on the course.\n","authors":["Sarah Sebo"],"pdf_url":"https://arxiv.org/pdf/2403.18692v1.pdf","comment":"4 pages, 2 tables, Presented at the Designing an Intro to HRI Course\n  Workshop at HRI 2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2311.15803v3","updated":"2024-03-27T15:05:19Z","published":"2023-11-27T13:25:47Z","title":"SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields","summary":"  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n","authors":["Quentin Herau","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2311.15803v3.pdf","comment":"Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/"},{"id":"http://arxiv.org/abs/2403.07091v2","updated":"2024-03-27T15:04:18Z","published":"2024-03-11T18:35:32Z","title":"Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym","summary":"  This paper explores policy-learning approaches in the context of sim-to-real\ntransfer for robotic manipulation using a TIAGo mobile manipulator, focusing on\ntwo state-of-art simulators, Isaac Gym and Isaac Sim, both developed by Nvidia.\nControl architectures are discussed, with a particular emphasis on achieving\ncollision-less movement in both simulation and the real environment. Presented\nresults demonstrate successful sim-to-real transfer, showcasing similar\nmovements executed by an RL-trained model in both simulated and real setups.\n","authors":["Jaume Albardaner","Alberto San Miguel","Néstor García","Magí Dalmau-Moreno"],"pdf_url":"https://arxiv.org/pdf/2403.07091v2.pdf","comment":"Accepted in ERF24 workshop \"Towards Efficient and Portable Robot\n  Learning for Real-World Settings\". To be published in Springer Proceedings in\n  Advanced Robotics"},{"id":"http://arxiv.org/abs/2403.18643v1","updated":"2024-03-27T14:46:54Z","published":"2024-03-27T14:46:54Z","title":"Sampling-Based Motion Planning with Online Racing Line Generation for\n  Autonomous Driving on Three-Dimensional Race Tracks","summary":"  Existing approaches to trajectory planning for autonomous racing employ\nsampling-based methods, generating numerous jerk-optimal trajectories and\nselecting the most favorable feasible trajectory based on a cost function\npenalizing deviations from an offline-calculated racing line. While successful\non oval tracks, these methods face limitations on complex circuits due to the\nsimplistic geometry of jerk-optimal edges failing to capture the complexity of\nthe racing line. Additionally, they only consider two-dimensional tracks,\npotentially neglecting or surpassing the actual dynamic potential. In this\npaper, we present a sampling-based local trajectory planning approach for\nautonomous racing that can maintain the lap time of the racing line even on\ncomplex race tracks and consider the race track's three-dimensional effects. In\nsimulative experiments, we demonstrate that our approach achieves lower lap\ntimes and improved utilization of dynamic limits compared to existing\napproaches. We also investigate the impact of online racing line generation, in\nwhich the time-optimal solution is planned from the current vehicle state for a\nlimited spatial horizon, in contrast to a closed racing line calculated\noffline. We show that combining the sampling-based planner with the online\nracing line generation can significantly reduce lap times in multi-vehicle\nscenarios.\n","authors":["Levent Ögretmen","Matthias Rowold","Boris Lohmann"],"pdf_url":"https://arxiv.org/pdf/2403.18643v1.pdf","comment":"8 pages, submitted to be published at the 35th IEEE Intelligent\n  Vehicles Symposium, June 2 - 5, 2024, Jeju Shinhwa World, Jeju Island, Korea"},{"id":"http://arxiv.org/abs/2403.18616v1","updated":"2024-03-27T14:30:56Z","published":"2024-03-27T14:30:56Z","title":"Will You Participate? Exploring the Potential of Robotics Competitions\n  on Human-centric Topics","summary":"  This paper presents findings from an exploratory needfinding study\ninvestigating the research current status and potential participation of the\ncompetitions on the robotics community towards four human-centric topics:\nsafety, privacy, explainability, and federated learning. We conducted a survey\nwith 34 participants across three distinguished European robotics consortia,\nnearly 60% of whom possessed over five years of research experience in\nrobotics. Our qualitative and quantitative analysis revealed that current\nmainstream robotic researchers prioritize safety and explainability, expressing\na greater willingness to invest in further research in these areas. Conversely,\nour results indicate that privacy and federated learning garner less attention\nand are perceived to have lower potential. Additionally, the study suggests a\nlack of enthusiasm within the robotics community for participating in\ncompetitions related to these topics. Based on these findings, we recommend\ntargeting other communities, such as the machine learning community, for future\ncompetitions related to these four human-centric topics.\n","authors":["Yuchong Zhang","Miguel Vasco","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.18616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.05362v2","updated":"2024-03-27T14:22:34Z","published":"2023-11-09T13:43:36Z","title":"Modeling and Control of Intrinsically Elasticity Coupled Soft-Rigid\n  Robots","summary":"  While much work has been done recently in the realm of model-based control of\nsoft robots and soft-rigid hybrids, most works examine robots that have an\ninherently serial structure. While these systems have been prevalent in the\nliterature, there is an increasing trend toward designing soft-rigid hybrids\nwith intrinsically coupled elasticity between various degrees of freedom. In\nthis work, we seek to address the issues of modeling and controlling such\nstructures, particularly when underactuated. We introduce several simple models\nfor elastic coupling, typical of those seen in these systems. We then propose a\ncontroller that compensates for the elasticity, and we prove its stability with\nLyapunov methods without relying on the elastic dominance assumption. This\ncontroller is applicable to the general class of underactuated soft robots.\nAfter evaluating the controller in simulated cases, we then develop a simple\nhardware platform to evaluate both the models and the controller. Finally,\nusing the hardware, we demonstrate a novel use case for underactuated,\nelastically coupled systems in \"sensorless\" force control.\n","authors":["Zach J. Patterson","Cosimo Della Santina","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2311.05362v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.03189v2","updated":"2024-03-27T14:21:13Z","published":"2023-11-06T15:28:24Z","title":"Safe Control for Soft-Rigid Robots with Self-Contact using Control\n  Barrier Functions","summary":"  Incorporating both flexible and rigid components in robot designs offers a\nunique solution to the limitations of traditional rigid robotics by enabling\nboth compliance and strength. This paper explores the challenges and solutions\nfor controlling soft-rigid hybrid robots, particularly addressing the issue of\nself-contact. Conventional control methods prioritize precise state tracking,\ninadvertently increasing the system's overall stiffness, which is not always\ndesirable in interactions with the environment or within the robot itself. To\naddress this, we investigate the application of Control Barrier Functions\n(CBFs) and High Order CBFs to manage self-contact scenarios in serially\nconnected soft-rigid hybrid robots. Through an analysis based on Piecewise\nConstant Curvature (PCC) kinematics, we establish CBFs within a classical\ncontrol framework for self-contact dynamics. Our methodology is rigorously\nevaluated in both simulation environments and physical hardware systems. The\nfindings demonstrate that our proposed control strategy effectively regulates\nself-contact in soft-rigid hybrid robotic systems, marking a significant\nadvancement in the field of robotics.\n","authors":["Zach J. Patterson","Wei Xiao","Emily Sologuren","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2311.03189v2.pdf","comment":"6 pages, 6 figures, submitted to IEEE Robosoft 2024 Conference"},{"id":"http://arxiv.org/abs/2309.10718v2","updated":"2024-03-27T13:54:19Z","published":"2023-09-19T16:02:23Z","title":"DRIVE: Data-driven Robot Input Vector Exploration","summary":"  An accurate motion model is a fundamental component of most autonomous\nnavigation systems. While much work has been done on improving model\nformulation, no standard protocol exists for gathering empirical data required\nto train models. In this work, we address this issue by proposing Data-driven\nRobot Input Vector Exploration (DRIVE), a protocol that enables characterizing\nuncrewed ground vehicles (UGVs) input limits and gathering empirical model\ntraining data. We also propose a novel learned slip approach outperforming\nsimilar acceleration learning approaches. Our contributions are validated\nthrough an extensive experimental evaluation, cumulating over 7 km and 1.8 h of\ndriving data over three distinct UGVs and four terrain types. We show that our\nprotocol offers increased predictive performance over common human-driven\ndata-gathering protocols. Furthermore, our protocol converges with 46 s of\ntraining data, almost four times less than the shortest human dataset gathering\nprotocol. We show that the operational limit for our model is reached in\nextreme slip conditions encountered on surfaced ice. DRIVE is an efficient way\nof characterizing UGV motion in its operational conditions. Our code and\ndataset are both available online at this link:\nhttps://github.com/norlab-ulaval/DRIVE.\n","authors":["Dominic Baril","Simon-Pierre Deschênes","Luc Coupal","Cyril Goffin","Julien Lépine","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2309.10718v2.pdf","comment":"8 pages, 7 figures, 1 table, accepted for publication at the 2024\n  IEEE International Conference on Robotics and Automation (ICRA2024),\n  Yokohama, Japan"},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2403.18524v1","updated":"2024-03-27T12:55:16Z","published":"2024-03-27T12:55:16Z","title":"Bridging the Gap: Regularized Reinforcement Learning for Improved\n  Classical Motion Planning with Safety Modules","summary":"  Classical navigation planners can provide safe navigation, albeit often\nsuboptimally and with hindered human norm compliance. ML-based, contemporary\nautonomous navigation algorithms can imitate more natural and humancompliant\nnavigation, but usually require large and realistic datasets and do not always\nprovide safety guarantees. We present an approach that leverages a classical\nalgorithm to guide reinforcement learning. This greatly improves the results\nand convergence rate of the underlying RL algorithm and requires no\nhuman-expert demonstrations to jump-start the process. Additionally, we\nincorporate a practical fallback system that can switch back to a classical\nplanner to ensure safety. The outcome is a sample efficient ML approach for\nmobile navigation that builds on classical algorithms, improves them to ensure\nhuman compliance, and guarantees safety.\n","authors":["Elias Goldsztejn","Ronen I. Brafman"],"pdf_url":"https://arxiv.org/pdf/2403.18524v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.11617v2","updated":"2024-03-27T12:49:06Z","published":"2024-03-18T09:50:05Z","title":"Frontier-Based Exploration for Multi-Robot Rendezvous in\n  Communication-Restricted Unknown Environments","summary":"  Multi-robot rendezvous and exploration are fundamental challenges in the\ndomain of mobile robotic systems. This paper addresses multi-robot rendezvous\nwithin an initially unknown environment where communication is only possible\nafter the rendezvous. Traditionally, exploration has been focused on rapidly\nmapping the environment, often leading to suboptimal rendezvous performance in\nlater stages. We adapt a standard frontier-based exploration technique to\nintegrate exploration and rendezvous into a unified strategy, with a mechanism\nthat allows robots to re-visit previously explored regions thus enhancing\nrendezvous opportunities. We validate our approach in 3D realistic simulations\nusing ROS, showcasing its effectiveness in achieving faster rendezvous times\ncompared to exploration strategies.\n","authors":["Mauro Tellaroli","Matteo Luperto","Michele Antonazzi","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17392v2","updated":"2024-03-27T12:10:00Z","published":"2024-03-26T05:23:12Z","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in\n  unknown obstructed soft terrain","summary":"  Navigating multi-robot systems in complex terrains has always been a\nchallenging task. This is due to the inherent limitations of traditional robots\nin collision avoidance, adaptation to unknown environments, and sustained\nenergy efficiency. In order to overcome these limitations, this research\nproposes a solution by integrating living insects with miniature electronic\ncontrollers to enable robotic-like programmable control, and proposing a novel\ncontrol algorithm for swarming. Although these creatures, called cyborg\ninsects, have the ability to instinctively avoid collisions with neighbors and\nobstacles while adapting to complex terrains, there is a lack of literature on\nthe control of multi-cyborg systems. This research gap is due to the difficulty\nin coordinating the movements of a cyborg system under the presence of insects'\ninherent individual variability in their reactions to control input. In\nresponse to this issue, we propose a novel swarm navigation algorithm\naddressing these challenges. The effectiveness of the algorithm is demonstrated\nthrough an experimental validation in which a cyborg swarm was successfully\nnavigated through an unknown sandy field with obstacles and hills. This\nresearch contributes to the domain of swarm robotics and showcases the\npotential of integrating biological organisms with robotics and control theory\nto create more intelligent autonomous systems with real-world applications.\n","authors":["Yang Bai","Phuoc Thanh Tran Ngoc","Huu Duoc Nguyen","Duc Long Le","Quang Huy Ha","Kazuki Kai","Yu Xiang See To","Yaosheng Deng","Jie Song","Naoki Wakamiya","Hirotaka Sato","Masaki Ogura"],"pdf_url":"https://arxiv.org/pdf/2403.17392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18459v1","updated":"2024-03-27T11:18:01Z","published":"2024-03-27T11:18:01Z","title":"CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration","summary":"  Assembly processes involving humans and robots are challenging scenarios\nbecause the individual activities and access to shared workspace have to be\ncoordinated. Fixed robot programs leave no room to diverge from a fixed\nprotocol. Working on such a process can be stressful for the user and lead to\nineffective behavior or failure. We propose a novel approach of online\nconstraint-based scheduling in a reactive execution control framework\nfacilitating behavior trees called CoBOS. This allows the robot to adapt to\nuncertain events such as delayed activity completions and activity selection\n(by the human). The user will experience less stress as the robotic coworkers\nadapt their behavior to best complement the human-selected activities to\ncomplete the common task. In addition to the improved working conditions, our\nalgorithm leads to increased efficiency, even in highly uncertain scenarios. We\nevaluate our algorithm using a probabilistic simulation study with 56000\nexperiments. We outperform all baselines by a margin of 4-10%. Initial real\nrobot experiments using a Franka Emika Panda robot and human tracking based on\nHTC Vive VR gloves look promising.\n","authors":["Marina Ionova","Jan Kristof Behrens"],"pdf_url":"https://arxiv.org/pdf/2403.18459v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.18456v1","updated":"2024-03-27T11:16:04Z","published":"2024-03-27T11:16:04Z","title":"Inverse kinematics learning of a continuum manipulator using limited\n  real time data","summary":"  Data driven control of a continuum manipulator requires a lot of data for\ntraining but generating sufficient amount of real time data is not cost\nefficient. Random actuation of the manipulator can also be unsafe sometimes.\nMeta learning has been used successfully to adapt to a new environment. Hence,\nthis paper tries to solve the above mentioned problem using meta learning. We\nconsider two cases for that. First, this paper proposes a method to use\nsimulation data for training the model using MAML(Model-Agnostic\nMeta-Learning). Then, it adapts to the real world using gradient steps.\nSecondly,if the simulation model is not available or difficult to formulate,\nthen we propose a CGAN(Conditional Generative adversial network)-MAML based\nmethod for it. The model is trained using a small amount of real time data and\naugmented data for different loading conditions. Then, adaptation is done in\nthe real environment. It has been found out from the experiments that the\nrelative positioning error for both the cases are below 3%. The proposed models\nare experimentally verified on a real continuum manipulator.\n","authors":["Alok Ranjan Sahoo","Pavan Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2403.18456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2311.08787v2","updated":"2024-03-27T11:06:49Z","published":"2023-11-15T08:59:05Z","title":"Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation\n  in cluttered environments","summary":"  In fields such as mining, search and rescue, and archaeological exploration,\nensuring real-time, collision-free navigation of robots in confined, cluttered\nenvironments is imperative. Despite the value of established path planning\nalgorithms, they often face challenges in convergence rates and handling\ndynamic infeasibilities. Alternative techniques like collision cones struggle\nto accurately represent complex obstacle geometries. This paper introduces a\nnovel category of control barrier functions, known as Polygonal Cone Control\nBarrier Function (PolyC2BF), which addresses overestimation and computational\ncomplexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming\n(QP) problem, proves effective in facilitating collision-free movement of\nmultiple robots in complex environments. The efficacy of this approach is\nfurther demonstrated through PyBullet simulations on quadruped (unicycle\nmodel), and crazyflie 2.1 (quadrotor model) in cluttered environments.\n","authors":["Manan Tayal","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2311.08787v2.pdf","comment":"6 Pages, 6 Figures. Accepted at European Control Conference (ECC)\n  2024. arXiv admin note: text overlap with arXiv:2303.15871"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18413v1","updated":"2024-03-27T10:01:14Z","published":"2024-03-27T10:01:14Z","title":"HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion\n  Planning Algorithm for Hybrid Systems","summary":"  This paper proposes a bidirectional rapidly-exploring random trees (RRT)\nalgorithm to solve the motion planning problem for hybrid systems. The proposed\nalgorithm, called HyRRT-Connect, propagates in both forward and backward\ndirections in hybrid time until an overlap between the forward and backward\npropagation results is detected. Then, HyRRT-Connect constructs a motion plan\nthrough the reversal and concatenation of functions defined on hybrid time\ndomains, ensuring the motion plan thoroughly satisfies the given hybrid\ndynamics. To address the potential discontinuity along the flow caused by\ntolerating some distance between the forward and backward partial motion plans,\nwe reconstruct the backward partial motion plan by a forward-in-hybrid-time\nsimulation from the final state of the forward partial motion plan. By applying\nthe reversed input of the backward partial motion plan, the reconstruction\nprocess effectively eliminates the discontinuity and ensures that as the\ntolerance distance decreases to zero, the distance between the endpoint of the\nreconstructed motion plan and the final state set approaches zero. The proposed\nalgorithm is applied to an actuated bouncing ball example and a walking robot\nexample so as to highlight its generality and computational improvement.\n","authors":["Nan Wang","Ricardo G. Sanfelice"],"pdf_url":"https://arxiv.org/pdf/2403.18413v1.pdf","comment":"Accepted by the 8th IFAC International Conference on Analysis and\n  Design of Hybrid Systems (ADHS 2024)"},{"id":"http://arxiv.org/abs/2309.12857v2","updated":"2024-03-27T09:26:56Z","published":"2023-09-22T13:31:29Z","title":"Risk-aware Control for Robots with Non-Gaussian Belief Spaces","summary":"  This paper addresses the problem of safety-critical control of autonomous\nrobots, considering the ubiquitous uncertainties arising from unmodeled\ndynamics and noisy sensors. To take into account these uncertainties,\nprobabilistic state estimators are often deployed to obtain a belief over\npossible states. Namely, Particle Filters (PFs) can handle arbitrary\nnon-Gaussian distributions in the robot's state. In this work, we define the\nbelief state and belief dynamics for continuous-discrete PFs and construct safe\nsets in the underlying belief space. We design a controller that provably keeps\nthe robot's belief state within this safe set. As a result, we ensure that the\nrisk of the unknown robot's state violating a safety specification, such as\navoiding a dangerous area, is bounded. We provide an open-source implementation\nas a ROS2 package and evaluate the solution in simulations and hardware\nexperiments involving high-dimensional belief spaces.\n","authors":["Matti Vahs","Jana Tumova"],"pdf_url":"https://arxiv.org/pdf/2309.12857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14864v2","updated":"2024-03-27T09:24:55Z","published":"2024-03-21T22:18:59Z","title":"Learning Quadruped Locomotion Using Differentiable Simulation","summary":"  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n","authors":["Yunlong Song","Sangbae Kim","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.14864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06494v2","updated":"2024-03-27T09:21:50Z","published":"2023-09-12T18:07:27Z","title":"Non-smooth Control Barrier Functions for Stochastic Dynamical Systems","summary":"  Uncertainties arising in various control systems, such as robots that are\nsubject to unknown disturbances or environmental variations, pose significant\nchallenges for ensuring system safety, such as collision avoidance. At the same\ntime, safety specifications are getting more and more complex, e.g., by\ncomposing multiple safety objectives through Boolean operators resulting in\nnon-smooth descriptions of safe sets. Control Barrier Functions (CBFs) have\nemerged as a control technique to provably guarantee system safety. In most\nsettings, they rely on an assumption of having deterministic dynamics and\nsmooth safe sets. This paper relaxes these two assumptions by extending CBFs to\nencompass control systems with stochastic dynamics and safe sets defined by\nnon-smooth functions. By explicitly considering the stochastic nature of system\ndynamics and accommodating complex safety specifications, our method enables\nthe design of safe control strategies in uncertain and complex systems. We\nprovide formal guarantees on the safety of the system by leveraging the\ntheoretical foundations of stochastic CBFs and non-smooth safe sets. Numerical\nsimulations demonstrate the effectiveness of the approach in various scenarios.\n","authors":["Matti Vahs","Jana Tumova"],"pdf_url":"https://arxiv.org/pdf/2309.06494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18376v1","updated":"2024-03-27T09:15:45Z","published":"2024-03-27T09:15:45Z","title":"Extensible Hook System for Rendesvouz and Docking of a Cubesat Swarm","summary":"  The use of cubesat swarms is being proposed for different missions where\ncooperation between satellites is required. Commonly, the cube swarm requires\nformation flight and even rendezvous and docking, which are very challenging\ntasks since they required more energy and the use of advanced guidance,\nnavigation and control techniques. In this paper, we propose the use of an\nextensible hook system to mitigate these drawbacks,i.e. it allows to save fuel\nand reduce the system complexity by including techniques that have been\npreviously demonstrated on Earth. This system is based on a scissor boom\nstructure, which could reach up to five meters for a 4U dimension, including\nthree degrees of freedom to place the end effector at any pose within the\nsystem workspace. We simulated the dynamic behaviour of a cubesat with the\nproposed system, demonstrating the required power for a 16U cubesat equipped\nwith one extensible hook system is considered acceptable according to the\ncurrent state of the art actuators.\n","authors":["Carlos J. Pérez-del-Pulgar","Antonio López-Palomeque","Jesús Juli","Matteo Madi"],"pdf_url":"https://arxiv.org/pdf/2403.18376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18358v1","updated":"2024-03-27T08:49:30Z","published":"2024-03-27T08:49:30Z","title":"Imaging radar and LiDAR image translation for 3-DOF extrinsic\n  calibration","summary":"  The integration of sensor data is crucial in the field of robotics to take\nfull advantage of the various sensors employed. One critical aspect of this\nintegration is determining the extrinsic calibration parameters, such as the\nrelative transformation, between each sensor. The use of data fusion between\ncomplementary sensors, such as radar and LiDAR, can provide significant\nbenefits, particularly in harsh environments where accurate depth data is\nrequired. However, noise included in radar sensor data can make the estimation\nof extrinsic calibration challenging. To address this issue, we present a novel\nframework for the extrinsic calibration of radar and LiDAR sensors, utilizing\nCycleGAN as amethod of image-to-image translation. Our proposed method employs\ntranslating radar bird-eye-view images into LiDAR-style images to estimate the\n3-DOF extrinsic parameters. The use of image registration techniques, as well\nas deskewing based on sensor odometry and B-spline interpolation, is employed\nto address the rolling shutter effect commonly present in spinning sensors. Our\nmethod demonstrates a notable improvement in extrinsic calibration compared to\nfilter-based methods using the MulRan dataset.\n","authors":["Sangwoo Jung","Hyesu Jang","Minwoo Jung","Ayoung Kim","Myung-Hwan Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17072v3","updated":"2024-03-27T07:04:58Z","published":"2023-10-26T00:28:37Z","title":"MMP++: Motion Manifold Primitives with Parametric Curve Models","summary":"  Motion Manifold Primitives (MMP), a manifold-based approach for encoding\nbasic motion skills, can produce diverse trajectories, enabling the system to\nadapt to unseen constraints. Nonetheless, we argue that current MMP models lack\ncrucial functionalities of movement primitives, such as temporal and via-points\nmodulation, found in traditional approaches. This shortfall primarily stems\nfrom MMP's reliance on discrete-time trajectories. To overcome these\nlimitations, we introduce Motion Manifold Primitives++ (MMP++), a new model\nthat integrates the strengths of both MMP and traditional methods by\nincorporating parametric curve representations into the MMP framework.\nFurthermore, we identify a significant challenge with MMP++: performance\ndegradation due to geometric distortions in the latent space, meaning that\nsimilar motions are not closely positioned. To address this, Isometric Motion\nManifold Primitives++ (IMMP++) is proposed to ensure the latent space\naccurately preserves the manifold's geometry. Our experimental results across\nvarious applications, including 2-DoF planar motions, 7-DoF robot arm motions,\nand SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing\nmethods in trajectory generation tasks, achieving substantial improvements in\nsome cases. Moreover, they enable the modulation of latent coordinates and\nvia-points, thereby allowing efficient online adaptation to dynamic\nenvironments.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17072v3.pdf","comment":"12 pages. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2403.17367v2","updated":"2024-03-27T06:16:06Z","published":"2024-03-26T04:05:01Z","title":"RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment","summary":"  Combining the mobility of legged robots with the manipulation skills of arms\nhas the potential to significantly expand the operational range and enhance the\ncapabilities of robotic systems in performing various mobile manipulation\ntasks. Existing approaches are confined to imprecise six degrees of freedom\n(DoF) manipulation and possess a limited arm workspace. In this paper, we\npropose a novel framework, RoboDuet, which employs two collaborative policies\nto realize locomotion and manipulation simultaneously, achieving whole-body\ncontrol through interactions between each other. Surprisingly, going beyond the\nlarge-range pose tracking, we find that the two-policy framework may enable\ncross-embodiment deployment such as using different quadrupedal robots or other\narms. Our experiments demonstrate that the policies trained through RoboDuet\ncan accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot\nexchange of legged robots, and can be deployed in the real world to perform\nvarious mobile manipulation tasks. Our project page with demo videos is at\nhttps://locomanip-duet.github.io .\n","authors":["Guoping Pan","Qingwei Ben","Zhecheng Yuan","Guangqi Jiang","Yandong Ji","Jiangmiao Pang","Houde Liu","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18259v1","updated":"2024-03-27T05:15:48Z","published":"2024-03-27T05:15:48Z","title":"RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based\n  3D Keypoint Generation","summary":"  Estimating robot pose and joint angles is significant in advanced robotics,\nenabling applications like robot collaboration and online hand-eye\ncalibration.However, the introduction of unknown joint angles makes prediction\nmore complex than simple robot pose estimation, due to its higher\ndimensionality.Previous methods either regress 3D keypoints directly or utilise\na render&compare strategy. These approaches often falter in terms of\nperformance or efficiency and grapple with the cross-camera gap problem.This\npaper presents a novel framework that bifurcates the high-dimensional\nprediction task into two manageable subtasks: 2D keypoints detection and\nlifting 2D keypoints to 3D. This separation promises enhanced performance\nwithout sacrificing the efficiency innate to keypoint-based techniques.A vital\ncomponent of our method is the lifting of 2D keypoints to 3D keypoints. Common\ndeterministic regression methods may falter when faced with uncertainties from\n2D detection errors or self-occlusions.Leveraging the robust modeling potential\nof diffusion models, we reframe this issue as a conditional 3D keypoints\ngeneration task. To bolster cross-camera adaptability, we introduce\ntheNormalised Camera Coordinate Space (NCCS), ensuring alignment of estimated\n2D keypoints across varying camera intrinsics.Experimental results demonstrate\nthat the proposed method outperforms the state-of-the-art render\\&compare\nmethod and achieves higher inference speed.Furthermore, the tests accentuate\nour method's robust cross-camera generalisation capabilities.We intend to\nrelease both the dataset and code in https://nimolty.github.io/Robokeygen/\n","authors":["Yang Tian","Jiyao Zhang","Guowei Huang","Bin Wang","Ping Wang","Jiangmiao Pang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18259v1.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.18256v1","updated":"2024-03-27T04:56:48Z","published":"2024-03-27T04:56:48Z","title":"Manipulating Neural Path Planners via Slight Perturbations","summary":"  Data-driven neural path planners are attracting increasing interest in the\nrobotics community. However, their neural network components typically come as\nblack boxes, obscuring their underlying decision-making processes. Their\nblack-box nature exposes them to the risk of being compromised via the\ninsertion of hidden malicious behaviors. For example, an attacker may hide\nbehaviors that, when triggered, hijack a delivery robot by guiding it to a\nspecific (albeit wrong) destination, trapping it in a predefined region, or\ninducing unnecessary energy expenditure by causing the robot to repeatedly\ncircle a region. In this paper, we propose a novel approach to specify and\ninject a range of hidden malicious behaviors, known as backdoors, into neural\npath planners. Our approach provides a concise but flexible way to define these\nbehaviors, and we show that hidden behaviors can be triggered by slight\nperturbations (e.g., inserting a tiny unnoticeable object), that can\nnonetheless significantly compromise their integrity. We also discuss potential\ntechniques to identify these backdoors aimed at alleviating such risks. We\ndemonstrate our approach on both sampling-based and search-based neural path\nplanners.\n","authors":["Zikang Xiong","Suresh Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2403.18256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11542v2","updated":"2024-03-27T04:39:26Z","published":"2024-01-21T16:51:07Z","title":"Nigel -- Mechatronic Design and Robust Sim2Real Control of an\n  Over-Actuated Autonomous Vehicle","summary":"  Simulation to reality (sim2real) transfer from a dynamics and controls\nperspective usually involves re-tuning or adapting the designed algorithms to\nsuit real-world operating conditions, which often violates the performance\nguarantees established originally. This work presents a generalizable framework\nfor achieving reliable sim2real transfer of autonomy-oriented control systems\nusing multi-model multi-objective robust optimal control synthesis, which lends\nwell to uncertainty handling and disturbance rejection with theoretical\nguarantees. Particularly, this work is centered around a novel\nactuation-redundant scaled autonomous vehicle called Nigel, with independent\nall-wheel drive and independent all-wheel steering architecture, whose enhanced\nconfiguration space bodes well for robust control applications. To this end, we\npresent the mechatronic design, dynamics modeling, parameter identification,\nand robust stabilizing as well as tracking control of Nigel using the proposed\nframework, with exhaustive experimentation and benchmarking in simulation as\nwell as real-world settings.\n","authors":["Chinmay Vilas Samak","Tanmay Vilas Samak","Javad Mohammadpour Velni","Venkat Narayan Krovi"],"pdf_url":"https://arxiv.org/pdf/2401.11542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08100v3","updated":"2024-03-27T04:00:07Z","published":"2023-11-14T11:53:24Z","title":"PPAD: Iterative Interactions of Prediction and Planning for End-to-end\n  Autonomous Driving","summary":"  We present a new interaction mechanism of prediction and planning for\nend-to-end autonomous driving, called PPAD (Iterative Interaction of Prediction\nand Planning Autonomous Driving), which considers the timestep-wise interaction\nto better integrate prediction and planning. An ego vehicle performs motion\nplanning at each timestep based on the trajectory prediction of surrounding\nagents (e.g., vehicles and pedestrians) and its local road conditions. Unlike\nexisting end-to-end autonomous driving frameworks, PPAD models the interactions\namong ego, agents, and the dynamic environment in an autoregressive manner by\ninterleaving the Prediction and Planning processes at every timestep, instead\nof a single sequential process of prediction followed by planning.\nSpecifically, we design ego-to-agent, ego-to-map, and ego-to-BEV interaction\nmechanisms with hierarchical dynamic key objects attention to better model the\ninteractions. The experiments on the nuScenes benchmark show that our approach\noutperforms state-of-the-art methods.\n","authors":["Zhili Chen","Maosheng Ye","Shuangjie Xu","Tongyi Cao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08100v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01616v2","updated":"2024-03-27T03:56:35Z","published":"2023-12-04T04:14:09Z","title":"SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation\n  System","summary":"  Accuracy and computational efficiency are the most important metrics to\nVisual Inertial Navigation System (VINS). The existing VINS algorithms with\neither high accuracy or low computational complexity, are difficult to provide\nthe high precision localization in resource-constrained devices. To this end,\nwe propose a novel filter-based VINS framework named SchurVINS, which could\nguarantee both high accuracy by building a complete residual model and low\ncomputational complexity with Schur complement. Technically, we first formulate\nthe full residual model where Gradient, Hessian and observation covariance are\nexplicitly modeled. Then Schur complement is employed to decompose the full\nmodel into ego-motion residual model and landmark residual model. Finally,\nExtended Kalman Filter (EKF) update is implemented in these two models with\nhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our method\nnotably outperforms state-of-the-art (SOTA) methods in both accuracy and\ncomputational complexity. The experimental code of SchurVINS is available at\nhttps://github.com/bytedance/SchurVINS.\n","authors":["Yunfei Fan","Tianyu Zhao","Guidong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18236v1","updated":"2024-03-27T03:53:30Z","published":"2024-03-27T03:53:30Z","title":"Multi-AGV Path Planning Method via Reinforcement Learning and Particle\n  Filters","summary":"  The Reinforcement Learning (RL) algorithm, renowned for its robust learning\ncapability and search stability, has garnered significant attention and found\nextensive application in Automated Guided Vehicle (AGV) path planning. However,\nRL planning algorithms encounter challenges stemming from the substantial\nvariance of neural networks caused by environmental instability and significant\nfluctuations in system structure. These challenges manifest in slow convergence\nspeed and low learning efficiency. To tackle this issue, this paper presents\nthe Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which\nincorporates the Particle Filter (PF) into multi-AGV reinforcement learning\npath planning. The PF-DDQN method leverages the imprecise weight values of the\nnetwork as state values to formulate the state space equation. Through the\niterative fusion process of neural networks and particle filters, the DDQN\nmodel is optimized to acquire the optimal true weight values, thus enhancing\nthe algorithm's efficiency. The proposed method's effectiveness and superiority\nare validated through numerical simulations. Overall, the simulation results\ndemonstrate that the proposed algorithm surpasses the traditional DDQN\nalgorithm in terms of path planning superiority and training time indicators by\n92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses\nthe challenges encountered by RL planning algorithms in AGV path planning. By\nintegrating the Particle Filter and optimizing the DDQN model, the proposed\nmethod achieves enhanced efficiency and outperforms the traditional DDQN\nalgorithm in terms of path planning superiority and training time indicators.\n","authors":["Shao Shuo"],"pdf_url":"https://arxiv.org/pdf/2403.18236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18222v1","updated":"2024-03-27T03:19:36Z","published":"2024-03-27T03:19:36Z","title":"Uncertainty-Aware Deployment of Pre-trained Language-Conditioned\n  Imitation Learning Policies","summary":"  Large-scale robotic policies trained on data from diverse tasks and robotic\nplatforms hold great promise for enabling general-purpose robots; however,\nreliable generalization to new environment conditions remains a major\nchallenge. Toward addressing this challenge, we propose a novel approach for\nuncertainty-aware deployment of pre-trained language-conditioned imitation\nlearning agents. Specifically, we use temperature scaling to calibrate these\nmodels and exploit the calibrated model to make uncertainty-aware decisions by\naggregating the local information of candidate actions. We implement our\napproach in simulation using three such pre-trained models, and showcase its\npotential to significantly enhance task completion rates. The accompanying code\nis accessible at the link:\nhttps://github.com/BobWu1998/uncertainty_quant_all.git\n","authors":["Bo Wu","Bruce D. Lee","Kostas Daniilidis","Bernadette Bucher","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2403.18222v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.04181v2","updated":"2024-03-27T02:51:24Z","published":"2023-10-06T11:53:04Z","title":"DiffPrompter: Differentiable Implicit Visual Prompts for\n  Semantic-Segmentation in Adverse Conditions","summary":"  Semantic segmentation in adverse weather scenarios is a critical task for\nautonomous driving systems. While foundation models have shown promise, the\nneed for specialized adaptors becomes evident for handling more challenging\nscenarios. We introduce DiffPrompter, a novel differentiable visual and latent\nprompting mechanism aimed at expanding the learning capabilities of existing\nadaptors in foundation models. Our proposed $\\nabla$HFC image processing block\nexcels particularly in adverse weather conditions, where conventional methods\noften fall short. Furthermore, we investigate the advantages of jointly\ntraining visual and latent prompts, demonstrating that this combined approach\nsignificantly enhances performance in out-of-distribution scenarios. Our\ndifferentiable visual prompts leverage parallel and series architectures to\ngenerate prompts, effectively improving object segmentation tasks in adverse\nconditions. Through a comprehensive series of experiments and evaluations, we\nprovide empirical evidence to support the efficacy of our approach. Project\npage at https://diffprompter.github.io.\n","authors":["Sanket Kalwar","Mihir Ungarala","Shruti Jain","Aaron Monis","Krishna Reddy Konda","Sourav Garg","K Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.04181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18212v1","updated":"2024-03-27T02:46:09Z","published":"2024-03-27T02:46:09Z","title":"Preference-Based Planning in Stochastic Environments: From\n  Partially-Ordered Temporal Goals to Most Preferred Policies","summary":"  Human preferences are not always represented via complete linear orders: It\nis natural to employ partially-ordered preferences for expressing incomparable\noutcomes. In this work, we consider decision-making and probabilistic planning\nin stochastic systems modeled as Markov decision processes (MDPs), given a\npartially ordered preference over a set of temporally extended goals.\nSpecifically, each temporally extended goal is expressed using a formula in\nLinear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially\nordered preference, we introduce order theory to map a preference over temporal\ngoals to a preference over policies for the MDP. Accordingly, a most preferred\npolicy under a stochastic ordering induces a stochastic nondominated\nprobability distribution over the finite paths in the MDP. To synthesize a most\npreferred policy, our technical approach includes two key steps. In the first\nstep, we develop a procedure to transform a partially ordered preference over\ntemporal goals into a computational model, called preference automaton, which\nis a semi-automaton with a partial order over acceptance conditions. In the\nsecond step, we prove that finding a most preferred policy is equivalent to\ncomputing a Pareto-optimal policy in a multi-objective MDP that is constructed\nfrom the original MDP, the preference automaton, and the chosen stochastic\nordering relation. Throughout the paper, we employ running examples to\nillustrate the proposed preference specification and solution approaches. We\ndemonstrate the efficacy of our algorithm using these examples, providing\ndetailed analysis, and then discuss several potential future directions.\n","authors":["Hazhar Rahmani","Abhishek N. Kulkarni","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18212v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.12267"},{"id":"http://arxiv.org/abs/2403.18209v1","updated":"2024-03-27T02:41:52Z","published":"2024-03-27T02:41:52Z","title":"Long and Short-Term Constraints Driven Safe Reinforcement Learning for\n  Autonomous Driving","summary":"  Reinforcement learning (RL) has been widely used in decision-making tasks,\nbut it cannot guarantee the agent's safety in the training process due to the\nrequirements of interaction with the environment, which seriously limits its\nindustrial applications such as autonomous driving. Safe RL methods are\ndeveloped to handle this issue by constraining the expected safety violation\ncosts as a training objective, but they still permit unsafe state occurrence,\nwhich is unacceptable in autonomous driving tasks. Moreover, these methods are\ndifficult to achieve a balance between the cost and return expectations, which\nleads to learning performance degradation for the algorithms. In this paper, we\npropose a novel algorithm based on the long and short-term constraints (LSTC)\nfor safe RL. The short-term constraint aims to guarantee the short-term state\nsafety that the vehicle explores, while the long-term constraint ensures the\noverall safety of the vehicle throughout the decision-making process. In\naddition, we develop a safe RL method with dual-constraint optimization based\non the Lagrange multiplier to optimize the training process for end-to-end\nautonomous driving. Comprehensive experiments were conducted on the MetaDrive\nsimulator. Experimental results demonstrate that the proposed method achieves\nhigher safety in continuous state and action tasks, and exhibits higher\nexploration performance in long-distance decision-making tasks compared with\nstate-of-the-art methods.\n","authors":["Xuemin Hu","Pan Chen","Yijun Wen","Bo Tang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17320v2","updated":"2024-03-27T02:39:30Z","published":"2024-03-26T02:02:35Z","title":"Leveraging Symmetry in RL-based Legged Locomotion Control","summary":"  Model-free reinforcement learning is a promising approach for autonomously\nsolving challenging robotics control problems, but faces exploration difficulty\nwithout information of the robot's kinematics and dynamics morphology. The\nunder-exploration of multiple modalities with symmetric states leads to\nbehaviors that are often unnatural and sub-optimal. This issue becomes\nparticularly pronounced in the context of robotic systems with morphological\nsymmetries, such as legged robots for which the resulting asymmetric and\naperiodic behaviors compromise performance, robustness, and transferability to\nreal hardware. To mitigate this challenge, we can leverage symmetry to guide\nand improve the exploration in policy learning via equivariance/invariance\nconstraints. In this paper, we investigate the efficacy of two approaches to\nincorporate symmetry: modifying the network architectures to be strictly\nequivariant/invariant, and leveraging data augmentation to approximate\nequivariant/invariant actor-critics. We implement the methods on challenging\nloco-manipulation and bipedal locomotion tasks and compare with an\nunconstrained baseline. We find that the strictly equivariant policy\nconsistently outperforms other methods in sample efficiency and task\nperformance in simulation. In addition, symmetry-incorporated approaches\nexhibit better gait quality, higher robustness and can be deployed zero-shot in\nreal-world experiments.\n","authors":["Zhi Su","Xiaoyu Huang","Daniel Ordoñez-Apraez","Yunfei Li","Zhongyu Li","Qiayuan Liao","Giulio Turrisi","Massimiliano Pontil","Claudio Semini","Yi Wu","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2403.17320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18207v1","updated":"2024-03-27T02:35:36Z","published":"2024-03-27T02:35:36Z","title":"Road Obstacle Detection based on Unknown Objectness Scores","summary":"  The detection of unknown traffic obstacles is vital to ensure safe autonomous\ndriving. The standard object-detection methods cannot identify unknown objects\nthat are not included under predefined categories. This is because\nobject-detection methods are trained to assign a background label to pixels\ncorresponding to the presence of unknown objects. To address this problem, the\npixel-wise anomaly-detection approach has attracted increased research\nattention. Anomaly-detection techniques, such as uncertainty estimation and\nperceptual difference from reconstructed images, make it possible to identify\npixels of unknown objects as out-of-distribution (OoD) samples. However, when\napplied to images with many unknowns and complex components, such as driving\nscenes, these methods often exhibit unstable performance. The purpose of this\nstudy is to achieve stable performance for detecting unknown objects by\nincorporating the object-detection fashions into the pixel-wise anomaly\ndetection methods. To achieve this goal, we adopt a semantic-segmentation\nnetwork with a sigmoid head that simultaneously provides pixel-wise anomaly\nscores and objectness scores. Our experimental results show that the objectness\nscores play an important role in improving the detection performance. Based on\nthese results, we propose a novel anomaly score by integrating these two\nscores, which we term as unknown objectness score. Quantitative evaluations\nshow that the proposed method outperforms state-of-the-art methods when applied\nto the publicly available datasets.\n","authors":["Chihiro Noguchi","Toshiaki Ohgushi","Masao Yamanaka"],"pdf_url":"https://arxiv.org/pdf/2403.18207v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.18206v1","updated":"2024-03-27T02:33:15Z","published":"2024-03-27T02:33:15Z","title":"Sailing Through Point Clouds: Safe Navigation Using Point Cloud Based\n  Control Barrier Functions","summary":"  The capability to navigate safely in an unstructured environment is crucial\nwhen deploying robotic systems in real-world scenarios. Recently, control\nbarrier function (CBF) based approaches have been highly effective in\nsynthesizing safety-critical controllers. In this work, we propose a novel\nCBF-based local planner comprised of two components: Vessel and Mariner. The\nVessel is a novel scaling factor based CBF formulation that synthesizes CBFs\nusing only point cloud data. The Mariner is a CBF-based preview control\nframework that is used to mitigate getting stuck in spurious equilibria during\nnavigation. To demonstrate the efficacy of our proposed approach, we first\ncompare the proposed point cloud based CBF formulation with other point cloud\nbased CBF formulations. Then, we demonstrate the performance of our proposed\napproach and its integration with global planners using experimental studies on\nthe Unitree B1 and Unitree Go2 quadruped robots in various environments.\n","authors":["Bolun Dai","Rooholla Khorrambakht","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2403.18206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00911v2","updated":"2024-03-27T02:31:36Z","published":"2023-08-02T02:37:24Z","title":"Optimal Sensor Deception to Deviate from an Allowed Itinerary","summary":"  In this work, we study a class of deception planning problems in which an\nagent aims to alter a security monitoring system's sensor readings so as to\ndisguise its adversarial itinerary as an allowed itinerary in the environment.\nThe adversarial itinerary set and allowed itinerary set are captured by regular\nlanguages. To deviate without being detected, we investigate whether there\nexists a strategy for the agent to alter the sensor readings, with a minimal\ncost, such that for any of those paths it takes, the system thinks the agent\ntook a path within the allowed itinerary. Our formulation assumes an offline\nsensor alteration where the agent determines the sensor alteration strategy and\nimplement it, and then carry out any path in its deviation itinerary. We prove\nthat the problem of solving the optimal sensor alteration is NP-hard, by a\nreduction from the directed multi-cut problem. Further, we present an exact\nalgorithm based on integer linear programming and demonstrate the correctness\nand the efficacy of the algorithm in case studies.\n","authors":["Hazhar Rahmani","Arash Ahadi","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2308.00911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18197v1","updated":"2024-03-27T02:13:24Z","published":"2024-03-27T02:13:24Z","title":"LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight\n  Loco-Manipulators","summary":"  Quadrupedal robots have emerged as versatile agents capable of locomoting and\nmanipulating in complex environments. Traditional designs typically rely on the\nrobot's inherent body parts or incorporate top-mounted arms for manipulation\ntasks. However, these configurations may limit the robot's operational\ndexterity, efficiency and adaptability, particularly in cluttered or\nconstrained spaces. In this work, we present LocoMan, a dexterous quadrupedal\nrobot with a novel morphology to perform versatile manipulation in diverse\nconstrained environments. By equipping a Unitree Go1 robot with two low-cost\nand lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan\nleverages the combined mobility and functionality of the legs and grippers for\ncomplex manipulation tasks that require precise 6D positioning of the end\neffector in a wide workspace. To harness the loco-manipulation capabilities of\nLocoMan, we introduce a unified control framework that extends the whole-body\ncontroller (WBC) to integrate the dynamics of loco-manipulators. Through\nexperiments, we validate that the proposed whole-body controller can accurately\nand stably follow desired 6D trajectories of the end effector and torso, which,\nwhen combined with the large workspace from our design, facilitates a diverse\nset of challenging dexterous loco-manipulation tasks in confined spaces, such\nas opening doors, plugging into sockets, picking objects in narrow and\nlow-lying spaces, and bimanual manipulation.\n","authors":["Changyi Lin","Xingyu Liu","Yuxiang Yang","Yaru Niu","Wenhao Yu","Tingnan Zhang","Jie Tan","Byron Boots","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18197v1.pdf","comment":"Project page: https://linchangyi1.github.io/LocoMan"},{"id":"http://arxiv.org/abs/2403.18195v1","updated":"2024-03-27T02:08:12Z","published":"2024-03-27T02:08:12Z","title":"SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly\n  Network","summary":"  Autonomous assembly in robotics and 3D vision presents significant\nchallenges, particularly in ensuring assembly correctness. Presently,\npredominant methods such as MEPNet focus on assembling components based on\nmanually provided images. However, these approaches often fall short in\nachieving satisfactory results for tasks requiring long-term planning.\nConcurrently, we observe that integrating a self-correction module can\npartially alleviate such issues. Motivated by this concern, we introduce the\nsingle-step assembly error correction task, which involves identifying and\nrectifying misassembled components. To support research in this area, we\npresent the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising\nmanual images for assembly steps and instances of assembly failures.\nAdditionally, we propose the Self-Correct Assembly Network (SCANet), a novel\nmethod to address this task. SCANet treats assembled components as queries,\ndetermining their correctness in manual images and providing corrections when\nnecessary. Finally, we utilize SCANet to correct the assembly results of\nMEPNet. Experimental results demonstrate that SCANet can identify and correct\nMEPNet's misassembled results, significantly improving the correctness of\nassembly. Our code and dataset are available at\nhttps://github.com/Yaser-wyx/SCANet.\n","authors":["Yuxuan Wan","Kaichen Zhou","jinhong Chen","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18178v1","updated":"2024-03-27T01:12:31Z","published":"2024-03-27T01:12:31Z","title":"Online Embedding Multi-Scale CLIP Features into 3D Maps","summary":"  This study introduces a novel approach to online embedding of multi-scale\nCLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By\nharnessing CLIP, this methodology surpasses the constraints of conventional\nvocabulary-limited methods and enables the incorporation of semantic\ninformation into the resultant maps. While recent approaches have explored the\nembedding of multi-modal features in maps, they often impose significant\ncomputational costs, lacking practicality for exploring unfamiliar environments\nin real time. Our approach tackles these challenges by efficiently computing\nand embedding multi-scale CLIP features, thereby facilitating the exploration\nof unfamiliar environments through real-time map generation. Moreover, the\nembedding CLIP features into the resultant maps makes offline retrieval via\nlinguistic queries feasible. In essence, our approach simultaneously achieves\nreal-time object search and mapping of unfamiliar environments. Additionally,\nwe propose a zero-shot object-goal navigation system based on our mapping\napproach, and we validate its efficacy through object-goal navigation, offline\nobject retrieval, and multi-object-goal navigation in both simulated\nenvironments and real robot experiments. The findings demonstrate that our\nmethod not only exhibits swifter performance than state-of-the-art mapping\nmethods but also surpasses them in terms of the success rate of object-goal\nnavigation tasks.\n","authors":["Shun Taguchi","Hideki Deguchi"],"pdf_url":"https://arxiv.org/pdf/2403.18178v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.18172v1","updated":"2024-03-27T00:58:31Z","published":"2024-03-27T00:58:31Z","title":"Vision-Based Force Estimation for Minimally Invasive Telesurgery Through\n  Contact Detection and Local Stiffness Models","summary":"  In minimally invasive telesurgery, obtaining accurate force information is\ndifficult due to the complexities of in-vivo end effector force sensing. This\nconstrains development and implementation of haptic feedback and force-based\nautomated performance metrics, respectively. Vision-based force sensing\napproaches using deep learning are a promising alternative to intrinsic end\neffector force sensing. However, they have limited ability to generalize to\nnovel scenarios, and require learning on high-quality force sensor training\ndata that can be difficult to obtain. To address these challenges, this paper\npresents a novel vision-based contact-conditional approach for force estimation\nin telesurgical environments. Our method leverages supervised learning with\nhuman labels and end effector position data to train deep neural networks.\nPredictions from these trained models are optionally combined with robot joint\ntorque information to estimate forces indirectly from visual data. We benchmark\nour method against ground truth force sensor data and demonstrate generality by\nfine-tuning to novel surgical scenarios in a data-efficient manner. Our methods\ndemonstrated greater than 90% accuracy on contact detection and less than 10%\nforce prediction error. These results suggest potential usefulness of\ncontact-conditional force estimation for sensory substitution haptic feedback\nand tissue handling skill evaluation in clinical settings.\n","authors":["Shuyuan Yang","My H. Le","Kyle R. Golobish","Juan C. Beaver","Zonghe Chua"],"pdf_url":"https://arxiv.org/pdf/2403.18172v1.pdf","comment":"Preprint of an article accepted in Journal of Medical Robotics\n  Research \\copyright 2024 copyright World Scientific Publishing Company"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18820v1","updated":"2024-03-27T17:59:54Z","published":"2024-03-27T17:59:54Z","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering","summary":"  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.\n","authors":["Guoxing Sun","Rishabh Dabral","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2403.18820v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/"},{"id":"http://arxiv.org/abs/2403.18819v1","updated":"2024-03-27T17:59:53Z","published":"2024-03-27T17:59:53Z","title":"Benchmarking Object Detectors with COCO: A New Path Forward","summary":"  The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz\n","authors":["Shweta Singh","Aayan Yadav","Jitesh Jain","Humphrey Shi","Justin Johnson","Karan Desai"],"pdf_url":"https://arxiv.org/pdf/2403.18819v1.pdf","comment":"Technical report. Dataset website: https://cocorem.xyz and code:\n  https://github.com/kdexd/coco-rem"},{"id":"http://arxiv.org/abs/2403.18818v1","updated":"2024-03-27T17:59:52Z","published":"2024-03-27T17:59:52Z","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion","summary":"  Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.\n","authors":["Daniel Winter","Matan Cohen","Shlomi Fruchter","Yael Pritch","Alex Rav-Acha","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2403.18818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18816v1","updated":"2024-03-27T17:59:33Z","published":"2024-03-27T17:59:33Z","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","summary":"  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.\n","authors":["Nikolaos Sarafianos","Tuur Stuyck","Xiaoyu Xiang","Yilei Li","Jovan Popovic","Rakesh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.18816v1.pdf","comment":"Project Page: https://nsarafianos.github.io/garment3dgen"},{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18811v1","updated":"2024-03-27T17:57:02Z","published":"2024-03-27T17:57:02Z","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment","summary":"  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n","authors":["Li Siyao","Tianpei Gu","Zhitao Yang","Zhengyu Lin","Ziwei Liu","Henghui Ding","Lei Yang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18811v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2311.10319v4","updated":"2024-03-27T17:41:50Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v4.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.18795v1","updated":"2024-03-27T17:40:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v1","updated":"2024-03-27T17:35:24Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.18784v1","updated":"2024-03-27T17:32:04Z","published":"2024-03-27T17:32:04Z","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface","summary":"  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n","authors":["Jiahao Luo","Jing Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.18784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01220v2","updated":"2024-03-27T17:23:16Z","published":"2023-12-02T20:11:48Z","title":"Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation","summary":"  Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to low visibility. Previous methods mitigate this issue\nby exploring image enhancement or object detection techniques with real\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties about collecting and annotating low-light images. To address this\nchallenge, we propose to boost low-light object detection with zero-shot\nday-night domain adaptation, which aims to generalize a detector from well-lit\nscenarios to low-light ones without requiring real low-light data. Revisiting\nRetinex theory in the low-level vision, we first design a reflectance\nrepresentation learning module to learn Retinex-based illumination invariance\nin images with a carefully designed illumination invariance reinforcement\nstrategy. Next, an interchange-redecomposition-coherence procedure is\nintroduced to improve over the vanilla Retinex image decomposition process by\nperforming two sequential image decompositions and introducing a\nredecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and\nCODaN datasets show strong low-light generalizability of our method. Our code\nis available at https://github.com/ZPDu/DAI-Net.\n","authors":["Zhipeng Du","Miaojing Shi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2312.01220v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2303.09817v2","updated":"2024-03-27T16:52:59Z","published":"2023-03-17T07:53:18Z","title":"Interpretable machine learning for time-to-event prediction in medicine\n  and healthcare","summary":"  Time-to-event prediction, e.g. cancer survival analysis or hospital length of\nstay, is a highly prominent machine learning task in medical and healthcare\napplications. However, only a few interpretable machine learning methods comply\nwith its challenges. To facilitate a comprehensive explanatory analysis of\nsurvival models, we formally introduce time-dependent feature effects and\nglobal feature importance explanations. We show how post-hoc interpretation\nmethods allow for finding biases in AI systems predicting length of stay using\na novel multi-modal dataset created from 1235 X-ray images with textual\nradiology reports annotated by human experts. Moreover, we evaluate cancer\nsurvival models beyond predictive performance to include the importance of\nmulti-omics feature groups based on a large-scale benchmark comprising 11\ndatasets from The Cancer Genome Atlas (TCGA). Model developers can use the\nproposed methods to debug and improve machine learning algorithms, while\nphysicians can discover disease biomarkers and assess their significance. We\nhope the contributed open data and code resources facilitate future work in the\nemerging research direction of explainable survival analysis.\n","authors":["Hubert Baniecki","Bartlomiej Sobieski","Patryk Szatkowski","Przemyslaw Bombinski","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2303.09817v2.pdf","comment":"An extended version of an AIME 2023 paper submitted to Artificial\n  Intelligence in Medicine"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11107v2","updated":"2024-03-27T16:48:34Z","published":"2024-03-17T06:21:21Z","title":"Self-supervised co-salient object detection via feature correspondence\n  at multiple scales","summary":"  Our paper introduces a novel two-stage self-supervised approach for detecting\nco-occurring salient objects (CoSOD) in image groups without requiring\nsegmentation annotations. Unlike existing unsupervised methods that rely solely\non patch-level information (e.g. clustering patch descriptors) or on\ncomputation heavy off-the-shelf components for CoSOD, our lightweight model\nleverages feature correspondences at both patch and region levels,\nsignificantly improving prediction performance. In the first stage, we train a\nself-supervised network that detects co-salient regions by computing local\npatch-level feature correspondences across images. We obtain the segmentation\npredictions using confidence-based adaptive thresholding. In the next stage, we\nrefine these intermediate segmentations by eliminating the detected regions\n(within each image) whose averaged feature representations are dissimilar to\nthe foreground feature representation averaged across all the cross-attention\nmaps (from the previous stage). Extensive experiments on three CoSOD benchmark\ndatasets show that our self-supervised model outperforms the corresponding\nstate-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model\nhas a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,\nour self-supervised model also outperforms several recent fully supervised\nCoSOD models on the three test datasets (e.g., on the CoCA dataset, our model\nhas a 4.6% F-measure gain over a recent supervised CoSOD model).\n","authors":["Souradeep Chakraborty","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.11107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18734v1","updated":"2024-03-27T16:22:45Z","published":"2024-03-27T16:22:45Z","title":"A vascular synthetic model for improved aneurysm segmentation and\n  detection via Deep Neural Networks","summary":"  We hereby present a full synthetic model, able to mimic the various\nconstituents of the cerebral vascular tree: the cerebral arteries, the\nbifurcations and the intracranial aneurysms. By building this model, our goal\nwas to provide a substantial dataset of brain arteries which could be used by a\n3D Convolutional Neural Network (CNN) to either segment or detect/recognize\nvarious vascular diseases (such as artery dissection/thrombosis) or even some\nportions of the cerebral vasculature, such as the bifurcations or aneurysms. In\nthis study, we will particularly focus on Intra-Cranial Aneurysm (ICA)\ndetection and segmentation. The cerebral aneurysms most often occur on a\nparticular structure of the vascular tree named the Circle of Willis. Various\nstudies have been conducted to detect and monitor the ICAs and those based on\nDeep Learning (DL) achieve the best performances. Specifically, in this work,\nwe propose a full synthetic 3D model able to mimic the brain vasculature as\nacquired by Magnetic Resonance Angiography (MRA), and more particularly the\nTime Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF\nallows to have a relatively good rendering of the blood vessels and is\nnon-invasive (no contrast liquid injection). Our model has been designed to\nsimultaneously mimic the arteries geometry, the ICA shape and the background\nnoise. The geometry of the vascular tree is modeled thanks to an interpolation\nwith 3D Spline functions, and the statistical properties of the background MRI\nnoise is collected from MRA acquisitions and reproduced within the model. In\nthis work, we thoroughly describe the synthetic vasculature model, we build up\na neural network designed for ICA segmentation and detection, and finally, we\ncarry out an in-depth evaluation of the performance gap gained thanks to the\nsynthetic model data augmentation.\n","authors":["Rafic Nader","Florent Autrusseau","Vincent L'Allinec","Romain Bourcier"],"pdf_url":"https://arxiv.org/pdf/2403.18734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18730v1","updated":"2024-03-27T16:20:55Z","published":"2024-03-27T16:20:55Z","title":"Towards Image Ambient Lighting Normalization","summary":"  Lighting normalization is a crucial but underexplored restoration task with\nbroad applications. However, existing works often simplify this task within the\ncontext of shadow removal, limiting the light sources to one and\noversimplifying the scene, thus excluding complex self-shadows and restricting\nsurface classes to smooth ones. Although promising, such simplifications hinder\ngeneralizability to more realistic settings encountered in daily use. In this\npaper, we propose a new challenging task termed Ambient Lighting Normalization\n(ALN), which enables the study of interactions between shadows, unifying image\nrestoration and shadow removal in a broader context. To address the lack of\nappropriate datasets for ALN, we introduce the large-scale high-resolution\ndataset Ambient6K, comprising samples obtained from multiple light sources and\nincluding self-shadows resulting from complex geometries, which is the first of\nits kind. For benchmarking, we select various mainstream methods and rigorously\nevaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong\nbaseline that maximizes Image-Frequency joint entropy to selectively restore\nlocal areas under different lighting conditions, without relying on shadow\nlocalization priors. Experiments show that IFBlend achieves SOTA scores on\nAmbient6K and exhibits competitive performance on conventional shadow removal\nbenchmarks compared to shadow-specific models with mask priors. The dataset,\nbenchmark, and code are available at https://github.com/fvasluianu97/IFBlend.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zongwei Wu","Rakesh Ranjan","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2403.18730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09992v3","updated":"2024-03-27T16:20:52Z","published":"2023-03-17T14:07:55Z","title":"LION: Implicit Vision Prompt Tuning","summary":"  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n","authors":["Haixin Wang","Jianlong Chang","Xiao Luo","Jinan Sun","Zhouchen Lin","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09992v3.pdf","comment":"Accepted by AAAI2024; 9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.18711v1","updated":"2024-03-27T15:58:25Z","published":"2024-03-27T15:58:25Z","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery","summary":"  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n","authors":["Camille Billouard","Dawa Derksen","Emmanuelle Sarrazin","Bruno Vallet"],"pdf_url":"https://arxiv.org/pdf/2403.18711v1.pdf","comment":"5 pages, 3 figures, 1 table; Accepted to International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2024; Code available at\n  https://github.com/Ellimac0/SAT-NGP"},{"id":"http://arxiv.org/abs/2403.18708v1","updated":"2024-03-27T15:56:42Z","published":"2024-03-27T15:56:42Z","title":"Dense Vision Transformer Compression with Few Samples","summary":"  Few-shot model compression aims to compress a large model into a more compact\none with only a tiny training set (even without labels). Block-level pruning\nhas recently emerged as a leading technique in achieving high accuracy and low\nlatency in few-shot CNN compression. But, few-shot compression for Vision\nTransformers (ViT) remains largely unexplored, which presents a new challenge.\nIn particular, the issue of sparse compression exists in traditional CNN\nfew-shot methods, which can only produce very few compressed models of\ndifferent model sizes. This paper proposes a novel framework for few-shot ViT\ncompression named DC-ViT. Instead of dropping the entire block, DC-ViT\nselectively eliminates the attention module while retaining and reusing\nportions of the MLP module. DC-ViT enables dense compression, which outputs\nnumerous compressed models that densely populate the range of model complexity.\nDC-ViT outperforms state-of-the-art few-shot compression methods by a\nsignificant margin of 10 percentage points, along with lower latency in the\ncompression of ViT and its variants.\n","authors":["Hanxiao Zhang","Yifan Zhou","Guo-Hua Wang","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18708v1.pdf","comment":"Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for\n  the arXiv version of this paper but is not listed as an author in the CVPR\n  version due to his role as Program Chair"},{"id":"http://arxiv.org/abs/2401.15120v2","updated":"2024-03-27T15:49:52Z","published":"2024-01-26T03:44:58Z","title":"Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models","summary":"  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n","authors":["Lizhen Zhu","James Z. Wang","Wonseuk Lee","Brad Wyble"],"pdf_url":"https://arxiv.org/pdf/2401.15120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18690v1","updated":"2024-03-27T15:41:23Z","published":"2024-03-27T15:41:23Z","title":"Annolid: Annotate, Segment, and Track Anything You Need","summary":"  Annolid is a deep learning-based software package designed for the\nsegmentation, labeling, and tracking of research targets within video files,\nfocusing primarily on animal behavior analysis. Based on state-of-the-art\ninstance segmentation methods, Annolid now harnesses the Cutie video object\nsegmentation model to achieve resilient, markerless tracking of multiple\nanimals from single annotated frames, even in environments in which they may be\npartially or entirely concealed by environmental features or by one another.\nOur integration of Segment Anything and Grounding-DINO strategies additionally\nenables the automatic masking and segmentation of recognizable animals and\nobjects by text command, removing the need for manual annotation. Annolid's\ncomprehensive approach to object segmentation flexibly accommodates a broad\nspectrum of behavior analysis applications, enabling the classification of\ndiverse behavioral states such as freezing, digging, pup huddling, and social\ninteractions in addition to the tracking of animals and their body parts.\n","authors":["Chen Yang","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2403.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08479v2","updated":"2024-03-27T15:38:27Z","published":"2023-12-13T19:38:50Z","title":"Vision Transformer-Based Deep Learning for Histologic Classification of\n  Endometrial Cancer","summary":"  Endometrial cancer, the fourth most common cancer in females in the United\nStates, with the lifetime risk for developing this disease is approximately\n2.8% in women. Precise histologic evaluation and molecular classification of\nendometrial cancer is important for effective patient management and\ndetermining the best treatment modalities. This study introduces EndoNet, which\nuses convolutional neural networks for extracting histologic features and a\nvision transformer for aggregating these features and classifying slides based\non their visual characteristics into high- and low- grade. The model was\ntrained on 929 digitized hematoxylin and eosin-stained whole-slide images of\nendometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies\nthese slides into low-grade (Endometroid Grades 1 and 2) and high-grade\n(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)\ncategories. EndoNet was evaluated on an internal test set of 110 patients and\nan external test set of 100 patients from the public TCGA database. The model\nachieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of\n0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for\nF1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending\nfurther validation, EndoNet has the potential to support pathologists without\nthe need of manual annotations in classifying the grades of gynecologic\npathology tumors.\n","authors":["Manu Goyal","Laura J. Tafe","James X. Feng","Kristen E. Muller","Liesbeth Hondelink","Jessica L. Bentz","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2312.08479v2.pdf","comment":"4 Tables and 3 Figures"},{"id":"http://arxiv.org/abs/2308.06098v2","updated":"2024-03-27T15:26:44Z","published":"2023-08-11T12:18:53Z","title":"Automated Construction of Time-Space Diagrams for Traffic Analysis Using\n  Street-View Video Sequence","summary":"  Time-space diagrams are essential tools for analyzing traffic patterns and\noptimizing transportation infrastructure and traffic management strategies.\nTraditional data collection methods for these diagrams have limitations in\nterms of temporal and spatial coverage. Recent advancements in camera\ntechnology have overcome these limitations and provided extensive urban data.\nIn this study, we propose an innovative approach to constructing time-space\ndiagrams by utilizing street-view video sequences captured by cameras mounted\non moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and\nphotogrammetry techniques for distance calculation, we can infer vehicle\ntrajectories from the video data and generate time-space diagrams. To evaluate\nthe effectiveness of our proposed method, we utilized datasets from the KITTI\ncomputer vision benchmark suite. The evaluation results demonstrate that our\napproach can generate trajectories from video data, although there are some\nerrors that can be mitigated by improving the performance of the detector,\ntracker, and distance calculation components. In conclusion, the utilization of\nstreet-view video sequences captured by cameras mounted on moving vehicles,\ncombined with state-of-the-art computer vision techniques, has immense\npotential for constructing comprehensive time-space diagrams. These diagrams\noffer valuable insights into traffic patterns and contribute to the design of\ntransportation infrastructure and traffic management strategies.\n","authors":["Tanay Rastogi","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2308.06098v2.pdf","comment":"The paper is published in 2023 IEEE 26th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2403.18674v1","updated":"2024-03-27T15:17:10Z","published":"2024-03-27T15:17:10Z","title":"Deep Learning for Robust and Explainable Models in Computer Vision","summary":"  Recent breakthroughs in machine and deep learning (ML and DL) research have\nprovided excellent tools for leveraging enormous amounts of data and optimizing\nhuge models with millions of parameters to obtain accurate networks for image\nprocessing. These developments open up tremendous opportunities for using\nartificial intelligence (AI) in the automation and human assisted AI industry.\nHowever, as more and more models are deployed and used in practice, many\nchallenges have emerged. This thesis presents various approaches that address\nrobustness and explainability challenges for using ML and DL in practice.\n  Robustness and reliability are the critical components of any model before\ncertification and deployment in practice. Deep convolutional neural networks\n(CNNs) exhibit vulnerability to transformations of their inputs, such as\nrotation and scaling, or intentional manipulations as described in the\nadversarial attack literature. In addition, building trust in AI-based models\nrequires a better understanding of current models and developing methods that\nare more explainable and interpretable a priori.\n  This thesis presents developments in computer vision models' robustness and\nexplainability. Furthermore, this thesis offers an example of using vision\nmodels' feature response visualization (models' interpretations) to improve\nrobustness despite interpretability and robustness being seemingly unrelated in\nthe related research. Besides methodological developments for robust and\nexplainable vision models, a key message of this thesis is introducing model\ninterpretation techniques as a tool for understanding vision models and\nimproving their design and robustness. In addition to the theoretical\ndevelopments, this thesis demonstrates several applications of ML and DL in\ndifferent contexts, such as medical imaging and affective computing.\n","authors":["Mohammadreza Amirian"],"pdf_url":"https://arxiv.org/pdf/2403.18674v1.pdf","comment":"150 pages, 37 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.15803v3","updated":"2024-03-27T15:05:19Z","published":"2023-11-27T13:25:47Z","title":"SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields","summary":"  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n","authors":["Quentin Herau","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2311.15803v3.pdf","comment":"Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/"},{"id":"http://arxiv.org/abs/2403.18660v1","updated":"2024-03-27T15:03:38Z","published":"2024-03-27T15:03:38Z","title":"InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing","summary":"  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n","authors":["Ruoyu Zhao","Qingnan Fan","Fei Kou","Shuai Qin","Hong Gu","Wei Wu","Pengcheng Xu","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18660v1.pdf","comment":"Project Page: https://royzhao926.github.io/InstructBrush/"},{"id":"http://arxiv.org/abs/2311.12386v3","updated":"2024-03-27T15:01:44Z","published":"2023-11-21T06:55:21Z","title":"Point, Segment and Count: A Generalized Framework for Object Counting","summary":"  Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. In this paper, we propose a generalized framework for both few-shot\nand zero-shot object counting based on detection. Our framework combines the\nsuperior advantages of two foundation models without compromising their\nzero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask\nproposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate\nobject counts. However, this strategy meets the obstacles of efficiency\noverhead and the small crowded objects that cannot be localized and\ndistinguished. To address these issues, our framework, termed PseCo, follows\nthree steps: point, segment, and count. Specifically, we first propose a\nclass-agnostic object localization to provide accurate but least point prompts\nfor SAM, which consequently not only reduces computation costs but also avoids\nmissing small objects. Furthermore, we propose a generalized object\nclassification that leverages CLIP image/text embeddings as the classifier,\nfollowing a hierarchical knowledge distillation to obtain discriminative\nclassifications among hierarchical mask proposals. Extensive experimental\nresults on FSC-147, COCO, and LVIS demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection. Code: https://github.com/Hzzone/PseCo\n","authors":["Zhizhong Huang","Mingliang Dai","Yi Zhang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2311.12386v3.pdf","comment":"Accepted by CVPR 2024. Camera ready"},{"id":"http://arxiv.org/abs/2311.17532v3","updated":"2024-03-27T15:01:22Z","published":"2023-11-29T11:10:40Z","title":"Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation","summary":"  Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets. Our\ncode and dataset will be released on the project page:\nhttps://xingqunqi-lab.github.io/Emo-Transition-Gesture/.\n","authors":["Xingqun Qi","Jiahao Pan","Peng Li","Ruibin Yuan","Xiaowei Chi","Mengfei Li","Wenhan Luo","Wei Xue","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2311.17532v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18649v1","updated":"2024-03-27T14:56:44Z","published":"2024-03-27T14:56:44Z","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets","summary":"  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n","authors":["Ajinkya Khoche","Aron Asefaw","Alejandro Gonzalez","Bogdan Timus","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.18649v1.pdf","comment":"Accepted to European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11041v3","updated":"2024-03-27T14:29:27Z","published":"2022-04-23T10:19:58Z","title":"Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n","authors":["Meng Xing","Zhiyong Feng","Yong Su","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2204.11041v3.pdf","comment":"update new experimental results"},{"id":"http://arxiv.org/abs/2403.18605v1","updated":"2024-03-27T14:24:30Z","published":"2024-03-27T14:24:30Z","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing","summary":"  Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.\n","authors":["Trong-Tung Nguyen","Duc-Anh Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2403.18605v1.pdf","comment":"Our project page: https://flex-edit.github.io/"},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.18593v1","updated":"2024-03-27T14:18:09Z","published":"2024-03-27T14:18:09Z","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding","summary":"  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n","authors":["Run Shao","Zhaoyang Zhang","Chao Tao","Yunsheng Zhang","Chengli Peng","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18593v1.pdf","comment":"20 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.18589v1","updated":"2024-03-27T14:12:56Z","published":"2024-03-27T14:12:56Z","title":"Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG","summary":"  We performed pairwise comparisons by human raters of JPEG images from\nMozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a\nquality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely\nto be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8\nbits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits\nper pixel respectively. The raw ratings and source images are publicly\navailable for further analysis and study.\n","authors":["Martin Bruse","Luca Versari","Zoltan Szabadka","Jyrki Alakuijala"],"pdf_url":"https://arxiv.org/pdf/2403.18589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2312.07264v2","updated":"2024-03-27T14:09:10Z","published":"2023-12-12T13:44:53Z","title":"Dual Structure-Aware Image Filterings for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-aware image filterings (DSAIF) as\nthe image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying the\nproposed DSAIF to mutually supervised networks decreases the consensus of their\nerroneous predictions on unlabeled images. This helps to alleviate the\nconfirmation bias issue of overfitting to noisy pseudo labels of unlabeled\nimages, and thus effectively improves the segmentation performance. Extensive\nexperimental results on three benchmark datasets demonstrate that the proposed\nmethod significantly/consistently outperforms some state-of-the-art methods.\nThe source codes will be publicly available.\n","authors":["Yuliang Gu","Zhichao Sun","Tian Chen","Xin Xiao","Yepeng Liu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2312.07264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18575v1","updated":"2024-03-27T13:56:08Z","published":"2024-03-27T13:56:08Z","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional\n  Synthesis and Sampling of Hand-Object Interactions","summary":"  Reconstructing 3D hand mesh robustly from a single image is very challenging,\ndue to the lack of diversity in existing real-world datasets. While data\nsynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.\nIn this work, we present HandBooster, a new approach to uplift the data\ndiversity and boost the 3D hand-mesh reconstruction performance by training a\nconditional generative space on hand-object interactions and purposely sampling\nthe space to synthesize effective data samples. First, we construct versatile\ncontent-aware conditions to guide a diffusion model to produce realistic images\nwith diverse hand appearances, poses, views, and backgrounds; favorably,\naccurate 3D annotations are obtained for free. Then, we design a novel\ncondition creator based on our similarity-aware distribution sampling\nstrategies to deliberately find novel and realistic interaction poses that are\ndistinctive from the training set. Equipped with our method, several baselines\ncan be significantly improved beyond the SOTA on the HO3D and DexYCB\nbenchmarks. Our code will be released on\nhttps://github.com/hxwork/HandBooster_Pytorch.\n","authors":["Hao Xu","Haipeng Li","Yinqiao Wang","Shuaicheng Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v3","updated":"2024-03-27T13:51:59Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.0% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v3.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2403.18565v1","updated":"2024-03-27T13:46:01Z","published":"2024-03-27T13:46:01Z","title":"Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images\n  with Deep Learning -- A Review","summary":"  Deep learning based approaches have been used to improve image quality in\ncone-beam computed tomography (CBCT), a medical imaging technique often used in\napplications such as image-guided radiation therapy, implant dentistry or\northopaedics. In particular, while deep learning methods have been applied to\nreduce various types of CBCT image artifacts arising from motion, metal\nobjects, or low-dose acquisition, a comprehensive review summarizing the\nsuccesses and shortcomings of these approaches, with a primary focus on the\ntype of artifacts rather than the architecture of neural networks, is lacking\nin the literature. In this review, the data generation and simulation\npipelines, and artifact reduction techniques are specifically investigated for\neach type of artifact. We provide an overview of deep learning techniques that\nhave successfully been shown to reduce artifacts in 3D, as well as in\ntime-resolved (4D) CBCT through the use of projection- and/or volume-domain\noptimizations, or by introducing neural networks directly within the CBCT\nreconstruction algorithms. Research gaps are identified to suggest avenues for\nfuture exploration. One of the key findings of this work is an observed trend\ntowards the use of generative models including GANs and score-based or\ndiffusion models, accompanied with the need for more diverse and open training\ndatasets and simulations.\n","authors":["Mohammadreza Amirian","Daniel Barco","Ivo Herzig","Frank-Peter Schilling"],"pdf_url":"https://arxiv.org/pdf/2403.18565v1.pdf","comment":"16 pages, 4 figures, 1 Table, published in IEEE Access Journal"},{"id":"http://arxiv.org/abs/2403.09700v2","updated":"2024-03-27T13:42:25Z","published":"2024-03-05T22:19:21Z","title":"Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI","summary":"  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n","authors":["Alex Glinsky","Alexey Sokolsky"],"pdf_url":"https://arxiv.org/pdf/2403.09700v2.pdf","comment":"36 pages, 32 figures"},{"id":"http://arxiv.org/abs/2403.18554v1","updated":"2024-03-27T13:33:14Z","published":"2024-03-27T13:33:14Z","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency\n  Detection","summary":"  Co-salient object detection (CoSOD) aims to identify the common and salient\n(usually in the foreground) regions across a given group of images. Although\nachieving significant progress, state-of-the-art CoSODs could be easily\naffected by some adversarial perturbations, leading to substantial accuracy\nreduction. The adversarial perturbations can mislead CoSODs but do not change\nthe high-level semantic information (e.g., concept) of the co-salient objects.\nIn this paper, we propose a novel robustness enhancement framework by first\nlearning the concept of the co-salient objects based on the input group images\nand then leveraging this concept to purify adversarial perturbations, which are\nsubsequently fed to CoSODs for robustness enhancement. Specifically, we propose\nCosalPure containing two modules, i.e., group-image concept learning and\nconcept-guided diffusion purification. For the first module, we adopt a\npre-trained text-to-image diffusion model to learn the concept of co-salient\nobjects within group images where the learned concept is robust to adversarial\nexamples. For the second module, we map the adversarial image to the latent\nspace and then perform diffusion generation by embedding the learned concept\ninto the noise prediction function as an extra condition. Our method can\neffectively alleviate the influence of the SOTA adversarial attack containing\ndifferent adversarial patterns, including exposure and noise. The extensive\nresults demonstrate that our method could enhance the robustness of CoSODs\nsignificantly.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.18554v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.18551v1","updated":"2024-03-27T13:31:39Z","published":"2024-03-27T13:31:39Z","title":"Attention Calibration for Disentangled Text-to-Image Personalization","summary":"  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n","authors":["Yanbing Zhang","Mengping Yang","Qin Zhou","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18551v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18550v1","updated":"2024-03-27T13:30:48Z","published":"2024-03-27T13:30:48Z","title":"OrCo: Towards Better Generalization via Orthogonality and Contrast for\n  Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which\nthe problem space expands with limited data. FSCIL methods inherently face the\nchallenge of catastrophic forgetting as data arrives incrementally, making\nmodels susceptible to overwriting previously acquired knowledge. Moreover,\ngiven the scarcity of labeled samples available at any given time, models may\nbe prone to overfitting and find it challenging to strike a balance between\nextensive pretraining and the limited incremental data. To address these\nchallenges, we propose the OrCo framework built on two core principles:\nfeatures' orthogonality in the representation space, and contrastive learning.\nIn particular, we improve the generalization of the embedding space by\nemploying a combination of supervised and self-supervised contrastive losses\nduring the pretraining phase. Additionally, we introduce OrCo loss to address\nchallenges arising from data limitations during incremental sessions. Through\nfeature space perturbations and orthogonality between classes, the OrCo loss\nmaximizes margins and reserves space for the following incremental data. This,\nin turn, ensures the accommodation of incoming classes in the feature space\nwithout compromising previously acquired knowledge. Our experimental results\nshowcase state-of-the-art performance across three benchmark datasets,\nincluding mini-ImageNet, CIFAR100, and CUB datasets. Code is available at\nhttps://github.com/noorahmedds/OrCo\n","authors":["Noor Ahmed","Anna Kukleva","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2403.18550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18548v1","updated":"2024-03-27T13:27:02Z","published":"2024-03-27T13:27:02Z","title":"A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency\n  Aware and Realistic Brightness Constraint","summary":"  Existing research based on deep learning has extensively explored the problem\nof daytime image dehazing. However, few studies have considered the\ncharacteristics of nighttime hazy scenes. There are two distinctions between\nnighttime and daytime haze. First, there may be multiple active colored light\nsources with lower illumination intensity in nighttime scenes, which may cause\nhaze, glow and noise with localized, coupled and frequency inconsistent\ncharacteristics. Second, due to the domain discrepancy between simulated and\nreal-world data, unrealistic brightness may occur when applying a dehazing\nmodel trained on simulated data to real-world data. To address the above two\nissues, we propose a semi-supervised model for real-world nighttime dehazing.\nFirst, the spatial attention and frequency spectrum filtering are implemented\nas a spatial-frequency domain information interaction module to handle the\nfirst issue. Second, a pseudo-label-based retraining strategy and a local\nwindow-based brightness loss for semi-supervised training process is designed\nto suppress haze and glow while achieving realistic brightness. Experiments on\npublic benchmarks validate the effectiveness of the proposed method and its\nsuperiority over state-of-the-art methods. The source code and Supplementary\nMaterials are placed in the https://github.com/Xiaofeng-life/SFSNiD.\n","authors":["Xiaofeng Cong","Jie Gui","Jing Zhang","Junming Hou","Hao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.18548v1.pdf","comment":"This paper is accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2310.15081v3","updated":"2024-03-27T13:23:28Z","published":"2023-10-23T16:41:13Z","title":"E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion","summary":"  This paper proposes a novel approach to face swapping from the perspective of\nfine-grained facial editing, dubbed \"editing for swapping\" (E4S). The\ntraditional face swapping methods rely on global feature extraction and fail to\npreserve the detailed source identity. In contrast, we propose a Regional GAN\nInversion (RGI) method, which allows the explicit disentanglement of shape and\ntexture. Specifically, our E4S performs face swapping in the latent space of a\npretrained StyleGAN, where a multi-scale mask-guided encoder is applied to\nproject the texture of each facial component into regional style codes and a\nmask-guided injection module manipulating feature maps with the style codes.\nBased on this disentanglement, face swapping can be simplified as style and\nmask swapping. Besides, due to the large lighting condition gap, transferring\nthe source skin into the target image may lead to disharmony lighting. We\npropose a re-coloring network to make the swapped face maintain the target\nlighting condition while preserving the source skin. Further, to deal with the\npotential mismatch areas during mask exchange, we design a face inpainting\nmodule to refine the face shape. The extensive comparisons with\nstate-of-the-art methods demonstrate that our E4S outperforms existing methods\nin preserving texture, shape, and lighting. Our implementation is available at\nhttps://github.com/e4s2024/E4S2024.\n","authors":["Maomao Li","Ge Yuan","Cairong Wang","Zhian Liu","Yong Zhang","Yongwei Nie","Jue Wang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15081v3.pdf","comment":"Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text\n  overlap with arXiv:2211.14068"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04344v3","updated":"2024-03-27T12:44:55Z","published":"2023-06-07T11:18:53Z","title":"ViDA: Homeostatic Visual Domain Adapter for Continual Test Time\n  Adaptation","summary":"  Since real-world machine systems are running in non-stationary environments,\nContinual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained\nmodel to continually changing target domains. Recently, existing methods mainly\nfocus on model-based adaptation, which aims to leverage a self-training manner\nto extract the target domain knowledge. However, pseudo labels can be noisy and\nthe updated model parameters are unreliable under dynamic data distributions,\nleading to error accumulation and catastrophic forgetting in the continual\nadaptation process. To tackle these challenges and maintain the model\nplasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly\nhandling both domain-specific and domain-shared knowledge. Specifically, we\nfirst comprehensively explore the different domain representations of the\nadapters with trainable high-rank or low-rank embedding spaces. Then we inject\nViDAs into the pre-trained model, which leverages high-rank and low-rank\nfeatures to adapt the current domain distribution and maintain the continual\ndomain-shared knowledge, respectively. To exploit the low-rank and high-rank\nViDAs more effectively, we further propose a Homeostatic Knowledge Allotment\n(HKA) strategy, which adaptively combines different knowledge from each ViDA.\nExtensive experiments conducted on four widely used benchmarks demonstrate that\nour proposed method achieves state-of-the-art performance in both\nclassification and segmentation CTTA tasks. Note that, our method can be\nregarded as a novel transfer paradigm for large-scale models, delivering\npromising results in adaptation to continually changing distributions. Project\npage: https://sites.google.com/view/iclr2024-vida/home.\n","authors":["Jiaming Liu","Senqiao Yang","Peidong Jia","Renrui Zhang","Ming Lu","Yandong Guo","Wei Xue","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04344v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.18512v1","updated":"2024-03-27T12:41:30Z","published":"2024-03-27T12:41:30Z","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","summary":"  We study a challenging task: text-to-motion synthesis, aiming to generate\nmotions that align with textual descriptions and exhibit coordinated movements.\nCurrently, the part-based methods introduce part partition into the motion\nsynthesis process to achieve finer-grained generation. However, these methods\nencounter challenges such as the lack of coordination between different part\nmotions and difficulties for networks to understand part concepts. Moreover,\nintroducing finer-grained part concepts poses computational complexity\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\nmotions and communication among different part motion generators, ensuring a\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\nwhole-body motion into multiple part motions to establish the prior concept of\ndifferent parts. Afterward, we employ multiple lightweight generators designed\nto synthesize different part motions and coordinate them through our part\ncoordination module. Our approach demonstrates superior performance on common\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\nproviding substantial evidence of its effectiveness. Code is available at\nhttps://github.com/qrzou/ParCo .\n","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2312.06358v2","updated":"2024-03-27T12:24:29Z","published":"2023-12-11T13:05:54Z","title":"Intraoperative 2D/3D Image Registration via Differentiable X-ray\n  Rendering","summary":"  Surgical decisions are informed by aligning rapid portable 2D intraoperative\nimages (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,\nCT). 2D/3D image registration often fails in practice: conventional\noptimization methods are prohibitively slow and susceptible to local minima,\nwhile neural networks trained on small datasets fail on new patients or require\nimpractical landmark supervision. We present DiffPose, a self-supervised\napproach that leverages patient-specific simulation and differentiable\nphysics-based rendering to achieve accurate 2D/3D registration without relying\non manually labeled data. Preoperatively, a CNN is trained to regress the pose\nof a randomly oriented synthetic X-ray rendered from the preoperative CT. The\nCNN then initializes rapid intraoperative test-time optimization that uses the\ndifferentiable X-ray renderer to refine the solution. Our work further proposes\nseveral geometrically principled methods for sampling camera poses from\n$\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving\nregistration in the tangent space $\\mathfrak{se}(3)$ with geodesic and\nmultiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy\nacross surgical datasets at intraoperative speeds, improving upon existing\nunsupervised methods by an order of magnitude and even outperforming supervised\nbaselines. Our code is available at https://github.com/eigenvivek/DiffPose.\n","authors":["Vivek Gopalakrishnan","Neel Dey","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2312.06358v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18501v1","updated":"2024-03-27T12:24:20Z","published":"2024-03-27T12:24:20Z","title":"HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with\n  Dual-Branch Pix2pix Generator","summary":"  Computational analysis of multiplexed immunofluorescence histology data is\nemerging as an important method for understanding the tumour micro-environment\nin cancer. This work presents HEMIT, a dataset designed for translating\nHematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)\nimages, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC\nimages are multi-component and cellular-level aligned with H&E, enriching\nsupervised stain translation tasks. To our knowledge, HEMIT is the first\npublicly available cellular-level aligned dataset that enables H&E to\nmulti-target mIHC image translation. This dataset provides the computer vision\ncommunity with a valuable resource to develop novel computational methods which\nhave the potential to gain new insights from H&E slide archives.\n  We also propose a new dual-branch generator architecture, using residual\nConvolutional Neural Networks (CNNs) and Swin Transformers which achieves\nbetter translation outcomes than other popular algorithms. When evaluated on\nHEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the\nhighest overall score on key metrics including the Structural Similarity Index\nMeasure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio\n(PSNR). Additionally, downstream analysis has been used to further validate the\nquality of the generated mIHC images. These results set a new benchmark in the\nfield of stain translation tasks.\n","authors":["Chang Bian","Beth Philips","Tim Cootes","Martin Fergie"],"pdf_url":"https://arxiv.org/pdf/2403.18501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02203v5","updated":"2024-03-27T12:12:45Z","published":"2023-07-05T10:54:50Z","title":"Neural Fields for Interactive Visualization of Statistical Dependencies\n  in 3D Simulation Ensembles","summary":"  We present the first neural network that has learned to compactly represent\nand can efficiently reconstruct the statistical dependencies between the values\nof physical variables at different spatial locations in large 3D simulation\nensembles. Going beyond linear dependencies, we consider mutual information as\na measure of non-linear dependence. We demonstrate learning and reconstruction\nwith a large weather forecast ensemble comprising 1000 members, each storing\nmultiple physical variables at a 250 x 352 x 20 simulation grid. By\ncircumventing compute-intensive statistical estimators at runtime, we\ndemonstrate significantly reduced memory and computation requirements for\nreconstructing the major dependence structures. This enables embedding the\nestimator into a GPU-accelerated direct volume renderer and interactively\nvisualizing all mutual dependencies for a selected domain point.\n","authors":["Fatemeh Farokhmanesh","Kevin Höhlein","Christoph Neuhauser","Tobias Necker","Martin Weissmann","Takemasa Miyoshi","Rüdiger Westermann"],"pdf_url":"https://arxiv.org/pdf/2307.02203v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18493v1","updated":"2024-03-27T12:08:41Z","published":"2024-03-27T12:08:41Z","title":"VersaT2I: Improving Text-to-Image Models with Versatile Reward","summary":"  Recent text-to-image (T2I) models have benefited from large-scale and\nhigh-quality data, demonstrating impressive performance. However, these T2I\nmodels still struggle to produce images that are aesthetically pleasing,\ngeometrically accurate, faithful to text, and of good low-level quality. We\npresent VersaT2I, a versatile training framework that can boost the performance\nwith multiple rewards of any T2I model. We decompose the quality of the image\ninto several aspects such as aesthetics, text-image alignment, geometry,\nlow-level quality, etc. Then, for every quality aspect, we select high-quality\nimages in this aspect generated by the model as the training set to finetune\nthe T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a\ngating function to combine multiple quality aspects, which can avoid conflicts\nbetween different quality aspects. Our method is easy to extend and does not\nrequire any manual annotation, reinforcement learning, or model architecture\nchanges. Extensive experiments demonstrate that VersaT2I outperforms the\nbaseline methods across various quality criteria.\n","authors":["Jianshu Guo","Wenhao Chai","Jie Deng","Hsiang-Wei Huang","Tian Ye","Yichen Xu","Jiawei Zhang","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18490v1","updated":"2024-03-27T12:05:22Z","published":"2024-03-27T12:05:22Z","title":"I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation","summary":"  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n","authors":["Ayoub Karine","Thibault Napoléon","Maher Jridi"],"pdf_url":"https://arxiv.org/pdf/2403.18490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16943v2","updated":"2024-03-27T11:46:36Z","published":"2023-12-28T10:40:11Z","title":"SAR-Net: Multi-scale Direction-aware SAR Network via Global Information\n  Fusion","summary":"  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion. Among them, Multi-scale Alignment Module (MAM) and distinct\nMulti-level Fusion Module (MFM) enhance feature integration by capturing both\ntexture detail and semantic information. Then, Multi-feature Embedding Module\n(MEM) feeds back global features into the primary branches, further improving\ninformation transmission. Additionally, DAM, through bidirectional attention\npolymerization, captures direction-aware information, effectively eliminating\nbackground interference. Extensive experiments demonstrate the effectiveness of\nSAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and\nship datasets (SSDD, HRSID), confirming its generalization capability and\nrobustness.\n","authors":["Mingxiang Cao","Jie Lei","Weiying Xie","Jiaqing Zhang","Daixun Li","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2312.16943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18476v1","updated":"2024-03-27T11:45:08Z","published":"2024-03-27T11:45:08Z","title":"Modeling uncertainty for Gaussian Splatting","summary":"  We present Stochastic Gaussian Splatting (SGS): the first framework for\nuncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\nnovel-view synthesis field by achieving impressive reconstruction quality at a\nfraction of the computational cost of Neural Radiance Fields (NeRF). However,\ncontrary to the latter, it still lacks the ability to provide information about\nthe confidence associated with their outputs. To address this limitation, in\nthis paper, we introduce a Variational Inference-based approach that seamlessly\nintegrates uncertainty prediction into the common rendering pipeline of GS.\nAdditionally, we introduce the Area Under Sparsification Error (AUSE) as a new\nterm in the loss function, enabling optimization of uncertainty estimation\nalongside image reconstruction. Experimental results on the LLFF dataset\ndemonstrate that our method outperforms existing approaches in terms of both\nimage rendering quality and uncertainty estimation accuracy. Overall, our\nframework equips practitioners with valuable insights into the reliability of\nsynthesized views, facilitating safer decision-making in real-world\napplications.\n","authors":["Luca Savant","Diego Valsesia","Enrico Magli"],"pdf_url":"https://arxiv.org/pdf/2403.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18471v1","updated":"2024-03-27T11:32:44Z","published":"2024-03-27T11:32:44Z","title":"DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face\n  Forgery Analysis","summary":"  The rapid progress in deep learning has given rise to hyper-realistic facial\nforgery methods, leading to concerns related to misinformation and security\nrisks. Existing face forgery datasets have limitations in generating\nhigh-quality facial images and addressing the challenges posed by evolving\ngenerative techniques. To combat this, we present DiffusionFace, the first\ndiffusion-based face forgery dataset, covering various forgery categories,\nincluding unconditional and Text Guide facial image generation, Img2Img,\nInpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace\ndataset stands out with its extensive collection of 11 diffusion models and the\nhigh-quality of the generated images, providing essential metadata and a\nreal-world internet-sourced forgery facial image dataset for evaluation.\nAdditionally, we provide an in-depth analysis of the data and introduce\npractical evaluation protocols to rigorously assess discriminative models'\neffectiveness in detecting counterfeit facial images, aiming to enhance\nsecurity in facial image authentication processes. The dataset is available for\ndownload at \\url{https://github.com/Rapisurazurite/DiffFace}.\n","authors":["Zhongxi Chen","Ke Sun","Ziyin Zhou","Xianming Lin","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18469v1","updated":"2024-03-27T11:28:57Z","published":"2024-03-27T11:28:57Z","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds","summary":"  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n","authors":["Zhimin Yuan","Wankang Zeng","Yanfei Su","Weiquan Liu","Ming Cheng","Yulan Guo","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18469v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18468v1","updated":"2024-03-27T11:28:32Z","published":"2024-03-27T11:28:32Z","title":"Deep Learning Segmentation and Classification of Red Blood Cells Using a\n  Large Multi-Scanner Dataset","summary":"  Digital pathology has recently been revolutionized by advancements in\nartificial intelligence, deep learning, and high-performance computing. With\nits advanced tools, digital pathology can help improve and speed up the\ndiagnostic process, reduce human errors, and streamline the reporting step. In\nthis paper, we report a new large red blood cell (RBC) image dataset and\npropose a two-stage deep learning framework for RBC image segmentation and\nclassification. The dataset is a highly diverse dataset of more than 100K RBCs\ncontaining eight different classes. The dataset, which is considerably larger\nthan any publicly available hematopathology dataset, was labeled independently\nby two hematopathologists who also manually created masks for RBC cell\nsegmentation. Subsequently, in the proposed framework, first, a U-Net model was\ntrained to achieve automatic RBC image segmentation. Second, an EfficientNetB0\nmodel was trained to classify RBC images into one of the eight classes using a\ntransfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%\nand an average classification accuracy of 96.5% were attained on the test set.\nMoreover, we have performed experimental comparisons against several prominent\nCNN models. These comparisons show the superiority of the proposed model with a\ngood balance between performance and computational cost.\n","authors":["Mohamed Elmanna","Ahmed Elsafty","Yomna Ahmed","Muhammad Rushdi","Ahmed Morsy"],"pdf_url":"https://arxiv.org/pdf/2403.18468v1.pdf","comment":"15 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.18461v1","updated":"2024-03-27T11:19:34Z","published":"2024-03-27T11:19:34Z","title":"DiffStyler: Diffusion-based Localized Image Style Transfer","summary":"  Image style transfer aims to imbue digital imagery with the distinctive\nattributes of style targets, such as colors, brushstrokes, shapes, whilst\nconcurrently preserving the semantic integrity of the content. Despite the\nadvancements in arbitrary style transfer methods, a prevalent challenge remains\nthe delicate equilibrium between content semantics and style attributes. Recent\ndevelopments in large-scale text-to-image diffusion models have heralded\nunprecedented synthesis capabilities, albeit at the expense of relying on\nextensive and often imprecise textual descriptions to delineate artistic\nstyles. Addressing these limitations, this paper introduces DiffStyler, a novel\napproach that facilitates efficient and precise arbitrary image style transfer.\nDiffStyler lies the utilization of a text-to-image Stable Diffusion model-based\nLoRA to encapsulate the essence of style targets. This approach, coupled with\nstrategic cross-LoRA feature and attention injection, guides the style transfer\nprocess. The foundation of our methodology is rooted in the observation that\nLoRA maintains the spatial feature consistency of UNet, a discovery that\nfurther inspired the development of a mask-wise style transfer technique. This\ntechnique employs masks extracted through a pre-trained FastSAM model,\nutilizing mask prompts to facilitate feature fusion during the denoising\nprocess, thereby enabling localized style transfer that preserves the original\nimage's unaffected regions. Moreover, our approach accommodates multiple style\ntargets through the use of corresponding masks. Through extensive\nexperimentation, we demonstrate that DiffStyler surpasses previous methods in\nachieving a more harmonious balance between content preservation and style\nintegration.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.18461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10522v4","updated":"2024-03-27T11:18:51Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18454v1","updated":"2024-03-27T11:13:20Z","published":"2024-03-27T11:13:20Z","title":"Scaling Vision-and-Language Navigation With Offline RL","summary":"  The study of vision-and-language navigation (VLN) has typically relied on\nexpert trajectories, which may not always be available in real-world situations\ndue to the significant effort required to collect them. On the other hand,\nexisting approaches to training VLN agents that go beyond available expert data\ninvolve data augmentations or online exploration which can be tedious and\nrisky. In contrast, it is easy to access large repositories of suboptimal\noffline trajectories. Inspired by research in offline reinforcement learning\n(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using\nsuboptimal demonstration data. We introduce a simple and effective\nreward-conditioned approach that can account for dataset suboptimality for\ntraining VLN agents, as well as benchmarks to evaluate progress and promote\nresearch in this area. We empirically study various noise models for\ncharacterizing dataset suboptimality among other unique challenges in VLN-ORL\nand instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in\nthe R2R and RxR environments. Our experiments demonstrate that the proposed\nreward-conditioned approach leads to significant performance improvements, even\nin complex and intricate environments.\n","authors":["Valay Bundele","Mahesh Bhupati","Biplab Banerjee","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2403.18454v1.pdf","comment":"Published in Transactions on Machine Learning Research (04/2024)"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18443v1","updated":"2024-03-27T11:00:33Z","published":"2024-03-27T11:00:33Z","title":"$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation\n  via Optical Flow Consistency and Feature Map Synthesis","summary":"  Self-supervised monocular depth estimation methods have been increasingly\ngiven much attention due to the benefit of not requiring large, labelled\ndatasets. Such self-supervised methods require high-quality salient features\nand consequently suffer from severe performance drop for indoor scenes, where\nlow-textured regions dominant in the scenes are almost indiscriminative. To\naddress the issue, we propose a self-supervised indoor monocular depth\nestimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow\nestimation network is introduced to supervise depth learning. To improve\noptical flow estimation performance in low-textured areas, only some patches of\npoints with more discriminative features are adopted for finetuning based on\nour well-designed patch-based photometric loss. The finetuned optical flow\nestimation network generates high-accuracy optical flow as a supervisory signal\nfor depth estimation. Correspondingly, an optical flow consistency loss is\ndesigned. Multi-scale feature maps produced by finetuned optical flow\nestimation network perform warping to compute feature map synthesis loss as\nanother supervisory signal for depth learning. Experimental results on the NYU\nDepth V2 dataset demonstrate the effectiveness of the framework and our\nproposed losses. To evaluate the generalization ability of our\n$\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of\napproximately 1500 points selected from 99 images in 18 scenes. Zero-shot\ngeneralization experiments on 7-Scenes dataset and Campus Indoor achieve\n$\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show\nthat our model can generalize well to monocular images captured in unknown\nindoor scenes.\n","authors":["Xiaotong Guo","Huijie Zhao","Shuwei Shao","Xudong Li","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17126v2","updated":"2024-03-27T10:50:54Z","published":"2022-11-30T16:03:24Z","title":"BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D\n  Object Detection","summary":"  Vision-centric bird-eye-view (BEV) perception has shown promising potential\nin autonomous driving. Recent works mainly focus on improving efficiency or\naccuracy but neglect the challenges when facing environment changing, resulting\nin severe degradation of transfer performance. For BEV perception, we figure\nout the significant domain gaps existing in typical real-world cross-domain\nscenarios and comprehensively solve the Domain Adaption (DA) problem for\nmulti-view 3D object detection. Since BEV perception approaches are complicated\nand contain several components, the domain shift accumulation on multiple\ngeometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In\nthis paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework\nto ease the domain shift accumulation, which consists of a Depth-Aware Teacher\n(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines\ntarget lidar and reliable depth prediction to construct depth-aware\ninformation, extracting target domain-specific knowledge in Voxel and BEV\nfeature spaces. It then transfers the sufficient domain knowledge of multiple\nspaces to the student model. In order to jointly alleviate the domain shift,\nGAS projects multi-geometric space features to a shared geometric embedding\nspace and decreases data distribution distance between two domains. To verify\nthe effectiveness of our method, we conduct BEV 3D object detection experiments\non three cross-domain scenarios and achieve state-of-the-art performance.\n","authors":["Jiaming Liu","Rongyu Zhang","Xiaoqi Li","Xiaowei Chi","Zehui Chen","Ming Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.17126v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.18442v1","updated":"2024-03-27T10:50:24Z","published":"2024-03-27T10:50:24Z","title":"Backpropagation-free Network for 3D Test-time Adaptation","summary":"  Real-world systems often encounter new data over time, which leads to\nexperiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods\ntend to apply computationally heavy and memory-intensive backpropagation-based\napproaches to handle this. Here, we propose a novel method that uses a\nbackpropagation-free approach for TTA for the specific case of 3D data. Our\nmodel uses a two-stream architecture to maintain knowledge about the source\ndomain as well as complementary target-domain-specific information. The\nbackpropagation-free property of our model helps address the well-known\nforgetting problem and mitigates the error accumulation issue. The proposed\nmethod also eliminates the need for the usually noisy process of\npseudo-labeling and reliance on costly self-supervised training. Moreover, our\nmethod leverages subspace learning, effectively reducing the distribution\nvariance between the two domains. Furthermore, the source-domain-specific and\nthe target-domain-specific streams are aligned using a novel entropy-based\nadaptive fusion strategy. Extensive experiments on popular benchmarks\ndemonstrate the effectiveness of our method. The code will be available at\nhttps://github.com/abie-e/BFTT3D.\n","authors":["Yanshuo Wang","Ali Cheraghian","Zeeshan Hayder","Jie Hong","Sameera Ramasinghe","Shafin Rahman","David Ahmedt-Aristizabal","Xuesong Li","Lars Petersson","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2403.18442v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.18113v2","updated":"2024-03-27T10:46:59Z","published":"2023-11-29T21:58:41Z","title":"Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features","summary":"  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n","authors":["Thomas Wimmer","Peter Wonka","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2311.18113v2.pdf","comment":"Accepted to CVPR 2024, Project page:\n  https://wimmerth.github.io/back-to-3d.html"},{"id":"http://arxiv.org/abs/2401.08742v2","updated":"2024-03-27T10:33:02Z","published":"2024-01-16T18:58:36Z","title":"Fast Dynamic 3D Object Generation from a Single-view Video","summary":"  Generating dynamic 3D object from a single-view video is challenging due to\nthe lack of 4D labeled data. Extending image-to-3D pipelines by transferring\noff-the-shelf image generation models such as score distillation sampling,\nexisting methods tend to be slow and expensive to scale due to the need for\nback-propagating the information-limited supervision signals through a large\npretrained model. To address this, we propose an efficient video-to-4D object\ngeneration framework called Efficient4D. It generates high-quality\nspacetime-consistent images under different camera views, and then uses them as\nlabeled data to directly train a novel 4D Gaussian splatting model with\nexplicit point cloud geometry, enabling real-time rendering under continuous\ncamera trajectories. Extensive experiments on synthetic and real videos show\nthat Efficient4D offers a remarkable 20-fold increase in speed when compared to\nprior art alternatives while preserving the quality of novel view synthesis.\nFor example, Efficient4D takes only 6 mins to model a dynamic object, vs 120\nmins by Consistent4D.\n","authors":["Zijie Pan","Zeyu Yang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.08742v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v2","updated":"2024-03-27T10:26:23Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12359v2","updated":"2024-03-27T10:18:04Z","published":"2023-12-19T17:40:27Z","title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary\n  semantic segmentation","summary":"  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose an\nopen-vocabulary semantic segmentation method, which does not require any\nannotations. We propose to locally improve dense MaskCLIP features, which are\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features. Our method CLIP-DINOiser needs only a\nsingle forward pass of CLIP and two light convolutional layers at inference, no\nextra supervision nor extra memory and reaches state-of-the-art results on\nchallenging and fine-grained benchmarks such as COCO, Pascal Context,\nCityscapes and ADE20k. The code to reproduce our results is available at\nhttps://github.com/wysoczanska/clip_dinoiser.\n","authors":["Monika Wysoczańska","Oriane Siméoni","Michaël Ramamonjisoa","Andrei Bursuc","Tomasz Trzciński","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2312.12359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12480v2","updated":"2024-03-27T10:12:32Z","published":"2023-12-19T15:34:52Z","title":"Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual\n  Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\npre-trained model to continually changing target distributions, addressing\nreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\nor teacher-student pseudo-labeling schemes for knowledge extraction in\nunlabeled target domains. However, dynamic data distributions cause\nmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\nlearning methods, hindering the effective mitigation of error accumulation and\ncatastrophic forgetting problems during the continual adaptation process. To\ntackle these issues, we propose a continual self-supervised method, Adaptive\nDistribution Masked Autoencoders (ADMA), which enhances the extraction of\ntarget domain knowledge while mitigating the accumulation of distribution\nshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\nto adaptively sample masked positions, followed by establishing consistency\nconstraints between the masked target samples and the original target samples.\nAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\na hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\nleveraging its invariant properties to boost task-relevant representations.\nThrough conducting extensive experiments on four widely recognized benchmarks,\nour proposed method attains state-of-the-art performance in both classification\nand segmentation CTTA tasks. Our project page:\nhttps://sites.google.com/view/continual-mae/home.\n","authors":["Jiaming Liu","Ran Xu","Senqiao Yang","Renrui Zhang","Qizhe Zhang","Zehui Chen","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12480v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18417v1","updated":"2024-03-27T10:09:38Z","published":"2024-03-27T10:09:38Z","title":"ECNet: Effective Controllable Text-to-Image Diffusion Models","summary":"  The conditional text-to-image diffusion models have garnered significant\nattention in recent years. However, the precision of these models is often\ncompromised mainly for two reasons, ambiguous condition input and inadequate\ncondition guidance over single denoising loss. To address the challenges, we\nintroduce two innovative solutions. Firstly, we propose a Spatial Guidance\nInjector (SGI) which enhances conditional detail by encoding text inputs with\nprecise annotation information. This method directly tackles the issue of\nambiguous control inputs by providing clear, annotated guidance to the model.\nSecondly, to overcome the issue of limited conditional supervision, we\nintroduce Diffusion Consistency Loss (DCL), which applies supervision on the\ndenoised latent code at any given time step. This encourages consistency\nbetween the latent code at each time step and the input signal, thereby\nenhancing the robustness and accuracy of the output. The combination of SGI and\nDCL results in our Effective Controllable Network (ECNet), which offers a more\naccurate controllable end-to-end text-to-image generation framework with a more\nprecise conditioning input and stronger controllable supervision. We validate\nour approach through extensive experiments on generation under various\nconditions, such as human body skeletons, facial landmarks, and sketches of\ngeneral objects. The results consistently demonstrate that our method\nsignificantly enhances the controllability and robustness of the generated\nimages, outperforming existing state-of-the-art controllable text-to-image\nmodels.\n","authors":["Sicheng Li","Keqiang Sun","Zhixin Lai","Xiaoshi Wu","Feng Qiu","Haoran Xie","Kazunori Miyata","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2403.18407v1","updated":"2024-03-27T09:49:37Z","published":"2024-03-27T09:49:37Z","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is\n  Critical for Semi-supervised Classification","summary":"  Semi-supervised learning (SSL) is a practical challenge in computer vision.\nPseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of\nThe Art (SOTA) performances in SSL. These approaches employ a\nthreshold-to-pseudo-label (T2L) process to generate PLs by truncating the\nconfidence scores of unlabeled data predicted by the self-training method.\nHowever, self-trained models typically yield biased and high-variance\npredictions, especially in the scenarios when a little labeled data are\nsupplied. To address this issue, we propose a lightweight channel-based\nensemble method to effectively consolidate multiple inferior PLs into the\ntheoretically guaranteed unbiased and low-variance one. Importantly, our\napproach can be readily extended to any SSL framework, such as FixMatch or\nFreeMatch. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art techniques on CIFAR10/100 in terms of\neffectiveness and efficiency.\n","authors":["Jiaqi Wu","Junbiao Pang","Baochang Zhang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.05262v2","updated":"2024-03-27T09:43:41Z","published":"2024-03-08T12:35:07Z","title":"Debiasing Multimodal Large Language Models","summary":"  In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.\n","authors":["Yi-Fan Zhang","Weichen Yu","Qingsong Wen","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.05262v2.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.01647v2","updated":"2024-03-27T09:39:41Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v2.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.11656v2","updated":"2024-03-27T09:34:44Z","published":"2024-03-18T10:53:00Z","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model","summary":"  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n","authors":["Yuxin Cao","Jinghao Li","Xi Xiao","Derui Wang","Minhui Xue","Hao Ge","Wei Liu","Guangwu Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11656v2.pdf","comment":"Accepted to 2024 IEEE Security and Privacy Workshops (SPW)"},{"id":"http://arxiv.org/abs/2403.18388v1","updated":"2024-03-27T09:25:20Z","published":"2024-03-27T09:25:20Z","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","summary":"  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient\ncomputing compared with Artificial Neural Networks (ANNs), closely mirroring\nbiological neural processes. However, this potential comes with inherent\nchallenges in directly training SNNs through spatio-temporal backpropagation --\nstemming from the temporal dynamics of spiking neurons and their discrete\nsignal processing -- which necessitates alternative ways of training, most\nnotably through ANN-SNN conversion. In this work, we introduce a lightweight\nForward Temporal Bias Correction (FTBC) technique, aimed at enhancing\nconversion accuracy without the computational overhead. We ground our method on\nprovided theoretical findings that through proper temporal bias calibration the\nexpected error of ANN-SNN conversion can be reduced to be zero after each time\nstep. We further propose a heuristic algorithm for finding the temporal bias\nonly in the forward pass, thus eliminating the computational burden of\nbackpropagation and we evaluate our method on CIFAR-10/100 and ImageNet\ndatasets, achieving a notable increase in accuracy on all datasets. Codes are\nreleased at a GitHub repository.\n","authors":["Xiaofeng Wu","Velibor Bojkovic","Bin Gu","Kun Suo","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2403.18388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06733v3","updated":"2024-03-27T09:24:56Z","published":"2023-12-11T10:43:28Z","title":"TULIP: Transformer for Upsampling of LiDAR Point Cloud","summary":"  LiDAR Upsampling is a challenging task for the perception systems of robots\nand autonomous vehicles, due to the sparse and irregular structure of\nlarge-scale scene contexts. Recent works propose to solve this problem by\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\nproblem in 2D image space. Although their methods can generate high-resolution\nrange images with fine-grained details, the resulting 3D point clouds often\nblur out details and predict invalid points. In this paper, we propose TULIP, a\nnew method to reconstruct high-resolution LiDAR point clouds from\nlow-resolution LiDAR input. We also follow a range image-based approach but\nspecifically modify the patch and window geometries of a Swin-Transformer-based\nnetwork to better fit the characteristics of range images. We conducted several\nexperiments on three public real-world and simulated datasets. TULIP\noutperforms state-of-the-art methods in all relevant metrics and generates\nrobust and more realistic point clouds than prior works.\n","authors":["Bin Yang","Patrick Pfreundschuh","Roland Siegwart","Marco Hutter","Peyman Moghadam","Vaishakh Patil"],"pdf_url":"https://arxiv.org/pdf/2312.06733v3.pdf","comment":"The paper was accepted by CVPR20224"},{"id":"http://arxiv.org/abs/2403.05218v2","updated":"2024-03-27T09:21:42Z","published":"2024-03-08T11:09:46Z","title":"3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder","summary":"  Monocular 3D face reconstruction plays a crucial role in avatar generation,\nwith significant demand in web-related applications such as generating virtual\nfinancial advisors in FinTech. Current reconstruction methods predominantly\nrely on deep learning techniques and employ 2D self-supervision as a means to\nguide model learning. However, these methods encounter challenges in capturing\nthe comprehensive 3D structural information of the face due to the utilization\nof 2D images for model training purposes. To overcome this limitation and\nenhance the reconstruction of 3D structural features, we propose an innovative\napproach that integrates existing 2D features with 3D features to guide the\nmodel learning process. Specifically, we introduce the 3D-ID Loss, which\nleverages the high-dimensional structure features extracted from a\nSpectral-Based Graph Convolution Encoder applied to the facial mesh. This\napproach surpasses the sole reliance on the 3D information provided by the\nfacial mesh vertices coordinates. Our model is trained using 2D-3D data pairs\nfrom a combination of datasets and achieves state-of-the-art performance on the\nNoW benchmark.\n","authors":["Haoxin Xu","Zezheng Zhao","Yuxin Cao","Chunyu Chen","Hao Ge","Ziyao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05218v2.pdf","comment":"4 pages, 3 figures. Accepted to WWW 2024"},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2401.17879v2","updated":"2024-03-27T09:17:14Z","published":"2024-01-31T14:36:49Z","title":"AEROBLADE: Training-Free Detection of Latent Diffusion Images Using\n  Autoencoder Reconstruction Error","summary":"  With recent text-to-image models, anyone can generate deceptively realistic\nimages with arbitrary contents, fueling the growing threat of visual\ndisinformation. A key enabler for generating high-resolution images with low\ncomputational cost has been the development of latent diffusion models (LDMs).\nIn contrast to conventional diffusion models, LDMs perform the denoising\nprocess in the low-dimensional latent space of a pre-trained autoencoder (AE)\ninstead of the high-dimensional image space. Despite their relevance, the\nforensic analysis of LDMs is still in its infancy. In this work we propose\nAEROBLADE, a novel detection method which exploits an inherent component of\nLDMs: the AE used to transform images between image and latent space. We find\nthat generated images can be more accurately reconstructed by the AE than real\nimages, allowing for a simple detection approach based on the reconstruction\nerror. Most importantly, our method is easy to implement and does not require\nany training, yet nearly matches the performance of detectors that rely on\nextensive training. We empirically demonstrate that AEROBLADE is effective\nagainst state-of-the-art LDMs, including Stable Diffusion and Midjourney.\nBeyond detection, our approach allows for the qualitative analysis of images,\nwhich can be leveraged for identifying inpainted regions. We release our code\nand data at https://github.com/jonasricker/aeroblade .\n","authors":["Jonas Ricker","Denis Lukovnikov","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2401.17879v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.00174v2","updated":"2024-03-27T09:13:19Z","published":"2024-02-29T22:58:13Z","title":"A citizen science toolkit to collect human perceptions of urban\n  environments using open street view images","summary":"  Street View-level Imagery (SVI) is a valuable data source for studies (e.g.,\nenvironmental assessments, green space identification or land cover\nclassification). While commercial SVI is available, such providers commonly\nrestrict copying or reuse in ways necessary for research. Open SVI datasets are\nreadily available from less restrictive sources, such as Mapillary, but due to\nthe heterogeneity of the images, these require substantial preprocessing,\nfiltering, and careful quality checks. We present an efficient method for\nautomated downloading, processing, cropping, and filtering open SVI, to be used\nin a survey of human perceptions of the streets portrayed in these images. We\ndemonstrate our open-source reusable SVI preparation and smartphone-friendly\nperception-survey software with Amsterdam (Netherlands) as the case study.\nUsing a citizen science approach, we collected from 331 people 22,637 ratings\nabout their perceptions for various criteria. We have published our software in\na public repository for future re-use and reproducibility.\n","authors":["Matthew Danish","SM Labib","Britta Ricker","Marco Helbich"],"pdf_url":"https://arxiv.org/pdf/2403.00174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18373v1","updated":"2024-03-27T09:10:01Z","published":"2024-03-27T09:10:01Z","title":"BAM: Box Abstraction Monitors for Real-time OoD Detection in Object\n  Detection","summary":"  Out-of-distribution (OoD) detection techniques for deep neural networks\n(DNNs) become crucial thanks to their filtering of abnormal inputs, especially\nwhen DNNs are used in safety-critical applications and interact with an open\nand dynamic environment. Nevertheless, integrating OoD detection into\nstate-of-the-art (SOTA) object detection DNNs poses significant challenges,\npartly due to the complexity introduced by the SOTA OoD construction methods,\nwhich require the modification of DNN architecture and the introduction of\ncomplex loss functions. This paper proposes a simple, yet surprisingly\neffective, method that requires neither retraining nor architectural change in\nobject detection DNN, called Box Abstraction-based Monitors (BAM). The novelty\nof BAM stems from using a finite union of convex box abstractions to capture\nthe learned features of objects for in-distribution (ID) data, and an important\nobservation that features from OoD data are more likely to fall outside of\nthese boxes. The union of convex regions within the feature space allows the\nformation of non-convex and interpretable decision boundaries, overcoming the\nlimitations of VOS-like detectors without sacrificing real-time performance.\nExperiments integrating BAM into Faster R-CNN-based object detection DNNs\ndemonstrate a considerably improved performance against SOTA OoD detection\ntechniques.\n","authors":["Changshun Wu","Weicheng He","Chih-Hong Cheng","Xiaowei Huang","Saddek Bensalem"],"pdf_url":"https://arxiv.org/pdf/2403.18373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18370v1","updated":"2024-03-27T09:06:36Z","published":"2024-03-27T09:06:36Z","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","summary":"  In recent years, remarkable advancements have been achieved in the field of\nimage generation, primarily driven by the escalating demand for high-quality\noutcomes across various image generation subtasks, such as inpainting,\ndenoising, and super resolution. A major effort is devoted to exploring the\napplication of super-resolution techniques to enhance the quality of\nlow-resolution images. In this context, our method explores in depth the\nproblem of ship image super resolution, which is crucial for coastal and port\nsurveillance. We investigate the opportunity given by the growing interest in\ntext-to-image diffusion models, taking advantage of the prior knowledge that\nsuch foundation models have already learned. In particular, we present a\ndiffusion-model-based architecture that leverages text conditioning during\ntraining while being class-aware, to best preserve the crucial details of the\nships during the generation of the super-resoluted image. Since the specificity\nof this task and the scarcity availability of off-the-shelf data, we also\nintroduce a large labeled ship dataset scraped from online ship images, mostly\nfrom ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method\nachieves more robust results than other deep learning models previously\nemployed for super resolution, as proven by the multiple experiments performed.\nMoreover, we investigate how this model can benefit downstream tasks, such as\nclassification and object detection, thus emphasizing practical implementation\nin a real-world scenario. Experimental results show flexibility, reliability,\nand impressive performance of the proposed framework over state-of-the-art\nmethods for different tasks. The code is available at:\nhttps://github.com/LuigiSigillo/ShipinSight .\n","authors":["Luigi Sigillo","Riccardo Fosco Gramaccioni","Alessandro Nicolosi","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.18370v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2312.10114v2","updated":"2024-03-27T09:00:54Z","published":"2023-12-15T09:49:21Z","title":"FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring\n  Benchmark for remote sensing foundation models","summary":"  Forests are an essential part of Earth's ecosystems and natural systems, as\nwell as providing services on which humanity depends, yet they are rapidly\nchanging as a result of land use decisions and climate change. Understanding\nand mitigating negative effects requires parsing data on forests at global\nscale from a broad array of sensory modalities, and recently many such problems\nhave been approached using machine learning algorithms for remote sensing. To\ndate, forest-monitoring problems have largely been addressed in isolation.\nInspired by the rise of foundation models for computer vision and remote\nsensing, we here present the first unified Forest Monitoring Benchmark\n(FoMo-Bench). FoMo-Bench consists of 15 diverse datasets encompassing\nsatellite, aerial, and inventory data, covering a variety of geographical\nregions, and including multispectral, red-green-blue, synthetic aperture radar\n(SAR) and LiDAR data with various temporal, spatial and spectral resolutions.\nFoMo-Bench includes multiple types of forest-monitoring tasks, spanning\nclassification, segmentation, and object detection. To further enhance the\ndiversity of tasks and geographies represented in FoMo-Bench, we introduce a\nnovel global dataset, TalloS, combining satellite imagery with ground-based\nannotations for tree species classification, encompassing 1,000+ categories\nacross multiple hierarchical taxonomic levels (species, genus, family).\nFinally, we propose FoMo-Net, a baseline foundation model with the capacity to\nprocess any combination of commonly used spectral bands in remote sensing,\nacross diverse ground sampling distances and geographical locations worldwide.\nThis work aims to inspire research collaborations between machine learning and\nforest biology researchers in exploring scalable multi-modal and multi-task\nmodels for forest monitoring. All code and data will be made publicly\navailable.\n","authors":["Nikolaos Ioannis Bountos","Arthur Ouaknine","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2312.10114v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2402.19473v2","updated":"2024-03-27T09:00:25Z","published":"2024-02-29T18:59:01Z","title":"Retrieval-Augmented Generation for AI-Generated Content: A Survey","summary":"  The development of Artificial Intelligence Generated Content (AIGC) has been\nfacilitated by advancements in model algorithms, the increasing scale of\nfoundation models, and the availability of ample high-quality datasets. While\nAIGC has achieved remarkable performance, it still faces several challenges,\nsuch as the difficulty of maintaining up-to-date and long-tail knowledge, the\nrisk of data leakage, and the high costs associated with training and\ninference. Retrieval-Augmented Generation(RAG) has recently emerged as a\nparadigm to address such challenges. In particular, RAG introduces the\ninformation retrieval process, which enhances the generation process by\nretrieving relevant objects from available data stores, leading to higher\naccuracy and better robustness. In this paper, we comprehensively review\nexisting efforts that integrate RAG technique into AIGC scenarios. We first\nclassify RAG foundations according to how the retriever augments the generator,\ndistilling the fundamental abstractions of the augmentation methodologies for\nvarious retrievers and generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies that help with\npotential future progress. We also summarize additional enhancements methods\nfor RAG, facilitating effective engineering and implementation of RAG systems.\nThen from another view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for researchers\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\nthe limitations of current RAG systems, and suggest potential directions for\nfuture research.Project Repo: https://github.com/hymie122/RAG-Survey.\n","authors":["Penghao Zhao","Hailin Zhang","Qinhan Yu","Zhengren Wang","Yunteng Geng","Fangcheng Fu","Ling Yang","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2402.19473v2.pdf","comment":"Citing 380 papers, 36 pages, 16 figures. Project:\n  https://github.com/hymie122/RAG-Survey"},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18361v1","updated":"2024-03-27T08:53:13Z","published":"2024-03-27T08:53:13Z","title":"ViTAR: Vision Transformer with Any Resolution","summary":"  his paper tackles a significant challenge faced by Vision Transformers\n(ViTs): their constrained scalability across different image resolutions.\nTypically, ViTs experience a performance decline when processing resolutions\ndifferent from those seen during training. Our work introduces two key\ninnovations to address this issue. Firstly, we propose a novel module for\ndynamic resolution adjustment, designed with a single Transformer block,\nspecifically to achieve highly efficient incremental token integration.\nSecondly, we introduce fuzzy positional encoding in the Vision Transformer to\nprovide consistent positional awareness across multiple resolutions, thereby\npreventing overfitting to any single training resolution. Our resulting model,\nViTAR (Vision Transformer with Any Resolution), demonstrates impressive\nadaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and\n80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational\ncosts. ViTAR also shows strong performance in downstream tasks such as instance\nand semantic segmentation and can easily combined with self-supervised learning\ntechniques like Masked AutoEncoder. Our work provides a cost-effective solution\nfor enhancing the resolution scalability of ViTs, paving the way for more\nversatile and efficient high-resolution image processing.\n","authors":["Qihang Fan","Quanzeng You","Xiaotian Han","Yongfei Liu","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2403.18361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18360v1","updated":"2024-03-27T08:52:44Z","published":"2024-03-27T08:52:44Z","title":"Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific\n  Boundaries for Domain Adaptation","summary":"  Most domain adaptation (DA) methods are based on either a convolutional\nneural networks (CNNs) or a vision transformers (ViTs). They align the\ndistribution differences between domains as encoders without considering their\nunique characteristics. For instance, ViT excels in accuracy due to its\nsuperior ability to capture global representations, while CNN has an advantage\nin capturing local representations. This fact has led us to design a hybrid\nmethod to fully take advantage of both ViT and CNN, called Explicitly\nClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine their\ndistinct strengths. In particular, we leverage ViT's properties to explicitly\nfind class-specific decision boundaries by maximizing the discrepancy between\nthe outputs of the two classifiers to detect target samples far from the source\nsupport. In contrast, the CNN encoder clusters target features based on the\npreviously defined class-specific boundaries by minimizing the discrepancy\nbetween the probabilities of the two classifiers. Finally, ViT and CNN mutually\nexchange knowledge to improve the quality of pseudo labels and reduce the\nknowledge discrepancies of these models. Compared to conventional DA methods,\nour ECB achieves superior performance, which verifies its effectiveness in this\nhybrid model. The project website can be found\nhttps://dotrannhattuong.github.io/ECB/website/.\n","authors":["Ba Hung Ngo","Nhat-Tuong Do-Tran","Tuan-Ngoc Nguyen","Hae-Gon Jeon","Tae Jong Choi"],"pdf_url":"https://arxiv.org/pdf/2403.18360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18356v1","updated":"2024-03-27T08:48:47Z","published":"2024-03-27T08:48:47Z","title":"MonoHair: High-Fidelity Hair Modeling from a Monocular Video","summary":"  Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic\nexpression, and immersion in computer graphics. While existing 3D hair modeling\nmethods have achieved impressive performance, the challenge of achieving\nhigh-quality hair reconstruction persists: they either require strict capture\nconditions, making practical applications difficult, or heavily rely on learned\nprior data, obscuring fine-grained details in images. To address these\nchallenges, we propose MonoHair,a generic framework to achieve high-fidelity\nhair reconstruction from a monocular video, without specific requirements for\nenvironments. Our approach bifurcates the hair modeling process into two main\nstages: precise exterior reconstruction and interior structure inference. The\nexterior is meticulously crafted using our Patch-based Multi-View Optimization\n(PMVO). This method strategically collects and integrates hair information from\nmultiple views, independent of prior data, to produce a high-fidelity exterior\n3D line map. This map not only captures intricate details but also facilitates\nthe inference of the hair's inner structure. For the interior, we employ a\ndata-driven, multi-view 3D hair reconstruction method. This method utilizes 2D\nstructural renderings derived from the reconstructed exterior, mirroring the\nsynthetic 2D inputs used during training. This alignment effectively bridges\nthe domain gap between our training data and real-world data, thereby enhancing\nthe accuracy and reliability of our interior structure inference. Lastly, we\ngenerate a strand model and resolve the directional ambiguity by our hair\ngrowth algorithm. Our experiments demonstrate that our method exhibits\nrobustness across diverse hairstyles and achieves state-of-the-art performance.\nFor more results, please refer to our project page\nhttps://keyuwu-cs.github.io/MonoHair/.\n","authors":["Keyu Wu","Lingchen Yang","Zhiyi Kuang","Yao Feng","Xutao Han","Yuefan Shen","Hongbo Fu","Kun Zhou","Youyi Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.18356v1.pdf","comment":"Accepted by IEEE CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18342v1","updated":"2024-03-27T08:32:48Z","published":"2024-03-27T08:32:48Z","title":"Learning Inclusion Matching for Animation Paint Bucket Colorization","summary":"  Colorizing line art is a pivotal task in the production of hand-drawn cel\nanimation. This typically involves digital painters using a paint bucket tool\nto manually color each segment enclosed by lines, based on RGB values\npredetermined by a color designer. This frame-by-frame process is both arduous\nand time-intensive. Current automated methods mainly focus on segment matching.\nThis technique migrates colors from a reference to the target frame by aligning\nfeatures within line-enclosed segments across frames. However, issues like\nocclusion and wrinkles in animations often disrupt these direct\ncorrespondences, leading to mismatches. In this work, we introduce a new\nlearning-based inclusion matching pipeline, which directs the network to\ncomprehend the inclusion relationships between segments rather than relying\nsolely on direct visual correspondences. Our method features a two-stage\npipeline that integrates a coarse color warping module with an inclusion\nmatching module, enabling more nuanced and accurate colorization. To facilitate\nthe training of our network, we also develope a unique dataset, referred to as\nPaintBucket-Character. This dataset includes rendered line arts alongside their\ncolorized counterparts, featuring various 3D characters. Extensive experiments\ndemonstrate the effectiveness and superiority of our method over existing\ntechniques.\n","authors":["Yuekun Dai","Shangchen Zhou","Qinyue Li","Chongyi Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18342v1.pdf","comment":"accepted to CVPR 2024. Project Page:\n  https://ykdai.github.io/projects/InclusionMatching"},{"id":"http://arxiv.org/abs/2403.18339v1","updated":"2024-03-27T08:28:14Z","published":"2024-03-27T08:28:14Z","title":"H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for\n  Tumor Segmentation in PET/CT Images","summary":"  Positron emission tomography (PET) combined with computed tomography (CT)\nimaging is routinely used in cancer diagnosis and prognosis by providing\ncomplementary information. Automatically segmenting tumors in PET/CT images can\nsignificantly improve examination efficiency. Traditional multi-modal\nsegmentation solutions mainly rely on concatenation operations for modality\nfusion, which fail to effectively model the non-linear dependencies between PET\nand CT modalities. Recent studies have investigated various approaches to\noptimize the fusion of modality-specific features for enhancing joint\nrepresentations. However, modality-specific encoders used in these methods\noperate independently, inadequately leveraging the synergistic relationships\ninherent in PET and CT modalities, for example, the complementarity between\nsemantics and structure. To address these issues, we propose a Hierarchical\nAdaptive Interaction and Weighting Network termed H2ASeg to explore the\nintrinsic cross-modal correlations and transfer potential complementary\ninformation. Specifically, we design a Modality-Cooperative Spatial Attention\n(MCSA) module that performs intra- and inter-modal interactions globally and\nlocally. Additionally, a Target-Aware Modality Weighting (TAMW) module is\ndeveloped to highlight tumor-related features within multi-modal features,\nthereby refining tumor segmentation. By embedding these modules across\ndifferent layers, H2ASeg can hierarchically model cross-modal correlations,\nenabling a nuanced understanding of both semantic and structural tumor\nfeatures. Extensive experiments demonstrate the superiority of H2ASeg,\noutperforming state-of-the-art methods on AutoPet-II and Hecktor2022\nbenchmarks. The code is released at https://github.com/G14nTDo4/H2ASeg.\n","authors":["Jinpeng Lu","Jingyun Chen","Linghan Cai","Songhan Jiang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18339v1.pdf","comment":"10 pages,4 figures"},{"id":"http://arxiv.org/abs/2403.17301v2","updated":"2024-03-27T08:23:09Z","published":"2024-03-26T01:06:47Z","title":"Physical 3D Adversarial Attacks against Monocular Depth Estimation in\n  Autonomous Driving","summary":"  Deep learning-based monocular depth estimation (MDE), extensively applied in\nautonomous driving, is known to be vulnerable to adversarial attacks. Previous\nphysical attacks against MDE models rely on 2D adversarial patches, so they\nonly affect a small, localized region in the MDE map but fail under various\nviewpoints. To address these limitations, we propose 3D Depth Fool\n(3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.\n3D$^2$Fool is specifically optimized to generate 3D adversarial textures\nagnostic to model types of vehicles and to have improved robustness in bad\nweather conditions, such as rain and fog. Experimental results validate the\nsuperior performance of our 3D$^2$Fool across various scenarios, including\nvehicles, MDE models, weather conditions, and viewpoints. Real-world\nexperiments with printed 3D textures on physical vehicle models further\ndemonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.\n","authors":["Junhao Zheng","Chenhao Lin","Jiahao Sun","Zhengyu Zhao","Qian Li","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.17301v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2306.02928v2","updated":"2024-03-27T08:21:17Z","published":"2023-06-05T14:45:38Z","title":"Weakly-Supervised Conditional Embedding for Referred Visual Search","summary":"  This paper introduces a new challenge for image similarity search in the\ncontext of fashion, addressing the inherent ambiguity in this domain stemming\nfrom complex images. We present Referred Visual Search (RVS), a task allowing\nusers to define more precisely the desired similarity, following recent\ninterest in the industry. We release a new large public dataset,\nLAION-RVS-Fashion, consisting of 272k fashion products with 842k images\nextracted from LAION, designed explicitly for this task. However, unlike\ntraditional visual search methods in the industry, we demonstrate that superior\nperformance can be achieved by bypassing explicit object detection and adopting\nweakly-supervised conditional contrastive learning on image tuples. Our method\nis lightweight and demonstrates robustness, reaching Recall at one superior to\nstrong detection-based baselines against 2M distractors. Code, data and models\nare available at https://www.github.com/Simon-Lepage/CondViT-LRVSF .\n","authors":["Simon Lepage","Jérémie Mary","David Picard"],"pdf_url":"https://arxiv.org/pdf/2306.02928v2.pdf","comment":"28 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.18334v1","updated":"2024-03-27T08:16:33Z","published":"2024-03-27T08:16:33Z","title":"DODA: Diffusion for Object-detection Domain Adaptation in Agriculture","summary":"  The diverse and high-quality content generated by recent generative models\ndemonstrates the great potential of using synthetic data to train downstream\nmodels. However, in vision, especially in objection detection, related areas\nare not fully explored, the synthetic images are merely used to balance the\nlong tails of existing datasets, and the accuracy of the generated labels is\nlow, the full potential of generative models has not been exploited. In this\npaper, we propose DODA, a data synthesizer that can generate high-quality\nobject detection data for new domains in agriculture. Specifically, we improve\nthe controllability of layout-to-image through encoding layout as an image,\nthereby improving the quality of labels, and use a visual encoder to provide\nvisual clues for the diffusion model to decouple visual features from the\ndiffusion model, and empowering the model the ability to generate data in new\ndomains. On the Global Wheat Head Detection (GWHD) Dataset, which is the\nlargest dataset in agriculture and contains diverse domains, using the data\nsynthesized by DODA improves the performance of the object detector by\n12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the\ntraining data.\n","authors":["Shuai Xiang","Pieter M. Blok","James Burridge","Haozhou Wang","Wei Guo"],"pdf_url":"https://arxiv.org/pdf/2403.18334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18330v1","updated":"2024-03-27T08:11:25Z","published":"2024-03-27T08:11:25Z","title":"Tracking-Assisted Object Detection with Event Cameras","summary":"  Event-based object detection has recently garnered attention in the computer\nvision community due to the exceptional properties of event cameras, such as\nhigh dynamic range and no motion blur. However, feature asynchronism and\nsparsity cause invisible objects due to no relative motion to the camera,\nposing a significant challenge in the task. Prior works have studied various\nmemory mechanisms to preserve as many features as possible at the current time,\nguided by temporal clues. While these implicit-learned memories retain some\nshort-term information, they still struggle to preserve long-term features\neffectively. In this paper, we consider those invisible objects as\npseudo-occluded objects and aim to reveal their features. Firstly, we introduce\nvisibility attribute of objects and contribute an auto-labeling algorithm to\nappend additional visibility labels on an existing event camera dataset.\nSecondly, we exploit tracking strategies for pseudo-occluded objects to\nmaintain their permanence and retain their bounding boxes, even when features\nhave not been available for a very long time. These strategies can be treated\nas an explicit-learned memory guided by the tracking objective to record the\ndisplacements of objects across frames. Lastly, we propose a spatio-temporal\nfeature aggregation module to enrich the latent features and a consistency loss\nto increase the robustness of the overall pipeline. We conduct comprehensive\nexperiments to verify our method's effectiveness where still objects are\nretained but real occluded objects are discarded. The results demonstrate that\n(1) the additional visibility labels can assist in supervised training, and (2)\nour method outperforms state-of-the-art approaches with a significant\nimprovement of 7.9% absolute mAP.\n","authors":["Ting-Kang Yen","Igor Morawski","Shusil Dangi","Kai He","Chung-Yi Lin","Jia-Fong Yeh","Hung-Ting Su","Winston Hsu"],"pdf_url":"https://arxiv.org/pdf/2403.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18328v1","updated":"2024-03-27T08:09:04Z","published":"2024-03-27T08:09:04Z","title":"PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans","summary":"  Information from neuroimaging examinations (CT, MRI) is increasingly used to\nsupport diagnoses of dementia, e.g., Alzheimer's disease. While current\nclinical practice is mainly based on visual inspection and feature engineering,\nDeep Learning approaches can be used to automate the analysis and to discover\nnew image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative\nto standard blackbox models, and have shown promising results in general\ncomputer vision. PP-NN's base their reasoning on prototypical image regions\nthat are learned fully unsupervised, and combined with a simple-to-understand\ndecision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply\nPIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from\nstructural Magnetic Resonance Imaging (sMRI). We assess the quality of\nprototypes under a systematic evaluation framework, propose new metrics to\nevaluate brain prototypes and perform an evaluation with domain experts. Our\nresults show that PIPNet3D is an interpretable, compact model for Alzheimer's\ndiagnosis with its reasoning well aligned to medical domain knowledge. Notably,\nPIPNet3D achieves the same accuracy as its blackbox counterpart; and removing\nthe remaining clinically irrelevant prototypes from its decision process does\nnot decrease predictive performance.\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Michael Scheschenja","Joel Wessendorf","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.18328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10030v2","updated":"2024-03-27T07:52:10Z","published":"2024-03-15T05:30:29Z","title":"Multi-criteria Token Fusion with One-step-ahead Attention for Efficient\n  Vision Transformers","summary":"  Vision Transformer (ViT) has emerged as a prominent backbone for computer\nvision. For more efficient ViTs, recent works lessen the quadratic cost of the\nself-attention layer by pruning or fusing the redundant tokens. However, these\nworks faced the speed-accuracy trade-off caused by the loss of information.\nHere, we argue that token fusion needs to consider diverse relations between\ntokens to minimize information loss. In this paper, we propose a Multi-criteria\nToken Fusion (MCTF), that gradually fuses the tokens based on multi-criteria\n(e.g., similarity, informativeness, and size of fused tokens). Further, we\nutilize the one-step-ahead attention, which is the improved approach to capture\nthe informativeness of the tokens. By training the model equipped with MCTF\nusing a token reduction consistency, we achieve the best speed-accuracy\ntrade-off in the image classification (ImageNet1K). Experimental results prove\nthat MCTF consistently surpasses the previous reduction methods with and\nwithout training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by\nabout 44% while improving the performance (+0.5%, and +0.3%) over the base\nmodel, respectively. We also demonstrate the applicability of MCTF in various\nVision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup\nwithout performance degradation. Code is available at\nhttps://github.com/mlvlab/MCTF.\n","authors":["Sanghyeok Lee","Joonmyung Choi","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10030v2.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2403.18321v1","updated":"2024-03-27T07:50:45Z","published":"2024-03-27T07:50:45Z","title":"Implementation of the Principal Component Analysis onto High-Performance\n  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and\n  Comparisons","summary":"  Dimensionality reduction represents a critical preprocessing step in order to\nincrease the efficiency and the performance of many hyperspectral imaging\nalgorithms. However, dimensionality reduction algorithms, such as the Principal\nComponent Analysis (PCA), suffer from their computationally demanding nature,\nbecoming advisable for their implementation onto high-performance computer\narchitectures for applications under strict latency constraints. This work\npresents the implementation of the PCA algorithm onto two different\nhigh-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and\na Kalray manycore, uncovering a highly valuable set of tips and tricks in order\nto take full advantage of the inherent parallelism of these high-performance\ncomputing platforms, and hence, reducing the time that is required to process a\ngiven hyperspectral image. Moreover, the achieved results obtained with\ndifferent hyperspectral images have been compared with the ones that were\nobtained with a field programmable gate array (FPGA)-based implementation of\nthe PCA algorithm that has been recently published, providing, for the first\ntime in the literature, a comprehensive analysis in order to highlight the pros\nand cons of each option.\n","authors":["E. Martel","R. Lazcano","J. Lopez","D. Madroñal","R. Salvador","S. Lopez","E. Juarez","R. Guerra","C. Sanz","R. Sarmiento"],"pdf_url":"https://arxiv.org/pdf/2403.18321v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18318v1","updated":"2024-03-27T07:40:51Z","published":"2024-03-27T07:40:51Z","title":"Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via\n  Bayesian Neural Networks","summary":"  Adversarial attacks have demonstrated the vulnerability of Machine Learning\n(ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) systems. An adversarial attack can deceive the classifier\ninto making incorrect predictions by perturbing the input SAR images, for\nexample, with a few scatterers attached to the on-ground objects. Therefore, it\nis critical to develop robust SAR ATR systems that can detect potential\nadversarial attacks by leveraging the inherent uncertainty in ML classifiers,\nthereby effectively alerting human decision-makers. In this paper, we propose a\nnovel uncertainty-aware SAR ATR for detecting adversarial attacks.\nSpecifically, we leverage the capability of Bayesian Neural Networks (BNNs) in\nperforming image classification with quantified epistemic uncertainty to\nmeasure the confidence for each input SAR image. By evaluating the uncertainty,\nour method alerts when the input SAR image is likely to be adversarially\ngenerated. Simultaneously, we also generate visual explanations that reveal the\nspecific regions in the SAR image where the adversarial scatterers are likely\nto to be present, thus aiding human decision-making with hints of evidence of\nadversarial attacks. Experiments on the MSTAR dataset demonstrate that our\napproach can identify over 80% adversarial SAR images with fewer than 20% false\nalarms, and our visual explanations can identify up to over 90% of scatterers\nin an adversarial SAR image.\n","authors":["Tian Ye","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2403.18318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08251v2","updated":"2024-03-27T07:33:42Z","published":"2022-12-16T02:43:52Z","title":"Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental\n  Learning","summary":"  Exemplar-free Class Incremental Learning (EFCIL) aims to sequentially learn\ntasks with access only to data from the current one. EFCIL is of interest\nbecause it mitigates concerns about privacy and long-term storage of data,\nwhile at the same time alleviating the problem of catastrophic forgetting in\nincremental learning. In this work, we introduce task-adaptive saliency for\nEFCIL and propose a new framework, which we call Task-Adaptive Saliency\nSupervision (TASS), for mitigating the negative effects of saliency drift\nbetween different tasks. We first apply boundary-guided saliency to maintain\ntask adaptivity and \\textit{plasticity} on model attention. Besides, we\nintroduce task-agnostic low-level signals as auxiliary supervision to increase\nthe \\textit{stability} of model attention. Finally, we introduce a module for\ninjecting and recovering saliency noise to increase the robustness of saliency\npreservation. Our experiments demonstrate that our method can better preserve\nsaliency maps across tasks and achieve state-of-the-art results on the\nCIFAR-100, Tiny-ImageNet, and ImageNet-Subset EFCIL benchmarks. Code is\navailable at \\url{https://github.com/scok30/tass}.\n","authors":["Xialei Liu","Jiang-Tian Zhai","Andrew D. Bagdanov","Ke Li","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.08251v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2307.09136v2","updated":"2024-03-27T07:16:28Z","published":"2023-07-18T10:34:21Z","title":"The Effects of Mixed Sample Data Augmentation are Class Dependent","summary":"  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and\nPuzzleMix, have been widely acknowledged for enhancing performance in a variety\nof tasks. A previous study reported the class dependency of traditional data\naugmentation (DA), where certain classes benefit disproportionately compared to\nothers. This paper reveals a class dependent effect of MSDA, where some classes\nexperience improved performance while others experience degraded performance.\nThis research addresses the issue of class dependency in MSDA and proposes an\nalgorithm to mitigate it. The approach involves training on a mixture of MSDA\nand non-MSDA data, which not only mitigates the negative impact on the affected\nclasses, but also improves overall accuracy. Furthermore, we provide in-depth\nanalysis and discussion of why MSDA introduced class dependencies and which\nclasses are most likely to have them.\n","authors":["Haeil Lee","Hansang Lee","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2307.09136v2.pdf","comment":"21 pages, 18 figures, Overall Revision"},{"id":"http://arxiv.org/abs/2402.18920v5","updated":"2024-03-27T07:16:21Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v5.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2308.13356v3","updated":"2024-03-27T07:12:09Z","published":"2023-08-25T13:05:06Z","title":"CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions\n  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and\n  Classification from Ultrasound Images","summary":"  Undoubtedly breast cancer identifies itself as one of the most widespread and\nterrifying cancers across the globe. Millions of women are getting affected\neach year from it. Breast cancer remains the major one for being the reason of\nlargest number of demise of women. In the recent time of research, Medical\nImage Computing and Processing has been playing a significant role for\ndetecting and classifying breast cancers from ultrasound images and mammograms,\nalong with the celestial touch of deep neural networks. In this research, we\nfocused mostly on our rigorous implementations and iterative result analysis of\ndifferent cutting-edge modified versions of EfficientNet architectures namely\nEfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,\nnamed as CEIMVEN. We utilized transfer learning approach here for using the\npre-trained models of EfficientNet versions. We activated the hyper-parameter\ntuning procedures, added fully connected layers, discarded the unprecedented\noutliers and recorded the accuracy results from our custom modified\nEfficientNet architectures. Our deep learning model training approach was\nrelated to both identifying the cancer affected areas with region of interest\n(ROI) techniques and multiple classifications (benign, malignant and normal).\nThe approximate testing accuracies we got from the modified versions of\nEfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,\nb5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-\n99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong\npotentials of deep learning approach for the successful detection and\nclassification of breast cancers from the ultrasound images at a very early\nstage. The code for this research is available here:\nhttps://github.com/ac005sheekar/CEIMVEN-Breast.\n","authors":["Sheekar Banerjee","Md. Kamrul Hasan Monir"],"pdf_url":"https://arxiv.org/pdf/2308.13356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.07392v3","updated":"2024-03-27T06:44:13Z","published":"2024-03-12T07:59:41Z","title":"ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature\n  Interaction for Dense Predictions","summary":"  Although Vision Transformer (ViT) has achieved significant success in\ncomputer vision, it does not perform well in dense prediction tasks due to the\nlack of inner-patch information interaction and the limited diversity of\nfeature scale. Most existing studies are devoted to designing vision-specific\ntransformers to solve the above problems, which introduce additional\npre-training costs. Therefore, we present a plain, pre-training-free, and\nfeature-enhanced ViT backbone with Convolutional Multi-scale feature\ninteraction, named ViT-CoMer, which facilitates bidirectional interaction\nbetween CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has\nthe following advantages: (1) We inject spatial pyramid multi-receptive field\nconvolutional features into the ViT architecture, which effectively alleviates\nthe problems of limited local information interaction and single-feature\nrepresentation in ViT. (2) We propose a simple and efficient CNN-Transformer\nbidirectional fusion interaction module that performs multi-scale fusion across\nhierarchical features, which is beneficial for handling dense prediction tasks.\n(3) We evaluate the performance of ViT-CoMer across various dense prediction\ntasks, different frameworks, and multiple advanced pre-training. Notably, our\nViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and\n62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art\nmethods. We hope ViT-CoMer can serve as a new backbone for dense prediction\ntasks to facilitate future research. The code will be released at\nhttps://github.com/Traffic-X/ViT-CoMer.\n","authors":["Chunlong Xia","Xinliang Wang","Feng Lv","Xin Hao","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07392v3.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18294v1","updated":"2024-03-27T06:40:26Z","published":"2024-03-27T06:40:26Z","title":"Multi-scale Unified Network for Image Classification","summary":"  Convolutional Neural Networks (CNNs) have advanced significantly in visual\nrepresentation learning and recognition. However, they face notable challenges\nin performance and computational efficiency when dealing with real-world,\nmulti-scale image inputs. Conventional methods rescale all input images into a\nfixed size, wherein a larger fixed size favors performance but rescaling small\nsize images to a larger size incurs digitization noise and increased\ncomputation cost. In this work, we carry out a comprehensive, layer-wise\ninvestigation of CNN models in response to scale variation, based on Centered\nKernel Alignment (CKA) analysis. The observations reveal lower layers are more\nsensitive to input image scale variations than high-level layers. Inspired by\nthis insight, we propose Multi-scale Unified Network (MUSN) consisting of\nmulti-scale subnets, a unified network, and scale-invariant constraint. Our\nmethod divides the shallow layers into multi-scale subnets to enable feature\nextraction from multi-scale inputs, and the low-level features are unified in\ndeep layers for extracting high-level semantic features. A scale-invariant\nconstraint is posed to maintain feature consistency across different scales.\nExtensive experiments on ImageNet and other scale-diverse datasets, demonstrate\nthat MSUN achieves significant improvements in both model performance and\ncomputational efficiency. Particularly, MSUN yields an accuracy increase up to\n44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18293v1","updated":"2024-03-27T06:37:51Z","published":"2024-03-27T06:37:51Z","title":"Efficient Test-Time Adaptation of Vision-Language Models","summary":"  Test-time adaptation with pre-trained vision-language models has attracted\nincreasing attention for tackling distribution shifts during the test time.\nThough prior studies have achieved very promising performance, they involve\nintensive computation which is severely unaligned with test-time adaptation. We\ndesign TDA, a training-free dynamic adapter that enables effective and\nefficient test-time adaptation with vision-language models. TDA works with a\nlightweight key-value cache that maintains a dynamic queue with few-shot pseudo\nlabels as values and the corresponding test-sample features as keys. Leveraging\nthe key-value cache, TDA allows adapting to test data gradually via progressive\npseudo label refinement which is super-efficient without incurring any\nbackpropagation. In addition, we introduce negative pseudo labeling that\nalleviates the adverse impact of pseudo label noises by assigning pseudo labels\nto certain negative classes when the model is uncertain about its pseudo label\npredictions. Extensive experiments over two benchmarks demonstrate TDA's\nsuperior effectiveness and efficiency as compared with the state-of-the-art.\nThe code has been released in \\url{https://kdiaaa.github.io/tda/}.\n","authors":["Adilbek Karmanov","Dayan Guan","Shijian Lu","Abdulmotaleb El Saddik","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2403.18293v1.pdf","comment":"Accepted to CVPR 2024. The code has been released in\n  \\url{https://kdiaaa.github.io/tda/}"},{"id":"http://arxiv.org/abs/2403.18291v1","updated":"2024-03-27T06:28:19Z","published":"2024-03-27T06:28:19Z","title":"Towards Non-Exemplar Semi-Supervised Class-Incremental Learning","summary":"  Deep neural networks perform remarkably well in close-world scenarios.\nHowever, novel classes emerged continually in real applications, making it\nnecessary to learn incrementally. Class-incremental learning (CIL) aims to\ngradually recognize new classes while maintaining the discriminability of old\nones. Existing CIL methods have two limitations: a heavy reliance on preserving\nold data for forgetting mitigation and the need for vast labeled data for\nknowledge adaptation. To overcome these issues, we propose a non-exemplar\nsemi-supervised CIL framework with contrastive learning and semi-supervised\nincremental prototype classifier (Semi-IPC). On the one hand, contrastive\nlearning helps the model learn rich representations, easing the trade-off\nbetween learning representations of new classes and forgetting that of old\nclasses. On the other hand, Semi-IPC learns a prototype for each class with\nunsupervised regularization, enabling the model to incrementally learn from\npartially labeled new data while maintaining the knowledge of old classes.\nExperiments on benchmark datasets demonstrate the strong performance of our\nmethod: without storing any old samples and only using less than 1% of labels,\nSemi-IPC outperforms advanced exemplar-based methods. We hope our work offers\nnew insights for future CIL research. The code will be made publicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15864v2","updated":"2024-03-27T06:26:09Z","published":"2023-11-27T14:32:33Z","title":"InterControl: Generate Human Motion Interactions by Controlling Every\n  Joint","summary":"  Text-conditioned human motion synthesis has made remarkable progress with the\nemergence of diffusion models in recent research. However, the majority of\nthese motion diffusion models are primarily designed for a single character and\noverlook multi-human interactions. In our approach, we strive to explore this\nproblem by synthesizing human motion with interactions for a group of\ncharacters of any size. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators.\n","authors":["Zhenzhi Wang","Jingbo Wang","Yixuan Li","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2311.15864v2.pdf","comment":"Generate human interactions with only single-person data via joint\n  contact pairs, code https://github.com/zhenzhiwang/intercontrol"},{"id":"http://arxiv.org/abs/2403.18282v1","updated":"2024-03-27T06:18:40Z","published":"2024-03-27T06:18:40Z","title":"SGDM: Static-Guided Dynamic Module Make Stronger Visual Models","summary":"  The spatial attention mechanism has been widely used to improve object\ndetection performance. However, its operation is currently limited to static\nconvolutions lacking content-adaptive features. This paper innovatively\napproaches from the perspective of dynamic convolution. We propose Razor\nDynamic Convolution (RDConv) to address thetwo flaws in dynamic weight\nconvolution, making it hard to implement in spatial mechanism: 1) it is\ncomputation-heavy; 2) when generating weights, spatial information is\ndisregarded. Firstly, by using Razor Operation to generate certain features, we\nvastly reduce the parameters of the entire dynamic convolution operation.\nSecondly, we added a spatial branch inside RDConv to generate convolutional\nkernel parameters with richer spatial information. Embedding dynamic\nconvolution will also bring the problem of sensitivity to high-frequency noise.\nWe propose the Static-Guided Dynamic Module (SGDM) to address this limitation.\nBy using SGDM, we utilize a set of asymmetric static convolution kernel\nparameters to guide the construction of dynamic convolution. We introduce the\nmechanism of shared weights in static convolution to solve the problem of\ndynamic convolution being sensitive to high-frequency noise. Extensive\nexperiments illustrate that multiple different object detection backbones\nequipped with SGDM achieve a highly competitive boost in performance(e.g., +4%\nmAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible\nparameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).\n","authors":["Wenjie Xing","Zhenchao Cui","Jing Qi"],"pdf_url":"https://arxiv.org/pdf/2403.18282v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.18281v1","updated":"2024-03-27T06:17:21Z","published":"2024-03-27T06:17:21Z","title":"AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation","summary":"  State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on\nimage retrieval (IR) techniques to establish 2D-3D correspondences by selecting\nthe $k$ most similar images from a reference image database for a given query\nimage. Although higher values of $k$ enhance localisation robustness, the\ncomputational cost for feature matching increases linearly with $k$. In this\npaper, we observe that queries that are the most similar to images in the\ndatabase result in a higher proportion of feature matches and, thus, more\naccurate positioning. Thus, a small number of images is sufficient for queries\nvery similar to images in the reference database. We then propose a novel\napproach, AIR-HLoc, which divides query images into different localisation\ndifficulty levels based on their similarity to the reference image database. We\nconsider an image with high similarity to the reference image as an easy query\nand an image with low similarity as a hard query. Easy queries show a limited\nimprovement in accuracy when increasing $k$. Conversely, higher values of $k$\nsignificantly improve accuracy for hard queries. Given the limited improvement\nin accuracy when increasing $k$ for easy queries and the significant\nimprovement for hard queries, we adapt the value of $k$ to the query's\ndifficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively\nassigning different values of $k$ based on the similarity between the query and\nreference images without losing accuracy. Our extensive experiments on the\nCambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate\nour algorithm's efficacy, reducing 30\\%, 26\\%, and 11\\% in computational\noverhead while maintaining SOTA accuracy compared to HLoc with fixed image\nretrieval.\n","authors":["Changkun Liu","Huajian Huang","Zhengyang Ma","Tristan Braud"],"pdf_url":"https://arxiv.org/pdf/2403.18281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07711v2","updated":"2024-03-27T06:02:38Z","published":"2024-03-12T14:53:56Z","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with\n  Structured State Spaces","summary":"  Given the remarkable achievements in image generation through diffusion\nmodels, the research community has shown increasing interest in extending these\nmodels to video generation. Recent diffusion models for video generation have\npredominantly utilized attention layers to extract temporal features. However,\nattention layers are limited by their memory consumption, which increases\nquadratically with the length of the sequence. This limitation presents\nsignificant challenges when attempting to generate longer video sequences using\ndiffusion models. To overcome this challenge, we propose leveraging state-space\nmodels (SSMs). SSMs have recently gained attention as viable alternatives due\nto their linear memory consumption relative to sequence length. In the\nexperiments, we first evaluate our SSM-based model with UCF101, a standard\nbenchmark of video generation. In addition, to investigate the potential of\nSSMs for longer video generation, we perform an experiment using the MineRL\nNavigate dataset, varying the number of frames to 64, 200, and 400. In these\nsettings, our SSM-based model can considerably save memory consumption for\nlonger sequences, while maintaining competitive FVD scores to the\nattention-based models. Our codes are available at\nhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\n","authors":["Yuta Oshima","Shohei Taniguchi","Masahiro Suzuki","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2403.07711v2.pdf","comment":"Accepted as workshop paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2303.08231v3","updated":"2024-03-27T06:00:18Z","published":"2023-03-14T20:55:27Z","title":"Rotation-Invariant Transformer for Point Cloud Matching","summary":"  The intrinsic rotation invariance lies at the core of matching point clouds\nwith handcrafted descriptors. However, it is widely despised by recent deep\nmatchers that obtain the rotation invariance extrinsically via data\naugmentation. As the finite number of augmented rotations can never span the\ncontinuous SO(3) space, these methods usually show instability when facing\nrotations that are rarely seen. To this end, we introduce RoITr, a\nRotation-Invariant Transformer to cope with the pose variations in the point\ncloud matching task. We contribute both on the local and global levels.\nStarting from the local level, we introduce an attention mechanism embedded\nwith Point Pair Feature (PPF)-based coordinates to describe the pose-invariant\ngeometry, upon which a novel attention-based encoder-decoder architecture is\nconstructed. We further propose a global transformer with rotation-invariant\ncross-frame spatial awareness learned by the self-attention mechanism, which\nsignificantly improves the feature distinctiveness and makes the model robust\nwith respect to the low overlap. Experiments are conducted on both the rigid\nand non-rigid public benchmarks, where RoITr outperforms all the\nstate-of-the-art models by a considerable margin in the low-overlapping\nscenarios. Especially when the rotations are enlarged on the challenging\n3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5\npercentage points in terms of Inlier Ratio and Registration Recall,\nrespectively.\n","authors":["Hao Yu","Zheng Qin","Ji Hou","Mahdi Saleh","Dongsheng Li","Benjamin Busam","Slobodan Ilic"],"pdf_url":"https://arxiv.org/pdf/2303.08231v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2403.18274v1","updated":"2024-03-27T05:57:45Z","published":"2024-03-27T05:57:45Z","title":"DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and\n  Bi-Directional Structure Alignment","summary":"  Information inside visual and LiDAR data is well complementary derived from\nthe fine-grained texture of images and massive geometric information in point\nclouds. However, it remains challenging to explore effective visual-LiDAR\nfusion, mainly due to the intrinsic data structure inconsistency between two\nmodalities: Images are regular and dense, but LiDAR points are unordered and\nsparse. To address the problem, we propose a local-to-global fusion network\nwith bi-directional structure alignment. To obtain locally fused features, we\nproject points onto image plane as cluster centers and cluster image pixels\naround each center. Image pixels are pre-organized as pseudo points for\nimage-to-point structure alignment. Then, we convert points to pseudo images by\ncylindrical projection (point-to-image structure alignment) and perform\nadaptive global feature fusion between point features with local fused\nfeatures. Our method achieves state-of-the-art performance on KITTI odometry\nand FlyingThings3D scene flow datasets compared to both single-modal and\nmulti-modal methods. Codes will be released later.\n","authors":["Jiuming Liu","Dong Zhuo","Zhiheng Feng","Siting Zhu","Chensheng Peng","Zhe Liu","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18271v1","updated":"2024-03-27T05:55:16Z","published":"2024-03-27T05:55:16Z","title":"Unleashing the Potential of SAM for Medical Adaptation via Hierarchical\n  Decoding","summary":"  The Segment Anything Model (SAM) has garnered significant attention for its\nversatile segmentation abilities and intuitive prompt-based interface. However,\nits application in medical imaging presents challenges, requiring either\nsubstantial training costs and extensive medical datasets for full model\nfine-tuning or high-quality prompts for optimal performance. This paper\nintroduces H-SAM: a prompt-free adaptation of SAM tailored for efficient\nfine-tuning of medical images via a two-stage hierarchical decoding procedure.\nIn the initial stage, H-SAM employs SAM's original decoder to generate a prior\nprobabilistic mask, guiding a more intricate decoding process in the second\nstage. Specifically, we propose two key designs: 1) A class-balanced,\nmask-guided self-attention mechanism addressing the unbalanced label\ndistribution, enhancing image embedding; 2) A learnable mask cross-attention\nmechanism spatially modulating the interplay among different image regions\nbased on the prior mask. Moreover, the inclusion of a hierarchical pixel\ndecoder in H-SAM enhances its proficiency in capturing fine-grained and\nlocalized details. This approach enables SAM to effectively integrate learned\nmedical priors, facilitating enhanced adaptation for medical image segmentation\nwith limited samples. Our H-SAM demonstrates a 4.78% improvement in average\nDice compared to existing prompt-free SAM variants for multi-organ segmentation\nusing only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM\neven outperforms state-of-the-art semi-supervised models relying on extensive\nunlabeled training data across various medical datasets. Our code is available\nat https://github.com/Cccccczh404/H-SAM.\n","authors":["Zhiheng Cheng","Qingyue Wei","Hongru Zhu","Yan Wang","Liangqiong Qu","Wei Shao","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.18271v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18270v1","updated":"2024-03-27T05:52:39Z","published":"2024-03-27T05:52:39Z","title":"Image Deraining via Self-supervised Reinforcement Learning","summary":"  The quality of images captured outdoors is often affected by the weather. One\nfactor that interferes with sight is rain, which can obstruct the view of\nobservers and computer vision applications that rely on those images. The work\naims to recover rain images by removing rain streaks via Self-supervised\nReinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain\nstreak pixels from the input rain image via dictionary learning and use\npixel-wise RL agents to take multiple inpainting actions to remove rain\nprogressively. To our knowledge, this work is the first attempt where\nself-supervised RL is applied to image deraining. Experimental results on\nseveral benchmark image-deraining datasets show that the proposed SRL-Derain\nperforms favorably against state-of-the-art few-shot and self-supervised\nderaining and denoising methods.\n","authors":["He-Hao Liao","Yan-Tsung Peng","Wen-Tao Chu","Ping-Chun Hsieh","Chung-Chi Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.18270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18266v1","updated":"2024-03-27T05:38:48Z","published":"2024-03-27T05:38:48Z","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has emerged as an effective paradigm for\nderiving general representations from vast amounts of unlabeled data. However,\nas real-world applications continually integrate new content, the high\ncomputational and resource demands of SSL necessitate continual learning rather\nthan complete retraining. This poses a challenge in striking a balance between\nstability and plasticity when adapting to new information. In this paper, we\nemploy Centered Kernel Alignment for quantitatively analyzing model stability\nand plasticity, revealing the critical roles of batch normalization layers for\nstability and convolutional layers for plasticity. Motivated by this, we\npropose Branch-tuning, an efficient and straightforward method that achieves a\nbalance between stability and plasticity in continual SSL. Branch-tuning\nconsists of branch expansion and compression, and can be easily applied to\nvarious SSL methods without the need of modifying the original methods,\nretaining old data or models. We validate our method through incremental\nexperiments on various benchmark datasets, demonstrating its effectiveness and\npractical value in real-world scenarios. We hope our work offers new insights\nfor future continual self-supervised learning research. The code will be made\npublicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03532v2","updated":"2024-03-27T05:28:55Z","published":"2024-03-06T08:18:02Z","title":"Extend Your Own Correspondences: Unsupervised Distant Point Cloud\n  Registration by Progressive Distance Extension","summary":"  Registration of point clouds collected from a pair of distant vehicles\nprovides a comprehensive and accurate 3D view of the driving scenario, which is\nvital for driving safety related applications, yet existing literature suffers\nfrom the expensive pose label acquisition and the deficiency to generalize to\nnew data distributions. In this paper, we propose EYOC, an unsupervised distant\npoint cloud registration method that adapts to new point cloud distributions on\nthe fly, requiring no global pose labels. The core idea of EYOC is to train a\nfeature extractor in a progressive fashion, where in each round, the feature\nextractor, trained with near point cloud pairs, can label slightly farther\npoint cloud pairs, enabling self-supervision on such far point cloud pairs.\nThis process continues until the derived extractor can be used to register\ndistant point clouds. Particularly, to enable high-fidelity correspondence\nlabel generation, we devise an effective spatial filtering scheme to select the\nmost representative correspondences to register a point cloud pair, and then\nutilize the aligned point clouds to discover more correct correspondences.\nExperiments show that EYOC can achieve comparable performance with\nstate-of-the-art supervised methods at a lower training cost. Moreover, it\noutwits supervised methods regarding generalization performance on new data\ndistributions.\n","authors":["Quan Liu","Hongzi Zhu","Zhenxi Wang","Yunsong Zhou","Shan Chang","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.03532v2.pdf","comment":"In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2024"},{"id":"http://arxiv.org/abs/2402.02561v2","updated":"2024-03-27T05:23:40Z","published":"2024-02-04T16:27:37Z","title":"Foundation Model Makes Clustering A Better Initialization For Cold-Start\n  Active Learning","summary":"  Active learning selects the most informative samples from the unlabelled\ndataset to annotate in the context of a limited annotation budget. While\nnumerous methods have been proposed for subsequent sample selection based on an\ninitialized model, scant attention has been paid to the indispensable phase of\nactive learning: selecting samples for model cold-start initialization. Most of\nthe previous studies resort to random sampling or naive clustering. However,\nrandom sampling is prone to fluctuation, and naive clustering suffers from\nconvergence speed, particularly when dealing with high-dimensional data such as\nimaging data. In this work, we propose to integrate foundation models with\nclustering methods to select samples for cold-start active learning\ninitialization. Foundation models refer to those trained on massive datasets by\nthe self-supervised paradigm and capable of generating informative and\ncompacted embeddings for various downstream tasks. Leveraging these embeddings\nto replace raw features such as pixel values, clustering quickly converges and\nidentifies better initial samples. For a comprehensive comparison, we included\na classic ImageNet-supervised model to acquire embeddings. Experiments on two\nclinical tasks of image classification and segmentation demonstrated that\nfoundation model-based clustering efficiently pinpointed informative initial\nsamples, leading to models showcasing enhanced performance than the baseline\nmethods. We envisage that this study provides an effective paradigm for future\ncold-start active learning.\n","authors":["Han Yuan","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2402.02561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17456v3","updated":"2024-03-27T05:22:18Z","published":"2023-11-29T08:56:24Z","title":"DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with\n  Iterative Diffusion-Based Refinement","summary":"  Scene flow estimation, which aims to predict per-point 3D displacements of\ndynamic scenes, is a fundamental task in the computer vision field. However,\nprevious works commonly suffer from unreliable correlation caused by locally\nconstrained searching ranges, and struggle with accumulated inaccuracy arising\nfrom the coarse-to-fine structure. To alleviate these problems, we propose a\nnovel uncertainty-aware scene flow estimation network (DifFlow3D) with the\ndiffusion probabilistic model. Iterative diffusion-based refinement is designed\nto enhance the correlation robustness and resilience to challenging cases, e.g.\ndynamics, noisy inputs, repetitive patterns, etc. To restrain the generation\ndiversity, three key flow-related features are leveraged as conditions in our\ndiffusion model. Furthermore, we also develop an uncertainty estimation module\nwithin diffusion to evaluate the reliability of estimated scene flow. Our\nDifFlow3D achieves state-of-the-art performance, with 24.0% and 29.1% EPE3D\nreduction respectively on FlyingThings3D and KITTI 2015 datasets. Notably, our\nmethod achieves an unprecedented millimeter-level accuracy (0.0078m in EPE3D)\non the KITTI dataset. Additionally, our diffusion-based refinement paradigm can\nbe readily integrated as a plug-and-play module into existing scene flow\nnetworks, significantly increasing their estimation accuracy. Codes are\nreleased at https://github.com/IRMVLab/DifFlow3D.\n","authors":["Jiuming Liu","Guangming Wang","Weicai Ye","Chaokang Jiang","Jinru Han","Zhe Liu","Guofeng Zhang","Dalong Du","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17456v3.pdf","comment":"Camera-ready version of CVPR 2024. Codes are released at\n  https://github.com/IRMVLab/DifFlow3D"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2209.02200v3","updated":"2024-03-27T05:16:02Z","published":"2022-09-06T03:42:18Z","title":"Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection\n  in Aerial Images","summary":"  Arbitrary-oriented object detection (AOOD) has been widely applied to locate\nand classify objects with diverse orientations in remote sensing images.\nHowever, the inconsistent features for the localization and classification\ntasks in AOOD models may lead to ambiguity and low-quality object predictions,\nwhich constrains the detection performance. In this article, an AOOD method\ncalled task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv\nadaptively samples task-wise features from respective sensitive regions and\nmaps these features together in alignment to guide a dynamic label assignment\nfor better predictions. Specifically, sampling positions of the localization\nconvolution in TS-Conv are supervised by the oriented bounding box (OBB)\nprediction associated with spatial coordinates, while sampling positions and\nconvolutional kernel of the classification convolution are designed to be\nadaptively adjusted according to different orientations for improving the\norientation robustness of features. Furthermore, a dynamic\ntask-consistent-aware label assignment (DTLA) strategy is developed to select\noptimal candidate positions and assign labels dynamically according to ranked\ntask-aware scores obtained from TS-Conv. Extensive experiments on several\npublic datasets covering multiple scenes, multimodal images, and multiple\ncategories of objects demonstrate the effectiveness, scalability, and superior\nperformance of the proposed TS-Conv.\n","authors":["Zhanchao Huang","Wei Li","Xiang-Gen Xia","Hao Wang","Ran Tao"],"pdf_url":"https://arxiv.org/pdf/2209.02200v3.pdf","comment":"15 pages, 13 figures, 11 tables"},{"id":"http://arxiv.org/abs/2403.07359v4","updated":"2024-03-27T05:14:09Z","published":"2024-03-12T06:45:34Z","title":"FSC: Few-point Shape Completion","summary":"  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n","authors":["Xianzu Wu","Xianfeng Wu","Tianyu Luan","Yajing Bai","Zhongyuan Lai","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.07359v4.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18258v1","updated":"2024-03-27T05:10:38Z","published":"2024-03-27T05:10:38Z","title":"Enhancing Generative Class Incremental Learning Performance with Model\n  Forgetting Approach","summary":"  This study presents a novel approach to Generative Class Incremental Learning\n(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing\nclass information for better adaptation to streaming data. GCIL is one of the\nhot topics in the field of computer vision, and this is considered one of the\ncrucial tasks in society, specifically the continual learning of generative\nmodels. The ability to forget is a crucial brain function that facilitates\ncontinual learning by selectively discarding less relevant information for\nhumans. However, in the field of machine learning models, the concept of\nintentionally forgetting has not been extensively investigated. In this study\nwe aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,\nthereby examining their impact on the models' ability to learn in continual\nlearning. Through our experiments, we have found that integrating the\nforgetting mechanisms significantly enhances the models' performance in\nacquiring new knowledge, underscoring the positive role that strategic\nforgetting plays in the process of continual learning.\n","authors":["Taro Togo","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2403.18258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00374v4","updated":"2024-03-27T04:06:36Z","published":"2023-12-31T02:25:41Z","title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via\n  Expressive Masked Audio Gesture Modeling","summary":"  We propose EMAGE, a framework to generate full-body human gestures from audio\nand masked gestures, encompassing facial, local body, hands, and global\nmovements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new\nmesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with\nFLAME head parameters and further refines the modeling of head, neck, and\nfinger movements, offering a community-standardized, high-quality 3D motion\ncaptured dataset. EMAGE leverages masked body gesture priors during training to\nboost inference performance. It involves a Masked Audio Gesture Transformer,\nfacilitating joint training on audio-to-gesture generation and masked gesture\nreconstruction to effectively encode audio and body gesture hints. Encoded body\nhints from masked gestures are then separately employed to generate facial and\nbody movements. Moreover, EMAGE adaptively merges speech features from the\naudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\nthe results' fidelity and diversity. Experiments demonstrate that EMAGE\ngenerates holistic gestures with state-of-the-art performance and is flexible\nin accepting predefined spatial-temporal gesture inputs, generating complete,\naudio-synchronized results. Our code and dataset are available at\nhttps://pantomatrix.github.io/EMAGE/\n","authors":["Haiyang Liu","Zihao Zhu","Giorgio Becherini","Yichen Peng","Mingyang Su","You Zhou","Xuefei Zhe","Naoya Iwamoto","Bo Zheng","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2401.00374v4.pdf","comment":"Conflict of Interest Disclosure; CVPR Camera Ready; Project Page:\n  https://pantomatrix.github.io/EMAGE/"},{"id":"http://arxiv.org/abs/2403.18238v1","updated":"2024-03-27T04:03:55Z","published":"2024-03-27T04:03:55Z","title":"TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint\n  Prediction in Aerial Scenes","summary":"  As drone technology advances, using unmanned aerial vehicles for aerial\nsurveys has become the dominant trend in modern low-altitude remote sensing.\nThe surge in aerial video data necessitates accurate prediction for future\nscenarios and motion states of the interested target, particularly in\napplications like traffic management and disaster response. Existing video\nprediction methods focus solely on predicting future scenes (video frames),\nsuffering from the neglect of explicitly modeling target's motion states, which\nis crucial for aerial video interpretation. To address this issue, we introduce\na novel task called Target-Aware Aerial Video Prediction, aiming to\nsimultaneously predict future scenes and motion states of the target. Further,\nwe design a model specifically for this task, named TAFormer, which provides a\nunified modeling approach for both video and target motion states.\nSpecifically, we introduce Spatiotemporal Attention (STA), which decouples the\nlearning of video dynamics into spatial static attention and temporal dynamic\nattention, effectively modeling the scene appearance and motion. Additionally,\nwe design an Information Sharing Mechanism (ISM), which elegantly unifies the\nmodeling of video and target motion by facilitating information interaction\nthrough two sets of messenger tokens. Moreover, to alleviate the difficulty of\ndistinguishing targets in blurry predictions, we introduce Target-Sensitive\nGaussian Loss (TSGL), enhancing the model's sensitivity to both target's\nposition and content. Extensive experiments on UAV123VP and VisDroneVP (derived\nfrom single-object tracking datasets) demonstrate the exceptional performance\nof TAFormer in target-aware video prediction, showcasing its adaptability to\nthe additional requirements of aerial video interpretation for target\nawareness.\n","authors":["Liangyu Xu","Wanxuan Lu","Hongfeng Yu","Yongqiang Mao","Hanbo Bi","Chenglong Liu","Xian Sun","Kun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18238v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.08100v3","updated":"2024-03-27T04:00:07Z","published":"2023-11-14T11:53:24Z","title":"PPAD: Iterative Interactions of Prediction and Planning for End-to-end\n  Autonomous Driving","summary":"  We present a new interaction mechanism of prediction and planning for\nend-to-end autonomous driving, called PPAD (Iterative Interaction of Prediction\nand Planning Autonomous Driving), which considers the timestep-wise interaction\nto better integrate prediction and planning. An ego vehicle performs motion\nplanning at each timestep based on the trajectory prediction of surrounding\nagents (e.g., vehicles and pedestrians) and its local road conditions. Unlike\nexisting end-to-end autonomous driving frameworks, PPAD models the interactions\namong ego, agents, and the dynamic environment in an autoregressive manner by\ninterleaving the Prediction and Planning processes at every timestep, instead\nof a single sequential process of prediction followed by planning.\nSpecifically, we design ego-to-agent, ego-to-map, and ego-to-BEV interaction\nmechanisms with hierarchical dynamic key objects attention to better model the\ninteractions. The experiments on the nuScenes benchmark show that our approach\noutperforms state-of-the-art methods.\n","authors":["Zhili Chen","Maosheng Ye","Shuangjie Xu","Tongyi Cao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08100v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01616v2","updated":"2024-03-27T03:56:35Z","published":"2023-12-04T04:14:09Z","title":"SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation\n  System","summary":"  Accuracy and computational efficiency are the most important metrics to\nVisual Inertial Navigation System (VINS). The existing VINS algorithms with\neither high accuracy or low computational complexity, are difficult to provide\nthe high precision localization in resource-constrained devices. To this end,\nwe propose a novel filter-based VINS framework named SchurVINS, which could\nguarantee both high accuracy by building a complete residual model and low\ncomputational complexity with Schur complement. Technically, we first formulate\nthe full residual model where Gradient, Hessian and observation covariance are\nexplicitly modeled. Then Schur complement is employed to decompose the full\nmodel into ego-motion residual model and landmark residual model. Finally,\nExtended Kalman Filter (EKF) update is implemented in these two models with\nhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our method\nnotably outperforms state-of-the-art (SOTA) methods in both accuracy and\ncomputational complexity. The experimental code of SchurVINS is available at\nhttps://github.com/bytedance/SchurVINS.\n","authors":["Yunfei Fan","Tianyu Zhao","Guidong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08106v3","updated":"2024-03-27T03:55:39Z","published":"2023-10-12T08:01:11Z","title":"Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing\n  Label Bias in Foundation Models","summary":"  Foundation models like CLIP allow zero-shot transfer on various tasks without\nadditional training data. Yet, the zero-shot performance is less competitive\nthan a fully supervised one. Thus, to enhance the performance, fine-tuning and\nensembling are also commonly adopted to better fit the downstream tasks.\nHowever, we argue that such prior work has overlooked the inherent biases in\nfoundation models. Due to the highly imbalanced Web-scale training set, these\nfoundation models are inevitably skewed toward frequent semantics, and thus the\nsubsequent fine-tuning or ensembling is still biased. In this study, we\nsystematically examine the biases in foundation models and demonstrate the\nefficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that\nbias estimation in foundation models is challenging, as most pre-train data\ncannot be explicitly accessed like in traditional long-tailed classification\ntasks. To this end, GLA has an optimization-based bias estimation approach for\ndebiasing foundation models. As our work resolves a fundamental flaw in the\npre-training, the proposed GLA demonstrates significant improvements across a\ndiverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large\naverage improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on\nlong-tailed classification. Codes are in \\url{https://github.com/BeierZhu/GLA}.\n","authors":["Beier Zhu","Kaihua Tang","Qianru Sun","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08106v3.pdf","comment":"V2 proposed a more effective method for label distribution\n  estimation. V1 fixed a typo in abstract; Accepted by NeurIPS2023"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2403.04125v2","updated":"2024-03-27T03:53:14Z","published":"2024-03-07T00:44:21Z","title":"Scalable and Robust Transformer Decoders for Interpretable Image\n  Classification with Foundation Models","summary":"  Interpretable computer vision models can produce transparent predictions,\nwhere the features of an image are compared with prototypes from a training\ndataset and the similarity between them forms a basis for classification.\nNevertheless these methods are computationally expensive to train, introduce\nadditional complexity and may require domain knowledge to adapt\nhyper-parameters to a new dataset. Inspired by developments in object\ndetection, segmentation and large-scale self-supervised foundation vision\nmodels, we introduce Component Features (ComFe), a novel explainable-by-design\nimage classification approach using a transformer-decoder head and hierarchical\nmixture-modelling. With only global image labels and no segmentation or part\nannotations, ComFe can identify consistent image components, such as the head,\nbody, wings and tail of a bird, and the image background, and determine which\nof these features are informative in making a prediction. We demonstrate that\nComFe obtains higher accuracy compared to previous interpretable models across\na range of fine-grained vision benchmarks, without the need to individually\ntune hyper-parameters for each dataset. We also show that ComFe outperforms a\nnon-interpretable linear head across a range of datasets, including ImageNet,\nand improves performance on generalisation and robustness benchmarks.\n","authors":["Evelyn Mannix","Howard Bondell"],"pdf_url":"https://arxiv.org/pdf/2403.04125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11104v4","updated":"2024-03-27T03:47:20Z","published":"2023-01-26T13:58:46Z","title":"Discovering and Mitigating Visual Biases through Keyword Explanation","summary":"  Addressing biases in computer vision models is crucial for real-world AI\ndeployments. However, mitigating visual biases is challenging due to their\nunexplainable nature, often identified indirectly through visualization or\nsample statistics, which necessitates additional human supervision for\ninterpretation. To tackle this issue, we propose the Bias-to-Text (B2T)\nframework, which interprets visual biases as keywords. Specifically, we extract\ncommon keywords from the captions of mispredicted images to identify potential\nbiases in the model. We then validate these keywords by measuring their\nsimilarity to the mispredicted images using a vision-language scoring model.\nThe keyword explanation form of visual bias offers several advantages, such as\na clear group naming for bias discovery and a natural extension for debiasing\nusing these group names. Our experiments demonstrate that B2T can identify\nknown biases, such as gender bias in CelebA, background bias in Waterbirds, and\ndistribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in\nlarger datasets, such as Dollar Street and ImageNet. For example, we discovered\na contextual bias between \"bee\" and \"flower\" in ImageNet. We also highlight\nvarious applications of B2T keywords, including debiased training, CLIP\nprompting, and model comparison.\n","authors":["Younghyun Kim","Sangwoo Mo","Minkyu Kim","Kyungmin Lee","Jaeho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2301.11104v4.pdf","comment":"CVPR 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18233v1","updated":"2024-03-27T03:39:57Z","published":"2024-03-27T03:39:57Z","title":"Benchmarking Image Transformers for Prostate Cancer Detection from\n  Ultrasound Data","summary":"  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in\nultrasound images typically employ convolutional networks (CNNs) to detect\ncancer in small regions of interest (ROI) along a needle trace region. However,\nthis approach suffers from weak labelling, since the ground-truth\nhistopathology labels do not describe the properties of individual ROIs.\nRecently, multi-scale approaches have sought to mitigate this issue by\ncombining the context awareness of transformers with a CNN feature extractor to\ndetect cancer from multiple ROIs using multiple-instance learning (MIL). In\nthis work, we present a detailed study of several image transformer\narchitectures for both ROI-scale and multi-scale classification, and a\ncomparison of the performance of CNNs and transformers for ultrasound-based\nprostate cancer classification. We also design a novel multi-objective learning\nstrategy that combines both ROI and core predictions to further mitigate label\nnoise. METHODS: We evaluate 3 image transformers on ROI-scale cancer\nclassification, then use the strongest model to tune a multi-scale classifier\nwith MIL. We train our MIL models using our novel multi-objective learning\nstrategy and compare our results to existing baselines. RESULTS: We find that\nfor both ROI-scale and multi-scale PCa detection, image transformer backbones\nlag behind their CNN counterparts. This deficit in performance is even more\nnoticeable for larger models. When using multi-objective learning, we can\nimprove performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a\nspecificity of 66.3%. CONCLUSION: Convolutional networks are better suited for\nmodelling sparse datasets of prostate ultrasounds, producing more robust\nfeatures than transformers in PCa detection. Multi-scale methods remain the\nbest architecture for this task, with multi-objective learning presenting an\neffective way to improve performance.\n","authors":["Mohamed Harmanani","Paul F. R. Wilson","Fahimeh Fooladgar","Amoon Jamzad","Mahdi Gilany","Minh Nguyen Nhat To","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2403.18233v1.pdf","comment":"early draft, 7 pages; Accepted to SPIE Medical Imaging 2024"},{"id":"http://arxiv.org/abs/2403.02649v2","updated":"2024-03-27T03:34:00Z","published":"2024-03-05T04:38:13Z","title":"Few-shot Learner Parameterization by Diffusion Time-steps","summary":"  Even when using large multi-modal foundation models, few-shot learning is\nstill challenging -- if there is no proper inductive bias, it is nearly\nimpossible to keep the nuanced class attributes while removing the visually\nprominent attributes that spuriously correlate with class labels. To this end,\nwe find an inductive bias that the time-steps of a Diffusion Model (DM) can\nisolate the nuanced class attributes, i.e., as the forward diffusion adds noise\nto an image at each time-step, nuanced attributes are usually lost at an\nearlier time-step than the spurious attributes that are visually prominent.\nBuilding on this, we propose Time-step Few-shot (TiF) learner. We train\nclass-specific low-rank adapters for a text-conditioned DM to make up for the\nlost attributes, such that images can be accurately reconstructed from their\nnoisy ones given a prompt. Hence, at a small time-step, the adapter and prompt\nare essentially a parameterization of only the nuanced class attributes. For a\ntest image, we can use the parameterization to only extract the nuanced class\nattributes for classification. TiF learner significantly outperforms OpenCLIP\nand its adapters on a variety of fine-grained and customized few-shot learning\ntasks. Codes are in https://github.com/yue-zhongqi/tif.\n","authors":["Zhongqi Yue","Pan Zhou","Richang Hong","Hanwang Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2403.02649v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18228v1","updated":"2024-03-27T03:31:16Z","published":"2024-03-27T03:31:16Z","title":"Fourier or Wavelet bases as counterpart self-attention in spikformer for\n  efficient visual classification","summary":"  Energy-efficient spikformer has been proposed by integrating the biologically\nplausible spiking neural network (SNN) and artificial Transformer, whereby the\nSpiking Self-Attention (SSA) is used to achieve both higher accuracy and lower\ncomputational cost. However, it seems that self-attention is not always\nnecessary, especially in sparse spike-form calculation manners. In this paper,\nwe innovatively replace vanilla SSA (using dynamic bases calculating from Query\nand Key) with spike-form Fourier Transform, Wavelet Transform, and their\ncombinations (using fixed triangular or wavelets bases), based on a key\nhypothesis that both of them use a set of basis functions for information\ntransformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is\nproposed and verified in visual classification tasks, including both static\nimage and event-based video datasets. The FWformer can achieve comparable or\neven higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$\nfor training and $19\\%$-$70\\%$ for inference), reduced theoretical energy\nconsumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$),\ncompared to the standard spikformer. Our result indicates the continuous\nrefinement of new Transformers, that are inspired either by biological\ndiscovery (spike-form), or information theory (Fourier or Wavelet Transform),\nis promising.\n","authors":["Qingyu Wang","Duzhen Zhang","Tilelin Zhang","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18228v1.pdf","comment":"18 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2308.02557"},{"id":"http://arxiv.org/abs/2304.14394v3","updated":"2024-03-27T03:23:12Z","published":"2023-04-27T17:56:29Z","title":"Unified Sequence-to-Sequence Learning for Single- and Multi-Modal Visual\n  Object Tracking","summary":"  In this paper, we introduce a new sequence-to-sequence learning framework for\nRGB-based and multi-modal object tracking. First, we present SeqTrack for\nRGB-based tracking. It casts visual tracking as a sequence generation task,\nforecasting object bounding boxes in an autoregressive manner. This differs\nfrom previous trackers, which depend on the design of intricate head networks,\nsuch as classification and regression heads. SeqTrack employs a basic\nencoder-decoder transformer architecture. The encoder utilizes a bidirectional\ntransformer for feature extraction, while the decoder generates bounding box\nsequences autoregressively using a causal transformer. The loss function is a\nplain cross-entropy. Second, we introduce SeqTrackv2, a unified\nsequence-to-sequence framework for multi-modal tracking tasks. Expanding upon\nSeqTrack, SeqTrackv2 integrates a unified interface for auxiliary modalities\nand a set of task-prompt tokens to specify the task. This enables it to manage\nmulti-modal tracking tasks using a unified model and parameter set. This\nsequence learning paradigm not only simplifies the tracking framework, but also\nshowcases superior performance across 14 challenging benchmarks spanning five\nsingle- and multi-modal tracking tasks. The code and models are available at\nhttps://github.com/chenxin-dlut/SeqTrackv2.\n","authors":["Xin Chen","Ben Kang","Jiawen Zhu","Dong Wang","Houwen Peng","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2304.14394v3.pdf","comment":"This is a new expanded version of our previous CVPR2023 paper\n  \"SeqTrack: Sequence to Sequence Learning for Visual Object Tracking.\"\n  SeqTrackv2 extends SeqTrack to four multi-modal tracking tasks with a unified\n  model and parameter set"},{"id":"http://arxiv.org/abs/2402.17464v3","updated":"2024-03-27T03:13:52Z","published":"2024-02-27T12:42:06Z","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","summary":"  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly. Code is available at\nhttps://github.com/pkudba/3DHPA.\n","authors":["Bi'an Du","Xiang Gao","Wei Hu","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17464v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16421v2","updated":"2024-03-27T03:07:20Z","published":"2023-09-28T13:12:18Z","title":"Distilling ODE Solvers of Diffusion Models into Smaller Steps","summary":"  Abstract Diffusion models have recently gained prominence as a novel category\nof generative models. Despite their success, these models face a notable\ndrawback in terms of slow sampling speeds, requiring a high number of function\nevaluations (NFE) in the order of hundreds or thousands. In response, both\nlearning-free and learning-based sampling strategies have been explored to\nexpedite the sampling process. Learning-free sampling employs various ordinary\ndifferential equation (ODE) solvers based on the formulation of diffusion ODEs.\nHowever, it encounters challenges in faithfully tracking the true sampling\ntrajectory, particularly for small NFE. Conversely, learning-based sampling\nmethods, such as knowledge distillation, demand extensive additional training,\nlimiting their practical applicability. To overcome these limitations, we\nintroduce Distilled-ODE solvers (D-ODE solvers), a straightforward distillation\napproach grounded in ODE solver formulations. Our method seamlessly integrates\nthe strengths of both learning-free and learning-based sampling. D-ODE solvers\nare constructed by introducing a single parameter adjustment to existing ODE\nsolvers. Furthermore, we optimize D-ODE solvers with smaller steps using\nknowledge distillation from ODE solvers with larger steps across a batch of\nsamples. Comprehensive experiments demonstrate the superior performance of\nD-ODE solvers compared to existing ODE solvers, including DDIM, PNDM,\nDPM-Solver, DEIS, and EDM, particularly in scenarios with fewer NFE. Notably,\nour method incurs negligible computational overhead compared to previous\ndistillation techniques, facilitating straightforward and rapid integration\nwith existing samplers. Qualitative analysis reveals that D-ODE solvers not\nonly enhance image quality but also faithfully follow the target ODE\ntrajectory.\n","authors":["Sanghwan Kim","Hao Tang","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2309.16421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04181v2","updated":"2024-03-27T02:51:24Z","published":"2023-10-06T11:53:04Z","title":"DiffPrompter: Differentiable Implicit Visual Prompts for\n  Semantic-Segmentation in Adverse Conditions","summary":"  Semantic segmentation in adverse weather scenarios is a critical task for\nautonomous driving systems. While foundation models have shown promise, the\nneed for specialized adaptors becomes evident for handling more challenging\nscenarios. We introduce DiffPrompter, a novel differentiable visual and latent\nprompting mechanism aimed at expanding the learning capabilities of existing\nadaptors in foundation models. Our proposed $\\nabla$HFC image processing block\nexcels particularly in adverse weather conditions, where conventional methods\noften fall short. Furthermore, we investigate the advantages of jointly\ntraining visual and latent prompts, demonstrating that this combined approach\nsignificantly enhances performance in out-of-distribution scenarios. Our\ndifferentiable visual prompts leverage parallel and series architectures to\ngenerate prompts, effectively improving object segmentation tasks in adverse\nconditions. Through a comprehensive series of experiments and evaluations, we\nprovide empirical evidence to support the efficacy of our approach. Project\npage at https://diffprompter.github.io.\n","authors":["Sanket Kalwar","Mihir Ungarala","Shruti Jain","Aaron Monis","Krishna Reddy Konda","Sourav Garg","K Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.04181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18211v1","updated":"2024-03-27T02:42:52Z","published":"2024-03-27T02:42:52Z","title":"NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual\n  Pretraining and Multi-level Modulation","summary":"  Recent fMRI-to-image approaches mainly focused on associating fMRI signals\nwith specific conditions of pre-trained diffusion models. These approaches,\nwhile producing high-quality images, capture only a limited aspect of the\ncomplex information in fMRI signals and offer little detailed control over\nimage creation. In contrast, this paper proposes to directly modulate the\ngeneration process of diffusion models using fMRI signals. Our approach,\nNeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI\ncalibrated-encoding, to tackle multi-individual pre-training for a shared\nlatent space to minimize individual difference and enable the subsequent\ncross-subject training; ii) fMRI-to-image cross-subject pre-training,\nperceptually learning to guide diffusion model with high- and low-level\nconditions across different individuals; iii) fMRI-to-image single-subject\nrefining, similar with step ii but focus on adapting to particular individual.\nNeuroPictor extracts high-level semantic features from fMRI signals that\ncharacterizing the visual stimulus and incrementally fine-tunes the diffusion\nmodel with a low-level manipulation network to provide precise structural\ninstructions. By training with over 60,000 fMRI-image pairs from various\nindividuals, our model enjoys superior fMRI-to-image decoding capacity,\nparticularly in the within-subject setting, as evidenced in benchmark datasets.\nProject page: https://jingyanghuo.github.io/neuropictor/.\n","authors":["Jingyang Huo","Yikai Wang","Xuelin Qian","Yun Wang","Chong Li","Jianfeng Feng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18208v1","updated":"2024-03-27T02:39:23Z","published":"2024-03-27T02:39:23Z","title":"An Evolutionary Network Architecture Search Framework with Adaptive\n  Multimodal Fusion for Hand Gesture Recognition","summary":"  Hand gesture recognition (HGR) based on multimodal data has attracted\nconsiderable attention owing to its great potential in applications. Various\nmanually designed multimodal deep networks have performed well in multimodal\nHGR (MHGR), but most of existing algorithms require a lot of expert experience\nand time-consuming manual trials. To address these issues, we propose an\nevolutionary network architecture search framework with the adaptive multimodel\nfusion (AMF-ENAS). Specifically, we design an encoding space that\nsimultaneously considers fusion positions and ratios of the multimodal data,\nallowing for the automatic construction of multimodal networks with different\narchitectures through decoding. Additionally, we consider three input streams\ncorresponding to intra-modal surface electromyography (sEMG), intra-modal\naccelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to\nvarious datasets, the ENAS framework is designed to automatically search a MHGR\nnetwork with appropriate fusion positions and ratios. To the best of our\nknowledge, this is the first time that ENAS has been utilized in MHGR to tackle\nissues related to the fusion position and ratio of multimodal data.\nExperimental results demonstrate that AMF-ENAS achieves state-of-the-art\nperformance on the Ninapro DB2, DB3, and DB7 datasets.\n","authors":["Yizhang Xia","Shihao Song","Zhanglu Hou","Junwen Xu","Juan Zou","Yuan Liu","Shengxiang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.18208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18207v1","updated":"2024-03-27T02:35:36Z","published":"2024-03-27T02:35:36Z","title":"Road Obstacle Detection based on Unknown Objectness Scores","summary":"  The detection of unknown traffic obstacles is vital to ensure safe autonomous\ndriving. The standard object-detection methods cannot identify unknown objects\nthat are not included under predefined categories. This is because\nobject-detection methods are trained to assign a background label to pixels\ncorresponding to the presence of unknown objects. To address this problem, the\npixel-wise anomaly-detection approach has attracted increased research\nattention. Anomaly-detection techniques, such as uncertainty estimation and\nperceptual difference from reconstructed images, make it possible to identify\npixels of unknown objects as out-of-distribution (OoD) samples. However, when\napplied to images with many unknowns and complex components, such as driving\nscenes, these methods often exhibit unstable performance. The purpose of this\nstudy is to achieve stable performance for detecting unknown objects by\nincorporating the object-detection fashions into the pixel-wise anomaly\ndetection methods. To achieve this goal, we adopt a semantic-segmentation\nnetwork with a sigmoid head that simultaneously provides pixel-wise anomaly\nscores and objectness scores. Our experimental results show that the objectness\nscores play an important role in improving the detection performance. Based on\nthese results, we propose a novel anomaly score by integrating these two\nscores, which we term as unknown objectness score. Quantitative evaluations\nshow that the proposed method outperforms state-of-the-art methods when applied\nto the publicly available datasets.\n","authors":["Chihiro Noguchi","Toshiaki Ohgushi","Masao Yamanaka"],"pdf_url":"https://arxiv.org/pdf/2403.18207v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.10066v3","updated":"2024-03-27T02:25:51Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18201v1","updated":"2024-03-27T02:24:00Z","published":"2024-03-27T02:24:00Z","title":"Few-shot Online Anomaly Detection and Segmentation","summary":"  Detecting anomaly patterns from images is a crucial artificial intelligence\ntechnique in industrial applications. Recent research in this domain has\nemphasized the necessity of a large volume of training data, overlooking the\npractical scenario where, post-deployment of the model, unlabeled data\ncontaining both normal and abnormal samples can be utilized to enhance the\nmodel's performance. Consequently, this paper focuses on addressing the\nchallenging yet practical few-shot online anomaly detection and segmentation\n(FOADS) task. Under the FOADS framework, models are trained on a few-shot\nnormal dataset, followed by inspection and improvement of their capabilities by\nleveraging unlabeled streaming data containing both normal and abnormal samples\nsimultaneously.\n  To tackle this issue, we propose modeling the feature distribution of normal\nimages using a Neural Gas network, which offers the flexibility to adapt the\ntopology structure to identify outliers in the data flow. In order to achieve\nimproved performance with limited training samples, we employ multi-scale\nfeature embedding extracted from a CNN pre-trained on ImageNet to obtain a\nrobust representation. Furthermore, we introduce an algorithm that can\nincrementally update parameters without the need to store previous samples.\nComprehensive experimental results demonstrate that our method can achieve\nsubstantial performance under the FOADS setting, while ensuring that the time\ncomplexity remains within an acceptable range on MVTec AD and BTAD datasets.\n","authors":["Shenxing Wei","Xing Wei","Zhiheng Ma","Songlin Dong","Shaochen Zhang","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2403.18201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00154v2","updated":"2024-03-27T02:21:03Z","published":"2024-02-29T22:11:20Z","title":"LLMs in Political Science: Heralding a New Era of Visual Analysis","summary":"  Interest is increasing among political scientists in leveraging the extensive\ninformation available in images. However, the challenge of interpreting these\nimages lies in the need for specialized knowledge in computer vision and access\nto specialized hardware. As a result, image analysis has been limited to a\nrelatively small group within the political science community. This landscape\ncould potentially change thanks to the rise of large language models (LLMs).\nThis paper aims to raise awareness of the feasibility of using Gemini for image\ncontent analysis. A retrospective analysis was conducted on a corpus of 688\nimages. Content reports were elicited from Gemini for each image and then\nmanually evaluated by the authors. We find that Gemini is highly accurate in\nperforming object detection, which is arguably the most common and fundamental\ntask in image analysis for political scientists. Equally important, we show\nthat it is easy to implement as the entire command consists of a single prompt\nin natural language; it is fast to run and should meet the time budget of most\nresearchers; and it is free to use and does not require any specialized\nhardware. In addition, we illustrate how political scientists can leverage\nGemini for other image understanding tasks, including face identification,\nsentiment analysis, and caption generation. Our findings suggest that Gemini\nand other similar LLMs have the potential to drastically stimulate and\naccelerate image research in political science and social sciences more\nbroadly.\n","authors":["Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.00154v2.pdf","comment":"7 pages, 3 tables"},{"id":"http://arxiv.org/abs/2403.18198v1","updated":"2024-03-27T02:16:04Z","published":"2024-03-27T02:16:04Z","title":"Generative Medical Segmentation","summary":"  Rapid advancements in medical image segmentation performance have been\nsignificantly driven by the development of Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). However, these models introduce high\ncomputational demands and often have limited ability to generalize across\ndiverse medical imaging datasets. In this manuscript, we introduce Generative\nMedical Segmentation (GMS), a novel approach leveraging a generative model for\nimage segmentation. Concretely, GMS employs a robust pre-trained Variational\nAutoencoder (VAE) to derive latent representations of both images and masks,\nfollowed by a mapping model that learns the transition from image to mask in\nthe latent space. This process culminates in generating a precise segmentation\nmask within the image space using the pre-trained VAE decoder. The design of\nGMS leads to fewer learnable parameters in the model, resulting in a reduced\ncomputational burden and enhanced generalization capability. Our extensive\nexperimental analysis across five public datasets in different medical imaging\ndomains demonstrates GMS outperforms existing discriminative segmentation\nmodels and has remarkable domain generalization. Our experiments suggest GMS\ncould set a new benchmark for medical image segmentation, offering a scalable\nand effective solution. GMS implementation and model weights are available at\nhttps://github.com/King-HAW/GMS.\n","authors":["Jiayu Huo","Xi Ouyang","Sébastien Ourselin","Rachel Sparks"],"pdf_url":"https://arxiv.org/pdf/2403.18198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18196v1","updated":"2024-03-27T02:13:20Z","published":"2024-03-27T02:13:20Z","title":"Looking Beyond What You See: An Empirical Analysis on Subgroup\n  Intersectional Fairness for Multi-label Chest X-ray Classification Using\n  Social Determinants of Racial Health Inequities","summary":"  There has been significant progress in implementing deep learning models in\ndisease diagnosis using chest X- rays. Despite these advancements, inherent\nbiases in these models can lead to disparities in prediction accuracy across\nprotected groups. In this study, we propose a framework to achieve accurate\ndiagnostic outcomes and ensure fairness across intersectional groups in\nhigh-dimensional chest X- ray multi-label classification. Transcending\ntraditional protected attributes, we consider complex interactions within\nsocial determinants, enabling a more granular benchmark and evaluation of\nfairness. We present a simple and robust method that involves retraining the\nlast classification layer of pre-trained models using a balanced dataset across\ngroups. Additionally, we account for fairness constraints and integrate\nclass-balanced fine-tuning for multi-label settings. The evaluation of our\nmethod on the MIMIC-CXR dataset demonstrates that our framework achieves an\noptimal tradeoff between accuracy and fairness compared to baseline methods.\n","authors":["Dana Moukheiber","Saurabh Mahindre","Lama Moukheiber","Mira Moukheiber","Mingchen Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18196v1.pdf","comment":"ICCV CVAMD 2023"},{"id":"http://arxiv.org/abs/2403.18193v1","updated":"2024-03-27T02:06:25Z","published":"2024-03-27T02:06:25Z","title":"Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T\n  Tracking","summary":"  RGB-T tracking, a vital downstream task of object tracking, has made\nremarkable progress in recent years. Yet, it remains hindered by two major\nchallenges: 1) the trade-off between performance and efficiency; 2) the\nscarcity of training data. To address the latter challenge, some recent methods\nemploy prompts to fine-tune pre-trained RGB tracking models and leverage\nupstream knowledge in a parameter-efficient manner. However, these methods\ninadequately explore modality-independent patterns and disregard the dynamic\nreliability of different modalities in open scenarios. We propose M3PT, a novel\nRGB-T prompt tracking method that leverages middle fusion and multi-modal and\nmulti-stage visual prompts to overcome these challenges. We pioneer the use of\nthe middle fusion framework for RGB-T tracking, which achieves a balance\nbetween performance and efficiency. Furthermore, we incorporate the pre-trained\nRGB tracking model into the framework and utilize multiple flexible prompt\nstrategies to adapt the pre-trained model to the comprehensive exploration of\nuni-modal patterns and the improved modeling of fusion-modal features,\nharnessing the potential of prompt learning in RGB-T tracking. Our method\noutperforms the state-of-the-art methods on four challenging benchmarks, while\nattaining 46.1 fps inference speed.\n","authors":["Qiming Wang","Yongqiang Bai","Hongxing Song"],"pdf_url":"https://arxiv.org/pdf/2403.18193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00211v2","updated":"2024-03-27T01:50:06Z","published":"2024-03-01T01:07:40Z","title":"Trustworthy Self-Attention: Enabling the Network to Focus Only on the\n  Most Relevant References","summary":"  The prediction of optical flow for occluded points is still a difficult\nproblem that has not yet been solved. Recent methods use self-attention to find\nrelevant non-occluded points as references for estimating the optical flow of\noccluded points based on the assumption of self-similarity. However, they rely\non visual features of a single image and weak constraints, which are not\nsufficient to constrain the trained network to focus on erroneous and weakly\nrelevant reference points. We make full use of online occlusion recognition\ninformation to construct occlusion extended visual features and two strong\nconstraints, allowing the network to learn to focus only on the most relevant\nreferences without requiring occlusion ground truth to participate in the\ntraining of the network. Our method adds very few network parameters to the\noriginal framework, making it very lightweight. Extensive experiments show that\nour model has the greatest cross-dataset generalization. Our method achieves\nmuch greater error reduction, 18.6%, 16.2%, and 20.1% for all points,\nnon-occluded points, and occluded points respectively from the state-of-the-art\nGMA-base method, MATCHFlow(GMA), on Sintel Albedo pass. Furthermore, our model\nachieves state-of-the-art performance on the Sintel bench-marks, ranking \\#1\namong all published methods on Sintel clean pass. The code will be open-source.\n","authors":["Yu Jing","Tan Yujuan","Ren Ao","Liu Duo"],"pdf_url":"https://arxiv.org/pdf/2403.00211v2.pdf","comment":"Correct Figure 1"},{"id":"http://arxiv.org/abs/2403.18187v1","updated":"2024-03-27T01:40:21Z","published":"2024-03-27T01:40:21Z","title":"LayoutFlow: Flow Matching for Layout Generation","summary":"  Finding a suitable layout represents a crucial task for diverse applications\nin graphic design. Motivated by simpler and smoother sampling trajectories, we\nexplore the use of Flow Matching as an alternative to current diffusion-based\nlayout generation models. Specifically, we propose LayoutFlow, an efficient\nflow-based model capable of generating high-quality layouts. Instead of\nprogressively denoising the elements of a noisy layout, our method learns to\ngradually move, or flow, the elements of an initial sample until it reaches its\nfinal prediction. In addition, we employ a conditioning scheme that allows us\nto handle various generation tasks with varying degrees of conditioning with a\nsingle model. Empirically, LayoutFlow performs on par with state-of-the-art\nmodels while being significantly faster.\n","authors":["Julian Jorge Andrade Guerreiro","Naoto Inoue","Kento Masui","Mayu Otani","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2403.18187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09069v2","updated":"2024-03-27T01:32:10Z","published":"2024-03-14T03:21:33Z","title":"Dyadic Interaction Modeling for Social Behavior Generation","summary":"  Human-human communication is like a delicate dance where listeners and\nspeakers concurrently interact to maintain conversational dynamics. Hence, an\neffective model for generating listener nonverbal behaviors requires\nunderstanding the dyadic context and interaction. In this paper, we present an\neffective framework for creating 3D facial motions in dyadic interactions.\nExisting work consider a listener as a reactive agent with reflexive behaviors\nto the speaker's voice and facial motions. The heart of our framework is Dyadic\nInteraction Modeling (DIM), a pre-training approach that jointly models\nspeakers' and listeners' motions through masking and contrastive learning to\nlearn representations that capture the dyadic context. To enable the generation\nof non-deterministic behaviors, we encode both listener and speaker motions\ninto discrete latent representations, through VQ-VAE. The pre-trained model is\nfurther fine-tuned for motion generation. Extensive experiments demonstrate the\nsuperiority of our framework in generating listener motions, establishing a new\nstate-of-the-art according to the quantitative measures capturing the diversity\nand realism of generated motions. Qualitative results demonstrate the superior\ncapabilities of the proposed approach in generating diverse and realistic\nexpressions, eye blinks and head gestures.\n","authors":["Minh Tran","Di Chang","Maksim Siniukov","Mohammad Soleymani"],"pdf_url":"https://arxiv.org/pdf/2403.09069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18186v1","updated":"2024-03-27T01:28:36Z","published":"2024-03-27T01:28:36Z","title":"Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting","summary":"  We present a method for large-mask pluralistic image inpainting based on the\ngenerative framework of discrete latent codes. Our method learns latent priors,\ndiscretized as tokens, by only performing computations at the visible locations\nof the image. This is realized by a restrictive partial encoder that predicts\nthe token label for each visible block, a bidirectional transformer that infers\nthe missing labels by only looking at these tokens, and a dedicated synthesis\nnetwork that couples the tokens with the partial image priors to generate\ncoherent and pluralistic complete image even under extreme mask settings.\nExperiments on public benchmarks validate our design choices as the proposed\nmethod outperforms strong baselines in both visual quality and diversity\nmetrics.\n","authors":["Haiwei Chen","Yajie Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18186v1.pdf","comment":"cvpr 2024"},{"id":"http://arxiv.org/abs/2403.18180v1","updated":"2024-03-27T01:15:05Z","published":"2024-03-27T01:15:05Z","title":"Multi-Layer Dense Attention Decoder for Polyp Segmentation","summary":"  Detecting and segmenting polyps is crucial for expediting the diagnosis of\ncolon cancer. This is a challenging task due to the large variations of polyps\nin color, texture, and lighting conditions, along with subtle differences\nbetween the polyp and its surrounding area. Recently, vision Transformers have\nshown robust abilities in modeling global context for polyp segmentation.\nHowever, they face two major limitations: the inability to learn local\nrelations among multi-level layers and inadequate feature aggregation in the\ndecoder. To address these issues, we propose a novel decoder architecture aimed\nat hierarchically aggregating locally enhanced multi-level dense features.\nSpecifically, we introduce a novel module named Dense Attention Gate (DAG),\nwhich adaptively fuses all previous layers' features to establish local feature\nrelations among all layers. Furthermore, we propose a novel nested decoder\narchitecture that hierarchically aggregates decoder features, thereby enhancing\nsemantic features. We incorporate our novel dense decoder with the PVT backbone\nnetwork and conduct evaluations on five polyp segmentation datasets: Kvasir,\nCVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS. Our experiments and comparisons\nwith nine competing segmentation models demonstrate that the proposed\narchitecture achieves state-of-the-art performance and outperforms the previous\nmodels on four datasets. The source code is available at:\nhttps://github.com/krushi1992/Dense-Decoder.\n","authors":["Krushi Patel","Fengjun Li","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18178v1","updated":"2024-03-27T01:12:31Z","published":"2024-03-27T01:12:31Z","title":"Online Embedding Multi-Scale CLIP Features into 3D Maps","summary":"  This study introduces a novel approach to online embedding of multi-scale\nCLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By\nharnessing CLIP, this methodology surpasses the constraints of conventional\nvocabulary-limited methods and enables the incorporation of semantic\ninformation into the resultant maps. While recent approaches have explored the\nembedding of multi-modal features in maps, they often impose significant\ncomputational costs, lacking practicality for exploring unfamiliar environments\nin real time. Our approach tackles these challenges by efficiently computing\nand embedding multi-scale CLIP features, thereby facilitating the exploration\nof unfamiliar environments through real-time map generation. Moreover, the\nembedding CLIP features into the resultant maps makes offline retrieval via\nlinguistic queries feasible. In essence, our approach simultaneously achieves\nreal-time object search and mapping of unfamiliar environments. Additionally,\nwe propose a zero-shot object-goal navigation system based on our mapping\napproach, and we validate its efficacy through object-goal navigation, offline\nobject retrieval, and multi-object-goal navigation in both simulated\nenvironments and real robot experiments. The findings demonstrate that our\nmethod not only exhibits swifter performance than state-of-the-art mapping\nmethods but also surpasses them in terms of the success rate of object-goal\nnavigation tasks.\n","authors":["Shun Taguchi","Hideki Deguchi"],"pdf_url":"https://arxiv.org/pdf/2403.18178v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.13729v2","updated":"2024-03-27T00:51:01Z","published":"2024-02-21T11:46:16Z","title":"Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet\n  Representation","summary":"  Generating high-quality videos that synthesize desired realistic content is a\nchallenging task due to their intricate high-dimensionality and complexity of\nvideos. Several recent diffusion-based methods have shown comparable\nperformance by compressing videos to a lower-dimensional latent space, using\ntraditional video autoencoder architecture. However, such method that employ\nstandard frame-wise 2D and 3D convolution fail to fully exploit the\nspatio-temporal nature of videos. To address this issue, we propose a novel\nhybrid video diffusion model, called HVDM, which can capture spatio-temporal\ndependencies more effectively. The HVDM is trained by a hybrid video\nautoencoder which extracts a disentangled representation of the video\nincluding: (i) a global context information captured by a 2D projected latent\n(ii) a local volume information captured by 3D convolutions with wavelet\ndecomposition (iii) a frequency information for improving the video\nreconstruction. Based on this disentangled representation, our hybrid\nautoencoder provide a more comprehensive video latent enriching the generated\nvideos with fine structures and details. Experiments on video generation\nbenchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed\napproach achieves state-of-the-art video generation quality, showing a wide\nrange of video applications (e.g., long video generation, image-to-video, and\nvideo dynamics control).\n","authors":["Kihong Kim","Haneol Lee","Jihye Park","Seyeon Kim","Kwanghee Lee","Seungryong Kim","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2402.13729v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.17098v2","updated":"2024-03-27T00:46:26Z","published":"2024-01-30T15:29:32Z","title":"Deep Learning-Driven Approach for Handwritten Chinese Character\n  Classification","summary":"  Handwritten character recognition (HCR) is a challenging problem for machine\nlearning researchers. Unlike printed text data, handwritten character datasets\nhave more variation due to human-introduced bias. With numerous unique\ncharacter classes present, some data, such as Logographic Scripts or\nSino-Korean character sequences, bring new complications to the HCR problem.\nThe classification task on such datasets requires the model to learn\nhigh-complexity details of the images that share similar features. With recent\nadvances in computational resource availability and further computer vision\ntheory development, some research teams have effectively addressed the arising\nchallenges. Although known for achieving high accuracy while keeping the number\nof parameters small, many common approaches are still not generalizable and use\ndataset-specific solutions to achieve better results. Due to complex structure,\nexisting methods frequently prevent the solutions from gaining popularity. This\npaper proposes a highly scalable approach for detailed character image\nclassification by introducing the model architecture, data preprocessing steps,\nand testing design instructions. We also perform experiments to compare the\nperformance of our method with that of existing ones to show the improvements\nachieved.\n","authors":["Boris Kriuk","Fedor Kriuk"],"pdf_url":"https://arxiv.org/pdf/2401.17098v2.pdf","comment":"30 pages, 9 figures, 2 tables, preprint v2"}],"Graphics":[{"id":"http://arxiv.org/abs/2403.18811v1","updated":"2024-03-27T17:57:02Z","published":"2024-03-27T17:57:02Z","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment","summary":"  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n","authors":["Li Siyao","Tianpei Gu","Zhitao Yang","Zhengyu Lin","Ziwei Liu","Henghui Ding","Lei Yang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18811v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18761v1","updated":"2024-03-27T16:59:21Z","published":"2024-03-27T16:59:21Z","title":"MATTopo: Topology-preserving Medial Axis Transform with Restricted Power\n  Diagram","summary":"  We present a novel volumetric RPD (restricted power diagram) based framework\nfor approximating the medial axes of 3D CAD shapes adaptively, while preserving\ntopological equivalence, medial features, and geometric convergence. To solve\nthe topology preservation problem, we propose a volumetric RPD based strategy,\nwhich discretizes the input volume into sub-regions given a set of medial\nspheres. With this intermediate structure, we convert the homotopy equivalence\nbetween the generated medial mesh and the input 3D shape into a localized\nproblem between each primitive of the medial mesh (vertex, edge, face) and its\ndual restricted elements (power cell, power face, power edge), by checking\ntheir connected components and Euler characteristics. We further proposed a\nfractional Euler characteristic strategy for efficient GPU-based computation of\nEuler characteristic for each restricted element on the fly while computing the\nvolumetric RPD. Compared with existing voxel-based or sampling-based methods,\nour method is the first that can adaptively and directly revise the medial mesh\nwithout modifying the dependent structure globally, such as voxel size or\nsampling density. Compared with the feature preservation method MATFP, our\nmethod offers geometrically comparable results with fewer number of spheres,\nwhile more robustly captures the topology of the input shape.\n","authors":["Ningna Wang","Hui Huang","Shibo Song","Bin Wang","Wenping Wang","Xiaohu Guo"],"pdf_url":"https://arxiv.org/pdf/2403.18761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18660v1","updated":"2024-03-27T15:03:38Z","published":"2024-03-27T15:03:38Z","title":"InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing","summary":"  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n","authors":["Ruoyu Zhao","Qingnan Fan","Fei Kou","Shuai Qin","Hong Gu","Wei Wu","Pengcheng Xu","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18660v1.pdf","comment":"Project Page: https://royzhao926.github.io/InstructBrush/"},{"id":"http://arxiv.org/abs/2403.18476v1","updated":"2024-03-27T11:45:08Z","published":"2024-03-27T11:45:08Z","title":"Modeling uncertainty for Gaussian Splatting","summary":"  We present Stochastic Gaussian Splatting (SGS): the first framework for\nuncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\nnovel-view synthesis field by achieving impressive reconstruction quality at a\nfraction of the computational cost of Neural Radiance Fields (NeRF). However,\ncontrary to the latter, it still lacks the ability to provide information about\nthe confidence associated with their outputs. To address this limitation, in\nthis paper, we introduce a Variational Inference-based approach that seamlessly\nintegrates uncertainty prediction into the common rendering pipeline of GS.\nAdditionally, we introduce the Area Under Sparsification Error (AUSE) as a new\nterm in the loss function, enabling optimization of uncertainty estimation\nalongside image reconstruction. Experimental results on the LLFF dataset\ndemonstrate that our method outperforms existing approaches in terms of both\nimage rendering quality and uncertainty estimation accuracy. Overall, our\nframework equips practitioners with valuable insights into the reliability of\nsynthesized views, facilitating safer decision-making in real-world\napplications.\n","authors":["Luca Savant","Diego Valsesia","Enrico Magli"],"pdf_url":"https://arxiv.org/pdf/2403.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18113v2","updated":"2024-03-27T10:46:59Z","published":"2023-11-29T21:58:41Z","title":"Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features","summary":"  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n","authors":["Thomas Wimmer","Peter Wonka","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2311.18113v2.pdf","comment":"Accepted to CVPR 2024, Project page:\n  https://wimmerth.github.io/back-to-3d.html"},{"id":"http://arxiv.org/abs/2401.01647v2","updated":"2024-03-27T09:39:41Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v2.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.12820v2","updated":"2024-03-27T07:35:47Z","published":"2024-03-19T15:21:00Z","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","summary":"  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n","authors":["Zhiwei Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12820v2.pdf","comment":"A derivation is incomplete, and updations are being processed"},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null}]}}